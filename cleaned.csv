,year,title,abstract,paper_text
1317,1987,New Hardware for Massive Neural Networks,Abstract Missing,"201

NEW HARDWARE FOR MASSIVE NEURAL NETWORKS
D. D. Coon and A. G. U. Perera
Applied Technology Laboratory
University of Pittsburgh
Pittsburgh, PA 15260.

ABSTRACT
Transient phenomena associated with forward biased silicon p + - n - n + structures at 4.2K show remarkable similarities with biological neurons. The devices play
a role similar to the two-terminal switching elements in Hodgkin-Huxley equivalent
circuit diagrams. The devices provide simpler and more realistic neuron emulation
than transistors or op-amps. They have such low power and current requirements
that they could be used in massive neural networks. Some observed properties of
simple circuits containing the devices include action potentials, refractory periods,
threshold behavior, excitation, inhibition, summation over synaptic inputs, synaptic
weights, temporal integration, memory, network connectivity modification based on
experience, pacemaker activity, firing thresholds, coupling to sensors with graded signal outputs and the dependence of firing rate on input current. Transfer functions
for simple artificial neurons with spiketrain inputs and spiketrain outputs have been
measured and correlated with input coupling.
INTRODUCTION
Here we discuss the simulation of neuron phenomena by electronic processes in
silicon from the point of view of hardware for new approaches to electronic processing
of information which parallel the means by which information is processed in intelligent organisms. Development of this hardware basis is pursued through exploratory
work on circuits which exhibit some basic features of biological neural networks. Fig. 1
shows the basic circuit used to obtain spiketrain outputs. A distinguishing feature
of this hardware basis is the spontaneous generation of action potentials as a device
physics feature.

,----_O_u-f)tput

) ! -_ _

R

JLJLL

Figure 1: Spontaneous,
neuronlike spiketrain
generating circuit. The
spikes are nearly equal in
amplitude so that
information is contained in
the frequency and
temporal pattern of the
spiketrain generation.

? American Institute of Physics 1988

202

TWO-TERMINAL SWITCHING ELEMENTS
The use of transistor based circuitry 1 is avoided because transistor electrical
characteristics are not similar to neuron characteristics. The use of devices with
fundamentally non-neuronlike character increases the complexity of artificial neural
networks. Complexity would be an important drawback for massive neural networks
and most neural networks in nature achieve their remarkable performance through
their massive size. In addition) transistors have three terminals whereas the switching
elements of Hodgkin-Huxley equivalent circuits have two terminals. Motivated in
part by Hodgkin-Huxley equivalent circuit diagrams) we employ two-terminal p+ n - n+ devices which execute transient switching between low conductance and high
conductance states. (See Fig. 2) We call these devices injection mode devices (IMDs).
In the ""OFF-STATE"", a typical current through the devices is '"" 100fA/mm2) and
in the ""ON-STATE"" a typical current is '"" 10mA/mm2. Hence this device is an
extremely good switch with a ON / 0 F F ratio of 1011. As in real neurons 2, the current
in the device is a function of voltage and time, not only voltage. The devices require
cryogenic cooling but this results in an advantageously low quiescent power drain of
< 1 nanowatt/cm2 of chip area and the very low leakage currents mentioned above.
In addition, the highly unique ability of the neural networks described here to operate
in a cryogenic environment is an important advantage for infrared image processing
at the focal plane (see Fig. 3 and further discussion below). Vision systems begin
processing at the focal plane and there are many benefits to be gained from the
vision system approach to IR image processing.

/ \
-----/

-----

...-.

I( V, t)

I

I

R

VD C ~--~--VV~--~------~

IR

;;SS:Ulse
Output

1----0

+Q

C

- Q

Figure 2:
Switching element
in Hodgkin-Huxley equivalent circuits.

Figure 3: Single stage conversion of
infrared intensity to spiketrain frequency with a neuron-like semiconductor device. No pre-amplifiers
are necessary.

Coding of graded input signals (see Fig. 4) such as photocurrents into action potential spike trains with millimeter scale devices has been experimentally
demonstrated3 with currents from 1 IlA down to about 1 picoampere with coding
noise referred to input of < 10 femtoamperes. Coding of much smaller current levels
should be possible with smaller devices. Figure 5 clearly shows the threshold behavior
of the IMD. For devices studied to date, a transition from action potential output to
graded signal output is observed for input currents of the order of 0.5 picoamperes 1~

203

--..

o

Z

o

10

4

U

w

(f)

CURRENT (AMPERES)

Figure 4: Coding of NIR-VISmLE-UV intensity into firing frequency of a spiketrain
and the experimentally determined firing rate vs. the input current for one device.
Note that the dynamic range is about 107 .

>
'0

'>
o

UBI)

E2

Figure 5: mustration of the threshold firing of the
device in response to input step functions.

---PL
500 fLS/ div

This transition is remarkably well described in von Neumann's discussion 5 ,6 of
the mixed character of neural elements which he relates to the concept of subliminal stimulation levels which are too low to produce the stereotypical all-or-nothing
response. Neural network modelers frequently adopt viewpoints which ignore this
interesting mixed character. The von Neumann viewpoint links the mixed character
to concepts of nonlinear dynamics in a way which is not apparent in recent neural
network modeling literature. The scaling down of IMD size should result in even
lower current requirements for all-or-nothing response.
DEVICE PHYSICS
Recently, neuronlike action potential transients in IMDs have been the subject
of considerable research3 ,4,7,8,9,1O,1l,12,13. In the simple circuits of Fig. 1, the IMD
gives rise to a spontaneous neuronlike spiketrain output. Between pulses, the IMD is
polarized in the sense that it is in a low conductance state with a substantial voltage
occurring across it, even though it is forward biased. The low conductance has been
attributed to small interfacial work functions due to band offsets at the n+ -n and
p+ -n interfaces 8 ?
Low temperatures inhibit thermionic injection of electrons and holes into the
n-region from the n+ -layer and p+ -layer impurity bands 14 . Pulses are caused by

204

switching to depolarized states with low diode potential drops and large injection
currents which are believed to be triggered by the slow buildup of a small thermionic
injection current from the n+ -layer into the n-region. The injection current can cause
impact ionization of n-region donor impurities resulting in an increasingly positive
space charge which further enhances the injection current to the point where the IMD
abruptly switches to the low conductance state with large injection current. Switching
times are typically under lOOns. Charging of the load capacitance CL cuts off the
large injection current and resets the diode to its low conductance state. The load
capacitor CL then discharges through RL. During the CL discharging time constant
RLCL the voltage across the IMD itself is low and therefore the bias voltage would
have to be raised substantially to cause further firing. Thus, RLCL is analogous to
the refractory period of a neuron. The output pulses of an IMD generally have about
the same amplitude while the rate of pulsing varies over a wide range depending on
the bias voltage and the presence of electromagnetic radiation. 7,8,10

~ DETECTOR ARRAY

?=::I

TRANSIENT SENSING

?=::I MOTION

SENSING -

TRACKING

2-D PARALLEL OUTPUT

Figure 6: lllustrative
laminar architecture
showing stacked wafers in
3-dimensions.

LAMINAR
NEURAL NETWORK

REAL TIME PARALLEL ASYNCHRONOUS PROCESSING
The devices described here could form the hardware basis for a parallel asynchronous processor in much the same way that transistors form the basis for digital
computers. The devices could be used to construct networks which could perform real
time signal processing. Pulse propagation through silicon chips (parallel firethrough,
see Fig. 7) as opposed to the lateral planar propagation in conventional integrated
circuits has been proposed. 1S This would permit the use of laminar, stacked wafer
architectures. See Fig. 6.
Such architectures would eliminate the serial processing limitations of standard processors which utilize multiplexing and charge transfer. There are additional
advantages in terms of elimination of pre-amplifiers and reduction in power consumption. The approach would utilize the low power, low noise devices lO described here
to perform input signal-to-frequency conversion in every processing channel.
POWER CONSUMPTION FOR A BRAIN SCALE SYSTEM
The low power and low current requirements together with the electronic simplicity (lower parts-count as compared with transistor and op-amp approaches) and

205

INPUTS

;1""*""*""* *'""*""* '*""* '* '*""* ""*1
111111111111

;1* ***'* *""*""*""* *' **1

;1*
*
*
*
*
*
*
*
**
*
*1
;1*""*""*""*
*
**
*""*
'
*
""*
""*1
;1* **""* ***""*""* '*""* ""*1

Siwafer
Siwafer

Siwaf.r
Siwaf.r

Figure 7: Schematic illustration of the signal flow
pattern through a real time
parallel asynchronous processor consisting of stacked
silicon wafers.

wafer
; I I I I I I I I I I I I I 1SiSiwaf.r
! ! ! ! ! ! ! ! ! ! ! !
OUTPUTS

the natural emulation of neuron features means that the approach described here
would be especially advantageous for very large neural networks, e.g. systems comparable to supercomputers in which power dissipation and system complexity are important considerations. The power consumption of large scale analog 16 and digital 17
systems is always a major concern. For example, the power consumption of the
CRAY XMP-48 is of the order of 300 kilowatts. For the devices described here, the
power consumption is very low. For these devices, we have observed quiescent power
drains of about 1 n W /cm 2 and pulse power consumption of about 500 nJ/pulse/cm 2 ?
We estimate that a system with 1011 active 10~m x 10~m elements (comparable
to the number of neurons in the brain 18 ) all firing with an average pulse rate of 1
KHz (corresponding to a high neuronal firing rateS) would consume about 50 watts.
The quiescent power drain for this system would be 0.1 milliwatts. Thus, power
(P) requirements for such an artificial neural network with the size scale (1011 pulse
generating elements) of the human brain and a range of activity between zero and
the maximum conceivable sustained activity for neurons in the brain would be 0.1
milliwatts < P < 50 watts for 10 micron technology. For comparison, we note that
von Neumann's estimate for the power dissipation of the brain is of order 10 to 25
watts. S,6 Fabrication of a 1011 element 10 ~m artificial neural network would require
processing of about 1500 four inch wafers.
NETWORK CONNECTIVITY
For a network with coupling between many IMD's3 we have shown"" that

(1)
where Vj is the voltage across the diode and the input capacitance Cj of the i-th
network node, Rj represents a leakage resistance in parallel with Cil and Ij represents
an external current input to the i-th diode. iJ=1,2,3, ..... label different network nodes
and Tij incoporates coupling between network elements. Equation 1 has the same
form as equations which occur in the Hopfield modeI 2o ,21,22,23 for neural networks.
Sejnowski has also discussed similar equations in connection with skeleton filters in

206

OUTPUTS

INPUTS

;~
t----o
~f---r----t---t t----o
t----o

c);
o---j

R

o---j~~~~~'fi-~

o---j

R

:P---o

~:
TRANSMISSION LINE

Figure 8: a) Main features of a typical neuron from Kandel and Schwartz. 19 b) Our
artificial neuron) which shows the summation over synaptic inputs and fan-out.
the brain. 24 ?25 Nonlinear threshold behavior of IMD)s enters through F(V) as it does
in the neural network models.
In Fig. 8-b a range of input capacitances is possible. This range of capacitances
is related to the range of possible synaptic weights. The circuit in Fig. 8 accomplishes
pulse height discrimination and each pulse can contribute to the charge stored on
the central node capacitance C. The charge added to C during each input pulse is
linearly related to the input capacitance except at extreme limits. The range of input
capacitances for a particular experiment was .002 J-lF to .2 J-lF which differ by a factor
of about 100. The effect of various input capacitance values (synaptic weights) on
input-output firing rates is shown in Fig. 9. Also the Fig. 8-b shows many capacitive
inputs/outputs to/from a single IMD. i.e. fan-in and fan-out. For pulses which arrive
at different inputs at about the same time) the effect of the pulses is additive. The
time within which inputs are summed is just the stored charge lifetime. Summation
over many inputs is an important feature of neural information processing.
EXCITATION) INHIBITION) MEMORY
Both excitatory and inhibitory input circuits are shown in Fig. 10. Input pulses
cause the accumulation of charge on C in excitatory circuits and the depletion of
charge on C in inhibitory circuits. Charge associated with input spiketrains is integrated/stored on C. The temporally integrated charge is depleted by the firing of the
IMD. Thus) the storage time is related to the firing rate. After an input spiketrain
raises the potential across C to a value above the firing threshold) the resulting IMD

207

5

O.2}J F
O.03}JF

,--.,.
N

I

.;,<

4

Figure 9: Output pulse
rate vs. the input
pulse rate for different
input capacitance
values Ci values

W
t-

~

3

w

Vl

---1

::J
(L

2

t-

::J
CL
t-

6

1

20

40

80

60

100

INPUT PULSE RATE (Hz )

(0)
R

I NP~ I----'-~-r_{)l__--,----<>
OUTP UT

Figure 10: Circuits which incorporate rectifying synaptic inputs. a) an excitatory
input. b) an inhibitory input.

(b)
R
R'

INP~

c?

L

R'L

output spiketrain codes the input information. The output firing rate is linearly related to the input firing rate times the synaptic coupling strength (linearly related to
Ci). See Fig. 9. If the input ceases, then the potential across C relaxes back to a value
just below the firing threshold. When not firing, the IMD has a high impedance. If
there is negligible leakage of charge from C, then V can remain near V T (threshold
voltage) for a long time and a new input signal will quickly take the IMD over the
firing threshold. See Fig. 11. We have observed stored charge lifetimes of 56 days and
longer times may be acheivable. The lifetime of charge stored on C can be reduced
by adding a resistance in parallel with C.
From the discussion of integration, we see that long term storage of charge on C
is equivalent to long term memory. The memory can be read by seeing if a new input
pulse or spiketrain produces a prompt output pulse or spiketrain. The read signal
input channel in Fig. 8-b can be the same as or different from the channel which
resulted in the charge storage. In either case memory would produce a change in the
pattern of connectivity if the circuit was imbedded in a neural network. Changes in
patterns of connectivity are similar to Hebb's ruie considerations26 in which memory
is associated with increases in the strength (weight) of synaptic couplings. Frequently,

208

-

-QJ 13
o

a::

11

Figure 11: Firing rate vs. the bias voltage.
The region where the firing is negligible is
associated with memory. The state of the
memory is associated with the proximity
to the firing threshold.

Input Potential
the increase in synaptic weights is modeled by increased conductance whereas in the
circuits in Figs. lO(a) and 8-b memory is achieved by integration and charge storage.
Note that for these particular circuits, the memory is not eraseable although volatile
(short term) memory can easily be constructed by adding a resistor in parallel with
C. Thus, a continuous range of memory lifetimes can be achieved.
2-D PARALLEL ASYNCHRONOUS CHIP-TO-CHIP TRANSMISSION
For many IMD's the output pulse heights for a circuit like that in Fig. 1 are
>3 volts. Thus, output from the first stage or any later stage of the network could
easily be transmitted to other parts of an overall system. Two-dimensional arrays
of devices on different chips could be coupled by indium bump bonding to form
the laminar architecture described above. Planar technology could be used for local
lateral interconnections in the processor. (See Fig. 7) In addition to transmission of
electrical pulses, optical transmission is possible because the pulses can directly drive
LED's.
Emerging GaAs-on-Si technology is interesting as a means of fabricating two
dimensional emitter arrays. Optical transmission is not necessary but it might be
useful (A) for processed image data transfer, (B) for coupling to an optical processor, or (C) to provide 2-0 optical interconnects between chips bearing 2-D arrays of
p+ - n - n+ diodes. Note that with optical interconnects between chips, the circuits
employed here would be internal receivers. The p-i-n diodes employed in the present
work would be well suited to the receiver role. An interesting possibility would entail the use optical interconnects between chips to achieve local, lateral interaction.
This would be accomplished by having each optical emitter in a 2-D array broadcast
locally to multiple receivers rather than to a single receiver. Similarly, each receiver
would have a reeeptive field extending over multiple transmitters. It is also possible
that an optical element could be placed in the gap between parallel transmitter and
receiver planes to structure, control or alter 2-D patterns of interconnection. This
would be an alternative to a planar technology approach to lateral interconnection.
IT the optical elements were active then the system would constitute a hybrid optical/electronic processor, whereas if passive optical elements were employed, we would
regard the system as an optoelectronic processor. In either case, we picture the processing functions of temporal integration, spatial summation over inputs, coding and
pulse generation as residing on-chip.

209

ACKNOWLEDGEMENTS
The work was supported in part by U.S. DOE under contract #DE-AC0280ER10667 and NSF under grant # ECS-8603075.

References
[1] L. D. Harmon, Kybernetik 1,89 (1961).
[2] A. L. Hodgkin and A. F. Huxley, J. Physioll17, 500 (1952).
[3] D. D. Coon and A. G. U. Perera, Int. J. Electronics 63, 61 (1987).
[4] K. M. S. V. Bandara, D. D. Coon and R. P. G. Karunasiri, Infrared 'lransient
Sensing, to be published.
[5] J. von Neumann, The Computer and the Brain, Yale University Press, New
Haven and London, 1958.
[6] J. von Neumann, Collected Works, Pergamon Press, New York, 1961.
[7] D. D. Coon and A. G. U. Perera, Int. J. Infrared and Millimeter Waves 7, 1571
(1986).
[8] D. D. Coon and S. D. Gunapala, J. Appl. Phys 57, 5525 (1985).
[9] D. D. Coon, S. N. Ma and A. G. U. Perera, Phys. Rev. Let. 58, 1139 (1987).
[10] D. D. Coon and A. G. U. Perera, Applied Physics Letters 51, 1711 (1987).
[11] D. D. Coon and A. G. U. Perera, Solid-State Electronics 29, 929 (1986).
[12] D. D. Coon and A. G. U. Perera, Applied Physics Letters 51, 1086 (1987).
[13] K. M. S. V. Bandara, D.D. Coon and R. P. G. Karunasiri, Appl. Phys. Lett 51,
961 (1987).
[14] Y. N. Yang, D. D. Coon and P. F. Shepard, Applied Physics Letters 45, 752
(1984).
[15] D. D. Coon and A. G. U. Perera, Int. J. IR and Millimeter Waves 8, 1037 (1987).
[16] M. A. Sivilotti, M. R. Emerling and C. A. Mead, VLSI Arcbitectures for Implementation of Neural Networks, Neural Networks for Computing, A.J.P.,
1986, pp. 408-413.
[17] R. W. Keyes, Proc. IEEE 63, 740 (1975).
[18] E . R. Kandel and J. H. Schwartz, Principles of Neural Science, Elsevier, New
York, 1985.

210

[19] E. R. Kandel and J. H. Schwartz, Principles of Neural Science, Elsevier, New
York, 1985, page 15, Reproduced by permission of Elsevier Science Publishing
Co., N.Y ..
[20] J. J. Hopfield, Proc. Nat!. Acad. Sci. U.S.A 81, 3088 (1984).
[21] J. J. Hopfield and D. W. Tank, BioI. Cybern 52, 141 (1985).
[22] J. J. Hopfield and D. W. Tank, Science 233,625 (1986).
[23] D. W. Tank and J. J. Hopfield, IEEE. Circuits Syst. CAS-33, 533 (1986).
[24] T. J. Sejnowski, J. Math. Biology 4, 303 (1977).
[25] T. J. Sejnowski, Skeleton Filters in tbe Brain, Lawrence Erlbaum, New Jersey,
1981, pp. 189-212, edited by G. E. Hinton and J. A. Anderson.
[26] J. L. McClelland, D. E. Rumelhart and the PDP research group, Parallel Distributed Processing, The MIT Press, Cambridge, Massachusetts, 1986, two volumes.

"
4793,1991,Learning to Make Coherent Predictions in Domains with Discontinuities,Abstract Missing,"Learning to Make Coherent Predictions in
Domains with Discontinuities

Suzanna Becker and Geoffrey E. Hinton
Department of Computer Science, University of Toronto
Toronto, Ontario, Canada M5S 1A4

Abstract
We have previously described an unsupervised learning procedure that
discovers spatially coherent propertit>_<; of the world by maximizing the information that parameters extracted from different parts of the sensory
input convey about some common underlying cause. When given random
dot stereograms of curved surfaces, this procedure learns to extract surface depth because that is the property that is coherent across space. It
also learns how to interpolate the depth at one location from the depths
at nearby locations (Becker and Hint.oll. 1992). 1n this paper, we propose two new models which handle surfaces with discontinuities. The first
model attempts to detect cases of discontinuities and reject them. The
second model develops a mixture of expert interpolators. It learns to detect the locations of discontinuities and to invoke specialized, asymmetric
interpolators that do not cross the discontinuities .

1

Introd uction

Standard backpropagation is implausible as a model of perceptual learning because
it requires an external teacher to specify the desired output of the network. We
have shown (Becker and Hinton, 1992) how the external teacher can be replaced
by internally derived teaching signals. These signals are generated by using the
assumption that different parts of the perceptual input have common causes in
the external world. Small modules that look at separate but related parts of the
perceptual input discover these common causes by striving to produce outputs that
agree with each other (see Figure 1 a). The modules may look at different modalities
(e.g. vision and touch), or the same modality at different times (e.g. the consecutive
2-D views of a rotating 3-D object), or even spatially adjacent parts of the same
image. In previous work, we showed that when our learning procedure is applied

372

Learning to Make Coherent Predictions in Domains with Discontinuities

to adjacent patches of 2-dimensional images, it allows a neural network that has no
prior knowledge of the third dimension to discover depth in random dot stereograms
of curved surfaces. A more general version of the method allows the network to
discover the best way of interpolating the depth at one location from the depths
at nearby locations. We first summarize this earlier work, and then introduce
two new models which allow coherent predictions to be made in the presence of
discontinuities.

a)

left

rightm~m~
patch A

patch B

Figure 1: a) Two modules that receive input from corresponding parts of stereo
images. The first module receives input from stereo patch A, consisting of a horizontal strip from the left image (striped) and a corresponding strip from the right
image (hatched). The second module receives input from an adjacent stereo patch
B . The modules try to make their outputs, d a and db, convey as much information as possible about some underlying signal (i. e., the depth) which is common to
both patches. b) The architecture of the interpolating network, consisting of multiple
copies of modules like those in a) plus a layer of interpolating units. The network
tries to maximize the information that the locally extracted parameter de and the
contextually predicted parameter de convey about some common underlying signal.
We actually used 10 modules and the central 6 modules tried to maximize agreement
between their outputs and contextually predicted values. We used weight averaging
to constrain the interpolating function to be identical for all modules.

2

Learning spatially coherent features in images

The simplest way to get the outputs of two modules to agree is to use the squared
difference between the outputs as a cost function, and to adjust the weights in each
module so as to minimize this cost. Unfortunately, this usually causes each module
to produce the same constant output that is unaffected by the input to the module
and therefore conveys no information about it. What we want is for the outputs
of two modules to agree closely (i.e. to have a small expected squared difference)
relative to how much they both vary as the input is varied. When this happens, the
two modules must be responding to something that is common to their two inputs.
In the special case when the outputs, d a , db, of the two modules are scalars, a good

373

374

Becker and Hinton

measure of agreement is:

(1)
where V is the variance over the training cases. If d a and db are both versions
of the same underlying Gaussian signal that have been corrupted by independent
Gaussian noise, it can be shown that I is the mutual information between the
underlying signal and the average of d a and db. By maximizing I we force the two
modules to extract as pure a version as possible of the underlying common signal.

2.1

The basic stereo net

We have shown how this principle can be applied to a multi-layer network that learns
to extract depth from random dot stereograms (Becker and Hinton, 1992). Each
network module received input from a patch of a left image and a corresponding
patch of a right image, as shown in Figure 1 a). Adjacent modules received input
from adjacent stereo image patches, and learned to extract depth by trying to
maximize agreement between their outputs. The real-valued depth (relative to the
plane of fixation) of each patch of the surface gives rise to a disparity between
features in the left and right images; since that disparity is the only property that
is coherent across each stereo image, the output units of modules were able to learn
to accurately detect relative depth.

2.2

The interpolating net

The basic stereo net uses a very simple model of coherence in which an underlying
parameter at one location is assumed to be approximately equal to the parameter at
a neighbouring location. This model is fine for the depth of fronto-parallel surfaces
but it is far from the best model of slanted or curved surfaces. Fortunately, we can
use a far more general model of coherence in which the parameter at one location
is assumed to be an unknown linear function of the parameters at nearby locations.
The particular linear function that is appropriate can be learned by the network.
We used a network of the type shown in Figure 1 b). The depth computed locally
by a module, dc, was compared with the depth predicted by a linear combination de
of the outputs of nearby modules, and the network tried to maximize the agreement
between de and de.
The contextual prediction, dc, was produced by computing a weighted sum of
the outputs of two adjacent modules on either side. The interpolating weights
used in this sum, and all other weights in the network, were adjusted so as to
maximize agreement between locally computed and contextually predicted depths.
To speed the learning, we first trained the lower layers of the network as before, so that agreement was maximized between neighbouring locally computed
outputs. This made it easier to learn good interpolating weights. When the
network was trained on stereograms of cubic surfaces, it learned interpolating
weights of -0.147,0.675,0.656, -0.131 (Becker and Hinton, 1992). Given noise
free estimates of local depth, the optimal linear interpolator for a cubic surfa.ce
is -0.167,0.667,0.667, -0.167.

Learning to Make Coherent Predictions in Domains with Discontinuities

3

Throwing out discontinuities

If the surface is continuous, the depth at one patch can be accurately predicted from
the depths of two patches on either side. If, however, the training data contains cases
in which there are depth discontinuities (see figure 2) the interpolator will also try
to model these cases and this will contribute considerable noise to the interpolating
weights and to the depth estimates. One way of reducing this noise is to treat the
discontinuity cases as outliers and to throw them out. Rather than making a hard
decision about whether a case is an outlier, we make a soft decision by using a
mixture model. For each training case, the network compares the locally extracted
depth, dc, with the depth predicted from the nearby context, de. It assumes that
de - de is drawn from a zero-mean Gaussian if it is a continuity case and from a
uniform distribution if it is a discontinuity case. It can then estimate the probability
of a continuity case:

--------

Spline
curve

Left
Image

I 1

l

Right
Image

""I

I

II I II \

I I

I

II I

III

1111

Iii

II ill I

til

IIII ,i I I \

I,

I""'I

II

I I ,,\

II

I
1'1

111

II

III II

I

Figure 2: Top: A curved surface strip with a discontinuity created by fitting 2
cubic splines through randomly chosen control points, 25 pixels apart, separated by
a depth discontinuity. Feature points are randomly scattered on each spline with an
average of 0.22 features per pixel. Bottom: A stereo pair of ""intensity"" images
of the surface strip formed by taking two different projections of the feature points,
filtering them through a gaussian, and sampling the filtered projections at evenly
spaced sample points. The sample values in corresponding patches of the two images
are used as the inputs to a module. The depth of the surface for a particular zmage
region is directly related to the disparity between corresponding features in the left
and right patch. Disparity ranges continuously from -1 to + 1 image pixels. Each
stereo image was 120 pixels wide and divided into 10 receptive fields 10 pixels wide
and separated by 2 pixel gaps, as input for the networks shown in figure 1. The
receptive field of an interpolating unit spanned 58 image pixels, and discontinuities
were randomly located a minimum of 40 pixels apart, so only rarely would more
than one discontinuity lie within an interpolator's receptive field.

375

376

Becker and Hinton

(2)
where N is a gaussian, and kdi3eont is a constant representing a uniform density.

1

We can now optimize the average information de and de transmit about their common cause. We assume that no information is transmitted in discontinuity cases,
so the average information depends on the probability of continuity and on the
variance of de + de and de - de measured only in the continuity cases.

(3)
We tried several variations of this mixture approach. The network is quite good at
rejecting the discontinuity cases, but this leads to only a modest improvement in
the performance of the interpolator. In cases where there is a depth discontinuity
between d a and db or between dd and de the interpolator works moderately well
because the weights on d a or de are small. Because of the term Peont in equation
3 there is pressure to include these cases as continuity cases, so they probably
contribute noise to the interpolating weights. In the next section we show how to
avoid making a forced choice between rejecting these cases or treating them just
like all the other continuity cases.

4

Learning a mixture of expert interpolators

The presence of a depth discontinuity somewhere within a strip of five adjacent
patches does not entirely eliminate the coherence of depth across these patches. It
just restricts the range over which this coherence operates. So instead of throwing
out cases that contain a discontinuity, the network could try to develop a number
of different, specialized interpolators each of which captures the particular type of
coherence that remains in the presence of a discontinuity at a particular location.
If, for example, there is a depth discontinuity between de and de, an extrapolator
with weights of -1.0, +2.0,0, would be an appropriate predictor of de .

?

Figure 3 shows the system of five expert interpolators that we used for predicting
de from the neighboring depths. To allow the system to invoke the appropriate
interpolator, each expert has its own ""controller"" which must learn to detect the
presence of a discontinuity at a particular location (or the absence of a discontinuity in the case of the interpolator for pure continuity cases). The outputs of the
controllers are normalized, as shown in figure 3, so that they form a probability distribution. We can think of these normalized outputs as the probability with which
the system selects a particular expert. The controllers get to see all five local depth
estimates and most of them learn to detect particular depth discontinuities by using
large weights of opposite sign on the local depth estimates of neighboring patches.
lWe empirically select a good (fixed) value of kdiseont, and we choose a starting value
of Veont{de - de) (some proportion of the initial variance of de - de), and gradually shrink
it during learning.

Learning to Make Coherent Predictions in Domains with Discontinuities

expert 1
expert 2
expert 3

de , I

PI

de ,2

P2

de ,3

P3

Xl

Normalization

Pi

=

J

expert 4
expert 5

X2

controller 2

X3

controller 3

e x ,2

I: e

controller 1

x J? 2

de ,4

P4

X4

de ,5

P5

X5

controller 4
controller 5

Figure 3: The architecture of the mixture of interpolators and discontinuzty detec.
tors . We actually used a larger modular network and equality constraints between
modules, as described in figure 1 b), with 6 copies of the architecture shown here .
Each copy received input from different but overlapping parts of the input.

Figure 4 shows the weights learned by the experts and by their controllers. As
expected, there is one interpolator (the top one) that is appropriate for continuity
cases and four other interpolators that are appropriate for the four different locations of a discontinuity. In interpreting the weights of the controllers it is important
to remember that a controller which produces a small X value for a particular case
may nevertheless assign high probability to its expert if all the other controllers
produce even smaller x values.
4.1

The learning procedure

In the example presented here, we first trained the network shown in figure 1b)
on images with discontinuities. We then used the outputs of the depth extracting
layer, d a , ... ,de as the inputs to the expert interpolators and their controllers. The
system learned a set of expert interpolators without backpropagating derivatives all
the way down to the weights of the local depth extracting modules. So the local
depth estimates d a , ... ,de did not change as the interpolators were learned .
To train the system we used an unsupervised version of the competing experts
algorithm described by Jacobs, Jordan, Nowlan and Hinton (1991) . The output of
the ith expert, de,i, is treated as the mean of a Gaussian distribution with variance 0- 2
and the normalized output of each controller, Pi , is treated as the mixing proportion
of that Gaussian. So, for each training case, the outputs of the experts and their
controllers define a probability distribution that is a mixture of Gaussians . The aim

377

378

Becker and Hinton

,a)

b)
Mean output vs. distance
to nearest discontinuity

Interpolator Discontinuity
weights
detector weights
1.00

iiiiiI
iiiii2

~

0.95

~T-

..

""
iI' I ,,,""""
. \ , ,, ,~,,
, I
I i , ?, ,, \
,~ : I ,, ,, , \,
/~ ; iI ,
,
? ~ 1
I,
:, ,, ,\
.? I
l ~, \'
\
I'

0 .90
0 .15
0.10
0.75

YU.-4- uair -

.

0.70
0 .6.5

.:'
:
.

0 .60
0.55
0.50
0.45
0.40

ti.,
:;
I

~
I'

0.10
02S
020

I

,-~:'. -f
.....' ,
,

I

0.15

\

,

\

1

,

I
I

I
I

I

,

i

!

I '
I'
i I

\'

I

""I,

I

OAS

,

~

. ..

\

I

0.10

:'

\?

\

I

""

I~ :i'I'i """"
I: ' ! ,"" ,
I .
:\ ,
I
, ,
i
I ,,
/
i ~

O.lS

j'

I

0.00

.oAS

pudol
?60.00

.4()00

-20.00

0 .00

20.00

40.00

60.00

Figure 4: a) Typical weights lear~ed by the five competing interpolators and corresponding five discontinuity detectors. Positive weights are shown in white, and
negative weights in black. b) The mean probabilities computed by each discontinuity
detector are plotted against the the distance from the center of the units' receptive
field to the nearest discontinuity. The probabilistic outputs are averaged over an
ensemble of 1000 test cases. If the nearest discontinuity is beyond ? thirty pixels,
it is outside the units' receptive field and the case is therefore a continuity example.
of the learning is to maximize the log probability density of the desired output, de,
under this mixture of Gaussians distribution. For a particular training case this log
probability is given by :
'""
log P( de) = log L.,; Pi

.

I

1

to=
v2~u

exp

((d
-

e

ei )2)
-d
2 2 '

u

(4)

By taking derivatives of this objective function we can simultaneously learn the
weights in the experts and in the controllers. For the results shown here, the
nework was trained for 30 conjugate gradient iterations on a set of 1000 random
dot stereograms with discontinuities.
The rationale for the use of a variance ratio in equation 1 is to prevent the variances
of d a and db collapsing to zero. Because the local estimates d 1 , ... , d s did not change
as the system learned the expert interpolators, it was possible to use (de - dc ,d 2 in
the objective function without worrying about the possibility that the variance of
de across cases would collapse to zero during the learning . Ideally we would like to

Learning (0 Make Coherent Predictions in Domains with Discontinuities
refine the weights of the local depth estimators to maximize their agreement with
the contextually predicted depths produced by the mixture of expert interpolators.
One way to do this would be to generalize equation 3 to handle a mixture of expert
interpolators:

(5)
Alternatively we could modify equation 4 by normalizing the difference (de - de.i )2
by the actual variance of dc, though this makes the derivatives considerably more
complicated.

5

Discussion

The competing controllers in figure 3 explicitly represent which regularity applies in
a particular region. The outputs of the controllers for nearby regions may themselves
exhibit coherence at a larger spatial scale, so the same learning technique could be
applied recursively. In 2-D images this should allow the continuity of depth edges
to be discovered.
The approach presented here should be applicable to other domains which contain
a mixture of alternative local regularities aCl?OSS space or time. For example, a l?igiJ
shape causes a linear constraint between the locations of its parts in an image, so if
there are many possible shapes, there are many alternative local regularities (Zemel
and Hinton, 1991).
Our learning procedure differs from methods that try to capture as much information as possible about the input (Linsker, 1988; Atick and Redlich, 1990) because
we ignore information in the input that is not coherent across space.
Acknowledgements
This research was funded by grants from NSERC and the Ontario Information Technology Research Centre. Hinton is Noranda fellow of the Canadian Institute for Advanced
Research. Thanks to John Bridle and Steve Nowlan for helpful discussions.

References
Atick, J. J. and Redlich, A. N. (1990). Towards a theory of early visual processing.
Technical Report IASSNS-HEP-90j10, Institute for Advanced Study, Princeton.
Becker, S. and Hinton, G. E. (1992). A self-organizing neural network that discovers
surfaces in random-dot stereograms. January 1992 Nature.
Jacobs, R. A., Jordan, M. 1., Nowlan, S. J., and Hinton, G. E. (1991). Adaptive mixtures
of local experts. Neural Computation, 3(1).
Linsker, R. (1988). Self-organization in a perceptual network. IEEE Computer, March,
21:105-117.
Zemel, R. S. and Hinton, G. E. (1991). Discovering viewpoint-invariant relationships that
characterize objects. In Advances In Neural Information Processing Systems 3, pages
299-305. Morgan Kaufmann Publishers.

379

"
5988,2016,Recovery Guarantee of Non-negative Matrix Factorization  via Alternating Updates,"Non-negative matrix factorization is a popular tool for  decomposing data into feature and weight matrices under non-negativity constraints. It enjoys practical success but is poorly understood theoretically. This paper proposes an algorithm that alternates between decoding the weights and updating the features, and shows that assuming a generative model of the data, it provably recovers the ground-truth under fairly mild conditions. In particular, its only essential requirement on features is linear independence. Furthermore, the algorithm uses ReLU to exploit the non-negativity for decoding the weights, and thus can tolerate adversarial noise that can potentially be as large as the signal, and can tolerate unbiased noise much larger than the signal. The analysis relies on a carefully designed coupling between two potential functions, which we believe is of independent interest.","Recovery Guarantee of Non-negative Matrix
Factorization via Alternating Updates

Yuanzhi Li, Yingyu Liang, Andrej Risteski
Computer Science Department at Princeton University
35 Olden St, Princeton, NJ 08540
{yuanzhil, yingyul, risteski}@cs.princeton.edu

Abstract
Non-negative matrix factorization is a popular tool for decomposing data into
feature and weight matrices under non-negativity constraints. It enjoys practical
success but is poorly understood theoretically. This paper proposes an algorithm
that alternates between decoding the weights and updating the features, and shows
that assuming a generative model of the data, it provably recovers the groundtruth under fairly mild conditions. In particular, its only essential requirement on
features is linear independence. Furthermore, the algorithm uses ReLU to exploit
the non-negativity for decoding the weights, and thus can tolerate adversarial noise
that can potentially be as large as the signal, and can tolerate unbiased noise much
larger than the signal. The analysis relies on a carefully designed coupling between
two potential functions, which we believe is of independent interest.

1

Introduction

In this paper, we study the problem of non-negative matrix factorization (NMF), where given a matrix
Y ? Rm?N , the goal to find a matrix A ? Rm?n and a non-negative matrix X ? Rn?N such
that Y ? AX.1 A is often referred to as feature matrix and X referred as weights. NMF has been
extensively used in extracting a parts representation of the data (e.g., [LS97, LS99, LS01]). It has
been shown that the non-negativity constraint on the coefficients forcing features to combine, but not
cancel out, can lead to much more interpretable features and improved downstream performance of
the learned features.
Despite all the practical success, however, this problem is poorly understood theoretically, with only
few provable guarantees known. Moreover, many of the theoretical algorithms are based on heavy
tools from algebraic geometry (e.g., [AGKM12]) or tensors (e.g. [AKF+ 12]), which are still not
as widely used in practice primarily because of computational feasibility issues or sensitivity to
assumptions on A and X. Some others depend on specific structure of the feature matrix, such as
separability [AGKM12] or similar properties [BGKP16].
A natural family of algorithms for NMF alternate between decoding the weights and updating the
features. More precisely, in the decoding step, the algorithm represents the data as a non-negative
combination of the current set of features; in the updating step, it updates the features using the
decoded representations. This meta-algorithm is popular in practice due to ease of implementation,
computational efficiency, and empirical quality of the recovered features. However, even less
theoretical analysis exists for such algorithms.
1

In the usual formulation of the problem, A is also assumed to be non-negative, which we will not require in
this paper.

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

This paper proposes an algorithm in the above framework with provable recovery guarantees. To
be specific, the data is assumed to come from a generative model y = A? x? + ?. Here, A? is the
ground-truth feature matrix, x? are the non-negative ground-truth weights generated from an unknown
distribution, and ? is the noise. Our algorithm can provably recover A? under mild conditions, even
in the presence of large adversarial noise.
Overview of main results. The existing theoretical results on NMF can be roughly split into two
categories. In the first category, they make heavy structural assumptions on the feature matrix A?
such as separability ([AGM12]) or allowing running time exponential in n ( [AGKM12]). In the
second one, they impose strict distributional assumptions on x? ([AKF+ 12]), where the methods are
usually based on the method of moments and tensor decompositions and have poor tolerance to noise,
which is very important in practice.
In this paper, we present a very simple and natural alternating update algorithm that achieves the best
of both worlds. First, we have minimal assumptions on the feature matrix A? : the only essential
condition is linear independence of the features. Second, it is robust to adversarial noise ? which
in some parameter regimes be potentially be on the same order as the signal
A? x? , and is robust to
?
unbiased noise potentially even higher than the signal by a factor of O( n). The algorithm does not
require knowing the distribution of x? , and allows a fairly wide family of interesting distributions.
We get this at a rather small cost of a mild ?warm start?. Namely, we initialize each of the features to
be ?correlated? with the ground-truth features. This type of initialization is often used in practice as
well, for example in LDA-c, the most popular software for topic modeling ([lda16]).
A major feature of our algorithm is the significant robustness to noise. In the presence of adversarial
noise on each entry of y up to level C? , the noise level k?k1 can be in the same order as the signal
A? x? . Still, our algorithm is able to output a matrix A such that the final kA? ? Ak1 ? O(k?k1 ) in
the order of the?
noise in one data point. If the noise is unbiased (i.e., E[?|x? ] = 0), the noise level
k?k1 can?be ?( n) times larger than the signal A? x? , while we can still guarantee kA? ? Ak1 ?
O (k?k1 n) ? so our algorithm is not only tolerant to noise, but also has very strong denoising effect.
Note that even for the unbiased case the noise can potentially be correlated with the ground-truth in
very complicated manner, and also, all our results are obtained only requiring the columns of A? are
independent.
Technical contribution. The success of our algorithm crucially relies on exploiting the non-negativity
of x? by a ReLU thresholding step during the decoding procedure. Similar techniques have been
considered in prior works on matrix factorization, however to the best of our knowledge, the analysis
(e.g., [AGMM15]) requires that the decodings are correct in all the intermediate iterations, in the
sense that the supports of x? are recovered with no error. Indeed, we cannot hope for a similar
guarantee in our setting, since we consider adversarial noise that could potentially be the same order
as the signal. Our major technical contribution is a way to deal with the erroneous decoding through
out all the intermediate iterations. We achieve this by a coupling between two potential functions
that capture different aspects of the working matrix A. While analyzing iterative algorithms like
alternating minimization or gradient descent in non-convex settings is a popular topic in recent
years, the proof usually proceeds by showing that the updates are approximately performing gradient
descent on an objective with some local or hidden convex structure. Our technique diverges from the
common proof strategy, and we believe is interesting in its own right.
Organization. After reviewing related work, we define the problem in Section 3 and describe our
main algorithm in Section 4. To emphasize the key ideas, we first present the results and the proof
sketch for a simplified yet still interesting case in Section 5, and then present the results under much
more general assumptions in Section 6. The complete proof is provided in the appendix.

2

Related work

Non-negative matrix factorization relates to several different topics in machine learning.
Non-negative matrix factorization. The area of non-negative matrix factorization (NMF) has a rich
empirical history, starting with the practical algorithm of [LS97].On the theoretical side, [AGKM12]
provides a fixed-parameter tractable algorithm for NMF, which solves algebraic equations and thus has
poor noise tolerance. [AGKM12] also studies NMF under separability assumptions about the features.
2

[BGKP16] studies NMF under heavy noise, but also needs assumptions related to separability, such
as the existence of dominant features. Also, their noise model is different from ours.
Topic modeling. A closely related problem to NMF is topic modeling, a common generative model
for textual data [BNJ03, Ble12]. Usually, kx? k1 = 1 while there also exist work that assume
x?i ? [0, 1] and are independent [ZX12]. A popular heuristic in practice for learning A? is variational
inference, which can be interpreted as alternating minimization in KL divergence norm. On the
theory front, there is a sequence of works by based on either spectral or combinatorial approaches,
which need certain ?non-overlapping? assumptions on the topics. For example, [AGH+ 13] assume
the topic-word matrix contains ?anchor words?: words which appear in a single topic. Most related
is the work of [AR15] who analyze a version of the variational inference updates when documents
are long. However, they require strong assumptions on both the warm start, and the amount of
?non-overlapping? of the topics in the topic-word matrix.
ICA. Our generative model for x? will assume the coordinates are independent, therefore our problem
can be viewed as a non-negative variant of ICA with high levels of noise. Results here typically are
not robust to noise, with the exception of [AGMS12] that tolerates Gaussian noise. However, to best
of our knowledge, no result in this setting is provably robust to adversarial noise.
Non-convex optimization. The framework of having a ?decoding? for the samples, along with
performing an update for the model parameters has proven successful for dictionary learning as
well. The original empirical work proposing such an algorithm (in fact, it suggested that the V1
layer processes visual signals in the same manner) was due to [OF97]. Even more, similar families
of algorithms based on ?decoding? and gradient-descent are believed to be neurally plausible as
mechanisms for a variety of tasks like clustering, dimension-reduction, NMF, etc ([PC15, PC14]). A
theoretical analysis came latter for dictionary learning due to [AGMM15] under the assumption that
the columns of A? are incoherent. The technique is not directly applicable to our case, as we don?t
wish to have any assumptions on the matrix A? . For instance, if A? is non-negative and columns
with l1 norm 1, incoherence effectively means the the columns of A? have very small overlap.

3

Problem definition and assumptions

Given a matrix Y ? Rm?N , the goal of non-negative matrix factorization (NMF) is to find a matrix
A ? Rm?n and a non-negative matrix X ? Rn?N , so that Y ? AX. The columns of Y are
called data points, those of A are features, and those of X are weights. We note that in the original
NMF, A is also assumed to be non-negative, which is not required here. We also note that typically
m  n, i.e., the features are a few representative components in the data space. This is different
from dictionary learning where overcompleteness is often assumed.
The problem in the worst case is NP-hard [AGKM12], so some assumptions are needed to design
provable efficient algorithms. In this paper, we consider a generative model for the data point
y = A ? x? + ?

(1)

where A? is the ground-truth feature matrix, x? is the ground-truth non-negative weight from some
unknown distribution, and ? is the noise. Our focus is to recover A? given access to the data
distribution, assuming some properties of A? , x? , and ?. To describe our assumptions, we let [M]i
denote the i-th row of a matrix M, [M]j its i-th column, Mi,j
its column
P its (i, j)-th entry. DenoteP
norm, row norm, and symmetrized norm as kMk1 = maxj i |Mi,j |, kMk? = maxi j |Mi,j |,
and kMks = max {kMk1 , kMk? } , respectively.
We assume the following hold for parameters C1 , c2 , C2 , `, C? to be determined in our theorems.
(A1) The columns of A? are linearly independent.
(A2) For all i ? [n], x?i ? [0, 1], E[x?i ] ?

C1
n

and

c2
n

? E[(x?i )2 ] ?

C2
n ,

and x?i ?s are independent.

(A3) The initialization A(0) = A? (?(0) + E(0) ) + N(0) , where ?(0) is diagonal, E(0) is offdiagonal, and




?(0)  (1 ? `)I, 
E(0) 
 ? `.
s

We consider two noise models.
3

(N1) Adversarial noise: only assume that maxi |?i | ? C? almost surely.
(N2) Unbiased noise: maxi |?i | ? C? almost surely, and E[?|x? ] = 0.
Remarks. We make several remarks about each of the assumptions.
(A1) is the assumption about A? . It only requires the columns of A? to be linear independent, which
is very mild and needed to ensure identifiability. Otherwise, for instance, if (A? )3 = ?1 (A? )1 +
?2 (A? )2 , it is impossible to distinguish between the case when x?3 = 1 and the case when x?2 = ?1
and x?1 = ?2 . In particular, we do not restrict the feature matrix to be non-negative, which is more
general than the traditional NMF and is potentially useful for many applications. We also do not
make incoherence or anchor word assumptions that are typical in related work.
(A2) is the assumption on x? . First, the coordinates are non-negative and bounded by 1; this is simply
a matter of scaling. Second, the assumption on the moments requires that, roughly speaking, each
feature should appear with reasonable probability. This is expected: if the occurrences of the features
are extremely unbalanced, then it will be difficult to recover the rare ones. The third requirement
on independence is motivated by that the features should be different so that their occurrences are
not correlated. Here we do not stick to a specific distribution, since the moment conditions are more
general, and highlight the essential properties our algorithm needs. Example distributions satisfying
our assumptions will be discussed later.
(0)

The warm start required by (A3) means that each feature Ai has a large fraction of the groundtruth feature A?i and a small fraction of the other features, plus some noise outside the span of the
ground-truth features. We emphasize that N(0) is the component of A(0) outside the column space
of A? , and is not the difference between A(0) and A? . This requirement is typically achieved in
practice by setting the columns of A(0) to reasonable ?pure? data points that contains one major
feature and a small fraction of some other features (e.g. [lda16, AR15]); in this initialization, it is
generally believed that N(0) = 0. But we state our theorems to allow some noise N(0) for robustness
in the initialization.
The adversarial noise model (N1) is very general, only imposing an upper bound on the entry-wise
noise level. Thus, ? can be correlated with x? in some complicated unknown way. (N2) additionally
requires it to be zero mean, which is commonly assumed and will be exploited by our algorithm to
tolerate larger noise.

4

Main algorithm

Algorithm 1 Purification
Input: initialization A(0) , threshold ?, step size ?, scaling factor r, sample size N , iterations T
1: for t = 0, 1, 2, ..., T ? 1 do
2:
Draw examples y1 , . . . , yN .
3:
(Decode) Compute A? , the pseudo-inverse of A(t) with minimum k(A)? k? .
Set x = ?? (A? y) for each example y.
// ?? is ReLU activation; see (2) for the
definition
4:
(Update) Update the feature matrix


? (y ? y 0 )(x ? x0 )>
A(t+1) = (1 ? ?) A(t) + r? E
? is over independent uniform y, y 0 from {y1 , . . . , yN }, and x, x0 are their decodings.
where E
Output: A = A(T )
Our main algorithm is presented in Algorithm 1. It keeps a working feature matrix and operates in
iterations. In each iteration, it first compute the weights for a batch of N examples (decoding), and
then uses the computed weights to update the feature matrix (updating).
The decoding is simply multiplying the example by the pseudo-inverse of the current feature matrix
and then passing it through the rectified linear unit (ReLU) ?? with offset ?. The pseudo-inverse
with minimum infinity norm is used so as to maximize the robustness to noise (see the theorems).
The ReLU function ?? operates element-wisely on the input vector v, and for an element vi , it is
4

defined as
?? (vi ) = max {vi ? ?, 0} .

(2)

To get an intuition why the decoding makes sense, suppose the current feature matrix is the groundtruth. Then A? y = A? A? x? + A? ? = x? + A? ?. So we would like to use a small A? and use
threshold to remove the noise term.


In the encoding step, the algorithm move the feature matrix along the direction E (y ? y 0 )(x ? x0 )> .
To see intuitively
why this isa good direction, note that when the decoding is perfect and there is no

noise, E (y ? y 0 )(x ? x0 )> = A? , and thus it is moving towards the ground-truth. Without those
ideal conditions, we need to choose a proper step size, which is tuned by the parameters ? and r.

5

Results for a simplified case

Our intuitions can be demonstrated in a simplified setting with (A1), (A2?), (A3), and (N1), where
(A2?) x?i ?s are independent, and x?i = 1 with probability s/n and 0 otherwise for a constant s > 0.
Furthermore, let N(0) = 0. This is a special case of our general assumptions, with C1 = c2 = C2 = s
where s is the parameter in (A2?). It is still an interesting setting; as far as we know, there is no
existing guarantee of alternating type algorithms for it.
To present our results, we let (A? )? denote the matrix satisfying (A? )? A? = I; if there are multiple
such matrices we let it denote the one with minimum k(A? )? k? .
Theorem 1 (Simplified case, adversarial noise). There exists a absolute constant G such that when
Assumption (A1)(A2?)(A3) and (N1) are satisfied with l = 1/10, C? ? max m,nGc(A? )?
for
{ k
k? }
(0)
some 0 ? c ? 1, and N
= 0, then there exist ?, ?, r such that for every 0 < , ? < 1 and
N = poly(n, m, 1/, 1/?) the following holds with probability at least 1 ? ?.

After T = O ln 1 iterations, Algorithm 1 outputs a solution A = A? (? + E) + N where
?  (1 ? `)I is diagonal, kEk1 ?  + c is off-diagonal, and kNk1 ? c.
? i = Ai /kAi k , and the
Remarks. Consequently, when kA? k1 = 1, we can do normalization A
1
?
normalized output A satisfies
? ? A? k1 ?  + 2c.
kA
So under mild conditions and with proper parameters, our algorithm recovers the ground-truth in a
geometric rate. It can achieve arbitrary small recovery error in the noiseless setting, and achieve error
up to the noise limit even with adversarial noise whose level is comparable to the signal.
The condition on ` means that a constant warm start is sufficient for our algorithm to converge, which
is much better than previous work such as [AR15]. Indeed, in that work, the ` needs to even depend
on the dynamic range of the entries of A? which is problematic in practice.
It is shown that with large adversarial noise, the algorithm can still recover the features up to the
?
noise limit. When m ? nk (A? ) k? , each data point has adversarial noise with `1 norm as large
as k?k1 = C? m = ?(c), which is in the same order as the signal kA? x? k1 = O(1). Our algorithm
still works in this regime. Furthermore, the final error kA ? A? k1 is O(c), in the same order as the
adversarial noise in one data point.
?

Note the appearance of k (A? ) k? is not surprising. The case when the columns are the canonical
?
unit vectors for instance, which corresponds to k (A? ) k? = 1, is expected to be easier than the
?
case when the columns are nearly the same, which corresponds to large k (A? ) k? .
A similar theorem holds for the unbiased noise model.
Theorem 2 (Simplified?case, unbiased noise). If Assumption (A1)(A2?)(A3) and (N2) are satisfied
Gc n
with C? = max m,n
and the other parameters set as in Theorem 1, then the same
{ k(A? )? k? }
guarantee in holds.
5

Remarks. With unbiased noise which is commonly assumed in many applications, the algorithm can
?
?
tolerate noise level n larger than the adversarial case. When m ?
? nk (A? ) k? , each ?
data point
has adversarial noise with `1 norm as large as k?k1 = C? m = ?(c n), which can be ?( n) times
larger than the signal kA? x? k1 = O(1). The algorithm can recover ?
the ground-truth in this heavy
?
noise regime. Furthermore, the final error kA ? A? k1 is O (k?k1 / n), which is only O(1/ n)
fraction of the noise in one data point. This is very strong denoising effect and a bit counter-intuitive.
It is possible since we exploit the average of the noise for cancellation, and also use thresholding to
remove noise spread out in the coordinates.
5.1

Analysis: intuition

A natural approach typically employed to analyze algorithms for non-convex problems is to define a
function on the intermediate solution A and the ground-truth A? measuring their distance and then
show that the function decreases at each step. However, a single potential function will not be enough
in our case, as we argue below, so we introduce a novel framework of maintaining two potential
functions which capture different aspects of the intermediate solutions.
Let us denote the intermediate solution and the update as (omitting the superscript (t))
? ? y 0 )(x ? x0 )> ] = A? (?
e + E)
e + N,
e
E[(y

A = A? (? + E) + N,

e are diagonal, E and E
e are off-diagonal, and N and N
e are the terms outside the span
where ? and ?
?
of A which is caused by the noise. To cleanly illustrate the intuition behind ReLU and the coupled
potential functions, we focus on the noiseless case and assume that we have infinite samples.
P
P
Since Ai = ?i,i A?i + j6=i Ej,i A?j , if the ratio between kEi k1 = j6=i |Ej,i | and ?i,i gets smaller,
then the algorithm is making progress; if the ratio is large at the end, a normalization of Ai gives a
good approximation of A?i . So it suffices to show that ?i,i is always about a constant while kEi k1
decreases at each iteration. We will focus on E and consider the update rule in more detail to argue
this. After some calculation, we have
e = E[(x? ? (x0 )? ) (x ? x0 )> ],
E

(3)

where x, x are the decoding for x , (x ) respectively:


x = ?? (? + E)?1 x? ,
x0 = ?? (? + E)?1 (x0 )? .

(4)

e
E ? (1 ? ?)E + r? E,
0

?

0 ?

To see why the ReLU function matters, consider the case when we do not use it.





e = E(x? ? (x0 )? ) A? A? (x? ? (x0 )? ) > = E (x? ? (x0 )? )(x? ? (x0 )? )> (? + E)?1 >
E

>
? (? + E)?1 ? ??1 ? ??1 E??1 .


where we used Taylor expansion and the fact that E (x? ? (x0 )? )(x? ? (x0 )? )> is a scaling of
identity. Hence, if we think of ? as approximately I and take an appropriate r, the update to the
matrix E is approximately E ? E ? ?E> . Since we do not have control over the signs of E
throughout the iterations, the problematic case is when the entries of E> and E roughly match in
signs, which would lead to the entries of E increasing.
Now we consider the decoding to see why ReLU is important. Ignoring the higher order terms and
regarding ? = I, we have


x = ?? (? + E)?1 x? ? ?? ??1 x? ? ??1 E??1 x? ? ?? (x? ? Ex? ) .
(5)
The problematic term is Ex? . These errors when summed up will be comparable or even larger
than the signals, and the algorithm will fail. However, since the signals are non-negative and most
coordinates with errors only have small values, thresholding with ReLU properly can remove those
e i,i and small E
e j,i ?s, and then
errors while keeping a large fraction of the signals. This leads to large ?
we can choose an r such that Ej,i ?s keep decreasing while ?i,i ?s stay in a certain range.
To get a quantitative bound, we divide E into its positive part E+ and its negative part E? :
[E+ ]i,j = max {Ei,j , 0} ,

[E? ]i,j = max {?Ei,j , 0} .
6

(6)

The reason
to do so is the following: when Ei,j is negative, by the Taylor expansion approxima
tion, (? + E)?1 x? i will tend to be more positive and will not be thresholded most of the time.
Therefore,
Ej,iwill turn more positive at next iteration. On the other hand, when Ei,j is positive,

(? + E)?1 x? i will tend to be more negative and zeroed out by the threshold function. Therefore,
Ej,i will not be more negative at next iteration. We will show for positive and negative parts of E:
postive(t+1) ? (1??)positive(t) +(?)negative(t) , negative(t+1) ? (1??)negative(t) +(??)positive(t)
for a small ?  1. Due to , we can couple the two parts so that a weighted average of them will
decrease, which implies that kEks is small at the end. This leads to our coupled potential function.2
5.2

Analysis: proof sketch

Here we describe a proof sketch for the simplified case while the complete proof for the general case
is presented in the appendix. The lemmas here are direct corollaries of those in the appendix.
One iteration. We focus on one update and omit the superscript (t). Recall the definitions of E, ?
e ?
e and N
e in (5.1). Our goal is to derive lower and upper bounds for E,
e ?
e
and N in (5.1), and E,
e
and N, assuming that ?i,i falls into some range around 1, while E and N are small. This will allow
doing induction on them.
First, begin with the decoding. Some calculation shows that, the decoding for y = A? x? + ? is
x = ?? (Zx? + ?) , where Z = (? + E)

?1

, ? = ?A? NZx? + A? ?.

(7)

e ?,
e and N.
e
Now, we can present our key lemmas bounding E,


e informal). (1) if Zi,j < 0, then E
e j,i  ? O
Lemma 3 (Simplified bound on E,

 

e j,i  ? O 1 kZi,j k .
(2) if Zi,j ? 0, then ?O nc2 + nc |Zi,j | + n12 |Zi,j | ? E
n

1
n2


(|Zi,j | + c) ,

Note that Z ? ??1 ? ??1 E??1 , so Zi,j < 0 corresponds roughly to Ei,j > 0. In this case,
e j,i | is very small and thus |Ej,i | decreases, as described in the intuition.
the upper bound on |E
What is most interesting is the case when Zi,j ? 0 (roughly Ei,j < 0). The upper bound is much
larger, corresponding to the intuition that negative Ei,j can contribute a large positive value to Ej,i .
Fortunately, the lower bounds are of much smaller absolute value, which allows us to show that a
potential function that couples Case (1) and Case (2) in Lemma 3 actually decreases; see the induction
below.
e informal). ?
e i,i ? ?(??1 ? ?)/n.
Lemma 4 (Simplified bound on ?,
i,i


e 
e
Lemma 5 (Simplified bound on N, adversarial noise, informal). N
i,j  ? O(C? /n).
Induction by iterations. We now show how to use the three lemmas to prove the theorem for the
adversarial noise, and that for the unbiased noise is similar.





 (t) 

 (t) 
Let at := 
E+ 
 and bt := 
E? 
 , and choose ? = `/6. We begin with proving the following
s
s
three claims by induction on t: at the beginning of iteration t,
(1) (1 ? `)I  ?(t)


(2) 
E(t) 
s ? 1/8, and if t > 0, then at + ?bt ? 1 ?
? ? (1, 8), and some small value h,


(3) 
N(t) 
 ? c/10.

1
25 ?



(at?1 + ?bt?1 ) + ?h, for some

s

The most interesting part is the second claim. At a high level, by Lemma 3, we can show that




3
24
1
at+1 ? 1 ? ? at + 7?bt + ?h,
bt+1 ? 1 ? ? bt +
?at + ?h.
25
25
100
2
Note that since intuitively, Ei,j gets affected by Ej,i after an update, if we have a row which contains
negative entries, it is possible that kAi ? A?i k1 increases. So we cannot simply use maxi kAi ? A?i k1 as a
potential function.

7

Notice that the contribution of bt to at+1 is quite large (due to the larger upper bound in Case (2)
in Lemma 3), but the other terms are much nicer, such as the small contribution of at to bt+1 . This
allows to choose a ? ? (1, 8) so that at+1 + ?bt+1 leads to the desired recurrence in the second
claim. In other words, at+1 + ?bt+1 is our potential function which decreases at each iteration up to
the level h. The other claims can also be proved by the corresponding lemmas. Then the theorem
follows from the induction claims.

6

More general results

More general weight distributions. Our argument holds under more general assumptions on x? .
Theorem 6 (Adversarial noise). There exists an absolute constant G such thatwhen Assumption (A0)
c2 Gc

c4 Gc

2
(A3) and (N1) are satisfied with l = 1/10, C2 ? 2c2 , C13 ? Gc22 n, C? ? C22 m , C 5 n (A
? )?
k?
1
1 k

 (0) 
2
c
Gc
2
for 0 ? c ? 1, and 
N 
? ? C 3 k(A
, then there exist ?, ?, r such that for every 0 < , ? < 1
? )? k
1
?
and N = poly(n, m, 1/, 1/?), with probability at least 1 ? ? the following holds.

After T = O ln 1 iterations, Algorithm 1 outputs a solution A = A? (? + E) + N where
?  (1 ? `)I is diagonal, kEk1 ?  + c/2 is off-diagonal, and kNk1 ? c/2.

Theorem 7? (Unbiased noise). If Assumption (A0)-(A3) and (N2) are satisfied with C? =
c2 G cn
and the other parameters set as in Theorem 6, then the same guarantee
C1 max{m,nk(A? )? k }
?
holds.
The conditions on C1 , c2 , C2 intuitively mean that each feature needs to appear with reasonable
probability. C2 ? 2c2 means that their proportions are reasonably balanced. This may be a mild
restriction for some applications, and additionally we propose a pre-processing step that can relax
this in the next subsection. The conditions allow a rather general family of distributions, so we point
out an important special case to provide a more concrete sense of the parameters. For example, for
the uniform independent distribution considered in the simplified case, we can actually allow s to be
much larger than a constant; our algorithm just requires s ? Gn for a fixed constant G. So it works
for uniform sparse distributions even when the sparsity is linear, which is an order of magnitude larger
than in the dictionary learning regime. Furthermore, the distributions of x?i can be very different,
since we only require C13 = O(c22 n). Moreover, all these can be handled without specific structural
assumptions on A? .
More general proportions. A mild restriction in Theorem 6 and 7 is that C2 ? 2c2 , that is,
maxi?[n] E[(x?i )2 ] ? 2 mini?[n] E[(x?i )2 ]. To satisfy this, we propose a preprocessing algorithm for
balancing E[(x?i )2 ]. The idea is quite simple: instead of solving Y ? A? X, we could also solve
Y ? [A? D][(D)?1 X] for a positive diagonal matrix D, where E[(x?i )2 ]/D2i,i is with in a factor of
2 from each other. We show in the appendix that this can be done under assumptions as the above
theorems, and additionally ?  (1 + `)I and E(0) ? entry-wise. After balancing, one can use
Algorithm 1 on the new ground-truth matrix [A? D] to get the final result.

7

Conclusion

A simple and natural algorithm that alternates between decoding and updating is proposed for
non-negative matrix factorization and theoretical guarantees are provided. The algorithm provably
recovers a feature matrix close to the ground-truth and is robust to noise. Our analysis provides
insights on the effect of the ReLU units in the presence of the non-negativity constraints, and the
resulting interesting dynamics of the convergence.

Acknowledgements
This work was supported in part by NSF grants CCF-1527371, DMS-1317308, Simons Investigator
Award, Simons Collaboration Grant, and ONR-N00014-16-1-2329.
8

References
[AGH+ 13] S. Arora, R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu, and M. Zhu. A
practical algorithm for topic modeling with provable guarantees. In ICML, 2013.
[AGKM12] Sanjeev Arora, Rong Ge, Ravindran Kannan, and Ankur Moitra. Computing a nonnegative matrix factorization?provably. In STOC, pages 145?162. ACM, 2012.
[AGM12] S. Arora, R. Ge, and A. Moitra. Learning topic models ? going beyond svd. In FOCS,
2012.
[AGMM15] S. Arora, R. Ge, T. Ma, and A. Moitra. Simple, efficient, and neural algorithms for
sparse coding. In COLT, 2015.
[AGMS12] Sanjeev Arora, Rong Ge, Ankur Moitra, and Sushant Sachdeva. Provable ica with
unknown gaussian noise, with implications for gaussian mixtures and autoencoders. In
NIPS, pages 2375?2383, 2012.
[AKF+ 12] A. Anandkumar, S. Kakade, D. Foster, Y. Liu, and D. Hsu. Two svds suffice: Spectral
decompositions for probabilistic topic modeling and latent dirichlet allocation. Technical
report, 2012.
[AR15] Pranjal Awasthi and Andrej Risteski. On some provably correct cases of variational
inference for topic models. In NIPS, pages 2089?2097, 2015.
[BGKP16] Chiranjib Bhattacharyya, Navin Goyal, Ravindran Kannan, and Jagdeep Pani. Nonnegative matrix factorization under heavy noise. In Proceedings of the 33nd International Conference on Machine Learning, 2016.
[Ble12] David M Blei. Probabilistic topic models. Communications of the ACM, 2012.
[BNJ03] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. JMLR,
3:993?1022, 2003.
[lda16] Lda-c software. https://github.com/blei-lab/lda-c/blob/master/readme.
txt, 2016. Accessed: 2016-05-19.
[LS97] Daniel D Lee and H Sebastian Seung. Unsupervised learning by convex and conic
coding. NIPS, pages 515?521, 1997.
[LS99] Daniel D Lee and H Sebastian Seung. Learning the parts of objects by non-negative
matrix factorization. Nature, 401(6755):788?791, 1999.
[LS01] Daniel D Lee and H Sebastian Seung. Algorithms for non-negative matrix factorization.
In NIPS, pages 556?562, 2001.
[OF97] Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set:
A strategy employed by v1? Vision research, 37(23):3311?3325, 1997.
[PC14] Cengiz Pehlevan and Dmitri B Chklovskii. A hebbian/anti-hebbian network derived
from online non-negative matrix factorization can cluster and discover sparse features.
In Asilomar Conference on Signals, Systems and Computers, pages 769?775. IEEE,
2014.
[PC15] Cengiz Pehlevan and Dmitri Chklovskii. A normative theory of adaptive dimensionality
reduction in neural networks. In NIPS, pages 2260?2268, 2015.
[ZX12] Jun Zhu and Eric P Xing. Sparse topical coding. arXiv preprint arXiv:1202.3778, 2012.

9

"
5971,2016,Quantum Perceptron Models,"We demonstrate how quantum computation can provide non-trivial improvements in the computational and statistical complexity of the perceptron model. We develop two quantum algorithms for perceptron learning. The first algorithm exploits quantum information processing to determine a separating hyperplane using a number of steps sublinear in the number of data points $N$, namely $O(\sqrt{N})$. The second algorithm illustrates how the classical mistake bound of $O(\frac{1}{\gamma^2})$ can be further improved to $O(\frac{1}{\sqrt{\gamma}})$ through quantum means, where $\gamma$ denotes the margin. Such improvements are achieved through the application of quantum amplitude amplification to the version space interpretation of the perceptron model.","Quantum Perceptron Models

Nathan Wiebe
Microsoft Research
Redmond WA, 98052
nawiebe@microsoft.com

Ashish Kapoor
Microsoft Research
Redmond WA, 98052
akapoor@microsoft.com

Krysta M Svore
Microsoft Research
Redmond WA, 98052
ksvore@microsoft.com

Abstract
We demonstrate how quantum computation can provide non-trivial improvements
in the computational and statistical complexity of the perceptron model. We
develop two quantum algorithms for perceptron learning. The first algorithm
exploits quantum information processing to determine a separating hyperplane
?
using a number of steps sublinear in the number of data points N , namely O( N ).
The second algorithm illustrates how the classical mistake bound of O( ?12 ) can be
further improved to O( ?1? ) through quantum means, where ? denotes the margin.
Such improvements are achieved through the application of quantum amplitude
amplification to the version space interpretation of the perceptron model.

1

Introduction

Quantum computation is an emerging technology that utilizes quantum effects to achieve significant,
and in some cases exponential, speed-ups of algorithms over their classical counterparts. The growing
importance of machine learning has in recent years led to a host of studies that investigate the promise
of quantum computers for machine learning [1, 2, 3, 4, 5, 6, 7, 8, 9].
While a number of important quantum speedups have been found, the majority of these speedups
are due to replacing a classical subroutine with an equivalent albeit faster quantum algorithm.
The true potential of quantum algorithms may therefore remain underexploited since quantum
algorithms have been constrainted to follow the same methodology behind traditional machine
learning methods [10, 8, 9]. Here we consider an alternate approach: we devise a new machine
learning algorithm that is tailored to the speedups that quantum computers can provide.
We illustrate our approach by focusing on perceptron training [11]. The perceptron is a fundamental
building block for various machine learning models including neural networks and support vector
machines [12]. Unlike many other machine learning algorithms, tight bounds are known for the
computational and statistical complexity of traditional perceptron training. Consequently, we are
able to rigorously show different performance improvements that stem from either using quantum
computers to improve traditional perceptron training or from devising a new form of perceptron
training that aligns with the capabilities of quantum computers.
We provide two quantum approaches to perceptron training. The first approach focuses on the
computational aspect of the problem and the proposed method quadratically reduces the scaling of
the complexity of training with respect to the number of training vectors. The second algorithm
focuses on statistical efficiency. In particular, we use the mistake bounds for traditional perceptron
training methods and ask if quantum computation lends any advantages. To this end, we propose an
algorithm that quadratically improves the scaling of the training algorithm with respect to the margin
between the classes in the training data. The latter algorithm combines quantum amplitude estimation
in the version space interpretation of the perceptron learning problem. Our approaches showcase the
trade-offs that one can consider in developing quantum algorithms, and the ultimate advantages of
performing learning tasks on a quantum computer.
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

Version Space

Feature Space

Figure 1: Version space and feature space views of classification. This figure is from [18].
The rest of the paper is organized as follows: we first cover the background on perceptrons, version
space and Grover search. We then present our two quantum algorithms and provide analysis of their
computational and statistical efficiency before concluding.

2
2.1

Background
Perceptrons and Version Space

Given a set of N separable training examples {?1 , .., ?N } ? IRD with corresponding labels
{y1 , .., yN }, yi ? {+1, ?1}, the goal of perceptron learning is to recover a hyperplane w that
perfectly classifies the training set [11]. Formally, we want w such that yi ? wT ?i > 0 for all i.
There are various simple online algorithms that start with a random initialization of the hyperplane
and make updates as they encounter more and more data [11, 13, 14, 15]; however, the rule that we
consider for online perceptron training is, upon misclassifying a vector (?, y), w ? w + y?.
A remarkable feature of the perceptron model is that upper bounds exist for the number of updates that
need to be made during this training procedure. In particular, if the training data is composed of unit
vectors, ?i ? IRD , that are separated by a margin of ? then there are perceptron training algorithms
that make at most O( ?12 ) mistakes [16], independent of the dimension of the training vectors. Similar
bounds also exist when the data is not separated [17] and also for other generalizations of perceptron
training [13, 14, 15]. Note that in the worst case, the algorithm will need to look at all points in the
training set at least once, consequently the computation complexity will be O(N ).
Our goal is to explore if the quantum procedures can provide improvements both in terms of
computational complexity (that is better than O(N )) and statistical efficiency (improve upon O( ?12 ).
Instead of solely applying quantum constructs to the feature space, we also consider the version space
interpretation of perceptrons which leads to the improved scaling with ?.
Formally, version space is defined as the set of all possible hyperplanes that perfectly separate the
data: VS := {w|yi ? wT ?i > 0 for all i}. Given a training datum, the traditional representation is
to depict data as points in the feature space and use hyperplanes to depict the classifiers. However,
there exists a dual representation where the hyperplanes are depicted as points and the data points are
represented as hyperplanes that induce constraints on the feasible set of classifiers. Figure 1, which is
borrowed from [18], illustrates the version space interpretation of perceptrons. Given three labeled
data points in a 2D space, the dual space illustrates the set of normalized hyperplanes as a yellow ball
with unit radius. The third dimension corresponds to the weights that multiply the two dimensions
of the input data and the bias term. The planes represent the constraints imposed by observing the
labeled data as every labeled data renders one-half of the space infeasible. The version space is then
the intersection of all the half-spaces that are valid. Naturally, classifiers including SVMs [12] and
Bayes point machines [19] lie in the version space.
We note that there are quantum constructs such as Grover search and amplitude amplification which
provide non-trivial speedups for the search task. This is the main reason why we resort to the version
space interpretation. We can use this formalism to simply pose the problem of determining the
2

P?/||P?||

P?/||P?||

P?/||P?||

U init U targ?

2 qa
qa

?

qa

qa

?

U targ?

qa

qa

?

U targ?

Figure 2: A geometric description of the action of Ugrover on an initial state vector ?.
separating hyperplane as a search problem in the dual space. For example given a set of candidates
hyperplanes, our problem reduces to searching amongst the sample set for the classifier that will
successfully classify the entire set. Therefore training the perceptron is equivalent to finding any
feasible point in the version space. We describe these quantum constructs in detail below.
2.2

Grover?s Search

Both quantum approaches introduced in this work and their corresponding speed-ups stem from a
quantum subroutine called Grover?s search [20, 21], which is a special case of a more general method
referred to as amplitude amplification [22]. Rather than sampling from a probability distribution until
a given marked element is found, the Grover search algorithm draws only one sample and then uses
quantum operations to modify the distribution from which it sampled. The probability distribution is
rotated, or more accurately the quantum state that yields the distribution is rotated, into one whose
probability is sharply concentrated on the marked element. Once a sharply peaked distribution is
identified, the marked item can be found using just one sample. In general, if p
the probability of
finding such an element is known to be a then amplitude amplification requires O( 1/a) operations
to find the marked item with certainty.
While Grover?s search is a quantum subroutine, it can in fact be understood using only geometric
arguments. The only notions from quantum mechanics used are those of the quantum state vector
and that of Born?s rule (measurement). A quantum state vector is a complex unit vector whose
components have magnitudes that are equal to the square?roots of the probabilities. In particular, if ?
is a quantum state vector and p is the corresponding probability distribution then
p = ? ? ? ?,

(1)

where the unit column vector ? is called the quantum state vector which sits in the vector space Cn , ?
is the Hadamard (pointwise) product and ? is the complex conjugate transpose. A quantum state can
be measured such that if we have a quantum state vector ? and a basis vector w then the probability
of measuring ? = w is |h?, wi|2 , where h?, ?i denotes the inner product.
We need to implement two unitary operations in order to perform the search algorithm:
Uinit = 2?? ? ? 11, Utarg = 11 ? 2P.

(2)

The operators Uinit and Utarg can be interpreted geometrically as reflections within a two?dimensional
space spanned by the vectors ? and P ?. If we assume that P ? 6= 0 and P ? 6= ? then these two
reflection operations can be used to rotate ? in the space span(?, P ?). Specifically this rotation
is Ugrover = Uinit Utarg . Its action is illustrated in Figure 2. If the angle between the vector ? and
P ?/kP ?k is ?/2 ? ?a , where ?a := sin?1 (|h?, P ?/kP ?ki|). It then follows from elementary
geometry and the rule for computing the probability distribution from a quantum state (known as
Born?s rule) that after j iterations of Grover?s algorithm the probability of finding a desirable outcome
is
p(? ? ?good |j) = sin2 ((2j + 1)?a ).
(3)
It is then easy
? to see that if ?a  1 and a probability of success greater than 1/4 is desired then
j ? O(1/ ?a ) suffices to find a marked outcome. This is quadratically faster than is possible
from statistical sampling, which requires O(1/?a ) samples on average. Simple modifications to this
algorithm allow it to be used in cases where ?a is not known [21, 22].
3

3

Online quantum perceptron

Now that we have discussed Grover?s search we turn our attention to applying it to speed up online
perceptron training. In order to do so, we first need to define the quantum model that we wish to
use as our quantum analogue of perceptron training. While there are many ways of defining such a
model but the following approach is perhaps the most direct. Although the traditional feature space
perceptron training algorithm is online [16], meaning that the training examples are provided one at a
time to it in a streaming fashion, we deviate from this model slightly by instead requiring that the
algorithm be fed training examples that are, in effect, sampled uniformly from the training set. This
is a slightly weaker model, as it allows for the possibility that some training examples will be drawn
multiple times. However, the ability to draw quantum states that are in a uniform superposition over
all vectors in the training set enables quantum computing to provide advantages over both classical
methods that use either access model.
We assume without loss of generality that the training set consists of N unit vectors, ?1 , . . . , ?N . If
we then define ?1 , . . . , ?N to be the basis vectors whose indices each coincide with a (B + 1)-bit
representation of the corresponding (?j , yj ) where yj ? {?1, 1} is the class assigned to ?j and let
?0 be a fixed unit vector that is chosen to represent a blank memory register.
We introduce the vectors ?j to make it clear that the quantum vectors states used to represent training
vectors do not live in the same vector space as the training vectors themselves. We choose the
quantum state vectors here to occupy a larger space than the training vectors because the Heisenberg
uncertainty principle makes it much more difficult for a quantum computer to compute the class that
the perceptron assigns to a training vector in such cases.
For example, the training vector (?j , yj ) ? ([0, 0, 1, 0]T , 1) can be encoded as an unsigned integer
00101 ? 5, which in turn can be represented by the unit vector ? = [0, 0, 0, 0, 0, 1]T . More generally,
if ?j ? IRD were a vector of floating point numbers then a similar vector could be constructed
by concatenating the binary representations of the D floating point numbers that comprise it with
(yj + 1)/2 and express the bit string as an unsigned integer, Q. The integer can then be expressed as
a unit vector ? : [?]q = ?q,Q . While encoding the training data as an exponentially long vector is
inefficient in a classical computer, it is not in a quantum computer because of the quantum computer?s
innate ability to store and manipulate exponentially large quantum state vectors.
Any machine learning algorithm, be it quantum or classical, needs to have a mechanism to access the
training data. We assume that the data is accessed via an oracle that not only accesses the training
data but also determines whether the data is misclassified. To clarify, let {uj : j = 1 : N } be an
orthonormal basis of quantum state vectors that serve as addresses for the training vectors in the
database. Given an input address for the training datum, the unitary operations U and U ? allow the
quantum computer to access the corresponding vector. Specifically, for all j
U [uj ? ?0 ]

U ? [uj ? ?j ]

= uj ? ?j ,

= uj ? ?0 .

(4)

Given an input address vector uj , the former corresponds to a database access and the latter inverts
the database access.
PN
P
Note that because U and U ? are linear operators we have that U j=1 uj ? ?0 = j uj ? ?j . A
quantum computer can therefore access each training vector simultaneously using a single operation.
The resultant vector is often called in the physics literature a quantum superposition of states and this
feature of linear transformations is referred to as quantum parallelism within quantum computing.
The next ingredient that we need is a method to test if the perceptron correctly assigns a training
vector addressed by a particular uj . This process can be pictured as being performed by a unitary
transformation that flips the sign of any basis-vector that is misclassified. By linearity, a single
application of this process flips the sign of any component of the quantum state vector that coincides
with a misclassified training vector. It therefore is no more expensive than testing if a given training
vector is misclassified in a classical setting. We denote the operator, which depends on the perceptron
weights w, Fw and require that
Fw [uj ? ?0 ] = (?1)fw (?j ,yj ) [uj ? ?0 ],

(5)

where fw (?j ) is a Boolean function that is 1 if and only if the perceptron with weights w misclassifies
training vector ?j . Since the classification step involves computing the dot?products of finite size
vectors, this process is efficient given that the ?j are efficiently computable.
4

We apply Fw in the following way. Let Fw be a unitary operation such that
Fw ?j = (?1)fw (?j ,yj ) ?j .

(6)

Fw is easy to implement in the quantum computer using a multiply controlled phase gate and a
quantum implementation of the perceptron classification algorithm, fw . We can then write
Fw = U ? (11 ? Fw )U.

(7)

Classifying the data based on the phases (the minus signs) output by Fw naturally leads to a very
memory efficient training algorithm because only one training vector is ever stored in memory during
the implementation of Fw given in Eq. (7). We can then use Fw to perform Grover?s search algorithm,
PN
by taking Utarg = Fw and Uinit = 2?? ? ? 11 with ? = ? := ?1N j=1 uj , to seek out training
vectors that the current perceptron model misclassifies. This leads to a quadratic reduction in the
number of times that the training vectors need to be accessed by Fw or its classical analogue.
In the classical setting, the natural object to query is slightly different. The oracle that is usually
assumed in online algorithms takes the form U c : Z 7? CD where U c (j) = ?j . We will assume that
a similar function exists in both the classical and the quantum settings for simplicity. In both cases,
we will consider the cost of a query to U c to be proportional to the cost of a query to Fw .
We use these operations in to implement a quantum search for training vectors that the perceptron
misclassifies. This leads to a quadratic speedup relative to classical methods as shown in the following
theorem. It is also worth noting that our algorithm uses a slight variant on the Grover search algorithm
to ensure that the runtime is finite.
Theorem 1. Given a training set that consists of unit vectors ?1 , . . . , ?N that are separated by a
margin of ? in feature space, the number of applications of Fw needed to infer a perceptron model,
w, such that P (? j : fw (?j ) = 1) ?  using a quantum computer is Nquant where
?

!
?
N
1
?( N ) 3 Nquant ? O
log
,
?2
? 2
whereas the number of queries to fw needed in the classical setting, Nclass , where the training vectors
are found by sampling uniformly from the training data is bounded by



1
N
log
.
?(N ) 3 Nclass ? O
?2
? 2
We assume in Theorem 1 that the training data in the classical case is accessed in a manner that
is analogous to the sampling procedure used in the quantum setting. If instead the training data is
supplied by a stream (as in the standard online model) then the upper bound changes to Nclass ?
O(N/? 2 ) because all N training vectors can be deterministically checked to see if they are correctly
classified by the perceptron. A quantum advantage is therefore obtained if N  log2 (1/? 2 ).
In order to prove Theorem 1 we need to have two technical lemmas (proven in the supplemental
material). The first bounds the complexity of the classical analogue to our training method:
Lemma 1. Given only the ability to sample uniformly from the training vectors, the number of queries
to fw needed to find a training vector that the current perceptron model fails to classify correctly, or
conclude that no such example exists, with probability 1 ? ? 2 is at most O(N log(1/? 2 )).
The second proves the correctness of our online quantum perceptron algorithm and bounds the
complexity of the algorithm:
Lemma 2. Assuming that the training vectors {?1 , . . . , ?N } are unit vectors and that they are drawn
from two classes separated by a margin of ? in feature space, Algorithm 2 will either update the
perceptron weights, or conclude that the current model provides a separating?hyperplane between
the two classes, using a number of queries to Fw that is bounded above by O( N log(1/? 2 )) with
probability of failure at most ? 2 .
After stating these results, we can now provide the proof of Theorem 1.
5

Proof of Theorem 1. The upper bounds follow as direct consequences of Lemma 2 and Lemma 1.
Novikoff?s theorem [16, 17] states that the algorithms described in both lemmas must be applied at
most 1/? 2 times before finding the result. However, either the classical or the quantum algorithm
may fail to find a misclassified vector at each of the O(1/? 2 ) steps. The union bound states that the
probability that this happens is at most the sum of the respective probabilities in each step. These
probabilities are constrained to be ? 2 , which means that the total probability of failing to correctly
find a mistake is at most  if both algorithms are repeated 1/? 2 times (which is the worst case number
of times that they need to be repeated).
The lower bound on the quantum query complexity follows from contradiction.
Assume that there
?
exists an algorithm that can train an arbitrary perceptron using o( N ) query operations. Now we
want to show that unstructured search with one marked element can be expressed as a perceptron
training algorithm. Let w be a known set of perceptron weights and assume that the perceptron only
misclassifies one vector ?1 . Thus if perceptron training succeeds then w the value of ?1 can be
extracted from the updated weights. This training problem is therefore equivalent to searching for a
misclassified vector. Now let ?j = [1 ? F (j), F (j)]T ? ?j where ?j is a unit vector that represents
the bit string j and F (j) is a Boolean function. Assume that F (0) = 1 and F (j) = 0 if j 6= 0,
which is without loss of generality equivalent ?
to Grover?s
[20, 21]. Now assume that ?j is
? T problem
P
1
?
assigned to class 2F (j) ? 1 and take w = [1/ 2, 1/ 2] ? N j ?j . This perceptron therefore
misclassifies ?0 and no other vector in the training set. Updating the weights yields ?j , which in turn
yields the value of j such that F (j) = 1, and so Grover?s search reduces to perceptron training.
Since?
Grover?s search reduces to perceptron training in the case of one marked item the lower bound
of ?( N ) queries for Grover?s
? search [21] applies to perceptron training. Since we assumed?that
perceptron training needs o( N ) queries this is a contradiction and the lower bound must be ?( N ).
We have assumed that in the classical setting that the user only has access to the training vectors
through an oracle that is promised to draw a uniform sample from {(?1 , y1 ), . . . , (?N , yN )}. Since
we are counting the number of queries to fw it is clear that in the worst possible case that the training
vector that the perceptron makes a mistake on can be the last unique value sampled from this list.
Thus the query complexity is ?(N ) classically.

4

Quantum version space perceptron

The strategy for our quantum version space training algorithm is to pose the problem of determining
a separating hyperplane as search. Specifically, the idea is to first generate K sample hyperplanes
w1 , . . . , wK from a spherical Gaussian distribution N (0, 11). Given a large enough K, we are
guaranteed to have at least one hyperplane amongst the samples that would lie in the version space
and perfectly separate the data. As discussed earlier Grover?s algorithm can provide quadratic speedup
over the classical search consequently the efficiency of the algorithm is determined by K. Theorem 2
provides an insight on how to determine this number of hyperplanes to be sampled.
Theorem 2. Given a training set that consists of d-dimensional unit vectors ?1 , . . . , ?N with labels
y1 , . . . , yN that are separated by a margin of ? in feature space, then a D-dimensional vector w
sampled from N (0, 11) perfectly separates the data with probability ?(?).
The proof of this theorem is provided in the supplementary material. The consequence of Theorem 2
stated below is that the expected number of samples K, required such that a separating hyperplane
exists in the set, only needs to scale as O( ?1 ). This is remarkable because, similar to Novikoff?s
theorem [16], the number of samples needed does not scale with D. Thus Theorem 2 implies that if
amplitude amplification is used to boost the probability of finding a vector in the version space then
the resulting quantum algorithm will need only O( ?1? ) quantum steps on average.
Next we show how to use Grover?s algorithm to search for a hyperplane that lies in the version space.
Let us take K = 2` , for positive integer `. Then given w1 , . . . , wK be the sampled hyperplanes, we
represent W1 , . . . , WK to be vectors that encode a binary representation of these random perceptron
vectors. In analogy to ?0 , we also define W0 to be a vector that represents an empty data register. We
define the unitary operator V to generate these weights given an address vector uj using the following
V [uj ? W0 ] = [uj ? Wj ].
6

(8)

In this context we can also think of the address vector, uj , as representing a seed for a pseudo?random
number generator that yields perceptron weights Wj .
Also let us define the classical analogue of V to be V c which obeys V c (j) = wj . Now using V (and
applying the Hadamard transform [23]) we can prepare the following quantum state
K
1 X
? := ?
uk ? Wk ,
K k=1

(9)

which corresponds to a uniform distribution over the randomly chosen w.
Now that we have defined the initial state, ?, for Grover?s search we need to define an oracle that
marks the vectors inside the version space. Let us define the operator F??,y via
F??,y [uj ? W0 ] = (?1)1+fwj (?,y) [uj ? W0 ].

(10)

This unitary operation looks at an address vector, uj , computes the corresponding perceptron model
Wj , flips the sign of any component of the quantum state vector that is in the half space in version
space specified by ? and then uncomputes Wj . This process can be realized using a quantum
subroutine that computes fw , an application of V and V ? and also the application of a conditional
phase gate (which is a fundamental quantum operation that is usually denoted Z) [23].
The oracle F??,y does not allow us to directly use Grover?s search to rotate a quantum state vector that
is outside the version space towards the version space boundary because it effectively only checks
one of the half?space inequalities that define the version space. It can, however, be used to build an
? that reflects about the version space:
operation, G,
? j ? W0 ] = (?1)1+(fwj (?1 ,y1 )?????fwj (?N ,yN )) [uj ? W0 ].
G[u

(11)

? can be implemented using 2N applications of F?? as well as a sequence of O(N )
The operation G
? as O(N ) queries to F??,y .
elementary quantum gates, hence we cost a query to G
We use these components in our version
? space training algorithm to, in effect, amplify the margin
between the two classes from ? to ?. We give the asymptotic scaling of this algorithm in the
following theorem.
Theorem 3. Given a training set that consists of unit vectors ?1 , . . . , ?N that are separated by a
margin of ? in feature space, the number of queries to F??,y needed to infer a perceptron model with
probability at least 1 ? , w, such that w is in the version space using a quantum computer is Nquant
where

 
N
3/2 1
Nquant ? O ? log
.
?

Proof. The proof of the theorem follows directly from bounds on K and the validity of our version
space training algorithm. It is clear from previous discussions that the algorithm carries out Grover?s
search, but instead of searching for a ? that is misclassified it instead searches for a w in version space.
Its validity therefore follows by following the exact same steps followed in the proof of Lemma 2
but with N = K. However, since the algorithm need is not repeated 1/? 2 times in this context we
can replace ? with 1 in the proof. Thus if ?
we wish to have a probability of failure of at most 0 then
? is in O( K log(1/0 )). This also guarantees that if any of the K
the number of queries made to G
vectors are in the version space then the probability of failing to find that vector is at most 0 .
? is costed at N queries to F??,y the query complexity (in units of queries
Next since one query to?G
?
to F?,y ) becomes O(N K log(1/0 )). The only thing that then remains is to bound the value of K
needed.
The probability of finding a vector in the version space is ?(?) from Theorem 2. This means that
there exists ? > 0 such that the probability of failing to find a vector in the version
space K

 times is
1
K
???K
at most (1 ? ??) ? e
. Thus this probability is at most ? for K ? ? ? log(1/?) . It then
suffices to pick K ? ?(log(1/?)/?) for the algorithm.
The union bound implies that the probability that either none of the vectors lie in the version space
or that Grover?s search failing to find such an element is at most 0 + ? ? . Thus it suffices to pick
7

0 ? ?() and ? ? ?() to ensure that the total probability is at most . Therefore the total number of
?
queries made to F??,y is in O(N log3/2 (1/)/ ?) as claimed.
The classical algorithm discussed previously has complexity O(N log(1/)/?), which follows from
the fact from Theorem 2 that K ? ?(log(1/)/?) suffices to make the probability of not drawing an
element of the version space at most . This demonstrates a quantum advantage if ?1  log(1/),
and illustrates that quantum computing can be used to boost the effective margins of the training data.
Quantum models of perceptrons therefore not only provide advantages in terms of the number of
vectors that need to be queried in the training process, they also can make the perceptron much more
perceptive by making training less sensitive to small margins.
These performance improvements can also be viewed as mistake bounds for the version space
perceptron. The inner loop in the version space algorithm attempts to sample from the version space
and then once it draws a sample it tests
? it against the training vectors to see if it errs on any example.
Since the inner loop is repeated O( K log(1/)) times, the maximum number of misclassified
vectors that arises from this training process is from Theorem 2 O( ?1? log3/2 (1/)) which, for
constant , constitutes a quartic improvement over the standard mistake bound of 1/? 2 [16].

5

Conclusion

We have provided two distinct ways to look at quantum perceptron training that each afford different
speedups relative to the other. The first provides a quadratic speedup with respect to the size of the
training data. We further show that this algorithm is asymptotically optimal in that if a super?quadratic
speedup were possible then it would violate known lower bounds for quantum searching. The second
provides a quadratic reduction in the scaling of the training time (as measured by the number of
interactions with the training data) with the margin between the two classes. This latter result is
especially interesting because it constitutes a quartic speedup relative to the typical perceptron training
bounds that are usually seen in the literature.
Perhaps the most significant feature of our work is that it demonstrates that quantum computing
can provide provable speedups for perceptron training, which is a foundational machine learning
method. While our work gives two possible ways of viewing the perceptron model through the lens of
quantum computing, other quantum variants of the perceptron model may exist. Seeking new models
for perceptron learning that deviate from these classical approaches may not only provide a more
complete understanding of what form learning takes within quantum systems, but also may lead to
richer classes of quantum models that have no classical analogue and are not efficiently simulatable
on classical hardware. Such models may not only revolutionize quantum learning but also lead to a
deeper understanding of the challenges and opportunities that the laws of physics place on our ability
to learn.

References
[1] M Lewenstein. Quantum perceptrons. Journal of Modern Optics, 41(12):2491?2501, 1994.
[2] Esma A??meur, Gilles Brassard, and S?ebastien Gambs. Machine learning in a quantum world. In
Advances in artificial intelligence, pages 431?442. Springer, 2006.
[3] Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum algorithms for supervised and
unsupervised machine learning. arXiv preprint arXiv:1307.0411, 2013.
[4] Nathan Wiebe, Ashish Kapoor, and Krysta Svore. Quantum nearest-neighbor algorithms for
machine learning. Quantum Information and Computation, 15:318?358, 2015.
[5] Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum principal component analysis.
Nature Physics, 10(9):631?633, 2014.
[6] Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. Quantum support vector machine for big
data classification. Physical review letters, 113(13):130503, 2014.
[7] Nathan Wiebe and Christopher Granade. Can small quantum systems learn? arXiv preprint
arXiv:1512.03145, 2015.
8

[8] Nathan Wiebe, Ashish Kapoor, and Krysta M Svore. Quantum deep learning. arXiv preprint
arXiv:1412.3489, 2014.
[9] Mohammad H Amin, Evgeny Andriyash, Jason Rolfe, Bohdan Kulchytskyy, and Roger Melko.
Quantum boltzmann machine. arXiv preprint arXiv:1601.02036, 2016.
[10] Silvano Garnerone, Paolo Zanardi, and Daniel A Lidar. Adiabatic quantum algorithm for search
engine ranking. Physical review letters, 108(23):230506, 2012.
[11] Frank Rosenblatt. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386, 1958.
[12] Johan AK Suykens and Joos Vandewalle. Least squares support vector machine classifiers.
Neural processing letters, 9(3):293?300, 1999.
[13] Yaoyong Li, Hugo Zaragoza, Ralf Herbrich, John Shawe-Taylor, and Jaz Kandola. The
perceptron algorithm with uneven margins. In ICML, volume 2, pages 379?386, 2002.
[14] Claudio Gentile. A new approximate maximal margin classification algorithm. The Journal of
Machine Learning Research, 2:213?242, 2002.
[15] Shai Shalev-Shwartz and Yoram Singer. A new perspective on an old perceptron algorithm. In
Learning Theory, pages 264?278. Springer, 2005.
[16] Albert BJ Novikoff. On convergence proofs for perceptrons. Technical report, DTIC Document,
1963.
[17] Yoav Freund and Robert E Schapire. Large margin classification using the perceptron algorithm.
Machine learning, 37(3):277?296, 1999.
[18] Thomas P Minka. A family of algorithms for approximate Bayesian inference. PhD thesis,
Massachusetts Institute of Technology, 2001.
[19] Ralf Herbrich, Thore Graepel, and Colin Campbell. Bayes point machines: Estimating the
bayes point in kernel space. In IJCAI Workshop SVMs, pages 23?27, 1999.
[20] Lov K Grover. A fast quantum mechanical algorithm for database search. In Proceedings of the
twenty-eighth annual ACM symposium on Theory of computing, pages 212?219. ACM, 1996.
[21] Michel Boyer, Gilles Brassard, Peter H?yer, and Alain Tapp. Tight bounds on quantum
searching. arXiv preprint quant-ph/9605034, 1996.
[22] Gilles Brassard, Peter Hoyer, Michele Mosca, and Alain Tapp. Quantum amplitude amplification
and estimation. Contemporary Mathematics, 305:53?74, 2002.
[23] Michael A Nielsen and Isaac L Chuang. Quantum computation and quantum information.
Cambridge university press, 2010.

9

"
3295,2010,Multiple Kernel Learning and the SMO Algorithm,"Our objective is to train $p$-norm Multiple Kernel Learning (MKL) and, more generally, linear MKL regularised by the Bregman divergence, using the Sequential Minimal Optimization (SMO) algorithm. The SMO algorithm is simple, easy to implement and adapt, and efficiently scales to large problems. As a result, it has gained widespread acceptance and SVMs are routinely trained using SMO in diverse real world applications. Training using SMO has been a long standing goal in MKL for the very same reasons. Unfortunately, the standard MKL dual is not differentiable, and therefore can not be optimised using SMO style co-ordinate ascent. In this paper, we demonstrate that linear MKL regularised with the $p$-norm squared, or with certain Bregman divergences, can indeed be trained using SMO. The resulting algorithm retains both simplicity and efficiency and is significantly faster than the state-of-the-art specialised $p$-norm MKL solvers. We show that we can train on a hundred thousand kernels in approximately seven minutes and on fifty thousand points in less than half an hour on a single core.","Multiple Kernel Learning and the SMO Algorithm

S. V. N. Vishwanathan, Zhaonan Sun, Nawanol Theera-Ampornpunt
Purdue University
vishy@stat.purdue.edu, sunz@stat.purdue.edu, ntheeraa@cs.purdue.edu
Manik Varma
Microsoft Research India
manik@microsoft.com

Abstract
Our objective is to train p-norm Multiple Kernel Learning (MKL) and, more generally, linear MKL regularised by the Bregman divergence, using the Sequential
Minimal Optimization (SMO) algorithm. The SMO algorithm is simple, easy to
implement and adapt, and efficiently scales to large problems. As a result, it has
gained widespread acceptance and SVMs are routinely trained using SMO in diverse real world applications. Training using SMO has been a long standing goal
in MKL for the very same reasons. Unfortunately, the standard MKL dual is not
differentiable, and therefore can not be optimised using SMO style co-ordinate ascent. In this paper, we demonstrate that linear MKL regularised with the p-norm
squared, or with certain Bregman divergences, can indeed be trained using SMO.
The resulting algorithm retains both simplicity and efficiency and is significantly
faster than state-of-the-art specialised p-norm MKL solvers. We show that we can
train on a hundred thousand kernels in approximately seven minutes and on fifty
thousand points in less than half an hour on a single core.

1

Introduction

Research on Multiple Kernel Learning (MKL) needs to follow a two pronged approach. It is important to explore formulations which lead to improvements in prediction accuracy. Recent trends
indicate that performance gains can be achieved by non-linear kernel combinations [7,18,21], learning over large kernel spaces [2] and by using general, or non-sparse, regularisation [6, 7, 12, 18].
Simultaneously, efficient optimisation techniques need to be developed to scale MKL out of the lab
and into the real world. Such algorithms can help in investigating new application areas and different
facets of the MKL problem including dealing with a very large number of kernels and data points.
Optimisation using decompositional algorithms such as Sequential Minimal Optimization
(SMO) [15] has been a long standing goal in MKL [3] as the algorithms are simple, easy to implement and efficiently scale to large problems. The hope is that they might do for MKL what SMO
did for SVMs ? allow people to play with MKL on their laptops, modify and adapt it for diverse real
world applications and explore large scale settings in terms of number of kernels and data points.
Unfortunately, the standard MKL formulation, which learns a linear combination of base kernels
subject to l1 regularisation, leads to a dual which is not differentiable. SMO can not be applied as a
result and [3] had to resort to expensive Moreau-Yosida regularisation to smooth the dual. State-ofthe-art algorithms today overcome this limitation by solving an intermediate saddle point problem
rather than the dual itself [12, 16].
Our focus, in this paper, is on training p-norm MKL, with p > 1, using the SMO algorithm. More
generally, we prove that linear MKL regularised by certain Bregman divergences, can also be trained
1

using SMO. We shift the emphasis firmly back towards solving the dual in such cases. The lp MKL dual is shown to be differentiable and thereby amenable to co-ordinate ascent. Placing the
p-norm squared regulariser in the objective lets us efficiently solve the core reduced two variable
optimisation problem analytically in some cases and algorithmically in others. Using results from [4,
9], we can compute the lp -MKL Hessian, which brings into play second order variable selection
methods which tremendously speed up the rate of convergence [8]. The standard decompositional
method proof of convergence [14] to the global optimum holds with minor modifications.
The resulting optimisation algorithm, which we call SMO-MKL, is straight forward to implement
and efficient. We demonstrate that SMO-MKL can be significantly faster than the state-of-the-art
specialised p-norm solvers [12]. We empirically show that the SMO-MKL algorithm is robust with
the desirable property that it is not greatly affected within large operating ranges of p. This implies
that our algorithm is well suited for learning both sparse, and non-sparse, kernel combinations.
Furthermore, SMO-MKL scales well to large problems. We show that we can efficiently combine
a hundred thousand kernels in approximately seven minutes or train on fifty thousand points in less
than half an hour using a single core on standard hardware where other solvers fail to produce results.
The SMO-MKL code can be downloaded from [20].

2

Related Work

Recent trends indicate that there are three promising directions of research for obtaining performance
improvements using MKL. The first involves learning non-linear kernel combinations. A framework
for learning general non-linear kernel combinations subject to general regularisation was presented
in [18]. It was demonstrated that, for feature selection, the non-linear GMKL formulation could
perform significantly better not only as compared to linear MKL but also state-of-the-art wrapper
methods and filter methods with averaging. Very significant performance gains in terms of pure
classification accuracy were reported in [21] by learning a different kernel combination per data
point or cluster. Again, the results were better not only as compared to linear MKL but also baselines
such as averaging. Similar trends were observed for regression while learning polynomial kernel
combinations [7]. Other promising directions which have resulted in performance gains are sticking
to standard MKL but combining an exponentially large number of kernels [2] and linear MKL with
p-norm regularisers [6, 12]. Thus MKL based methods are beginning to define the state-of-the-art
for very competitive applications, such as object recognition on the Caltech 101 database [21] and
object detection on the PASCAL VOC 2009 challenge [19].
In terms of optimisation, initial work on MKL leveraged general purpose SDP and QCQP
solvers [13]. The SMO+M.-Y. regularisation method of [3] was one of the first techniques that
could efficiently tackle medium scale problems. This was superseded by the SILP technique of [17]
which could, very impressively, train on a million point problem with twenty kernels using parallelism. Unfortunately, the method did not scale well with the number of kernels. In response, many
two-stage wrapper techniques came up [2, 10, 12, 16, 18] which could be significantly faster when
the number of training points was reasonable but the number of kernels large. SMO could indirectly
be used in some of these cases to solve the inner SVM optimisation. The primary disadvantage of
these techniques was that they solved the inner SVM to optimality. In fact, the solution needed to
be of high enough precision so that the kernel weight gradient computation was accurate and the
algorithm converged. In addition, Armijo rule based step size selection was also very expensive and
could involve tens of inner SVM evaluations in a single line search. This was particularly expensive
since the kernel cache would be invalidated from one SVM evaluation to the next. The one big
advantage of such two-stage methods for l1 -MKL was that they could quickly identify, and discard,
the kernels with zero weights and thus scaled well with the number of kernels. Most recently, [12]
have come up with specialised p-norm solvers which make substantial gains by not solving the inner
SVM to optimality and working with a small active set to better utilise the kernel cache.

3

The lp -MKL Formulation

The objective in MKL is to jointly learn kernel and SVM parameters from training data {(xi , yi )}.
Given a set of base kernels {Kk } and corresponding
P feature maps {?k }, linear MKL aims to learn
a linear combination of the base kernels as K = k dk Kk . If the kernel weights are restricted to
2

be non-negative, then the MKL task ?
corresponds to learning a standard SVM in the feature space
formed by concatenating the vectors dk ?k . The primal can therefore be formulated as
Xp
X
X
? X p p2
1
s. t. yi (
min
dk wtk ?k (xi )+b) ? 1??i (1)
wtk wk +C
?i + (
dk )
2
w,b,??0,d?0
2
i
k

k

k

The regularisation on the kernel weights is necessary to prevent them from shooting off to infinity.
Which regulariser one uses depends on the task at hand. In this Section, we limit ourselves to the
p-norm squared regulariser with p > 1. If it is felt that certain kernels are noisy and should be
discarded then a sparse solution can be obtained by letting p tend to unity from above. Alternatively,
if the application demands dense solutions, then larger values
? of p should be selected. Note that the
primal above can be made convex by substituting wk for dk wk to get
X
X
X
? X p p2
1
s. t. yi (
wtk ?k (xi ) + b) ? 1 ? ?i (2)
wtk wk /dk + C
?i + (
dk )
min
2
w,b,??0,d?0
2
i
k

k

k

We first derive an intermediate saddle point optimisation problem obtained by minimising only w,
b and ?. The Lagrangian is
X
X
X
? X p p2 X
L = 12
?i [yi (
wtk ?k (xi ) + b) ? 1 + ?i ] (3)
wtk wk /dk +
(C ? ?i )?i + (
dk ) ?
2
i
i
k

k

k

Differentiating with respect to w, b and ? to get the optimality conditions and substituting back
results in the following intermediate saddle point problem
X
? X p p2
(4)
d k ? t Hk ? + (
min max 1t ? ? 12
dk )
d?0 ??A
2
k

k

where A = {?|0 ? ? ? C1, 1t Y ? = 0}, Hk = Y Kk Y and Y is a diagonal matrix with the labels
on the diagonal. Note that most MKL methods end up optimising either this, or a very similar, saddle
point problem. To now eliminate d we again form the Lagrangian
X
? X p p2 X
L = 1t ? ? 21
d k ? t Hk ? + (
dk ) ?
? k dk
(5)
2
k
k
k
X p 2
?L
(6)
= 0 ? ?(
dk ) p ?1 dp?1
= ?k + 21 ?t Hk ?
k
?dk
k
X p 2
X
(7)
? ?(
dk ) p =
dk (?k + 21 ?t Hk ?)
k

k

? L = 1t ? ?

2
? X p p2
1 X
(
( (?k + 12 ?t Hk ?)q ) q
dk ) = 1t ? ?
2
2?

k

(8)

k

where p1 + 1q = 1. Since Hk is positive semi-definite, ?t Hk ? ? 0 and since ?k ? 0 it is clear that
the optimal value of ?k is zero. Our lp -MKL dual therefore becomes
2
1 X t
( (? Hk ?)q ) q
(9)
D ? max 1t ? ?
??A
8?
k

and the kernel weights can be recovered from the dual variables as
! q1 ? p1
q
1 X t
(?t Hk ?) p
(? Hk ?)q
dk =
2?

(10)

k

Note that our dual objective, unlike the objective in [3], is differentiable with respect to ?. The
SMO algorithm can therefore be brought to bear where two variables are selected and optimised
using gradient or Newton methods and the process repeated until convergence.
Also note that it has sometimes been observed that l2 regularisation can provide better results than
l1 [6, 7, 12, 18]. For this special case, when p = q = 2, the reduced two variable problem can
be solved analytically. This was one of the primary motivations for choosing the p-norm squared
regulariser and placing it in the primal objective (the other was to be consistent with other p-norm
formulations [9, 11]). Had we included the regulariser as a primal constraint then the dual would
have the q-norm rather than the q-norm squared. Our dual would then be near identical to Eq. (9)
in [12]. However, it would then no longer have been possible to solve the two variable reduced
problem analytically for the 2-norm special case.
3

4

SMO-MKL Optimisation

We now develop the SMO-MKL algorithm for optimising the lp MKL dual. The algorithm has three
main components: (a) reduced variable optimisation; (b) working set selection and (c) stopping
criterion and kernel caching. We build the SMO-MKL algorithm around the LibSVM code base [5].
4.1

The Reduced Variable Optimisation

The SMO algorithm works by repeatedly choosing two variables (assumed to be ?1 and ?2 without
loss of generality in this Subsection) and optimising them while holding all other variables constant.
If ?1 ? ?1 + ? and ?2 ? ?2 + s?, the dual simplifies to
?? = argmax (1 + s)? ?
L???U

2
1 X
( (ak ?2 + 2bk ? + ck )q ) q
8?

(11)

k

where s = ?y1 y2 , L = (s == +1) ? max(??1 , ??2 ) : max(??1 , ?2 ? C), U =
(s == +1) ? min(C ? ?1 , C ? ?2 ) : min(C ? ?1 , ?2 ), ak = H11k + H22k + 2sH12k ,
bk = ?t (H:1k + sH:2k ) and ck = ?t Hk ?. Unlike as in SMO, ?? can not be found analytically for arbitrary p. Nevertheless, since this is a simple one dimensional concave optimisation
problem, we can efficiently find the global optimum using a variety of methods. We tried bisection
search and Brent?s algorithm but the Newton-Raphson method worked best ? partly because the one
dimensional Hessian was already available from the working set selection step.
4.2

Working Set Selection

The choice of which two variables to select for optimisation can have a big impact on training time.
Very simple strategies, such as random sampling, can have very little cost per iteration but need many
iterations to converge. First and second order working set selection techniques are more expensive
per iteration but converge in far fewer iterations.
We implement the greedy second order working set selection strategy of [8]. We do not give the
variable selection equations due to lack of space but refer the interested reader to the WSS2 method
of [8] and our source code [20]. The critical thing is that the selection of the first (second) variable
involves computing the gradient (Hessian) of the dual. These are readily derived to be
X
?? D = 1 ?
dk Hk ? = 1 ? H?
(12)
k

?2? D = ?H ?

1X
??k f ?1 (?)(Hk ?)(Hk ?)t
?

(13)

k

where ??k f ?1 (?) = (2 ? q)? 2?2q
?k2q?2 + (q ? 1)? q2?q ?kq?2 and ?k
q

=

1 t
? Hk ?
2?

(14)

where D has been overloaded to now refer to the dual objective. Rather than compute the gradient
?? D repeatedly, we speed up variable selection by caching, separately for each kernel, Hk ?. The
cache needs to be updated every time we change ? in the reduced variable optimisation. However,
since only two variables are changed, Hk ? can be updated by summing along just two columns of
the kernel matrix. This involves only O(M ) work in all, where M is the number of kernels, since
the column sums can be pre-computed for each kernel. The Hessian is too expensive to cache and is
recomputed on demand.
4.3

Stopping Criterion and Kernel Caching

We terminate the SMO-MKL algorithm when the duality gap falls below a pre-specified threshold.
Kernel caching strategies can have a big impact on performance since kernel computations can
dominate everything else in some cases. While a few different kernel caching techniques have been
explored for SVMs, we stick to the standard one used in LibSVM [5]. A Least Recently Used
(LRU) cache is implemented as a circular queue. Each element in the queue is a pointer to a recently
accessed (common) row of each of the individual kernel matrices.
4

5

Special Cases and Extensions

We briefly discuss a few special cases and extensions which impact our SMO-MKL optimisation.
5.1

2-Norm MKL

As we noted earlier, 2-norm MKL has sometimes been found to outperform MKL trained with l1
regularisation [6, 7, 12, 18]. For this special case, when p = q = 2, our dual and reduced variable
optimisation problems simplify to polynomials of degree four
1 X t
D2 ? max 1t ? ?
(? Hk ?)2
(15)
??A
8?
k
1 X
?? = argmax (1 + s)? ?
(ak ?2 + 2bk ? + ck )2
(16)
8?
L???U
k

?

Just as in standard SMO, ? can now be found analytically by using the expressions for the roots of
a cubic. This makes our SMO-MKL algorithm particularly efficient for p = 2 and our code defaults
to the analytic solver for this special case.
5.2

The Bregman Divergence as a Regulariser

The Bregman divergence generalises the squared p-norm. It is not a metric as it is not symmetric and
does not obey the triangle inequality. In this Subsection, we demonstrate that our MKL formulation
can also incorporate the Bregman divergence as a regulariser.
Let F be any differentiable, strictly convex function and f = ?F represent its gradient. The
Bregman divergence generated by F is given by rF (d) = F (d) ? F (d0 ) ? (d ? d0 )t f (d0 ). Note
that ?rF (d) = f (d) ? f (d0 ). Incorporating the Bregman divergence as a regulariser in our primal
objective leads to the following intermediate saddle point problem and Lagrangian
X
IB ? min max 1t ? ? 12
dk ?t Hk ? + ?rF (d)
(17)
d?0 ??A

LB = 1t ? ?

X

k

dk (?k + 12 ?t Hk ?) + ?rF (d)

?d LB = 0 ? f (d) ? f (d0 ) = g(?, ?)/?
?d=f

?1

(18)

k

(19)

(f (d0 ) + g(?, ?)/?) = f

?1

(?(?, ?))

(20)

1 t
2 ? Hk ?

where g is a vector with entries gk (?, ?) = ?k +
and ?(?, ?) = f (d0 ) + g(?, ?)/?.
Substituting back in the Lagrangian and discarding terms dependent on just d0 results in the dual
DR ?

max

??A,??0

1t ? + ?(F (f ?1 (?)) ? ? t f ?1 (?))

(21)

In many cases the optimal value of ? will turn out to be zero and the optimisation can efficiently be
carried out over ? using our SMO-MKL algorithm.
Generalised KL Divergence To take a concrete example, different from the p-norm squared used
thus far, we investigate the use of the generalised KL divergence as a regulariser. Choosing F (d) =
P
k dk (log(dk ) ? 1) leads to the generalised KL divergence between d and d0
X
X
X
rKL (d) =
dk log(dk /d0k ) ?
dk +
d0k
(22)
k

k

k

Plugging in rKL in IB and following the steps above leads to the following dual problem
X
t
1
max 1t ? ? ?
d0k e 2? ? Hk ?
??A

(23)

k

which can be optimised straight forwardly using our SMO-MKL algorithm once we plug in the
gradient and hessian information. However, discussing this further would take us too far out of the
scope of this paper. We therefore stay focused on lp -MKL for the remainder of this paper.
5

5.3

Regression and Other Loss Functions

While we have discussed MKL based classification so far we can easily adapt our formulation to
handle other convex loss functions such as regression, novelty detection, etc. We demonstrate this
for the ?-insensitive loss function for regression. The primal, intermediate saddle point and final
dual problems are given by
X
X
? X p p2
+
?
t
1
w
w
/d
+
C
(?
+
?
)
+
(
dk )
PR ?
min
(24)
k
k
k
i
i
2
2
w,b,?? ?0,d?0
i
k
k
X
such that ? (
(25)
wtk ?k (xi ) + b ? yi ) ? ? + ?i?
k

IR ? min

max

DR ?

max

d?0 ?|?|?C1, 1t ?=0

0?|?|?C1, 1t ?=0

t

1 (Y ? ? ?|?|) ?
1t (Y ? ? ?|?|) ?

1
2

X

d k ? t Kk ? +

k

? X p p2
(
dk )
2

2
1 X t
( (? Kk ?)q ) q
8?

(26)

k

(27)

k

SMO has a slightly harder time optimising DR due to the |?| term which, though in itself not
differentiable, can be gotten around by substituting ? = ?+ ? ?? at the cost of doubling the
number of dual variables.

6

Experiments

In this Section, we empirically compare the performance of our proposed SMO-MKL algorithm
against the specialised lp -MKL solver of [12] which is referred to as Shogun. Code, scripts and
parameter settings were helpfully provided by the authors and we ensure that our stopping criteria
are compatible. All experiments are carried out on a single core of an AMD 2380 2.5 GHz processor
with 32 Gb RAM. Our focus in these experiments is purely on training time and speed of optimisation as the prediction accuracy improvements of lp -MKL have already been documented [12].
We carry out two sets of experiments. The first, on small scale UCI data sets, are carried out using
pre-computed kernels. This performs a direct comparison of the algorithmic components of SMOMKL and Shogun. We also carry out a few large scale experiments with kernels computed on the
fly. This experiment compares the two methods in totality. In this case, kernel caching can have an
effect, but not a significant one as the two methods have very similar caching strategies.
For each UCI data set we generated kernels as recommended in [16]. We generated RBF kernels
with ten bandwidths for each individual dimension of the feature vector as well as the full feature
vector itself. Similarly, we also generated polynomial kernels of degrees 1, 2 and 3. All kernels
matrices were pre-computed and normalised to have unit trace. We set C = 100 as it gives us a
reasonable accuracy on the test set. Note that for some value of ?, SMO-MKL and Shogun will
converge to exactly the same solution [12]. Since this value is not known a priori we arbitrarily set
? = 1.
Training times on the UCI data sets are presented in Table 1. Means and standard deviations are
reported for five fold cross-validation. As can be seen, SMO-MKL is significantly faster than Shogun
at converging to similar solutions and obtaining similar test accuracies. In many cases, SMO-MKL
is more than four times as fast and in some case more than ten or twenty times as fast. Note that our
test classification accuracy on Liver is a lot lower than Shogun?s. This is due to the arbitrary choice
of ?. We can vary our ? on Liver to recover the same accuracy and solution as Shogun with a further
decrease in our training time.
Another very positive thing is that SMO-MKL appears to be relatively stable across a large operating
range of p. The code is, in most of the cases as expected, fastest when p = 2 and gets slower as
one increases or decreases p. Interestingly though, the algorithm doesn?t appear to be significantly
slower for other values of p. Therefore, it is hoped that SMO-MKL can be used to learn sparse
kernel combinations as well as non-sparse ones.
Moving on to the large scale experiments with kernels computed on the fly, we first tried combining
a hundred thousand RBF kernels on the Sonar data set with 208 points and 59 dimensional features.
6

Table 1: Training times on UCI data sets with N training points, D dimensional features, M kernels
and T test points. Mean and standard deviations are reported for 5-fold cross validation.
(a) Australian: N =552, T =138, D=13, M =195.
Training Time (s)
Test Accuracy (%)
# Kernels Selected
p
SMO-MKL
Shogun
SMO-MKL
Shogun
SMO-MKL
Shogun
1.10 4.89 ? 0.31 58.52 ? 16.49 85.22 ? 2.96 85.22 ? 2.81 26.4 ? 0.8 137.2 ? 53.8
62.4 ? 4.7
1.33 4.16 ? 0.16 33.58 ? 2.58 85.36 ? 3.79 85.07 ? 2.85 40.8 ? 1.3
1.66 4.31 ? 0.19 31.89 ? 1.25 85.65 ? 3.73 85.07 ? 2.85 72.2 ? 4.8
100.2 ? 3.7
2.00 4.27 ? 0.10 27.08 ? 7.18 85.80 ? 3.74 85.22 ? 2.99 126.4 ? 4.3 134.4 ? 5.6
2.33 4.88 ? 0.18 24.92 ? 6.46 85.80 ? 3.74 85.07 ? 2.85 162.8 ? 3.6 177.8 ? 8.3
2.66 5.19 ? 0.05 26.90 ? 2.05 85.80 ? 3.68 85.22 ? 2.85 188.2 ? 4.7 188.8 ? 5.1
3.00 5.48 ? 0.21 27.06 ? 2.20 85.51 ? 3.69 85.22 ? 2.85 192.0 ? 2.6 194.4 ? 1.2
p
1.10
1.33
1.66
2.00
2.33
2.66
3.00

p
1.10
1.33
1.66
2.00
2.33
2.66
3.00

p
1.10
1.33
1.66
2.00
2.33
2.66
3.00

(b) Ionosphere: N =280, T =71, D=33, M =442.
Training Time (s)
Test Accuracy (%)
# Kernels Selected
SMO-MKL
Shogun
SMO-MKL
Shogun
SMO-MKL
Shogun
2.85 ? 0.16 19.82 ? 4.02 92.60 ? 1.35 92.03 ? 1.68 50.0 ? 2.7
125.2 ? 7.3
2.78 ? 1.18 8.49 ? 0.61 92.03 ? 1.42 92.60 ? 1.86 120.8 ? 6.0 217.0 ? 23.4
2.42 ? 0.28 10.49 ? 2.27 91.74 ? 2.08 91.74 ? 1.37 200.8 ? 4.4 291.4 ? 33.0
2.16 ? 0.16 13.99 ? 4.68 92.03 ? 1.68 91.17 ? 2.45 328.0 ? 6.6 364.2 ? 15.4
2.35 ? 0.25 24.90 ? 9.43 92.03 ? 1.68 91.74 ? 2.08 413.6 ? 5.6 412.2 ? 6.6
2.50 ? 0.32 33.05 ? 3.66 92.03 ? 1.68 92.03 ? 1.68 430.6 ? 4.6 436.6 ? 4.3
3.03 ? 0.99 36.23 ? 3.62 92.31 ? 1.41 91.75 ? 2.05 434.4 ? 4.8 442.0 ? 0.0
(c) Liver: N =276, T =69, D=5, M =91.
Training Time (s)
Test Accuracy (%)
SMO-MKL
Shogun
SMO-MKL
Shogun
0.53 ? 0.03 2.15 ? 0.12 62.90 ? 9.81 66.67 ? 9.91
0.54 ? 0.03 0.92 ? 0.05 66.09 ? 8.48 71.59 ? 8.92
0.56 ? 0.04 1.14 ? 0.23 66.96 ? 7.53 70.72 ? 9.28
0.54 ? 0.04 1.72 ? 0.57 66.96 ? 7.06 72.17 ? 6.94
0.63 ? 0.03 2.35 ? 0.36 66.38 ? 7.36 73.33 ? 6.71
0.65 ? 0.02 2.53 ? 0.44 65.22 ? 6.80 72.75 ? 7.96
0.67 ? 0.03 3.40 ? 0.55 65.22 ? 6.74 73.91 ? 7.28
(d) Sonar: N =166, T =42, D=59, M =793.
Training Time (s)
Test Accuracy (%)
SMO-MKL
Shogun
SMO-MKL
Shogun
4.95 ? 0.29 47.19 ? 3.85 85.15 ? 7.99 81.25 ? 8.71
4.00 ? 0.76 18.28 ? 1.63 84.65 ? 9.37 87.03 ? 6.85
4.48 ? 1.63 20.27 ? 8.84 88.47 ? 6.68 87.51 ? 6.28
3.31 ? 0.31 31.52 ? 5.07 88.94 ? 6.00 88.95 ? 6.33
3.54 ? 0.35 51.83 ? 17.96 88.94 ? 4.97 88.94 ? 5.41
3.83 ? 0.38 64.59 ? 9.19 88.94 ? 4.97 88.94 ? 4.97
3.96 ? 0.45 70.08 ? 9.18 88.94 ? 4.97 89.92 ? 5.13

# Kernels Selected
SMO-MKL
Shogun
9.40 ? 1.02 39.40 ? 1.50
24.40 ? 2.06 43.60 ? 2.42
44.20 ? 2.23 57.00 ? 3.29
71.00 ? 5.29 78.00 ? 2.28
82.40 ? 2.42 88.20 ? 1.72
83.20 ? 2.32 90.80 ? 0.40
85.20 ? 3.37 91.00 ? 0.00
# Kernels Selected
SMO-MKL
Shogun
91.2 ? 6.9
258.0 ? 24.8
247.8 ? 7.7 374.2 ? 20.9
383.0 ? 5.7 451.6 ? 12.0
661.2 ? 10.2 664.8 ? 35.2
770.8 ? 4.4
763.0 ? 7.0
782.0 ? 3.4
789.4 ? 2.8
786.0 ? 4.1
792.2 ? 1.1

Note that these kernels do not form any special hierarchy so approaches such as [2] are not applicable. Timing results on a log-log scale are given in Figure (1a). As can be seen, SMO-MKL appears
to be scaling linearly with the number of kernels and we converge in less than half an hour on all
hundred thousand kernels for both p = 2 and p = 1.33. If we were to run the same experiment using
pre-computed kernels then we converge in approximately seven minutes (see Fig (1b)). On the other
hand, Shogun took six hundred seconds to combine just ten thousand kernels computed on the fly.
The trend was the same when we increased the number of training points. Figure (1c) and (1d) plot
timing results on a log-log scale as the number of training points is varied on the Adult and Web
data sets (please see [1] for data set details and downloads). We used 50 kernels computed on the
7

Sonar

7

6

9
8

5.5

log(Time) (s)

log(Time) (s)

log(Time) (s)

6

8

10

SMO?MKL p=1.33
SMO?MKL p=2.00

5
6.5

Web

Adult

7

SMO?MKL p=1.33
SMO?MKL p=2.00

4
3
2

SMO?MKL p=1.33
SMO?MKL p=2.00
Shogun p=1.33
Shogun p=2.00

7

SMO?MKL p=1.33
SMO?MKL p=2.00

6
log(Time) (s)

Sonar
7.5

7
6
5
4

5
4
3

3

5
4.5
9

1
9.5

10
10.5
11
log(# Kernels)

11.5

12

(a) Sonar

0
6

2

2

7

8

9
10
log(# Kernels)

11

12

(b) Sonar Pre-computed

1
7

7.5

8
8.5
9
9.5
log(# Training Points)

(c) Adult

10

10.5

1
7.5

8

8.5
9
9.5
10
log(# Training Points)

10.5

11

(d) Web

Figure 1: Large scale experiments varying the number of kernels and points. See text for details.
fly for these experiments. On Adult, till about six thousand points, SMO-MKL is roughly 1.5 times
faster than Shogun for p = 1.33 and 5 times faster for p = 2. However, on reaching eleven thousand
points, Shogun starts taking more and more time to converge and we could not get results for sixteen
thousand points or more. SMO-MKL was unaffected and converged on the full data set with 32,561
points in 9245.80 seconds for p = 1.33 and 8511.12 seconds for p = 2. We tried the Web data set
to see whether the SMO-MKL algorithm would scale beyond 32K points. Training on all 49,749
points and 50 kernels took 1574.73 seconds (i.e. less than half an hour) with p = 1.33 and 2023.35
seconds with p = 2.

7

Conclusions

We developed the SMO-MKL algorithm for efficiently optimising the lp -MKL formulation. We
placed the emphasis firmly back on optimising the MKL dual rather than the intermediate saddle
point problem on which all state-of-the-art MKL solvers are based. We showed that the lp -MKL
dual is differentiable and that placing the p-norm squared regulariser in the primal objective lets us
analytically solve the reduced variable problem for p = 2. We could also solve the convex, onedimensional reduced variable problem when p 6= 2 by the Newton-Raphson method. A second-order
working set selection algorithm was implemented to speed up convergence. The resulting algorithm
is simple, easy to implement and efficiently scales to large problems. We also showed how to
generalise the algorithm to handle not just p-norms squared but also certain Bregman divergences.
In terms of empirical performance, we compared the SMO-MKL algorithm to the specialised lp MKL solver of [12] referred to as Shogun. It was demonstrated that SMO-MKL was significantly
faster than Shogun on both small and large scale data sets ? sometimes by an order of magnitude.
SMO-MKL was also found to be relatively stable for various values of p and could therefore be
used to learn both sparse, and non-sparse, kernel combinations. We demonstrated that the algorithm
could combine a hundred thousand kernels on Sonar in approximately seven minutes using precomputed kernels and in less than half an hour using kernels computed on the fly. This is significant
as many non-linear kernel combination forms, which lead to performance improvements but are
non-convex, can be recast as convex linear MKL with a much larger set of base kernels. The SMOMKL algorithm can now be used to tackle such problems as long as an appropriate regulariser can
be found. We were also able to train on the entire Web data set with nearly fifty thousand points
and fifty kernels computed on the fly in less than half an hour. Other solvers were not able to
return results on these problems. All experiments were carried out on a single core and therefore,
we believe, redefine the state-of-the-art in terms of MKL optimisation. The SMO-MKL code is
available for download from [20].

Acknowledgements
We are grateful to Saurabh Gupta, Marius Kloft and Soren SSonnenburg for helpful discussions,
feedback and help with Shogun.

References
[1] http://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/binary.html.

8

[2] F. R. Bach. Exploring large feature spaces with hierarchical multiple kernel learning. In NIPS, pages
105?112, 2008.
[3] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. Multiple kernel learning, conic duality, and the SMO
algorithm. In ICML, pages 6?13, 2004.
[4] A. Ben-Tal, T. Margalit, and A. Nemirovski. The ordered subsets mirror descent optimization method
with applications to tomography. SIAM Journal of Opimization, 12(1):79?108, 2001.
[5] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines, 2001. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
[6] C. Cortes, M. Mohri, and A. Rostamizadeh. L2 regularization for learning kernels. In UAI, 2009.
[7] C. Cortes, M. Mohri, and A. Rostamizadeh. Learning non-linear combinations of kernels. In NIPS, 2009.
[8] R. E. Fan, P. H. Chen, and C. J. Lin. Working set selection using second order information for training
SVM. JMLR, 6:1889?1918, 2005.
[9] C. Gentile. Robustness of the p-norm algorithms. ML, 53(3):265?299, 2003.
[10] M. Gonen and E. Alpaydin. Localized multiple kernel learning. In ICML, 2008.
[11] J. Kivinen, M. K. Warmuth, and B. Hassibi. The p-norm generaliziation of the LMS algorithm for adaptive
filtering. IEEE Trans. Signal Processing, 54(5):1782?1793, 2006.
[12] M. Kloft, U. Brefeld, S. Sonnenburg, P. Laskov, K.-R. Muller, and A. Zien. Efficient and accurate lp -norm
Multiple Kernel Learning. In NIPS, 2009.
[13] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the kernel matrix
with semidefinite programming. JMLR, 5:27?72, 2004.
[14] C. J. Lin, S. Lucidi, L. Palagi, A. Risi, and M. Sciandrone. Decomposition algorithm model for singly
linearly-constrained problems subject to lower and upper bounds. JOTA, 141(1):107?126, 2009.
[15] J. Platt. Fast training of support vector machines using sequential minimal optimization. In Advances in
Kernel Methods ? Support Vector Learning, pages 185?208, 1999.
[16] A. Rakotomamonjy, F. Bach, Y. Grandvalet, and S. Canu. SimpleMKL. JMLR, 9:2491?2521, 2008.
[17] S. Sonnenburg, G. Raetsch, C. Schaefer, and B. Schoelkopf. Large scale multiple kernel learning. JMLR,
7:1531?1565, 2006.
[18] M. Varma and B. R. Babu. More generality in efficient multiple kernel learning. In ICML, 2009.
[19] A. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman. Multiple kernels for object detection. In ICCV,
2009.
[20] S. V. N. Vishwanathan, Z. Sun, N. Theera-Ampornpunt, and M. Varma, 2010. The SMO-MKL code
http://research.microsoft.com/?manik/code/SMO-MKL/download.html.
[21] J. Yang, Y. Li, Y. Tian, L. Duan, and W. Gao. Group-sensitive multiple kernel learning for object categorization. In ICCV, 2009.

9

"
1083,1989,Learning to Control an Unstable System with Forward Modeling,Abstract Missing,"324

Jordan and Jacobs

Learning to Control an Unstable System with
Forward Modeling

Michael I. Jordan
Brain and Cognitive Sciences
MIT
Cambridge, MA 02139

Robert A. Jacobs
Computer and Information Sciences
University of Massachusetts
Amherst, MA 01003

ABSTRACT
The forward modeling approach is a methodology for learning control when data is available in distal coordinate systems. We extend
previous work by considering how this methodology can be applied
to the optimization of quantities that are distal not only in space
but also in time.
In many learning control problems, the output variables of the controller are not
the natural coordinates in which to specify tasks and evaluate performance. Tasks
are generally more naturally specified in ""distal"" coordinate systems (e.g., endpoint
coordinates for manipulator motion) than in the ""proximal"" coordinate system of
the controller (e.g., joint angles or torques). Furthermore, the relationship between
proximal coordinates and distal coordinates is often not known a priori and, if
known, not easily inverted.
The forward modeling approach is a methodology for learning control when training data is available in distal coordinate systems. A forward model is a network
that learns the transformation from proximal to distal coordinates so that distal
specifications can be used in training the controller (Jordan & Rumelhart, 1990).
The forward model can often be learned separately from the controller because it
depends only on the dynamics of the controlled system and not on the closed-loop
dynamics.
In previous work, we studied forward models of kinematic transformations (Jordan,
1988, 1990) and state transitions (Jordan & Rumelhart, 1990). In the current paper,

Learning to Control an Unstable System with Forward Modeling
we go beyond the spatial credit assignment problems studied in those papers and
broaden the application of forward modeling to include cases of temporal credit
assignment (cf. Barto, Sutton, & Anderson, 1983; Werbos, 1987). As discussed
below, the function to be modeled in such cases depends on a time integral of the
closed-loop dynamics. This fact has two important implications. First, the data
needed for learning the forward model can no longer be obtained solely by observing
the instantaneous state or output of the plant. Second, the forward model is no
longer independent of the controller: If the parameters of the controller are changed
by a learning algorithm, then the closed-loop dynamics change and so does the
mapping from proximal to distal variables. Thus the learning of the forward model
and the learning of the controller can no longer be separated into different phases.

1

FORWARD MODELING

In this section we briefly summarize our previous work on forward modeling (see
also Nguyen & Widrow, 1989 and Werbos, 1987).

1.1

LEARNING A FORWARD MODEL

Given a fixed control law , the learning of a forward model is a system identification
problem. Let z = g(s, u) be a system to be modeled, where z is the output or the
state-derivative, s is the state, and u is the control. We require the forward model
to minimize the cost functional
Jm =

~

J

(z - z)T(z - z)dt.

(1)

where z = 9(s, u, v) is the parameterized function computed by the model. Once
the minimum is found, backpropagation through the model provides an estimate
of the system Jacobian matrix :~ (cf. Jordan, 1988).

?u

1.2

LEARNING A CONTROLLER

Once the forward model is sufficiently accurate, it can be used in the training of the
controller. Backpropagation through the model provides derivatives that indicate
how to change the outputs of the controller. These derivatives can be used to
change the parameters of the controller by a further application of backpropagation.
Figure 1 illustrates the general procedure.
This procedure minimizes the ""distal"" cost functional

(2)
where z? is a reference signal. To see this, let the controller output be given as a
function u = f(s, z?, w) of the state s?, the reference signal z?, and a parameter
vector w. Differentiating J with respect to w yields

""w J

=-

J

ouT ozT
ow ou (z? - z)dt.

(3)

325

326

Jordan and Jacobs

\

z*
~

x

Feedforward
Controller

z

Plant

-

Forward
-Model -

+

-

-

Figure 1: Learning a Controller. The Dashed Line Represents Backpropagation.

?u

The Jacobian matrix
cannot be assumed to be available a priori, but can be
estimated by backpropagation through the forward model. Thus the error signal
available for learning the controller is the estimated gradient

.

ou = - J-ow

T 0' T

V'wJ

oz (z ? - z)dt.
OU

(4)

We now consider a task in which the foregoing framework must be broadened to
allow a more general form of distal task specification.

2

THE TASK

The task is to learn to regulate an unstable nonminimum-phase plant. We have
chosen the oft-studied (e.g., Barto, Sutton, & Anderson, 1983; \Vidrow & Smith,
1964) problem of learning to balance an inverted pendulum on a moving cart. The
plant dynamics are given by:

[ M+m
mlcos(J

mlcos(J ] [

I

~
(J

]

+ [ -mlsi~(J

-mglszn(J

]

iP

= [ F0

]

where m is the mass of the pole, M is the mass of the cart, I is half the pole length,
I is the inertia of the pole around its base, and F is the force applied to the cart.
The task we studied is similar to that studied by Barto, Sutton, & Anderson (1983).
A state-feedback controller provides forces to the cart, and the system evolves until
failure occurs (the cart reaches the end of the track or the pole reaches a critical
angle). The system learns from failure; indeed, it is assumed that the only teaching
information provided by the environment is the signal that failure has occurred.

Learning to Control an Unstable System with Forward Modeling
Forward Model

o

sgn (x)

o
o
o

0

lielO
sgn(

x)

lei
sgn(e)

lei
sgn(e)

0
0
0
0
0

o

e
Ii

o
o
o

?

Action
Unit

o
o
o
o
o
o

Temporal
Difference
Unit

-0

o

-0

Controller

~,.p..

nl

Figure 2: The Network Architecture

There are several differences between our task and that studied by Barto, Sutton, &.
Anderson (1983). First, disturbances (white noise) are provided by the environment
rather than by the learning algorithm. This implies that in our experiments the
level of noise seen by the controller does not diminish to zero over the course of
learning. Second, we used real-valued forces rather than binary forces. Finally, we
do not assume the existence of a ""reset button"" that reinitializes the system to the
origin of state space; upon failure the system is restarted in a random configuration.

3

OUR APPROACH

In our approach, the control system learns a model that relates the current state of
the plant and the current control signal to a prediction of future failure. We make
use of a temporal difference algorithm (Sutton, 1988) to learn the transformation
from (state, action) pairs to an estimate of the inverse of the time until failure.
This mapping is then used as a differentiable forward model in the learning of the
controller-the controller is changed so as to minimize the output of the model and
thereby maximize the time until failure .
The overall system architecture is shown in Figure 2. We describe each component
in detail in the following sections.
An important feature that distinguishes this architecture from previous work (e.g.,

327

328

Jordan and Jacobs
Barto, Sutton, & Anderson, 1983) is the path from the action unit into the forward
model. This path is necessary for supervised learning algorithms to be used (see
also Werbos, 1987).

3.1

LEARNING THE FORWARD MODEL

Temporal difference algorithms learn to make long term predictions by achieving
local consistency between predictions at neighboring time steps, and by grounding
the chain of predictions when information from the environment is obtained. In our
case, if z(t) is the inverse of the time until failure, then consistency is defined by
the requirement that z-l(t) = z-l(t + 1) + 1. The chain is grounded by defining
z(T) 1, where T is the time step on which failure occurs.

=

To learn to estimate the inverse of the time until failure, the following temporal
difference error terms are used. For time steps on which failure does not occur,
( )
et

= 1 + ?-11(t + 1) -

A( )

zt ,

where ?(t) denotes the output of the forward model. When failure occurs, the target
for the forward model is set to unity:

e(t)

= 1 -- ?(t)

The error signal e(t) is propagated backwards at time t + 1 using activations saved
from time t. Standard backpropagation is used to compute the changes to the
weights.

3.2

LEARNING THE CONTROLLER

If the controller is performing as desired, then the output of the forward model
is zero (that is, the predicted time-until-failure is infinity). This suggests that an
appropriate distal error signal for the controller is zero minus the output of the
forward model.
Given that the forward model has the control action as an input, the distal error
can be propagated backward to the hidden units of the forward model, through the
action unit, and into the controller where the weights are changed (see Figure 2).
Thus the controller is changed in such a way as to minimize the output of the
forward model and thereby maximize the time until failure.

3.3

LEARNING THE FORWARD MODEL AND THE CONTROLLER
SIMULTANEOUSLY

As the controller varies, the mapping that the forward model must learn also varies.
Thus, if the forward model is to provide reasonable derivatives, it must be continuously updated as the controller changes. We find that it is possible to train the
forward model and the controller simultaneously, provided that we use a larger
learning rate for the forward model than for the controller.

Learning to Control an Unstable System with Forward Modeling

4
4.1

MISCELLANY
RESET

Although previous studies have assumed the existence of a ""reset button"" that
can restart the system at the origin of state space, we prefer not to make such an
assumption. A reset button implies the existence of a controller that can stabilize
the system, and the task of learning is to find such a controller. In our simulations,
we restart the system at random points in state space after failure occurs.

4.2

REDUNDANCY

The mapping learned by the forward model depends on both the state and the action. The action, however, is itself a function of the state, so the action unit provides
redundant information. This implies that the forward model could have arbitrary
weights in the path from the action unit and yet make reasonable predictions. Such
a model, however, would yield meaningless derivatives for learning the controller.
Fortunately, backpropagation tends to produce meaningful weights for a path that
is correlated with the outcome, even if that path conveys redundant information.
To further bias things in our favor, we found it useful to employ a larger learning
rate in the path from the action unit to the hidden units of the forward model (0.9)
than in the path from the state units (0.3).

4.3

REPRESENTATION

As seen in Figure 2, we chose input representations that take advantage of symmetries in the dynamics of the cart-pole system. The forward model has even symmetry
with respect to the state variables, whereas the controller has odd symmetry.

4.4

LONG-TERM BEHAVIOR

There is never a need to ""turn off"" the learning of the forward model. Once the pole
is being successfully balanced in the presence of fluctuations, the average time until
failure goes to infinity. The forward model therefore learns to predict zero in the
region of state space around the origin, and the error propagated to the controller
also goes to zero.

5

RESULTS

We ran twenty simulations starting with random initial weights. The learning rate
for the controller was 0.05 and the learning rate for the forward model was 0.3,
except for the connection from the action unit where the learning rate was 0.9.
Eighteen runs converged to controller configurations that balanced the pole, and
two runs converged on local minima. Figure 3 shows representative learning curves
for six of the successful runs.
To obtain some idea of the size of the space of correct solutions, we performed an
exhaustive search of a lattice in a rectangular region of weight space that contained

329

330

Jordan and Jacobs
1000

800

600

Average time
until failure
.00

200

o

1000

500

1500

Bins
(1 bin ., 20 fillur ?? )

Figure 3: Learning Curves for Six Runs
all of the weight configurations found by our simulations. As shown in Figure 4,
only 15 out of 10,000 weight configurations were able to balance the pole.

6

CONCLUSIONS

Previous wor k within the forward modeling paradigm focused on models of fixed
kinematic or dynamic properties of the controlled plant (Jordan, 1988,1990; Jordan
&, Rumelhart, 1990). In the current paper, the notion of a forward model is broader.
The function that must be modeled depends not only on properties of the controlled
plant, but also on properties of the controller. Nonetheless, the mapping is welldefined, and the results demonstrate that it can be used to provide appropriate
incremental changes for the controller.
These results provide further demonstration of the applicability of supervised learning algorithms to learning control problems in which explicit target information is
not available.

Acknowledgments
The first author was supported by BRSG 2 S07 RR07047-23 awarded by the Biomedical Research Support Grant Program, Division of Research Resources, National
Institutes of Health and by a grant from Siemens Corporation. The second author was supported by the Air Force Office of Scientific Research, through grant
AFOSR-87-0030.

Learning to Control an Unstable System with Forward Modeling

?
3 ?

?
Log
Frequency

?
?
?
?
).

2

?

-.

-

??

0+---44.-----~--.-r_----r_--~

o

200

.00

100

100

1000

Median Time Steps Until Failure

Figure 4: Performance of Population of Controllers

References

Barto, A. G ., Sutton, R. S., & Anderson, C. W. (1983). Neuronlike adaptive elements that can solve difficult learning control problems. IEEE Transactions on
Systems, Man, and Cybernetics, SMC.19, 834-846.
Jordan, M. I. (1988). Supervised learning and systems with excess degress of freedom. (COINS Tech. Rep. 88-27). Amherst, MA: University of Massachusetts,
Computer and Information Sciences.
Jordan, M. I. (1990). Motor learning and the degrees of freedom problem. In M.
Jeannerod, (Ed). Attention and Performance, XIII. Hillsdale, NJ: Erlbaum.
Jordan, M. I. & Rumelhart, D. E. (1990). Supervised learning with a distal teacher.
Paper in preparation.
Nguyen, D. & Widrow, B. (1989). The truck backer-upper: An example of selflearning in neural networks. In: Proceedings of the International Joint Conference
on Neural Networks. Piscataway, NJ: IEEE Press.
Sutton, R. S. (1987). Learning to predict by the methods of temporal differences.
Machine Learning, 9, 9-44.
Werbos, P. (1987). Building and understanding adaptive systems: A statistical/numerical approach to factory automation and brain research. IEEE Transactions on Systems, Man, and Cybernetics, 17, 7-20.
Widrow, B. & Smith, F. W. (1964). Pattern-recognizing control systems. In: Computer and Information Sciences Proceedings, Washington, D.C.: Spartan.

331

"
4825,2014,Conditional Swap Regret and Conditional Correlated Equilibrium,"We introduce a natural extension of the notion of swap regret, conditional swap regret, that allows for action modifications conditioned on the player?s action history. We prove a series of new results for conditional swap regret minimization. We present algorithms for minimizing conditional swap regret with bounded conditioning history. We further extend these results to the case where conditional swaps are considered only for a subset of actions. We also define a new notion of equilibrium, conditional correlated equilibrium, that is tightly connected to the notion of conditional swap regret: when all players follow conditional swap regret minimization strategies, then the empirical distribution approaches this equilibrium. Finally, we extend our results to the multi-armed bandit scenario.","Conditional Swap Regret and
Conditional Correlated Equilibrium

Mehryar Mohri
Courant Institute and Google
251 Mercer Street
New York, NY 10012

Scott Yang
Courant Institute
251 Mercer Street
New York, NY 10012

mohri@cims?nyu?edu

yangs@cims?nyu?edu

Abstract
We introduce a natural extension of the notion of swap regret, conditional swap
regret, that allows for action modi?cations conditioned on the player?s action history. We prove a series of new results for conditional swap regret minimization.
We present algorithms for minimizing conditional swap regret with bounded conditioning history. We further extend these results to the case where conditional
swaps are considered only for a subset of actions. We also de?ne a new notion
of equilibrium, conditional correlated equilibrium, that is tightly connected to the
notion of conditional swap regret: when all players follow conditional swap regret
minimization strategies, then the empirical distribution approaches this equilibrium. Finally, we extend our results to the multi-armed bandit scenario.

1

Introduction

On-line learning has received much attention in recent years. In contrast to the standard batch
framework, the online learning scenario requires no distributional assumption. It can be described
in terms of sequential prediction with expert advice [13] or formulated as a repeated two-player
game between a player (the algorithm) and an opponent with an unknown strategy [7]: at each time
step, the algorithm probabilistically selects an action, the opponent chooses the losses assigned to
each action, and the algorithm incurs the loss corresponding to the action it selected.
The standard measure of the quality of an online algorithm is its regret, which is the difference
between the cumulative loss it incurs after some number of rounds and that of an alternative policy.
The cumulative loss can be compared to that of the single best action in retrospect [13] (external
regret), to the loss incurred by changing every occurrence of a speci?c action to another [9] (internal
regret), or, more generally, to the loss of action sequences obtained by mapping each action to some
other action [4] (swap regret). Swap regret, in particular, accounts for situations where the algorithm
could have reduced its loss by swapping every instance of one action with another (e.g. every time
the player bought Microsoft, he should have bought IBM).
There are many algorithms for minimizing external regret [7], such as, for example, the randomized
weighted-majority algorithm of [13]. It was also shown in [4] and [15] that there exist algorithms for
minimizing internal and swap regret. These regret minimization techniques have been shown to be
useful for approximating game-theoretic equilibria: external regret algorithms for Nash equilibria
and swap regret algorithms for correlated equilibria [14].
By de?nition, swap regret compares a player?s action sequence against all possible modi?cations at
each round, independently of the previous time steps. In this paper, we introduce a natural extension
of swap regret, conditional swap regret, that allows for action modi?cations conditioned on the
player?s action history. Our de?nition depends on the number of past time steps we condition upon.
1

As a motivating example, let us limit this history to just the previous one time step, and suppose we
design an online algorithm for the purpose of investing, where one of our actions is to buy bonds
and another to buy stocks. Since bond and stock prices are known to be negatively correlated, we
should always be wary of buying one immediately after the other ? unless our objective was to pay
for transaction costs without actually modifying our portfolio? However, this does not mean that we
should avoid purchasing one or both of the two assets completely, which would be the only available
alternative in the swap regret scenario. The conditional swap class we introduce provides precisely
a way to account for such correlations between actions. We start by introducing the learning set-up
and the key notions relevant to our analysis (Section 2).

2

Learning set?up and model

We consider the standard online learning set-up with a set of actions N = {1? . . . ? N }. At each
round t ? {1? . . . ? T }, T ? 1, the player selects an action xt ? N according to a distribution pt
over N , in response to which the adversary chooses a function f t : N t ? [0? 1] and causes the
player to incur a loss f t ?xt ? xt?1 ? . . . ? x1 ). The objective of the player is to choose a sequence of
?T
actions ?x1 ? . . . ? xT ) that minimizes his cumulative loss t=1 f t ?xt ? xt?1 ? . . . ? x1 ).

A standard metric used to measure the performance of an online algorithm ? over T rounds is its
?expected) external regret, which measures the player?s expected performance against the best ?xed
action in hindsight:
Reg??? T ) =
Ext

T
?
t=1

?

?xt ?..?x1 )?
?pt ?...?p1 )

[f t ?xt ? ..? x1 )] ? min
j?N

T
?

f t ?j? j? ...? j).

t=1

There are several common modi?cations to the above online learning scenario: (1) we may com?T
t
pare regret against stronger competitor classes: Reg? ??? T ) =
t=1 ?pt ?...?p1 f ?xt ? ..? x1 ) ?
?T
t
min??? t=1 ?pt ?...?p1 [f ???xt )? ??xt?1 )? ...? ??x1 ))] for some function class C ? N N ; (2)
the player may have access to only partial information about the loss, i.e. only knowledge
of f t ?xt ? ..? x1 ) as opposed to f t ?a? xt?1 ? . . . ? x1 )?a ? N (also known as the bandit scenario); (3) the loss function may have bounded memory: f t ?xt ? ...? xt?k ? xt?k?1 ? ...? x1 ) =
f t ?xt ? ...? xt?k ? yt?k?1 ? ...? y1 ), ?xj ? yj ? N .

The scenario where C = N N in (1) is called the swap regret case, and the case where k = 0 in (3) is
referred to as the oblivious adversary. (Sublinear) regret minimization is possible for loss functions
against any competitor class of the form described in (1), with only partial information, and with
at least some level of bounded memory. See [4] and [1] for a reference on (1), [2] and [5] for (2),
and [1] for (3). [6] also provides a detailed summary of the best known regret bounds in all of these
scenarios and more.
The introduction of adversaries with bounded memory naturally leads to an interesting question:
what if we also try to increase the power of the competitor class in this way?
While swap regret is a natural competitor class and has many useful game theoretic consequences
(see [14]), one important missing ingredient is that the competitor class of functions does not have
memory. In fact, in most if not all online learning scenarios and regret minimization algorithms
considered so far, the point of comparison has been against modi?cation of the player?s actions
at each point of time independently of the previous actions. But, as we discussed above in the
?nancial markets example, there exist cases where a player should be measured against alternatives
that depend on the past and the player should take into account the correlations between actions.
Speci?cally, we consider competitor functions of the form ?t : N t ? N t . Let Call = {?t : N t ?
?T
t
N t }?
t=1 denote the class of all such functions. This leads us to the expression:
t=1 ?p1 ?...?pt [f ] ?
?T
t
t
min?t ??all t=1 ?p1 ?...?pt [f ? ? ]. Call is clearly a substantially richer class of competitor functions than traditional swap regret. In fact, it is the most comprehensive class, since we can always
?T
?T
reach t=1 ?p1 ?...?pt [f t ] ? t=1 min?x1 ?..?xt ) f t ?x1 ? ..? xt ) by choosing ?t to map all points to
t
argmin?xt ?..?x1 ) f ?xt ? ...? x1 ). Not surprisingly, however, it is not possible to obtain a sublinear
regret bound against this general class.
2

???
????????
????????

??????
??????
??????

?

???

????????

?

?

????????
????????
???
????????

???

????????
????????
????????

?

?

(a)

(b)

Figure 1: (a) unigram conditional swap class interpreted as a ?nite-state transducer. This is the same
as the usual swap class and has only the trivial state; (b) bigram conditional swap class interpreted as
a ?nite-state transducer. The action at time t ? 1 de?nes the current state and in?uences the potential
swap at time t.
Theorem 1. No algorithm can achieve sublinear regret against the class Call , regardless of the loss
function?s memory.
This result is well-known in the on-line learning community, but, for completeness, we include a
proof in Appendix 9. Theorem 1 suggests examining more reasonable subclasses of Call . To simplify
the notation and proofs that follow in the paper, we will henceforth restrict ourselves to the scenario
of an oblivious adversary, as in the original study of swap regret [4]. However, an application of the
batching technique of [1] should produce analogous results in the non-oblivious case for all of the
theorems that we provide.
Now consider the collection of competitor functions Ck = {? : N k ? N }. Then, a player
who has played actions {as }t?1
s=1 in the past should have his performance compared against
??at ? at?1 ? at?2 ? . . . ? at??k?1) ) at time t, where ? ? Ck . We call this class Ck of functions the
k-gram conditional swap regret class, which also leads us to the regret de?nition:
Reg??? T ) =
?k

T
?
t=1

? t [f t ?xt )] ? min

xt ?p

???k

T
?
t=1

? [f t ???xt ? at?1 ? at?2 ? . . . ? at??k?1) ))].

xt ?pt

Note that this is a direct extension of swap regret to the scenario where we allow for swaps conditioned on the history of the previous ?k ? 1) actions. For k = 1, this precisely coincides with swap
regret.
One important remark about the k-gram conditional swap regret is that it is a random quantity that
depends on the particular sequence of actions played. A natural deterministic alternative would be
of the form:
T
?
t=1

? t [f t ?xt )] ? min

xt ?p

???k

T
?
t=1

?

?xt ?...?x1 )??pt ?...?p1 )

[f t ???xt ? xt?1 ? xt?2 ? . . . ? xt??k?1) ))].

However, by taking the expectation of Reg?k ??? T ) with respect to aT ?1 ? aT2 ? . . . ? a1 and applying
Jensen?s inequality, we obtain
T
T
?
?
Reg??? T )?
? t [f t ?xt )]? min
?k

t=1

xt ?p

???k

t=1

?

?xt ?...?x1 )??pt ?...?p1 )

[f t ???xt ? xt?1 ? xt?2 ? . . . ? xt??k?1) ))]?

and so no generality is lost by considering the randomized sequence of actions in our regret term.
Another interpretation of the bigram conditional swap class is in the context of ?nite-state transducers. Taking a player?s sequence of actions ?x1 ? ...? xT ), we may view each competitor function in
the conditional swap class as an application of a ?nite-state transducer with N states, as illustrated
by Figure 1. Each state encodes the history of actions ?xt?1 ? . . . ? xt??k?1) ) and admits N outgoing
transitions representing the next action along with its possible modi?cation. In this framework, the
original swap regret class is simply a transducer with a single state.
3

3

Full Information Scenario

Here, we prove that it is in fact possible to minimize k-gram conditional swap regret against an
oblivious adversary, starting with the easier to interpret bigram scenario. Our proof constructs a
meta-algorithm using external regret algorithms as subroutines, as in [4]. The key is to attribute
a fraction of the loss to each external regret algorithm, so that these losses sum up to our actual
realized loss and also press the subroutines to minimize regret against each of the conditional swaps.
Theorem 2. There? exists
algorithm ? with bigram swap regret bounded as follows:
?
? an online
Reg?2 ??? T ) ? O N T log N .
Proof. Since the distribution pt at round t is ?nite-dimensional, we can represent it as a vector
pt = ?pt1 ? ...? ptN ). Similarly, since oblivious adversaries take only N arguments, we can write f t
t
as the loss vector f t = ?f1t ? ...? fN
). Let {at }Tt=1 be a sequence of random variables denoting the
player?s actions at each time t, and let ?at t denote the (random) Dirac delta distribution concentrated
at at and applied to variable xt . Then, we can rewrite the bigram swap regret as follows:
Reg??? T ) =
?2

T
?
t=1

=

?t [f t ?xt )] ? min

???2

p

T ?
N
?
t=1 i=1

pti fit ? min

???2

T
?

?

t t?1
t=1 p ??at?1

N
T ?
?

[f t ???xt ? xt?1 )]

t?1
pti ?{a
ft
t?1 =j} ??i?j)

t=1 i?j=1

Our algorithm for achieving sublinear regret is de?ned as follows:

1. At t = 1, initialize N 2 external regret minimizing algorithms Ai?k , ?i? k) ? N 2 .
We can view these in the form of N matrices in RN ?N , {Qt?k }N
k=1 , where for each
is
a
row
vector
consisting
of
the
distribution
weights generated
k ? {1? . . . ? N }, Qt?k
i
by algorithm Ai?k at time t based on losses received at times 1? . . . ? t ? 1.
2. At each time t, let at?1 denote the random action played at time t ? 1 and let ?at?1
denote
t?1
the (random) Dirac delta distribution for this action. De?ne the N ? N matrix Qt =
?N
t?1
t?k
t
k=1 ?{at?1 =k} Q . Q is a Markov chain (i.e., its rows sum up to one), so it admits a
t
stationary distribution p which we we will use as our distribution for time t.
3. When we draw from pt , we play a random action at and receive loss f t . Attribute the
t?1
portion of loss pti ?{a
f t to algorithm Ai?k , and generate distributions Qt?k
i . Notice
t?1 =k}
?N
t t?1
t
t
that i?k=1 pi ?{at?1 =k} f = f , so that the actual realized loss is allocated completely.
Recall that an optimal external regret minimizing algorithm ? (e.g.
majority)
??randomized weighted
?
i?k
i?k
admits a regret bound of the form Ri?k = Ri?k ?Lmin ? T? N ) = O
Lmin log?N ) , where Li?k
min =
?
T
t?i?k
minN
for the sequence of loss vectors {f t?i?k }Tt=1 incurred by the algorithm. Since
j=1
t=1 fj
t
t t
p = p Q is a stationary distribution, we can write:
T
?
t=1

pt ? f t =

N
T ?
?
t=1 j=1

ptj fjt =

N ?
N
T ?
?

pti Qti?j fjt =

t=1 j=1 i=1

N ?
N
T ?
?
t=1 j=1 i=1

4

pti

N
?

k=1

t?1
t
?{i
Qt?k
i?j fj .
t?1 =k}

Rearranging leads to
T
?
t=1

pt ? f t =
?
=

T ?
N ?
N
?

t?1
t
pti ?{i
Qt?k
i?j fj
t?1 =k}

i?k=1 t=1 j=1
N
?

i?k=1
N
?

i?k=1

?? T
?

t?1
pti ?{i
ft
t?1 =k} ??i?k)

t=1

? T
?

t?1
pti ?{i
ft
t?1 =k} ??i?k)

t=1

?

?

?2

T
?
t=1

pt ? f t ? min

???2

+ Ri?k ?Lmin ? T? N )

+

N
?

(for arbitrary ? : N 2 ? N )

Ri?k ?Lmin ? T? N ).

i?k=1

Since ? is arbitrary, we obtain
Reg??? T ) =

?

T ?
N
?

t=1 i?k=1

t?1
pti ?{i
ft
?
t?1 =k} ??i?k)

N
?

Ri?k ?Lmin ? T? N ).

i?k=1

?
log?N ) and that we scaled the losses to algorithm Ai?k by
Using the fact that Ri?k = O
?N ?N
t t?1
pi ?{it?1 =k} , the following inequality holds: k=1 j=1 Lk?j
min ? T . By Jensen?s inequality, this
implies
?
?
?
N
N N ?
N ?
? 1 ?
1 ??
T
k?j
k?j
?
Lmin ?
Lmin ?
?
2
N2
N
N
k=1 j=1
k=1 j=1
?
?N ?N ?
or, equivalently, k=1 j=1 Lk?j
min ? N T . Combining this with our regret bound yields
??
?
N
N
?
? ?
?
?
Li?k
Ri?k ?Lmin ? T? N ) =
O
? O N T log N ?
Reg??? T ) ?
min log N
?2

i?k=1

??

Li?k
min

i?k=1

which concludes the proof.

Remark 1. The computational complexity of a standard external regret minimization algorithm such
as randomized weighted majority per round is in O?N ) ?update the distribution on each of the N
actions multiplicatively and then renormalize), which implies that updating the N 2 subroutines will
cost O?N 3 ) per round. Allocating losses to these subroutines and combining the distributions that
they return will cost an additional O?N 3 ) time. Finding the stationary distribution of a stochastic
3
matrix can be done
?via matrix inversion in O?N )3time. Thus, the total computational complexity
of achieving O?N T log?N )) regret is only O?N T ). We remark that in practice, one often uses
iterative methods to compute dominant eigenvalues ?see [16] for a standard reference and [11] for
recent improvements). [10] has also studied techniques to avoid computing the exact stationary
distribution at every iteration step for similar types of problems.
The meta-algorithm above can be interpreted in three equivalent ways: (1) the player draws an
action xt from distribution pt at time t; (2) the player uses distribution pt to choose among the
N subsets of algorithms Qt1 ? ...? QtN , picking one subset Qtj ; next, after drawing j from pt , the
t?N
t?1
to randomly choose among the algorithms Qt?1
player uses ?{a
j ? ...? Qj , picking algorithm
t?1 =k}
t?a

t?a

Qj t?1 ; after locating this algorithm, the player uses the distribution from algorithm Qj t?1 to draw
t t?1
an action; (3) the player chooses algorithm Qt?k
j with probability pj ?{at?1 =k} and draws an action
from its distribution.
The following more general bound can be given for an arbitrary k-gram swap scenario.
Theorem 3. There??
exists an online
? algorithm ? with k-gram swap regret bounded as follows:
Reg?k ??? T ) ? O N k T log N .

The algorithm used to derive this result is a straightforward extension of the algorithm provided in
the bigram scenario, and the proof is given in Appendix 11.
Remark 2. The computational complexity of achieving the above regret bound is O?N k+1 T ).
5

????????
????????

???
?

?

????????

???

????????
???????? ????????

???
???

?

Figure 2: bigram conditional swap class restricted to a ?nite number of active states. When the
action at time t ? 1 is 1 or 2, the transducer is in the same state, and the swap function is the same.

4

State?Dependent Bounds

In some situations, it may not be relevant to consider conditional swaps for every possible action,
either because of the speci?c problem at hand or simply for the sake of computational ef?ciency.
Thus, for any S ? N 2 , we de?ne the following competitor class of functions:
? for ?i? k) ? S where ?? : N ? N }.
C2?S = {? : N 2 ? N |??i? k) = ??i)

See Figure 2 for a transducer interpretation of this scenario.
We will now show that the algorithm above can be easily modi?ed to derive a tighter bound that
is dependent on the number of states in our competitor class. We will focus on the bigram case,
although a similar result can be shown for the general k-gram conditional swap regret.
?
Theorem 4. There exists an online algorithm ? such that Reg?2?? ??? T )
?
c
O? T ?|S | + N ) log?N )).

The proof of this result is given in Appendix 10. Note that when S = ?, we are in the scenario where
all the previous states matter, and our bound coincides with that of the previous section.
Remark 3. The computational complexity of achieving the above regret bound is O??N ?|?1 ?S)| +
|S c |) + N 3 )T ), where ?1 is projection onto the ?rst component. This follows from the fact that
we allocate the same loss to all {Ai?k }k:?i?k)?S ?i ? ?1 ?S), so we effectively only have to manage
|?1 ?S)| + |S c | subroutines.

5

Conditional Correlated Equilibrium and ??Dominated Actions

It is well-known that regret minimization in on-line learning is related to game-theoretic equilibria
[14]. Speci?cally, when both players in a two-player zero-sum game follow external regret minimizing strategies, then the product of their individual empirical distributions converges to a Nash
equilibrium. Moreover, if all players in a general K-player game follow swap regret minimizing
strategies, then their empirical joint distribution converges to a correlated equilibrium [7].
We will show in this section that when all players follow conditional swap regret minimization
strategies, then the empirical joint distribution will converge to a new stricter type of correlated
equilibrium.
?k)
:S ?
De?nition 1. Let Nk = {1? ...? Nk }, for k ? {1? ...? K} and G = ?S = ?K
k=1 Nk ? {l
K
[0? 1]}k=1 ) denote a K-player game. Let s = ?s1 ? ...? sK ) ? S denote the strategies of all players in
one instance of the game, and let s??k) denote the ?K ? 1)-vector of strategies played by all players
aside from player k. A joint distribution P on two rounds of this game is a conditional correlated
equilibrium if for any player k, actions j? j ? ? Nk , and map ?k : Nk2 ? Nk , we have
?
?
?
P ?s? r) l?k) ?sk ? s??k) ) ? l?k) ??k ?sk ? rk )? s??k) ) ? 0.
?s?r)?S 2 : sk =j?rk =j ?

The standard interpretation of correlated equilibrium, which was ?rst introduced by Aumann, is a
scenario where an external authority assigns mixed strategies to each player in such a way that no
player has an incentive to deviate from the recommendation, provided that no other player deviates
6

from his [3]. In the context of repeated games, a conditional correlated equilibrium is a situation
where an external authority assigns mixed strategies to each player in such a way that no player
has an incentive to deviate from the recommendation in the second round, even after factoring in
information from the previous round of the game, provided that no other player deviates from his.
It is important to note that the concept of conditional correlated equilibrium presented here is different from the notions of extensive form correlated equilibrium and repeated game correlated equilibrium that have been studied in the game theory and economics literature [8, 12].
Notice that when the values taken for ?k are indepndent of its second argument, we retrieve the
familiar notion of correlated equilibrium.
Theorem 5. Suppose that all players in a K-player repeated game follow bigram conditional swap
regret minimizing strategies. Then, the joint empirical distribution of all players converges to a
conditional correlated equilibrium.
Proof. Let I t ? S be a random vector denoting the actions played by all K players in the game
at round t. The empirical joint distribution of every two subsequent rounds of a K-player game
?T ?
played repeatedly for T total rounds has the form P?T = T1 t=1 ?s?r)?S 2 ?{I t =s?I t?1 =r} , where
I = ?I1 ? ..? IK ) and Ik ? p?k) denotes the action played by player k using the mixed strategy p?k) .

t?1
? pt?1??k?1) . Then, the conditional swap regret of each player k,
Let q t??k) denote ?{i
t?1 =k}
reg?k? T ), can be bounded as follows since he is playing with a conditional swap regret minimizing
strategy:

reg?k? T ) =

T
T
?
?
1?
1?
l?k) ?sk ? s??k) ) ? min
?
? T
T t=1 stk ?pt??k)
t=1

?
? ?
log?N )
.
?O N
T

?

?stk ?st?1
)
k
??pt??k) ?q t??k) )

?

?
t
l?k) ???stk ? st?1
k )? s??k) )

De?ne the instantaneous conditional swap regret vector as
? ? ?
?
??
?k)
t
r?t?j0 ?j1 = ?{I t =j0 ?I t?1 =j1 } l?k) I t ? l?k) ?k ?j0 ? j1 )? I??k)
?
?k)

?k)

and the expected instantaneous conditional swap regret vector as
? ?
?
?
??
?k)
t
t
rt?j0 ?j1 = P?stk = j0 )?{I t?1 =j1 } l?k) j0 ? I??k)
? l?k) ?k ?j0 ? j1 )? I??k)
.
?k)

Consider the ?ltration Gt = {information of opponents at time t and of the player?s actions up to
?
? ?k)
?k)
?k)
?k)
time t ? 1}. Then, we see that ? r?t?j0 ?j1 |Gt = rt?j0 ?j1 . Thus, {Rt = rt?j0 ?j1 ? r?t?j0 ?j1 }?
t=1 is a
sequence of bounded martingale differences, and by the Hoeffding-Azuma inequality, we can write
?T
for any ? > 0, that P[| t=1 Rt | > ?] ? 2 exp??C?2 /T ) for some constant C > 0.
? ?
?? ?
? 2 ??
?
?
T
. By our concentration bound, we
Now de?ne the sets AT := ? T1 t=1 Rt ? > C
T log ?T
?
have P ?AT ) ? ?T . Setting ?T = exp?? T ) and applying the Borel-Cantelli lemma, we obtain
?T
that lim supT ?? | T1 t=1 Rt | = 0 a.s..

Finally, since each player followed a conditional swap regret minimizing strategy, we can write
?T
?k)
lim supT ?? T1 t=1 r?t?j0 ?j1 ? 0. Now, if the empirical distribution did not converge to a conditional correlated equilibrium, then by Prokhorov?s theorem, there exists a subsequence {P?Tj }j
satisfying the conditional correlated equilibrium inequality but converging to some limit P ? that is
not a conditional correlated equilibrium. This cannot be true because the inequality is closed under
weak limits.
Convergence to equilibria over the course of repeated game-playing also naturally implies the
scarcity of ?very suboptimal? strategies.
7

De?nition 2. An action pair ?sk ? rk ) ? Nk2 played by player k is conditionally ??dominated if
there exists a map ?k : Nk2 ? Nk such that
l?k) ?sk ? s??k) ) ? l?k) ??k ?sk ? rk )? s??k) ) ? ?.

Theorem 6. Suppose player k follows a conditional swap regret minimizing strategy that produces
a regret R over T instances of the repeated game. Then, on average, an action pair of player k is
R
conditionally ?-dominated at most ?T
fraction of the time.
The proof of this result is provided in Appendix 12.

6

Bandit Scenario

As discussed earlier, the bandit scenario differs from the full-information scenario in that the player
only receives information about the loss of his action f t ?xt ) at each time and not the entire loss
function f t . One standard external regret minimizing algorithm is the Exp3 algorithm introduced
by [2], and it is the base learner off of which we will build a conditional swap regret minimizing
algorithm.
To derive a sublinear conditional swap regret bound, we require an external regret bound on Exp3:
T
?
t=1

? [f t ?xt )] ? min

pt

a?N

T
?
t=1

?
f t ?a) ? 2 Lmin N log?N )?

which can be found in Theorem 3.1 of [5]. Using this estimate, we can derive the following result.
??
?
N 3 log?N )T .

Theorem 7. There exists an algorithm ? such that Reg?2 ?bandit ??? T ) ? O

The proof is given in Appendix 13 and is very similar to the proof for the full information setting.

It can also easily be extended in the analogous way to provide a regret bound for the k-gram regret
in the bandit scenario.
??
?
Theorem 8. There exists an algorithm ? such that Reg?k ?bandit ??? T ) ? O N k+1 log?N )T .
See Appendix 14 for an outline of the algorithm.

7

Conclusion

We analyzed the extent to which on-line learning scenarios are learnable. In contrast to some of
the more recent work that has focused on increasing the power of the adversary (see e.g. [1]), we
increased the power of the competitor class instead by allowing history-dependent action swaps and
thereby extending the notion of swap regret. We proved that this stronger class of competitors can
still be beaten in the sense of sublinear regret as long as the memory of the competitor is bounded.
We also provided a state-dependent bound that gives a more favorable guarantee when only some
parts of the history are considered. In the bigram setting, we introduced the notion of conditional
correlated equilibrium in the context of repeated K-player games, and showed how it can be seen
as a generalization of the traditional correlated equilibrium. We proved that if all players follow
bigram conditional swap regret minimizing strategies, then the empirical joint distribution converges
to a conditional correlated equilibrium and that no player can play very suboptimal strategies too
often. Finally, we showed that sublinear conditional swap regret can also be achieved in the partial
information bandit setting.

8

Acknowledgements

We thank the reviewers for their comments, many of which were very insightful. We are particularly
grateful to the reviewer who found an issue in our discussion on conditional correlated equilibrium
and proposed a helpful resolution. This work was partly funded by the NSF award IIS-1117591. The
material is also based upon work supported by the National Science Foundation Graduate Research
Fellowship under Grant No. DGE 1342536.
8

References
[1] Raman Arora, Ofer Dekel, and Ambuj Tewari. Online bandit learning against an adaptive
adversary: from regret to policy regret. In ICML, 2012.
[2] Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic
multiarmed bandit problem. SIAM J. Comput., 32(1):48?77, 2002.
[3] Robert J. Aumann. Subjectivity and correlation in randomized strategies. Journal of Mathematical Economics, 1(1):67?96, March 1974.
[4] Avrim Blum and Yishay Mansour. From external to internal regret. Journal of Machine Learning Research, 8:1307?1324, 2007.
[5] S?ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret analysis of stochastic and nonstochastic
multi-armed bandit problems. CoRR, abs/1204.5721, 2012.
[6] Nicol`o Cesa-Bianchi, Ofer Dekel, and Ohad Shamir. Online learning with switching costs and
other adaptive adversaries. In NIPS, pages 1160?1168, 2013.
[7] Nicol`o Cesa-Bianchi and G?abor Lugosi. Prediction, Learning, and Games. Cambridge University Press, New York, NY, USA, 2006.
[8] Francoise Forges. An approach to communication equilibria. Econometrica, 54(6):pp. 1375?
1385, 1986.
[9] Dean P. Foster and Rakesh V. Vohra. Calibrated learning and correlated equilibrium. Games
and Economic Behavior, 21(12):40 ? 55, 1997.
[10] Amy Greenwald, Zheng Li, and Warren Schudy. More ef?cient internal-regret-minimizing
algorithms. In COLT, pages 239?250. Omnipress, 2008.
[11] N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decompositions. SIAM Rev., 53(2):217?288,
2011.
[12] Ehud Lehrer. Correlated equilibria in two-player repeated games with nonobservable actions.
Mathematics of Operations Research, 17(1):pp. 175?199, 1992.
[13] Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. Inf. Comput.,
108(2):212?261, 1994.
? Tardos, and Vijay V. Vazirani. Algorithmic Game Theory.
[14] Noam Nisan, Tim Roughgarden, Eva
Cambridge University Press, New York, NY, USA, 2007.
[15] Gilles Stoltz and G?abor Lugosi. Learning correlated equilibria in games with compact sets of
strategies. Games and Economic Behavior, 59(1):187?208, 2007.
[16] Lloyd N. Trefethen and David Bau. Numerical Linear Algebra. SIAM: Society for Industrial
and Applied Mathematics, 1997.

9

"
3533,1987,Static and Dynamic Error Propagation Networks with Application to Speech Coding,Abstract Missing,"632

STATIC AND DYNAMIC ERROR PROPAGATION
NETWORKS WITH APPLICATION TO SPEECH
CODING
A J Robinson, F Fallside
Cambridge University Engineering Department
Trumpington Street, Cambridge, England
Abstract
Error propagation nets have been shown to be able to learn a variety of tasks in
which a static input pattern is mapped outo a static output pattern. This paper
presents a generalisation of these nets to deal with time varying, or dynamic
patterns, and three possible architectures are explored. As an example, dynamic
nets are applied to tbe problem of speech coding, in which a time sequence of
speech data are coded by one net and decoded by another. The use of dynamic
nets gives a better signal to noise ratio than that achieved using static nets.

1. INTRODUCTION
This paper is based upon the use of the error propagation algorithm of Rumelbart, Hinton
and Williams l to train a connectionist net. The net is defined as a set of units, each witb an
activation, and weights between units which determine the activations. The algorithm uses a
gradient descent technique to calculate the direction by which each weight should be changed
in order to minimise the summed squared difference between the desired output and the actual
output. Using this algorithm it is believed that a net can be trained to make an arbitrary
non-linear mapping of the input units onto the output units if given enough intermediate
units. This 'static' net can be used as part of a larger system with more complex behaviour.
The static net has no memory for past inputs, but many problems require the context of
the input in order to c.ompute the answer. An extension to the static net is developed, the
'dynamic' net, which feeds back a section of the output to the input, so creating some internal
storage for context, and allowing a far greater class of problems to be learned. Previously this
method of training time dependence into uets has suffered from a computational requirement
which increases linearly with the time span of the desired context. The three architectures
for dynamic uets presented here overcome this difficulty.
To illustrate the power of these networks a general coder is developed and applied to the
problem of speech coding. The non-liuear solution found by training a dynamic net coder is
compared with an established linear solution, and found to have an increased performance as
measured by the signal to noise ratio .

2. STATIC ERROR PROPAGATION NETS
A static Ret is defined by a set of units and links between the units. Denoting 0i as the value
of the ith unit, and wi,l as the weight of the link between Oi and OJ, we may divide up the
units into input units, hidden units and output units. If we assign 00 to a. constant to form a

@ American Institute of Physics 1988

633

bias, the input units run from 01 up to on"",\., followed by the hidden units to onh?.t and then
the output units to On."".' The values of the input units are defined by the problem and the
values of the remaining units are defined by:
i-I

neti
?i

~1LJ'',1'0'J

(2.1)

j=O
!(net;)

(2.2)

where !( x) is any continuous monotonic non-linear function and is known as the activation
function. The function used the application is:
2
----1
1 + e- z"",

!(x)

(2 .3)

These equations define a net which has the maximum number of interconnections. This
arrangement is commonly restricted to a layered structure in which units are only connected
to the immediately preceding layer . The architecture of these nets is specified by the number
of input, output and hidden units. Diagrammatically the static net is transformation of an
input 'U, onto the output y, as in figure 1.

static
net

figure 1

The net is trained by using a gradient descent algorithm which mlDlsmises an energy
term, E, defined as the summed squared error between the actual outputs, ai, and the target
outputs, t i . The algorithm also defines an error signal, Oi, for each unit:

E
[Ii

1

2

""lint

~

(ti --

i=nl w l+1
!' (netd(t i

-

od 2

0;)

(2.4)
nhid

< i ::;

nout

(2.5 )

ninp

< i ::;

nhid

(2 .6)

"" lint

.f' (net;) ~

OiWj,i

j=i+l

where f' (x) is the derivative of !( x). The error signal and the adivations of the units define
the change in each weight, D. Wi,j'
(2.7)

where '1 is a constant of proportionality which determines the learning rate. The above
equations define the error signal, 0;, for the input units as well as for the hidden units. Thus
any number of static nets can be connected together, the values of Oi being passed from input
units of one net to output units of the preceding net. It is this ability of error propagation
nets to be 'glued' together in this way that enables the construction of dynamic nets.

3. DYNAMIC ERROR PROPAGATION NETS
The essential quality of the dynamic net is is that its behaviour is determined both by the
external input to the net, and also by its own internal state. This state is represented by the

634

activation of a group of units. These units form part of the output of a st.atic net and also
part of the input to another copy of the same static net in the next time period. Thus the
state units link multiple copies of static nets over time to form a dynamk net.

3.1. DEVELOPMENT FROM LINEAR CONTROL THEORY
The analogy of a dynamic net in linear systems 2 may be stated as:

(3.1.1)
(3.1.2)
where up is the input vector,

zp

the state vector, and Yp the output vector at the integer time

p. A, Band C are matrices.

The structure of the linear systems solution may be implemented as a non-linear dynamic
net by substituting the matrices A, Band C by statk nets, represented by the non-linear
functions A[.]' B[.] and C[.]. The summation operation of Azp and Bup could be achieved
using a net with one node for each element in z and u and with unity weights from the two
inputs to the identity activation function f( x) = z. Alternatively this net can be incorporated
into the A[.] net giving the architecture of figure 2.

B [.]

y(p+l)

dynamic t---of
A[.]

e[.]

y(p+l)
net
x(p+l)

Time

Time

Delay

Delay

figure 2

figure 3

The three networks may be combined into one, as in figure 3. Simplicity of architecture
is not just an aesthetic consideration. If three nets are used then each one must have enough
computational power for its part of the task, combining the nets means that only the combined
power must be sufficient and it allows common computations can be shared.
The error signal for thf' output Yp+l, can be calculated by comparison with the desired
output. However, the error signal for thf' state units, x P ' is only given by the net at time p+l,
which is not known at time p. Thus it is impossible to use a single backward pass to train
this net . It is this difficulty which introduces the variation in the architectures of dynamic
nets.

3.2. THE FINITE INPUT DURATION (FID) DYNAMIC NET
If the output of a dynamic net, YP' is df'pendf'nt on a finite number of previous inputs, up_p
to up, or if this assumption is a good approximation, then it is possible to formulate the

635

learning algorithm by expansion of the dynamk net for a finite time, as in figure 4. This
formulation is simlar to a restricted version of the recurrent net of Rumelhart, Hinton and
Williams. 1

x(p+l)
dynamic
net
(p)

y(p+l)
dynamic
net

(p-l)
yep)
dynamic
net
(p-2)

figure 4

Consider only the component of the error signal in past instantiations of the nets which
is the result of the error signal at time t. The errot signal for YP is calculated from the target
output and the ('rror signal for xr is zero. This combined error signal is propagated back
though the dynamic net at p to yield the error signals for up and xp' Similarly these error
signals can then be propagated back through the net at t - P, and so on for all relevant inputs.
The summed error signal is then used to change the weights as for a static net.
Formalising the FID dynamic net for a general time q, q ~ p:
n, is the number of state units
is the output value of unit i at time q
?q,i
is the target value of unit i at time q
tq,i
is
the error value of unit i at time q
6'1,'
is the weight between 0; and OJ
Wi,j
is the weight change for this iteration at time q
~Wq,i,i
is the total weight change for this iteration
~wi,i
These values are calculated in the same way as in a static net,
i-1

netq,i

L

(3.2.1)

Wi,jOq,j

j=O

(3 .2.2)

f(net q ,.)

f' (netq,d( tq,i

-

0'1,;)

+ n, < i :S nout
nhid < i :S nhid + n,

nhid

(3 .2.3)
(3.2.4)

nullt

!'(n('t q ,;)

L

6q ,jWj ,i

(3 .2.5)

j-=i+l

(3.2.6)

and the total weight change is given by the summation of the partial weight changes for all

636

previous times.
p

L

Llu'q,i,j

(3.2.7)

7]6 q,i O q,j

(3.2.8)

q=p-P
p

L
q=p-P

Thus, it is possible to train a dynamic net to incorporate the information from any time
period of finite length, and so l~arn any function which has a finite impulse response.?
In some situations the approximation to a finite length may not be valid, or the storage
and computational requirements of such a net may not be feasible. In such situations another
approach is possible, the infinite input duration dynamic net .

3.3. THE INFINITE INPUT DURATION (lID) DYNAMIC NET
Although the forward pass of the FID net of the previous section is a non-linear process, th..
backward pass computes the efred of small variations on the forward pass, and is a linear
process. Thus the recursive learning procedure described in the previous section may be
compressed into a single operation.
Given the target values for the output of the net at time p, equations (3.2.3) and (3.2.4)
define valu~s of 6p ,i at the outputs. If we denote this set of 6p ,i by Dp then equation (3.2.5)
states that any 6p ,i in the net at time p is simply a linear transformation o( Dp. Writing the
transformation matrix as S:

(3.3.1)
In particular the set of 6p ,i which is to be fed back into the network at time p - 1 is also
a linear transformation of Dp

(3.3.2)
or for an arbitrary time q:

(3.3.3)
so substituting equations (3.3.1) and (3.3.3) into equation (3.2.8):
p

LlU'i,j

7]L

Sq,i

q=-oo

( IT T,) D,o""j

(3.3.4)

7=q+l

(3.3.5)

7]Mp ,i,i D p
where:
p

M p,',)
.,

L

q=-oo

Sq,i

( IT T}""j

(3.3.6)

""=q+l

? This is a restriction on the class of functions which can be learned, the output will always be affected
in some way by all previous inputs giving an infinite impulse response performance.

637

and note that Mp,i,i can be written in terms of Mp-1,i,i :

MP,-.,,J

Sp,i (

IT

T,.) 0p,i

,.=p+l

Sp,iop,i

+

(I:

Sq,i

(3.3.7)

q=-oo

+ Mp-1,i,iTp

(3.3 .8)

Hence we can calculate the weight changes for an infinite recursion using only the finite
matrix M,

3.3. THE STATE COMPRESSION DYNAMIC NET
The previous architectures for dynamic nets rely on the propagation of the error signal hack
ill time to define the format of the information in the state units. All alternative approach
is to use another error propagation net to define the format of the state units. The overall
architecture is given in figure 5.

Bncoder
net

1-----\1 Tranlllatort---""""'""
x(p+1)
y(p+1)
net

Decoder
net

figure 5

The encoder net is trained to code the current input and current state onto the next state,
while the decoder net is trained to do the reverse operation. The tran81ator net code8 the
next state onto the desired output. This encoding/decoding attempts to represent the current
input and the current state in the next state, and by the recursion, it will try to represent all
previous inputs. Feeding errors back from the translator directs this coding of past inputs to
those which are useful in forming the output.

3.4. COMPARISON OF DYNAMIC NET ARCHITECTURES

III comparing the three architectures for dynamic nets, it is important to consider the computational and memory requirements, and how these requirements scale with increasing context.
To train an FID net the net must store the past activations of the all the units within
the time span of thel'necessary context, Using this minimal storage, the computational load
scales proportiona.lly to the time span considered, as for every new input/output pair the
net must propagate an error signal back though all the past nets. However, if more sets
of past activations are stored in a buffer, then it is possible to wait until this buffer is full
before computing the weight changes. As the buffer size increases the computational load in

638

calculating the weight changes tends to that of a single backward pass through the units, and
so becomes independent of the amount of coutext.
The largest matrix required to compute the 110 net is M, which requires a factor of the
number of outputs of the net more storage than the weight matrix. This must be updated
on each iteration, a computational requirement larger than that of the FlO net for smaJl
problems 3 . However, if this architecture were implemented on a paraJlel machine it would be
possible to store the matrix M in a distributed form over the processors, and locally calculate
the weight changes. Thus, whilst the FID net requires the error signal to be propagated back
in time in a strictly sequential manner, the 110 net may be implemented in paraJld, with
possible advantages on parallel machines.
The state compression net has memory and computational requirements independent of
the amount of context. This is achieved at the expense of storing recent information in the
state units whether it is required to compute the output or not . This results in an increased
computational and memory load over the more efficient FID net when implemented with a
buffer for past outputs. However, the exclusion of external storage during training gives this
architecture more biological plausibili ty, constrained of course by the plausibility of the error
propagation algorithm itself.
With these considerations in mind, the FlO net was chosen to investigate a 'real world'
problem, that of the coding of the speech waveform.

4. APPLICATION TO SPEECH CODING
The problem of speech coding is one of finding a suitable model to remove redundancy and
hence reduce the data rate of the speech. The Boltzmann machine learning algorithm has
already been extended to deal to the dynamic case and applied to speech recognition4. However, previous use of error propagation nets for speech processing has mainly been restricted to
explicit presentation of the context 5,6 or explicit feeding back the output units to the input 7,8,
with some work done in usillg units with feedback links to themselves 9 . In a similar area,
static error propagation nets have been used to perform image coding as well as cOllventional
techniques 1o .

4.1. THE ARCHITECTURE OF A GENERAL CODER
The coding principle used in this section is not restricted to c.oding speech data. The general
problem is one of encoding the present input using past input context to form the transmitted
signal, and decoding this signal using the context ofthe coded signals to regenerate the original
input. Previous sections have shown that dynamic nets are able to represent context, so two
dynamic, nets in series form the architecture of the coder, as in figure 6.
This architecture may be specified by the number of input, state, hidden and transmission
units. There are as many output units as input units and, in this application, both the
transmitter and receiver have the same number of state and hidden units.
The input is combined with the internal state of the transmitter to form the coded signal,
and then decoded by the receiver using its internal state. Training of the net involves the
comparison of the input and output to form the error signal, which is thell propagated back
through past instantiations of the receiver and transmitter in the same way as a for a FID
dynamic net.
It is useful to introduce noise into the coded signal during the training to reduce the
information capacity of the transmission line. This forces the dynamic 11ets to incorporate
time information, without this constraint both nets can learn a simple transformation without
any time dependence. The noise can be used to simulate quantisation of the coded signal so

639

input

,

coded signal

J

?

I

TX

r-\

output

?

ax

r-\

rI
io-

..,

rI

~

Time

V-

Delay

~

I-

I""""

Time

I

Delay

\--

figure 6

quantifying the transmission rate. Unfortunately, a
violates tbe requirement of the activation function
train the net. Instead quantisation to n levels may
distributed uniformly in the range + 1/ n to -1 / n to

straight implementation of quantisation
to be continuous, which is necessary to
be simulated by adding a random value
each of the channels in the coded signal.

4.2. TRAINING OF THE SPEECH CODER
The chosen problem was to present a sinJZ;le sample of digitised speech to the input, code to
a single value quantised to fifteen levels, and then to reconstruct tile original speech at the
output . Fifteen levels was chosen as the point where there is a marked loss in the intelligibility
of the speech, so implementation of these coding schemes gives an audible improvement. Two
version of the coder net were implemented, both nets had eight hidden units, with no state
units for the static time independent case and four state units for the dynamic time dependent
case.
The data for this problem was 40 seconds of speech from a single male speaker, digit,ised
to 12 bits at 10kHz and recorded in a laboratory environment. The speech was divided into
two halves, the first was used for training and the second for testing.
The static and the dynamic versions of the architecture were trained on about 20 passes
through the training data. After training the weights were frozen and the inclusion of random
noise was replaced by true quantisation of the coded representation. A further pass was then
made through the test data to yield the performance measurements.
The adaptive training algorithm of Chan 11 was used to dynamically alter the learning
rates during training. Previously these machines were trained with fixed learning rates and
weight update after every sample 3 , and the use of the adaptive t.raining algorithm has been
found to result in a substantially deeper energy minima. Weights were updated after every
1000 samples, that is about 200 times in one pass of the training data.

4.3. COMPARISON OF PERFORMANCE
The performance of a coding schemes can be measured by defining the noise energy as half the
summed squared difference between the actual output and the desired output. This energy
is the quantity minimised by the error propagation algorithm. The lower the noise energy in
relation to the energy of the signal, the higher the performance.
Three non-connectionist coding schemes were implemented for comparison with the static

640

and dynamic net coders. In the first the signal is linearly quantised within the dynamic range
of the original signal. In the second the quantiser is restricted to operate over a reduced
dynamic range, with values outside that range thresholded to the maximuJn and minimum
outputs of the quantiser. The thresholds of the quantiser were chosen to optimise the signal
to noise ratio. The third scheme used the technique of Differential Pulse Code Modulation
(DPCM)12 which involves a linear filter to predict the speech waveform, and the transmitted
signal is the difference between the real signal and the predicted signal. Another linear filter
reconstructs the original signal from the difference signal at the receiver. The filter order of
the DPCM coder was chosen to be the same as the number of state units in the dynamic net
coder, thus both coders can store the same amount of context enabling a comparison with
this established technique.
The resulting noise energy when the signal energy was normalised to unity, and the corresponding signal to noise ratio are given in table 1 for the five coding techniques.
coding method
linear, original thresholds
linear, optimum thresholds
static net
DPCM, optimum thresholds
dynamic net

normalised
nOise energy
0.071
0.041
0.049
0.037
0.028

signal to noise
ratio in dB
11.5
13.9
13.1
14.3
15.5

table 1
The static net may be compared with the two forms of the linear quantiser. Firstly note
that a considerable improvemeut in the signal to noise ratio may be achieved by reducing the
thresholds of the qllantiser from the extremes of the input. This improvement is achieved
because the distribution of samples in the input is concentrated around the mean value, with
very few values near the extremes. Thus many samples are represented with greater accuracy
at the expense of a few which are thresholded. The static net has a poorer performance than
the linear quantiser with optimum thresholds. The form of the linear quantiser solution is
within the class of problems which the static net can represent . It's failure to do so can be
attributed to finding a local minima, a plateau in weight space, or corruption of the true
steepest descent direction by noise introduced by updating the weights more than once per
pass through the training data.
The dynamic net may be compared with the DPCM coding. The output from both these
coders is no longer constrained to discrete signal levels and the resulting noise energy is lower
than all the previous examples. The dynamic net has a significantly lower noise energy than
any other coding scheme, although, from the static net example, this is unlikely to be an
optimal solution. The dynamic net achieves a lower noise energy than the DPCM coder by
virtue of the non-linear processing at each unit, and the flexibility of data storage in the state
units.
As expected from the measured noise energies, there is an improvement in signal quality
and intelligibility from the linear quantised speech through to the DCPM and dynamic net
quantised speech.

5. CONCLUSION
This report has developed three architectures for dynamic nets. Each architecture can be
formulated in a way where the computational requirement is independent of the degree of
context necessary to learn the solution. The FID architecture appears most suitable for

641
implementation on a sf'rial processor, t.hf' nn archit.f'd,11fe has possihle a(lvant,ages for implementation on parallel processors, and the state compression net has a higher degree of
biological plausibility.
Two FID dynamic nets have been coupled together to form a coder, and this has been
applied to speech coding. Although the dynamic net coder is unlikely to have learned the
optimum coding strategy, it does delUonstrate that dynamic nets can be used to 8.Chieve an
improved performance in a real world task over an estaBlished conventional technique.

One of the authors, A J Robinson, is supported by a maintenance grant from the U.K.
Science and Engineering Research Council, and gratefully acknowledges this support.

References
[1] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by
error propagation. In D. E. Rumelhart and J. L. McClelland, editors, Parallel Distributed
Processing: E2:plorations in the M1crostructure of Cognition, Vol. 1: Foundations., Bradford Books/MIT Press, Cambridge, MA , 1986,
[2] O. L. R. Jacobs. IntroductIOn to Contml Theory. Clarendon Press, Oxford, 1974.
[3J A. J. Robinson and F. Fallside. The Utility Drit'en Dynamic Error Propagation Network. Technical Report CUED/F-INFENG/TR.l, Cambridge University Engineering
Department, 1987.
[4J R. W. Prager, T. D. Harrison, and F. Fallside, Boltzmann machines for speech recognition. Compllter Speech and Language, 1:3-27, 1986,
[5] J. L. Elman and D. Zipser. Learning the Hidden Structure of Speech. ICS Report 8701,
University of California, San Diego, 1987.
[6] A. J. Robinson. Speech Rerognition wIth Associatille Networks. M.Phil Computer Speech
and Language Processing thesis, Cambridge University Engineering Department, 1986.
[7] M. I. Jordan. Serial Order: A Parallel Distributed Processing Approach. ICS Report 8604, Institute for Cognitive Science, University of California, San Diego, May
1986.
[8] D. J, C. MacKay. A Method of Increa,sing the Conte2:tual Input to Adaptive Pattern
Recognition Systems. Technical Report RIPRREP /1000 /14/87, Research Initiative in
Pattern Recognition, RSRE, Malvern, 1987.
[9) R. L. Watrous, L. Shastri, and A. H. Waibel. Learned phonetic discrimination using
connectionist networks. In J . Laver and M. A. Jack, editors, Proceedings of the Etl.ropea,n
Conference on Speech Technology, CEP Consultants Ltd, Edinburgh, September 1987.
(10) G. W. Cottrell, P. Munro, and D Zipser. Image Compression by Back Propagation: An
E2:ample of Existential Programming. ICS Report 8702, Institute for Cognitive Science,
University of California, San Diego, Febuary 1986.
[11) L. W . Chan and F. Fallside. An Adaptive Learning Algori.thm for Back Propaga.tion Networks . Technical Report CUED / F-INFENG/TR.2, Cambridge University Engineering
Department, 1987, submitted to Compute?' Speech and Language.
[12] L, R. Rabiner and R. W, Schefer . DIgital Processmg of Speech Signals. Prentice Hall,
Englewood Cliffs, New Jersey, 1978.

"
1865,2004,Saliency-Driven Image Acuity Modulation on a Reconfigurable Array of Spiking Silicon Neurons,Abstract Missing,"Saliency-Driven Image Acuity Modulation on a
Reconfigurable Silicon Array of Spiking Neurons

R. Jacob Vogelstein1 , Udayan Mallik2 , Eugenio Culurciello3 ,
Gert Cauwenberghs2 and Ralph Etienne-Cummings2
1
Dept. of Biomedical Engineering, Johns Hopkins University, Baltimore, MD
2
Dept. of Electrical & Computer Engineering, Johns Hopkins University, Baltimore, MD
3
Dept. of Electrical Engineering, Yale University, New Haven, CT
{jvogelst,umallik1,gert,retienne}@jhu.edu, eugenio.culurciello@yale.edu

Abstract
We have constructed a system that uses an array of 9,600 spiking silicon neurons, a fast microcontroller, and digital memory, to implement
a reconfigurable network of integrate-and-fire neurons. The system is
designed for rapid prototyping of spiking neural networks that require
high-throughput communication with external address-event hardware.
Arbitrary network topologies can be implemented by selectively routing address-events to specific internal or external targets according to a
memory-based projective field mapping. The utility and versatility of the
system is demonstrated by configuring it as a three-stage network that
accepts input from an address-event imager, detects salient regions of the
image, and performs spatial acuity modulation around a high-resolution
fovea that is centered on the location of highest salience.

1

Introduction

The goal of neuromorphic engineering is to design large-scale sensory information processing systems that emulate the brain. In many biological neural systems, the information
received by a sensory organ passes through multiple stages of neural computations before
a judgment is made. A convenient way to study this functionality is to design separate
chips for each stage of processing and connect them with a fast data bus. However, it is not
always advisable to fabricate a new chip to test a hypothesis regarding a particular neural
computation, and software models of spiking neural networks cannot typically execute or
communicate with external devices in real-time. Therefore, we have designed specialized
hardware that implements a reconfigurable array of spiking neurons for rapid prototyping
of large-scale neural networks.
Neuromorphic sensors can generate up to millions of spikes per second (see, e.g., [1]),
so a proper communication protocol is required for multi-chip systems. ?Address-Event
Representation? (AER) was developed for this purpose over a decade ago and has since
become the common ?language? of neuromorphic chips [2?7]. The central idea of AER
is to use time-multiplexing to emulate extensive connectivity between neurons. Although
it was originally proposed to implement a one-to-one connection topology, AER has been
extended to allow convergent and divergent connectivity [5, 8, 9], and has even been used

DAC

driving potential
(analog)

DIO
MCU

driving
potential
(digital)
postsynaptic
neuron addr.

presynaptic
neuron addr.

RAM

synapse
parameters
postsynaptic
neuron addr.

MCU

I&F

synaptic
weight

outgoing
spike addr.

incoming
spike addr.

I&F

recurrent
mode

DIO

(a)

feed-forward
mode

RAM
DAC

(b)

Figure 1: (a) Block diagram of IFAT system. Incoming and outgoing address-events are
communicated through the digital I/O port (DIO), with handshaking executed by the microcontroller (MCU). The digital-to-analog converter (DAC) is controlled by the MCU and
provides the synaptic driving potential (?E? in Figure 2) to the integrate-and-fire neurons
(I&F), according to the synapse parameters stored in memory (RAM). Modified from [18].
(b) Printed circuit board integrating all components of the IFAT system.

for functions in addition to inter-chip communication [10?12]. Within our hardware array, all inter-neuron communication is performed using AER; the absence of hardwired
connections is the feature that allows for reconfigurability.
A few examples of AER-based reconfigurable neural array transceivers can be found in the
literature [8, 9], but our Integrate-and-Fire Array Transceiver system (IFAT) differs in its
size and flexibility. With four custom aVLSI chips [13] operating in parallel and 128 MB
of digital RAM, the system contains 9,600 neurons and up to 4,194,304 synapses. Because
it was designed from the start for generality and biological realism, every silicon neuron
implements a discrete-time version of the classical biological ?membrane equation? [14],
a simple conductance-like model of neural function that allows for emulating an unlimited
number of synapse types by dynamically varying two parameters [13]. By using a memorybased projective field mapping to route incoming address-events to different target neurons,
the system can implement arbitrarily complex network topologies, limited only by the capacity of the RAM.
To demonstrate the functionality of the IFAT, we designed a three-stage feed-forward model
of salience-based attention and implemented it entirely on the reconfigurable array. The
model is based on a biologically-plausible architecture that has been used to explain human
visual search strategies [15, 16]. Unlike previous hardware implementations (e.g. [17]), we
use a multi-chip system and perform all computations with spiking neurons. The network
accepts spikes from an address-event imager as inputs, computes spatial derivatives of light
intensity as a measure of local information content, identifies regions of high salience, and
foveates a location of interest by reducing the resolution in the surrounding areas. These
capabilities are useful for smart, low-bandwidth, wide-angle surveillance networks.

2

Hardware

From the perspective of an external device, the IFAT system (Figure 1) operates as an
AER transceiver, both receiving and transmitting spikes over a bidirectional address-event
(AE) bus. Internally, incoming events are routed to any number of integrate-and-fire (I&F)

Vdd

?2

?1

Vbp

M5

Vdd

Vcomp

Rscan
M11

Vm
E

X1

Creq

Vdd

M6

M15
M7 M8

M4
C1

C2

M12

?1

?2
W0

M1

W1

M2

W2

M3

Rreq

M14

X2
C0

M13

Cack

Cm

M9

Rack

M10

Vthresh

Vbn

Vreset

Figure 2: Silicon neuron. The ?general-purpose? synapse is shown inside the dashed
box [13], with event generation circuitry shown right [9].
neurons according to a look-up table stored in RAM. When the inputs are sufficient to
cause a neuron to spike, the output is either directed to other internal neurons (for recurrent
networks) or to an external device via the AE bus. The following two sections will describe
the system and silicon neurons in more detail.
2.1

The IFAT system

A block diagram of the IFAT system and its physical implementation are shown in Figure 1.
The primary components are a 100 MHz FPGA (Xilinx XC2S100PQ208), 128 MB of nonvolatile SRAM (TI bq4017), a high-speed DAC (TI TLC7524CN), a 68-pin digital I/O
interface (DIO), and 4 custom aVLSI chips that implement a total of 9,600 I&F neurons.
The FPGA controls access to both an internal and external AE bus, and communicates
address-events between both the I&F neurons and external devices in bit-parallel using a
four-phase asynchronous handshaking scheme.
The 128 MB of RAM is arranged as a 4 MB ? 32 array. Each 32-bit entry contains a
complete description of a single synapse, specifying the postsynaptic target, the synaptic
equilibrium potential, and the synaptic weight. The weight field can be further subdivided
into three parts, corresponding to three ways in which biological neurons can control the
synaptic weight (w) [14, p. 91]:
w = npq
(1)
where n is the number of quantal neurotransmitter sites, p is the probability of neurotransmitter release per site, and q is a measure of the postsynaptic effect of the neurotransmitter.
In the IFAT system, the FPGA can implement p with a simple pseudo-random number algorithm, it can control n by sending multiple outgoing spikes for each incoming spike, and
it sends the value of q to the I&F neuron chips (see Section 2.2).
Instead of hardwired connections between neurons, the IFAT implements ?virtual connections? by serially routing incoming events to their appropriate targets at a rate of up to
1,000,000 events per second. When the IFAT receives an AE from an external device, the
FPGA observes the address, appends some ?chip identifier? (CID) bits, and stores the resulting binary number as a base address. It then adds additional offset bits to form
a complete 22-bit RAM address, which it uses to look up a set of synaptic parameters.
After configuring q and instructing the DAC to produce the analog synaptic equilibrium
potential, the FPGA activates a target neuron by placing its address on the internal AE
bus and initiating asynchronous handshaking with the appropriate I&F chip. It then increments the offset by one and repeats the process for the next synapse, stopping when it
sees a reserved code word in the data field. Recurrent connections can be implemented
simply by appending a different CID to events generated by the on-board I&F neurons,
while connections to external devices are achieved by specifying the appropriate CID for

Row Decoder
E (V) W

Column Decoder

Spike Threshold

V

m

1.5
1

60 x 40 Array of Neurons

0.5

Column Arbitration

111
000
5
0

0
0.1
0.2
0.3
Time (Arbitrary Units)

0.4

(a)

Row Arbitration

LFSR

0

(b)

Figure 3: (a) Data collected from one neuron during operation of the chip. The lower trace
illustrates the membrane potential (Vm ) of a single neuron in the array as a series of events
are sent at times marked at the bottom of the figure. The synaptic equilibrium potential (E)
and synaptic weight (W ) are drawn in the top two traces. Figure from [13]. (b) Integrateand-fire chip micrograph. The linear-feedback shift register (LFSR) implements a pseudorandom element for resolving arbitration conflicts. Modified from [13].
the postsynaptic target. With this infrastructure, arbitrary patterns of connectivity can be
implemented, limited only by the memory?s capacity.
2.2

Integrate-and-Fire Neurons

As described above, the IFAT system includes four custom aVLSI chips [13] that contain a
total of 9,600 integrate-and-fire neurons. All the neurons are identical and each implements
a simple conductance-like model of a single, ?general purpose? synapse using a switchedcapacitor architecture (Figure 2). The synapses have two internal parameters that can be
dynamically modulated for each incoming event: the synaptic equilibrium potential (E)
and the synaptic weight (W0-W2). Values for both parameters are stored in RAM; the
3-bit q is used by the FPGA to selectively enable binary-sized capacitors C0-C2, while E
is converted to an analog value by the DAC. By varying these parameters, it is possible
to emulate a large number of different kinds of synapses impinging on the same cell. An
example of one neuron in operation is shown in Figure 3a.
A micrograph of the integrate-and-fire chip is shown in Figure 3b. Incoming address-events
are decoded and sent to the appropriate neuron in the 60 ? 40 array. When a neuron?s
membrane potential exceeds an externally-provided threshold voltage, it requests service
from the peripheral arbitration circuitry. After request is acknowledged, the neuron is reset and its address is placed on the IFAT system?s internal AER bus. Conflicts between
simultaneously active neurons are resolved by a novel arbitration scheme that includes a
pseudo-random element on-chip [19].

3

Experimental Design and Results

To demonstrate the functionality of the IFAT system, we designed and implemented a threestage network for salience-based foveation [16] of an address-event imager. This work
is motivated by the fact that wide-angle image sensors in a monitoring sensor network

(a)

(b)

Figure 4: (a) Test image. (b) Output from Octopus Retina.
extract a large quantity of data from the environment, most of which is irrelevant. Because
bandwidth is limited and data transmission is energy-intensive, it is desirable to reduce the
amount of information sent over the communication channel. Therefore, if a particular
region of the visual field can be identified as having high salience, that part of the image
can be selectively transmitted with high resolution and the surrounding scene can be be
compressed.
The input to the first stage of the network is a stream of address-events generated by an
asynchronous imager called the ?Octopus Retina? (OR) [1,20]. The OR contains a 60 ? 80
array of light-sensitive ?neurons? that each represent local light intensity as a spike rate. In
other words, pixels that receive a lot of light spike more frequently than those that receive
a little light. For these experiments, we collected 100,000 events from the OR over the
course of about one second while it was viewing a grayscale picture mounted on a white
background. The test image and OR output are shown in Figure 4.
To identify candidate regions of salience, the first stage of the network is configured to
compute local changes in contrast. Every 2 ? 2 block of pixels in the OR corresponds
to four neurons on the IFAT that respond to light-to-dark or dark-to-light transitions in
the rightward or downward direction (Figure 5a). Each IFAT cell computes local changes
in contrast due to a receptive field (RF) that spans four OR pixels in either the horizontal or vertical dimension, with two of its inputs being excitatory and the other two being
inhibitory. When a given IFAT cell?s RF falls on a region of visual space with uniform
brightness, all of the OR pixels projecting to that cell will have the same mean firing rate,
so the excitatory and inhibitory inputs will cancel. However, if a cell?s excitatory inputs
are exposed to high light intensity and its inhibitory inputs are exposed to low light intensity, the cell will receive more excitatory inputs than inhibitory inputs and will generate an
output spike train with spike frequency proportional to the contrast. The output from the
4,800 IFAT neurons in the first stage of the network in response to the OR input is shown
in Figure 5b.
The second stage of processing is designed to pool inputs from neighboring contrastsensitive cells to identify locations of high salience. Our underlying assumption is that
regions of interest will contain more detail than their surroundings, producing a large output from the first stage. Blocks of 8 ? 8 IFAT cells from the first stage project to single
cells in the second stage, and each 8 ? 8 region overlaps the next by 4 neurons (Figure 6a).
Therefore, every IFAT cell in the second stage has an 8 ? 8 RF. Although it is not necessary to normalize the firing rates of the first and second stages, because every second stage
IFAT cell receives 64 inputs, we reduce the strength of the synaptic connections between
the two stages to conserve bandwidth. The output from the 300 IFAT neurons in the second

1

4

2

3

(a)

(b)

Figure 5: (a) Stage 1 network for computing local changes in contrast. Squares in the
center represent OR pixels. Circles represent IFAT neurons. Excitatory synapses are represented by triangles, and inhibitory synapses as circles. Only four IFAT neurons with nonoverlapping receptive fields are shown for clarity. (b) Output of stage 1, as implemented on
the IFAT, with Figure 4b from the OR as input.

stage of the network in response to the output from the first stage IFAT neurons is shown
in Figure 6b.
The final stage of processing modulates the spatial acuity of the original image to reduce the
resolution outside the region of highest salience. This is achieved by a foveation network
that pools inputs from neighboring pixels using overlapping Gaussian kernels (Figure 7a)
[18]. The shape of the kernel functions is implemented by varying the synaptic weight
and synaptic equilibrium potential between OR neurons and IFAT cells in the third stage:
within every pooled block, the strongest connections originate from the center pixels and
the weakest connections come from the outermost pixels. Instead of physically moving the
OR imager to center the fovea on the region of interest, we relocate the fovea by performing
simple manipulations in the address domain. First, the address space of incoming events
is enlarged beyond the range provided by the OR and the fovea is centered within this
virtual visual field (Figure 7a). Then, the row and column address of the second stage IFAT
neuron with the largest output is subtracted from the address of the center of the fovea,
and the result is stored as a constant offset. This offset is then added to the addresses of
all incoming events from the OR, resulting in a shift of the OR image in the virtual visual
field so that the fovea will be positioned over the region of highest salience. The output
from the 1,650 IFAT neurons in the third stage network is shown in Figure 7b. With a
32 ? 32 pixel high-resolution fovea, the network allows for a 66% reduction in the number
of address-events required to reconstruct the image.

4

Conclusion

We have demonstrated a multi-chip neuromorphic system for performing saliency-based
spatial acuity modulation. An asynchronous imager provides the input and communicates
with a reconfigurable array of spiking silicon neurons using address-events. The resulting
output is useful for efficient spatial and temporal bandwidth allocation in low-power vision
sensors for wide-angle video surveillance. Future work will concentrate on extending the
functionality of the multi-chip system to perform stereo processing on address-event data
from two imagers.

(a)

(b)

Figure 6: (a) Stage 2 network for computing local changes in contrast. Blocks of 8 ? 8
IFAT neurons from stage 1 (shown as regions alternately shaded white and gray) project
to single IFAT neurons in stage 2 (not shown). Blocks are shown as non-overlapping for
clarity. (b) Output of stage 2, as implemented on the IFAT, with Figure 5b from stage 1 as
input.
Acknowledgments
This work was partially funded by NSF Awards #0120369, #9896362, and IIS-0209289;
ONR Award #N00014-99-1-0612; and a DARPA/ONR MURI #N00014-95-1-0409. Additionally, RJV is supported by an NSF Graduate Research Fellowship.

References
[1] E. Culurciello, R. Etienne-Cummings, and K. A. Boahen, ?A biomorphic digital image sensor,?
IEEE J. Solid-State Circuits, vol. 38, no. 2, 2003.
[2] M. Sivilotti, Wiring considerations in analog VLSI systems, with application to fieldprogrammable networks. PhD thesis, California Institute of Technology, Pasadena, CA, 1991.
[3] M. Mahowald, An analog VLSI system for stereoscopic vision. Boston, MA: Kluwer Academic
Publishers, 1994.
[4] J. Lazzaro, J. Wawrzynek, M. Mahowald, M. Sivilotti, and D. Gillespie, ?Silicon auditory processors as computer peripherals,? IEEE Trans. Neural Networks, vol. 4, no. 3, pp. 523?528,
1993.
[5] K. A. Boahen, ?Point-to-point connectivity between neuromorphic chips using address events,?
IEEE Trans. Circuits & Systems II, vol. 47, no. 5, pp. 416?434, 2000.
[6] C. M. Higgins and C. Koch, ?Multi-chip neuromorphic motion processing,? in Proc. 20th Anniversary Conference on Advanced Research in VLSI (D. S. Wills and S. P. DeWeerth, eds.),
(Los Alamitos, CA), pp. 309?323, IEEE Computer Society, 1999.
[7] S.-C. Liu, J. Kramer, G. Indiveri, T. Delbru? ck, and R. Douglas, ?Orientation-selective aVLSI
spiking neurons,? in Advances in Neural Information Processing Systems 14 (T. G. Dietterich,
S. Becker, and Z. Ghahramani, eds.), Cambridge, MA: MIT Press, 2002.
[8] G. Indiveri, A. M. Whatley, and J. Kramer, ?A reconfigurable neuromorphic VLSI multi-chip
system applied to visual motion computation,? in Proc. MicroNeuro?99, Apr. 1999.
[9] D. H. Goldberg, G. Cauwenberghs, and A. G. Andreou, ?Probabilistic synaptic weighting in a
reconfigurable network of VLSI integrate-and-fire neurons,? Neural Networks, vol. 14, no. 6-7,
pp. 781?793, 2001.

(a)

(b)

Figure 7: (a) Stage 3 foveation network. The 32 ? 32 pixel high-resolution fovea (center)
is surrounded by lower-resolution areas where 2 ? 2, 4 ? 4, and 8 ? 8 blocks of OR
neurons (shown as non-overlapping for clarity) project to single IFAT cells. The address
space for inputs to the foveation network is 128 ? 128. [18]. (b) Output of stage 3, as
implemented on the IFAT, with the fovea centered on the location with the maximum firing
rate in Figure 6b, from stage 2. Peripheral pixels that receive no input are not shown.
[10] S. R. Deiss, R. J. Douglas, and A. M. Whatley, ?A pulse-coded communications infrastructure
for neuromorphic systems,? in Pulsed Neural Networks (W. Maass and C. M. Bishop, eds.),
pp. 157?178, Cambridge, MA: MIT Press, 1999.
[11] M. Mahowald and R. Douglas, ?A silicon neuron,? Nature, vol. 354, pp. 515?518, 1991.
[12] R. J. Vogelstein, F. Tenore, R. Philipp, M. S. Adlerstein, D. H. Goldberg, and G. Cauwenberghs,
?Spike timing-dependent plasticity in the address domain,? in Advances in Neural Information
Processing Systems 15 (S. Becker, S. Thrun, and K. Obermayer, eds.), Cambridge, MA: MIT
Press, 2003.
[13] R. J. Vogelstein, U. Mallik, and G. Cauwenberghs, ?Silicon spike-based synaptic array and
address-event transceiver,? in Proc. ISCAS?04, vol. 5, (Vancouver, BC), pp. 385?388, 2004.
[14] C. Koch, Biophysics of Computation: Information Processing in Single Neurons. New York,
NY: Oxford University Press, 1999.
[15] C. Koch and S. Ullman, ?Shifts in selective visual attention: towards the underlying neural
circuitry,? Human Neurobiology, vol. 4, pp. 219?227, 1985.
[16] L. Itti, E. Niebur, and C. Koch, ?A model of saliency-based fast visual attention for rapid scene
analysis,? IEEE Trans. Pattern Analysis & Machine Intelligence, vol. 20, no. 11, pp. 1254?1259,
1998.
[17] T. Horiuchi, T. Morris, C. Koch, and S. P. DeWeerth, ?Analog VLSI circuits for attentionbased, visual tracking,? in Advances in Neural Information Processing Systems 9, pp. 706?712,
Cambridge, MA: MIT Press, 1997.
[18] R. J. Vogelstein, U. Mallik, E. Culurciello, G. Cauwenberghs, and R. Etienne-Cummings, ?Spatial acuity modulation of an address-event imager,? in ICECS?04, 2004.
[19] R. J. Vogelstein, U. Mallik, and G. Cauwenberghs, ?Reconfigurable silicon array of spiking
neurons,? IEEE Trans. Neural Networks, 2005. (Submitted).
[20] E. Culurciello, R. Etienne-Cummings, and K. Boahen, ?Second generation of high dynamic
range, arbitrated digital imager,? in Proc. ISCAS?04, vol. 4, (Vancouver, BC), pp. 828?831,
2004.

"
127,1995,Stock Selection via Nonlinear Multi-Factor Models,Abstract Missing,"Stock Selection via Nonlinear
Multi-Factor Models
Asriel U. Levin
BZW Barclays Global Investors
Advanced Strategies and Research Group
45 Fremont Street
San Francisco CA 94105
email: asriel.levin@bglobal.com

Abstract
This paper discusses the use of multilayer feed forward neural networks for predicting a stock's excess return based on its exposure
to various technical and fundamental factors. To demonstrate the
effectiveness of the approach a hedged portfolio which consists of
equally capitalized long and short positions is constructed and its
historical returns are benchmarked against T-bill returns and the
S&P500 index.

1

Introduction

Traditional investment approaches (Elton and Gruber, 1991) assume that the return
of a security can be described by a multifactor linear model:

(1)
where Hi denotes the return on security i, Fl are a set of factor values and Uil are
security i exposure to factor I, ai is an intercept term (which under the CAPM
framework is assumed to be equal to the risk free rate of return (Sharpe, 1984))
and ei is a random term with mean zero which is assumed to be uncorrelated across
securities.
The factors may consist of any set of variables deemed to have explanatory power for
security returns . These could be aspects of macroeconomics, fundamental security
analysis, technical attributes or a combination of the above. The value of a factor
is the expected excess return above risk free rate of a security with unit exposure to
the factor and zero exposure to all other factors. The choice offactors can be viewed
as a proxy for the"" state of the world"" and their selection defines a metric imposed
on the universe of securities: Once the factors are set, the model assumption is that,

967

Stock Selection via Nonlinear Multi-factor Models

on average, two securities with similar factor loadings
manner.

(Uil)

will behave in a similar

The factor model (1) was not originally developed as a predictive model, but rather
as an explanatory model, with the returns It; and the factor values Pi assumed to
be contemporaneous. To utilize (1) in a predictive manner, each factor value must
be replaced by an estimate, resulting in the model
A

It;

A

A

= ai + UilFl + Ui2 F 2 + ... + UiLFL + ei

(2)

where Ri is a security's future return and F/ is an estimate of the future value
of factor 1, based on currently available information. The estimation of Fl can be
approached with varying degree of sophistication ranging from a simple use of the
historical mean to estimate the factor value (setting Fl(t) = Fi), to more elaborate
approaches attempting to construct a time series model for predicting the factor
values.
Factor models of the form (2) can be employed both to control risk and to enhance
return. In the first case, by capturing the major sources of correlation among
security returns, one can construct a well balanced portfolio which diversifies specific
risk away. For the latter, if one is able to predict the likely future value of a factor,
higher return can be achieved by constructing a portfolio that tilts toward ""good""
factors and away from ""bad"" ones.
While linear factor models have proven to be very useful tools for portfolio analysis
and investment management, the assumption of linear relationship between factor
values and expected return is quite restrictive. Specifically, the use of linear models
assumes that each factor affects the return independently and hence, they ignore the
possible interaction between different factors. Furthermore, with a linear model, the
expected return of a security can grow without bound as its exposure to a factor
increases. To overcome these shortcomings of linear models, one would have to
consider more general models that allow for nonlinear relationship among factor
values, security exposures and expected returns.
Generalizing (2), while maintaining the basic premise that the state of the world can
be described by a vector of factor values and that the expected return of a security
is determined through its coordinates in this factor world, leads to the nonlinear
model:
It; = j(Uil' Ui2,???, UiL, Fl , F2, ... , FL ) + ei
(3)
where JO is a nonlinear function and
""security specific risk"" .

ei

is the noise unexplained by the model, or

The prediction task for the nonlinear model (3) is substantially more complex than
in the linear case since it requires both the estimation of future factor values as
well as a determination of the unknown function j. The task can be somewhat
simplified if factor estimates are replaced with their historical means:
It;

J(Uil, Ui2, ... , UiL,

lA, F2, ... , FL) + ei
(4)

where now Uil are the security's factor exposure at the beginning of the period over
which we wish to predict.
To estimate the unknown function t(-), a family of models needs to be selected,
from which a model is to be identified. In the following we propose modeling the relationship between factor exposures and future returns using the class of multilayer
feedforward neural networks (Hertz et al., 1991). Their universal approximation

968

A. U. LEVIN

capabilities (Cybenko, 1989; Hornik et al., 1989), as well as the existence of an effective parameter tuning method (the backpropagation algorithm (Rumelhart et al.,
1986)) makes this family of models a powerful tool for the identification of nonlinear
mappings and hence a natural choice for modeling (4).

2

The stock selection problem

Our objective in this paper is to test the ability of neural network based models
of the form (4) to differentiate between attractive and unattractive stocks. Rather
than trying to predict the total return of a security, the objective is to predict its
performance relative to the market, hence eliminating the need to predict market
directions and movements.
The data set consists of monthly historical records (1989 through 1995) for the
largest 1200-1300 US companies as defined by the BARRA HiCap universe. Each
data record (::::::1300 per month) consists of an input vector composed of a security's
factor exposures recorded at the beginning of the month and the corresponding
output is the security's return over the month. The factors used to build the model
include Earning/Price, Book/Price, past price performance, consensus of analyst
sentiments etc, which have been suggested in the financial literature as having
explanatory power for security returns (e.g. (Fama and French, 1992)). To minimize
risk, exposure to other unwarranted factors is controlled using a quadratic optimizer.

3

Model construction and testing

Potentially, changes in a price of a security are a function of a very large number of
forces and events, of which only a small subset can be included in the factor model
(4). All other sources of return play the role of noise whose magnitude is probably
much larger than any signal that can be explained by the factor exposures. When
this information is used to train a neural network, the network attempts to replicate
the examples it sees and hence much of what it tries to learn will be the particular
realizations of noise that appeared in the training set.
To minimize this effect, both a validation set and regularization are used in the
training. The validation set is used to monitor the performance of the model with
data on which it has not been trained on. By stopping the learning process when
validation set error starts to increase, the learning of noise is minimized. Regularization further limits the complexity of the function realized by the network and,
through the reduction of model variance, improves generalization (Levin et al.,
1994).
The stock selection model is built using a rolling train/test window. First, M
""two layer"" feedforward networks are built for each month of data (result is rather
insensitive to the particular choice of M). Each network is trained using stochastic
gradient descent with one quarter of the monthly data (randomly selected) used as
a validation set. Regularization is done using principal component pruning (Levin
et al., 1994). Once training is completed, the models constructed over N consecutive
month of data (again, result is insensitive to particular choice of N) are combined
(thus increasing the robustness of the model (Breiman, 1994)) to predict the returns
in the following month. Thus the predicted (out of sample) return of stock i in
month k is given by

(5)

969

Stock Selection via Nonlinear Multi-factor Models

0.4
Nonlinear
Linear

0.35
0.3
c

0
:;:::;
ctI

........Q)
0

()

0.25
0.2
0.15

~

-.,

I :
i !
r---: !

0.1

,,,,,

0.05

!i

r---l

r- -I i -r- ~mf

i

0

,

I

L _J

0

5

10
Cell

bl

1! i

-I! I !

----_]'-T_-I

':

-l-.+-r-+-,-...L-J.-l-

-

15

:

-L J.

20

Figure 1 : Average correlation between predicted alphas and realized returns for
linear and nonlinear models
where k(k) is stock's i predicted return, N Nk-j(?) denoted the neural network
model built in month k - j and u71 are stock's i factor exposures as measured at
the beginning of month k.

4

Benchmarking to linear

As a first step in evaluating the added value of the nonlinear model, its performance
was benchmarked against a generalized least squares linear model. Each model was
run over three universes: all securities in the HiCap universe, the extreme 200 stocks
(top 100, bottom 100 as defined by each model), and the extreme 100 stocks. As
a comparative performance measure we use the Sharpe ratio (Elton and Gruber,
1991). As shown in Table 4, while the performance of the two models is quite
comparable over the whole universe of stocks, the neural network based model
performs better at the extremes, resulting in a substantially larger Sharpe ratio
(and of course, when constructing a portfolio , it is the extreme alphas that have
the most impact on performance).

I Portfolio\Model
All HiCap
100 long/100 short
50 long/50 short

II

Linear
6.43
4.07
3.07

Nonlinear
6.92
5.49
4.23

II

Table 1: Ex ante Sharpe ratios: Neural network vs. linear
While the numbers in the above table look quite impressive, it should be emphasised
that they do not represent returns of a practical strategy: turnover is huge and the
figures do not take transaction costs into account. The main purpose of the table

A. U. LEVIN

970

is to compare the information that can be captured by the different models and
specifically to show the added value of the neural network at the extremes. A
practical implementation scheme and the associated performance will be discussed
in the next section.
Finally, some insight as to the reason for the improved performance can be gained
by looking at the correlation between model predictions and realized returns for
different values of model predictions (commonly referred to as alphas). For that,
the alpha range was divided to 20 cells, 5% of observations in each and correlations
were calculated separately for each cell. As is shown in figure 1, while both neural
network and linear model seem to have more predictive power at the extremes, the
network's correlations are substantially larger for both positive and negative alphas.

5

Portfolio construction

Given the superior predictive ability of the nonlinear model at the extremes, a
natural way of translating its predictions into an investment strategy is through the
use of a long/short construct which fully captures the model information on both
the positive as well as the negative side.
The long/short portfolio (Jacobs and Levy, 1993) is constructed by allocating equal
capital to long and short positions. By monitoring and controlling the risk characteristics on both sides, one is able to construct a portfolio that has zero correlation
with the market ((3 = 0) - a ""market neutral"" portfolio. By construction, the return of a market neutral portfolio is insensitive to the market up or down swings
and its only source of return is the performance spread between the long and short
positions, which in turn is a direct function of the model (5) discernment ability.
Specifically, the translation of the model predictions into a realistically implementable strategy is done using a quadratic optimizer. Using the model predicted
returns and incorporating volatility information about the various stocks, the optimizer is utilized to construct a portfolio with the following characteristics:
? Market neutral (equal long and short capitalization).
? Total number of assets in the portfolio

<= 200.

? Average (one sided) monthly turnover

~

15%.

? Annual active risk ~ 5%.

In the following, all results are test set results (out of sample), net of estimated
transaction costs (assumed to be 1.5% round trip). The standard benchmark for
a market neutral portfolio is the return on 3 month T-bill and as can be seen
in Table 2, over the test period the market neutral portfolio has consistently and
decisively outperformed its benchmark. Furthermore, the results reported for 1995
were recorded in real-time (simulated paper portfolio).
An interesting feature of the long/short construct is its ease of transportability (Jacobs and Levy, 1993). Thus, while the base construction is insensitive to market
movement, if one wishes, full exposure to a desired market can be achieved through
the use of futures or swaps (Hull, 1993). As an example, by adding a permanent
S&P500 futures overlay in an amount equal to the invested capital, one is fully
exposed to the equity market at all time , and returns are the sum of the long/short
performance spread and the profits or losses resulting from the market price movements. This form of a long/short strategy is referred to as an ""equitized"" strategy
and the appropriate benchmark will be overlayed index. The relative performance

Stock Selection via Nonlinear Multi-factor Models

I Statistics

II

Total Return~%)
Annual total(Yr%)
Active Return(%)
Annual active(Yr%)
Active risk(Yr%)
Max draw down(%)
Turnover(Yr%)

T-Bill
27.8
4.6

971

I Neutral II
131.5
16.8
103.7
12.2
4.8
3.2
198.4

-

S&P500
102.0
10.4

-

13.9

-

I Equitized II
264.5
27.0
162.5
16.6
4.8
10.0
198.4

Table 2: Comparative summary of ex ante portfolio performance (net of transaction
costs) 8/90 - 12/95
4
3.5
3
(I)
;:)

?i

>

Equitized --SP500 -+--Neutral .-0 . .. .
T-bill ......_.

2.5

.2

~0

2

0..
C])

>

~
""S
E

1.5

::I
()

91

92

93

94

95

96

Year

Figure 2: Cumulative portfolio value 8/90 - 12/95 (net of estimated transaction
costs)
of the equitized strategy with an S&P500 futures overlay is presented in Table 2.
Summary of the accumulated returns over the test period for the market neutral
and equitized portfolios compared to T-bill and S&P500 are given in Figure 2.
Finally, even though the performance of the model is quite good, it is very difficult
to convince an investor to put his money on a ""black box"". A rather simple way to
overcome this problem of neural networks is to utilize a CART tree (Breiman et aI.,
1984) to explain the model's structure. While the performance of the tree on the
raw data in substantially inferior to the network's, it can serve as a very effective
tool for analyzing and interpreting the information that is driving the model.

6

Conclusion

We presented a methodology by which neural network based models can be used
for security selection and portfolio construction. In spite of the very low signal to
noise ratio of the raw data, the model was able to extract meaningful relationship

972

A. U. LEVIN

between factor exposures and expected returns. When utilized to construct hedged
portfolios, these predictions achieved persistent returns with very favorable risk
characteristics.
The model is currently being tested in real time and given its continued consistent
performance, is expected to go live soon.

References
Anderson, J. and Rosenfeld, E., editors (1988) . Neurocomputing: Foundations of
Research. MIT Press, Cambridge.
Breiman, L. (1994) . Bagging predictors. Technical Report 416, Department of
Statistics, VCB, Berkeley, CA.
Breiman, L., Friedman, J ., Olshen, R., and Stone, C. (1984). Classification and
Regression Trees. Chapman & Hall.
Cybenko, G . (1989) . Approximation by superpositions of a sigmoidal function .
Mathematics of Control, Signals, and Systems, 2:303-314.
Elton , E. and Gruber, M. (1991). Modern Portfolio Theory and Investment Analysis.
John Wiley.
Fama, E. and French, K. (1992). The cross section of expected stock returns. Journal
of Finance, 47:427- 465 .
Hertz, J., Krogh, A., and Palmer, R. (1991). Introduction to the theory of neural
computation, volume 1 of Santa Fe Institute studies in the sciences ofcomplexity. Addison Wesley Pub. Co.
Hornik, K. , Stinchcombe, M., and White, H. (1989). Multilayer feedforward networks are universal approximators. Neural Networks, 2:359-366.
Hull, J . (1993). Options, Futures and Other Derivative Securities. Prentice-Hall.
Jacobs , B. and Levy, K. (1993). Long/short equity investing. Journal of Portfolio
Management, pages 52-63.
Levin, A. V., Leen, T. K., and Moody, J . E. (1994) . Fast pruning using principal
components. In Cowan, J . D., Tesauro, G., and Alspector, J., editors , Advances
in Neural Information Processing Systems, volume 6. Morgan Kaufmann. to
apear.
Rumelhart, D., Hinton, G., and Williams, R. (1986) . Learning representations by
back-propagating errors. Nature, 323:533- 536. Reprinted in (Anderson and
Rosenfeld, 1988).
Sharpe, W . (1984) . Factor models, CAPMs and the APT. Journal of Portfolio
Management, pages 21-25.

"
4692,2014,Weighted importance sampling for off-policy learning with linear function approximation,"Importance sampling is an essential component of off-policy model-free reinforcement learning algorithms. However, its most effective variant, \emph{weighted} importance sampling, does not carry over easily to function approximation and, because of this, it is not utilized in existing off-policy learning algorithms. In this paper, we take two steps toward bridging this gap. First, we show that weighted importance sampling can be viewed as a special case of weighting the error of individual training samples, and that this weighting has theoretical and empirical benefits similar to those of weighted importance sampling. Second, we show that these benefits extend to a new weighted-importance-sampling version of off-policy LSTD(lambda). We show empirically that our new WIS-LSTD(lambda) algorithm can result in much more rapid and reliable convergence than conventional off-policy LSTD(lambda) (Yu 2010, Bertsekas & Yu 2009).","Weighted importance sampling for off-policy learning
with linear function approximation
A. Rupam Mahmood, Hado van Hasselt, Richard S. Sutton
Reinforcement Learning and Artificial Intelligence Laboratory
University of Alberta
Edmonton, Alberta, Canada T6G 1S2
{ashique,vanhasse,sutton}@cs.ualberta.ca

Abstract
Importance sampling is an essential component of off-policy model-free reinforcement learning algorithms. However, its most effective variant, weighted importance sampling, does not carry over easily to function approximation and, because of this, it is not utilized in existing off-policy learning algorithms. In this
paper, we take two steps toward bridging this gap. First, we show that weighted
importance sampling can be viewed as a special case of weighting the error of
individual training samples, and that this weighting has theoretical and empirical benefits similar to those of weighted importance sampling. Second, we show
that these benefits extend to a new weighted-importance-sampling version of offpolicy LSTD( ). We show empirically that our new WIS-LSTD( ) algorithm can
result in much more rapid and reliable convergence than conventional off-policy
LSTD( ) (Yu 2010, Bertsekas & Yu 2009).

1

Importance sampling and weighted importance sampling

Importance sampling (Kahn & Marshall 1953, Rubinstein 1981, Koller & Friedman 2009) is a wellknown Monte Carlo technique for estimating an expectation under one distribution given samples
from a different distribution. Consider that data samples Yk 2 R are generated i.i.d. from a sample
.
distribution l, but we are interested in estimating the expected value of these samples, vg = E g [Yk ],
under a different distribution g. In importance sampling this is achieved simply by averaging the
.
k)
samples weighted by the ratio of their likelihoods ?k = g(Y
l(Yk ) , called the importance-sampling
ratio. That is, vg is estimated as:
Pn
?k Y k
.
v?g = k=1
.
(1)
n
This is an unbiased estimate because each of the samples it averages is unbiased:
Z
Z
g(y)
E l [?k Yk ] = l(y)
y dy = g(y)y dy = E g [Yk ] = vg .
l(y)
y
y
Unfortunately, this importance sampling estimate is often of unnecessarily high variance. To see
how this can happen, consider a case in which the samples Yk are all nearly the same (under both
distributions) but the importance-sampling ratios ?k vary greatly from sample to sample. This should
be an easy case because the samples are so similar for the two distributions, but importance sampling
will average the ?k Yk , which will be of high variance, and thus its estimates will also be of high
variance. In fact, without further bounds on the importance-sampling ratios, v?g may have infinite
variance (Andrad?ottir et al. 1995, Robert & Casella 2004).

An important variation on importance sampling that often has much lower variance is weighted importance sampling (Rubinstein 1981, Koller & Friedman 2009). The weighted importance sampling
1

(WIS) estimate vg as a weighted average of the samples with importance-sampling ratios as weights:
Pn
.
k=1 ?k Yk
v?g = P
.
n
k=1 ?k

This estimate is biased, but consistent (asymptotically correct) and typically of much lower variance
than the ordinary importance-sampling (OIS) estimate, as acknowledged by many authors (Hesterberg 1988, Casella & Robert 1998, Precup, Sutton & Singh 2000, Shelton 2001, Liu 2001, Koller
& Friedman 2009). For example, in the problematic case sketched above (near constant Yk , widely
varying ?k ) the variance of the WIS estimate will be related to the variance of Yk . Note also that
when the samples are bounded, the WIS estimate has bounded variance, because the estimate itself
is bounded by the highest absolute value of Yk , no matter how large the ratios ?k are (Precup, Sutton
& Dasgupta 2001).
Although WIS is the more successful importance sampling technique, it has not yet been extended
to parametric function approximation. This is problematic for applications to off-policy reinforcement learning, in which function approximation is viewed as essential for large-scale applications
to sequential decision problems with large state and action spaces. Here an important subproblem is
the approximation of the value function?the expected sum of future discounted rewards as a function of state?for a designated target policy that may differ from that used to select actions. The
existing methods for off-policy value-function approximation either use OIS (Maei & Sutton 2010,
Yu 2010, Sutton et al. 2014, Geist & Scherrer 2014, Dann et al. 2014) or use WIS but are limited
to the tabular or non-parametric case (Precup et al. 2000, Shelton 2001). How to extend WIS to
parametric function approximation is important, but far from clear (as noted by Precup et al. 2001).

2

Importance sampling for linear function approximation

In this section, we take the first step toward bridging the gap between WIS and off-policy learning
with function approximation. In a general supervised learning setting with linear function approximation, we develop and analyze two importance-sampling methods. Then we show that these two
methods have theoretical properties similar to those of OIS and WIS. In the fully-representable case,
one of the methods becomes equivalent to the OIS estimate and the other to the WIS estimate.
The key idea is that OIS and WIS can be seen as least-squares solutions to two different empirical
objectives. The OIS estimate is the least-squares solution to an empirical mean-squared objective
where the samples are importance weighted:
Pn
n
n
X
?k Y k
1X
2
v?g = arg min
(?k Yk v) =)
(?k Yk v?g ) = 0 =) v?g = k=1
. (2)
n
n
v
k=1

k=1

Similarly, the WIS estimate is the least-squares solution to an empirical mean-squared objective
where the individual errors are importance weighted:
Pn
n
n
X
1X
2
k=1 ?k Yk
v?g = arg min
?k (Yk v) =)
?k (Yk v?g ) = 0 =) v?g = P
. (3)
n
n
v
k=1 ?k
k=1

k=1

We solve similar empirical objectives in a general supervised learning setting with linear function
approximation to derive the two new methods.
Consider two correlated random variables Xk and Yk , where Xk takes values from a finite set X ,
and where Yk 2 R. We want to estimate the conditional expectation of Yk for each x 2 X under
a target distribution gY |X . However, the samples (Xk , Yk ) are generated i.i.d. according to a joint
sample distribution lXY (?) with conditional probabilities lY |X that may differ from the conditional
.
target distribution. Each input is mapped to a feature vector k = (Xk ) 2 Rm , and the goal is to
estimate the expectation E gY |X [Yk | Xk = x] as a linear function of the features
.
? > (x) ? vg (x) = E gY |X [Yk |Xk = x] .

Estimating this expectation is again difficult because the target joint distribution of the input-output
pairs gXY can be different than the sample joint distribution lXY . Generally, the discrepancy in
2

the joint distribution may arise from two sources: difference in marginal distribution of inputs,
gX 6= lX , and difference in the conditional distribution of outputs, gY |X 6= lY |X . Problems where
only the former discrepancy arise are known as covariate shift problems (Shimodaira 2000). In
these problems the conditional expectation of the outputs is assumed unchanged between the target
and the sample distributions. In off-policy learning problems, the discrepancy between conditional
probabilities is more important. Most off-policy learning methods correct only the discrepancy
between the target and the sample conditional distributions of outputs (Hachiya et al. 2009, Maei &
Sutton 2010, Yu 2010, Maei 2011, Geist & Scherrer 2014, Dann et al. 2014). In this paper, we also
focus only on correcting the discrepancy between the conditional distributions.
The problem of estimating vg (x) as a linear function of features using samples generated from l can
be formulated as the minimization of the mean squared error (MSE) where the solution is as follows:
??
?2
?
? 1
?
?
?? =
? arg min E lX E gY |X [Yk |Xk ] ? > k
= E lX k >
E lX E gY |X [Yk |Xk ] k . (4)
k
?

Similar to the empirical mean-squared objectives defined in (2) and (3), two different empirical
objectives can be defined to approximate this solution. In one case the importance weighting is
applied to the output samples, Yk , and in the other case the importance weighting is applied to the
error, Yk ? > k ,
n ?
n
?2
?
?2
. 1X
. 1X
J?n (?) =
?k Y k ? > k ;
J?n (?) =
?k Y k ? > k ,
n
n
k=1

k=1

.
where importance-sampling ratios are defined by ?k = gY |X (Yk |Xk )/lY |X (Yk |Xk ).

We can minimize these objectives by equating the derivatives of the above empirical objectives to
zero. Provided the relevant matrix inverses exist, the resulting solutions are, respectively
! 1 n
n
X
X
.
>
?n =
?
?k Yk k , and
(5)
k
k

k=1

.
?n =
?

n
X

k=1

?k

k

>
k

!

k=1
1 n

X

?k Y k

k

.

(6)

k=1

? the OIS-LS estimator and ?
? the WIS-LS estimator.
We call ?
A least-squares method similar to WIS-LS above was introduced for covariate shift problems by
Hachiya, Sugiyama and Ueda (2012). Although superficially similar, that method uses importancesampling ratios to correct for the discrepancy in the marginal distributions of inputs, whereas
WIS-LS corrects for the discrepancy in the conditional expectations of the outputs. For the fullyrepresentable case, unlike WIS-LS, the method of Hachiya et al. becomes an ordinary Monte Carlo
estimator with no importance sampling.

3

Analysis of the least-squares importance-sampling methods

The two least-squares importance-sampling methods have properties similar to those of the OIS and
the WIS methods. In Theorems 1 and 2, we prove that when vg can be represented as a linear
function of the features, then OIS-LS is an unbiased estimator of ? ? , whereas WIS-LS is a biased
estimator, similar to the WIS estimator. If the solution is not linearly representable, least-squares
methods are not generally unbiased. In Theorem 3 and 4, we show that both least-squares estimators
are consistent for ? ? . Finally, we demonstrate that the least-squares methods are generalizations of
OIS and WIS by showing, in Theorem 5 and 6, that in the fully representable case (when the features
form an orthonormal basis) OIS-LS is equivalent to OIS and WIS-LS is equivalent to WIS.
Theorem 1. If vg is a linear function of the features, that is, vg (x) = ? >
? (x), then OIS-LS is an
? n ] = ?? .
unbiased estimator, that is, E lXY [?
Theorem 2. Even if vg is a linear function of the features, that is, vg (x) = ? >
? (x), WIS-LS is in
? n ] 6= ? ? .
general a biased estimator, that is, E lXY [?
3

? n is a consistent estimator of the MSE solution ? ? given in (4).
Theorem 3. The OIS-LS estimator ?
? n is a consistent estimator of the MSE solution ? ? given in (4).
Theorem 4. The WIS-LS estimator ?

? > (x) of input
Theorem 5. If the features form an orthonormal basis, then the OIS-LS estimate ?
n
x is equivalent to the OIS estimate of the outputs corresponding to x.
? > (x) of input
Theorem 6. If the features form an orthonormal basis, then the WIS-LS estimate ?
n
x is equivalent to the WIS estimate of the outputs corresponding to x.
Proofs of Theorem 1-6 are given in the Appendix.
The WIS-LS estimate is perhaps the most interesting of the two least-squares estimates, because it
generalizes WIS to parametric function approximation for the first time and extends its advantages.

4

A new off-policy LSTD( ) with WIS

In sequential decision problems, off-policy learning methods based on important sampling can suffer
from the same high-variance issues as discussed above for the supervised case. To address this, we
extend the idea of WIS-LS to off-policy reinforcement learning and construct a new off-policy WISLSTD( ) algorithm.
We first explain the problem setting. Consider a learning agent that interacts with an environment
where at each step t the state of the environment is St and the agent observes a feature vector
.
m
t = (St ) 2 R . The agent takes an action At based on a behavior policy b(?|St ), that is typically
a function of the state features. The environment provides the agent a scalar (reward) signal Rt+1
and transitions to state St+1 . This process continues, generating a trajectory of states, actions and
rewards. The goal is to estimate the values of the states under the target policy ?, defined as the
expected returns given by the sum of future discounted rewards:
""1
#
t
X
Y
.
v? (s) = E
Rt+1
(Sk ) | S0 = s, At ? ?(?|St ), 8t ,
t=0

k=1

where (Sk ) 2 [0, 1] is a state-dependent degree of discounting on arrival in Sk (as in Sutton et al.
2014). We assume the rewards and discounting are chosen such that v? (s) is well-defined and finite.
Our primary objective is to estimate v? as a linear function of the features: v? (s) ? ? > (s), where
? 2 Rm is a parameter vector to be estimated. As before, we need to correct for the difference
in sample distribution resulting from the behavior policy and the target distribution as induced by
the target policy. Consider a partial trajectory from time step k to time t, consisting of a sequence
Sk , Ak , Rk , Sk+1 , . . . , St . The probability of this trajectory occurring given it starts at Sk under the
target policy will generally differ from its probability under the behavior policy. The importancesampling ratio ?tk is defined to be the ratio of these probabilities. This importance-sampling ratio
can be written in terms of the product of action-selection probabilities without needing a model of
the environment (Sutton & Barto 1998):
Qt 1
t 1
t 1
?(Ai |Si ) Y ?(Ai |Si ) Y
t .
?k = Qi=k
=
=
?i ,
t 1
b(Ai |Si )
i=k b(Ai |Si )
i=k

i=k

.
where we use the shorthand ?i = ?i+1
= ?(Ai |Si )/b(Ai |Si ).
i

We incorporate a common technique to reinforcement learning (RL) where updates are estimated
by bootstrapping, fully or partially, on previously constructed state-value estimates. Bootstrapping
potentially reduces the variance of the updates compared to using full returns and makes RL algorithms applicable to non-episodic tasks. In this paper we assume that the bootstrapping parameter
(s) 2 [0, 1] may depend on the state s (as in Sutton & Barto 1998, Maei & Sutton 2010). In the
.
.
following, we use the notational shorthands k = (Sk ) and k = (Sk ).
Following Sutton et al. (2014), we construct an empirical loss as a sum of pairs of squared corrected
and uncorrected errors, each corresponding to a different number of steps of lookahead, and each
.
weighted as a function of the intervening discounting and bootstrapping. Let Gtk = Rk+1 + . . . + Rt
be the undiscounted return truncated after looking ahead t k steps. Imagine constructing the
4

empirical loss for time 0. After leaving S0 and observing R1 and S1 , the first uncorrected error is
G10 ? > 0 , with weight equal to the probability of terminating 1
1 . If we do not terminate, then
we continue to S1 and form the first corrected error G10 + v > 1 ? > 0 using the bootstrapping
estimate v > 1 . The weight on this error is 1 (1 1 ), and the parameter vector v may differ from ?.
Continuing to the next time step, we obtain the second uncorrected error G20 ? > 0 and the second
corrected error G20 +v > 2 ? > 0 , with respective weights 1 1 (1 2 ) and 1 1 2 (1
2 ). This
goes on until we reach the horizon of our data, say at time t, when we bootstrap fully with v > t ,
generating a final corrected return error of Gt0 + v > t ? > 0 with weight 1 1 ? ? ? t 1 t 1 t .
.
The general form for the uncorrected error is ?tk (?) = Gtk ? > k , and the general form for the
.
corrected error is ?kt (?, v) = Gtk + v > t ? > k . All these errors could be squared, weighted by
their weights, and summed to form the overall empirical loss. In the off-policy case, we need to also
weight the squares of the errors ?tk and ?kt by the importance-sampling ratio ?tk . Hence, the overall
empirical loss at time k for data up to time t can be written as
`tk (?, v)

t 1
X
.
= ?k
Cki

1

i=k+1

+

?k Ckt 1

?

(1

h

i)

(1

?

t)

?ik (?)

?2

2
?tk (?)

+
+

i (1

i)

?

?ki (?, v)

?kt (?, v) 2

t

i

?2

t
. Y
, where Ckt =

j

j ?j .

j=k+1

This loss differs from that used by other LSTD( ) methods in that importance weighting is applied
to the individual errors within `tk (?, v).
Now, we are ready to state the least-squares problem. As noted by Geist & Scherrer (2014), LSTD( )
methods can be derived by solving least-squares problems where estimates at each step are matched
with multi-step returns starting from those steps given that bootstrapping is done using the solution
itself. Our proposed new method, called WIS-LSTD( ), computes at each time t the solution to the
least-squares problem:
t 1
X
.
? t = arg min
`tk (?, ? t ).
?

k=0

At the solution, the derivative of the objective is zero:
Pt 1 ?
?
k=0 2 k,t (? t , ? t ) k = 0, where the errors k,t are defined by
?
k,t (?, v)

t 1
X
.
= ?k
Cki
i=k+1

1

?

Next, we separate the terms of
?
k,t (? t , ? t )

k

1

+

?i
i ) k (?, v)

i (1

+ ?k Ckt
?
k,t (? t , ? t )

k

?

1

?

Pt

1 t
k=0 `k (?, ? t ) ?=? t

t
t )?k (?)

(1

?t
t k (?, v)

+

that involve ? t from those that do not:

=

?

.

Ak,t ? t , where bk,t 2 Rm , Ak,t 2 Rm?m and they are defined as

= bk,t

t 1
X
.
bk,t = ?k
Cki

i
i )?k (?)

(1

@
@?

(1

i
i i )Gk

+ ?k Ckt

k

1

Gtk

k,

i=k+1

t 1
X
.
Ak,t = ?k
Cki

1

k ((1

i i)

k

i (1

i)

i)

>

+ ?k Ckt

1

k(

k

t

t)

>

.

i=k+1

Therefore, the solution can be found as follows:
t 1
X

k=0

t 1

(bk,t

. X
Ak,t ? t ) = 0 =) ? t = At 1 bt , where At =
Ak,t ,
k=0

t 1

. X
bt =
bk,t .

(7)

k=0

In the following we show that WIS-LS is a special case of the above algorithm defined by (7). As
Theorem 6 shows that WIS-LS generalizes WIS, it follows that the above algorithm generalizes WIS
as well.
Theorem 7. At termination, the algorithm defined by (7) is equivalent to the WIS-LS method in the
? t as
sense that if 0 = ? ? ? = t = 0 = ? ? ? = t 1 = 1 and t = 0, then ? t defined in (7) equals ?
.
defined in (6), with Yk = Gtk . (Proved in the Appendix).
5

Our last challenge is to find an equivalent efficient online algorithm for this method. The solution in
(7) cannot be computed incrementally in this form. When a new sample arrives at time t + 1, Ak,t+1
and bk,t+1 have to be computed for each k = 0, . . . , t, and hence the computational complexity of
this solution grows with time. It would be preferable if the solution at time t + 1 could be computed
incrementally based on the estimates from time t, requiring only constant computational complexity
per time step. It is not immediately obvious such an efficient update exists. For instance, for = 1
this method achieves full Monte Carlo (weighted) importance-sampling estimation, which means
whenever the target policy deviates from the behavior policy all previously made updates have to
be unmade so that no updates are made towards a trajectory which is impossible under the target
policy. Sutton et al. (2014) show it is possible to derive efficient updates in some cases with the use
of provisional parameters which keep track of the provisional updates that might need to be unmade
when a deviation occurs. In the following, we show that using such provisional parameters it is also
possible to achieve an equivalent efficient update for (7).
We first write both bk,t and Ak,t recursively in t (derivations in Appendix A.8):
bk,t+1 = bk,t + ?k Ckt Rt+1

k

+ (?t

t 1 t
t t ?k Ck Gk

1)

k,

>

>
Ak,t+1 = Ak,t +
1) t t ?k Ckt 1 k ( k
k( t
t+1 t+1 ) + (?t
t) .
Using the above recursions, we can write the updates of both bt and At incrementally. The vector
bt can be updated incrementally as
t
t 1
t 1
t 1
X
X
X
X
bt+1 =
bk,t+1 =
bk,t+1 + bt,t+1 =
bk,t + ?t Rt+1 t + Rt+1
?k Ckt k

?k Ckt

k=0

k=0

+ (?t

1)

t t

k=1

t 1
X

?k Ckt

1

Gtk

k=1

= bt + Rt+1 et + (?t

k

(8)

1)ut ,

k=1

where the eligibility trace et 2 Rm and the provisional vector ut 2 Rm are defined as follows:
e t = ?t

+

t

t 1
X

?k Ckt

= ?t

k

t

+ ?t

?t

t t

1

t 1

k=1

ut =

t t

t 1
X

?k Ckt 1

?k Ckt
t 2
X

1

Gtk

k

=

?t

t t

1 t 1 t 1

?k Ckt 1

k

+ ?t

1 Rt

t 1

k=1

t 2
X

?k Ckt

2

k=0

+

1

(?t

1 ut 1

k(

!

=

t

t+1

>
t+1 ) + (?t

1)

t t

t(

t t

t 1
X

t

t+1 )

t+1

+ (?t

1)Vt ,

1 t 1

k=1

+

t t

?k Ckt 1
?t

(9)

k

+ Rt e t

t

?k Ckt

t+1 )

t+1

(10)

1) .

>

1

k(

t)

k

>

k(

t 1

t)

>

+ ?t

t 1(

1

t 1

1 Vt 1

+ et

1(

t 1

t)

>

(11)

t 2
X

?k Ckt

2

k(

k

t 1)

>

k=1
t)

>

k=1

=

t t et 1 ),

k=1
>

where the provisional matrix Vt 2 Rm?m is defined as
t 1
X
>
Vt = t t
?k Ckt 1 k ( k
t ) = t t ?t 1 t
t 2
X

+

t

k=0

k=1

= At + et (

= ?t (

k=1

k=0

?k Ckt

!

Gtk

The matrix At can be updated incrementally as
t
t 1
t 1
X
X
X
At+1 =
Ak,t+1 =
Ak,t+1 + At,t+1 =
Ak,t + ?t
t 1
X

k

k=1

k=1

+ Rt

+

t 2
X

!
(12)

.

Then the parameter vector can be updated as: ? t+1 = (At+1 )

1

bt+1 .

(13)

Equations (8?13) comprise our WIS-LSTD( ). Its per-step computational complexity is O(m3 ),
where m is the number of features. The computational cost of this method does not increase with
time. At present we are unsure whether or not there is an O(m2 ) implementation.
6

Theorem 8. The off-policy LSTD( ) method defined in (8?13) is equivalent to the off-policy
LSTD( ) method defined in (7) in the sense that they compute the same ? t at each time t.
Proof. The result follows immediately from the above derivation.
It is easy to see that in the on-policy case this method becomes equivalent to on-policy LSTD( )
(Boyan 1999) by noting that the third term of both bt and At updates in (8) and (11) becomes zero,
because in the on-policy case all the importance-sampling ratios are 1.
Recently Dann et al. (2014) proposed another least-squares based off-policy method called recursive LSTD-TO( ). Unlike our algorithm, that algorithm does not specialize to WIS in the fully representable case, and it does not seem as closely related to WIS. The Adaptive Per-Decision Importance
Weighting (APDIW) method by Hachiya et al. (2009) is superficially similar to WIS-LSTD( ), there
are several important differences. APDIW is a one-step method that always fully bootstraps whereas
WIS-LSTD( ) covers the full spectrum of multi-step backups including both one-step backup and
Monte Carlo update. In the fully representable case, APDIW does not become equivalent to the WIS
estimate, whereas WIS-LSTD(1) does. Moreover, APDIW does not find a consistent estimation of
the off-policy target whereas WIS algorithms do.

5

Experimental results

We compared the performance of the proposed WIS-LSTD( ) method with the conventional offpolicy LSTD( ) by Yu (2010) on two random-walk tasks for off-policy policy evaluation. These
random-walk tasks consist of a Markov chain with 11 non-terminal and two terminal states. They
can be imagined to be laid out horizontally, where the two terminal states are at the left and the right
ends of the chain. From each non-terminal state, there are two actions available: left, which leads to
the state to the left and right, which leads to the state to the right. The reward is 0 for all transitions
except for the rightmost transition to the terminal state, where it is +1. The initial state was set to
the state in the middle of the chain. The behavior policy chooses an action uniformly randomly,
whereas the target policy chooses the right action with probability 0.99. The termination function
was set to 1 for the non-terminal states and 0 for the terminal states.
We used two tasks based on this Markov chain in our experiments. These tasks differ by how the
non-terminal states were mapped to features. The terminal states were always mapped to a vector
with all zero elements. For each non-terminal state, the features were normalized so that the L2 norm
of each feature vector was one. For the first task, the feature representation was tabular, that is, the
feature vectors were standard basis vectors. In this representation, each feature corresponded to only
one state. For the second task, the feature vectors were binary representations of state indices. There
were 11 non-terminal states, hence each feature vector had blog2 (11)c + 1 = 4 components. These
vectors for the states from left to right were (0, 0, 0, 1)> , (0, 0, 1, 0)> , (0, 0, 1, 1)> , . . . , (1, 0, 1, 1)> ,
which were then normalized to get unit vectors. These features heavily underrepresented the states,
due to the fact that 11 states were represented by only 4 features.
We tested both algorithms for different values of constant , from 0 to 0.9 in steps of 0.1 and from
0.9 to 1.0 in steps of 0.025. The matrix to be inverted in both methods was initialized to ?I, where the
regularization parameter ? was varied by powers of 10 with powers chosen from -3 to +3 in steps of
0.2. Performance was measured as the empirical mean squared error (MSE) between the estimated
value of the initial state and its true value under the target policy projected to the space spanned by
the given features. This error was measured at the end of each of 200 episodes for 100 independent
runs.
Figure 1 shows the results for the two tasks in terms of empirical convergence rate, optimum performance and parameter sensitivity. Each curve shows MSE together with standard errors. The first row
shows results for the tabular task and the second row shows results for the function approximation
task. The first column shows learning curves using ( , ?) = (0, 1) for the first task and (0.95, 10) for
the second. It shows that in both cases WIS-LSTD( ) learned faster and gave lower error throughout
the period of learning. The second column shows performance with respect to different optimized
over ?. The x-axis is plotted in a reverse log scale, where higher values are more spread out than the
lower values. In both tasks, WIS-LSTD( ) outperformed the conventional LSTD( ) for all values of
. For the best parameter setting (best and ?), WIS-LSTD( ) outperformed LSTD( ) by an order
7

Tabular task
o?-policy LSTD( )
MSE

MSE

MSE
? 0.0
... 0.5
?? 0.9

WIS-LSTD( )

episodes

regularization parameter ?

Func. approx. task

o?-policy LSTD( )
MSE

MSE

MSE
? 0.5
... 0.9
?? 1.0

WIS-LSTD( )

episodes

regularization parameter ?

Figure 1: Empirical comparison of our WIS-LSTD( ) with conventional off-policy LSTD( ) on two
random-walk tasks. The empirical Mean Squared Error shown is for the initial state at the end of
each episode, averaged over 100 independent runs (and also over 200 episodes in column 2 and 3).
of magnitude. The third column shows performance with respect to the regularization parameter ?
for three representative values of . For a wide range of ?, WIS-LSTD( ) outperformed conventional LSTD( ) by an order of magnitude. Both methods performed similarly for large ?, as such
large values essentially prevent learning for a long period of time. In the function approximation
task when smaller values of ? were chosen, close to 1 led to more stable estimates, whereas smaller
introduced high variance for both methods. In both tasks, the better-performing regions of ? (the
U-shaped depressions) were wider for WIS-LSTD( ).

6

Conclusion

Although importance sampling is essential to off-policy learning and has become a key part of modern reinforcement learning algorithms, its most effective form?WIS?has been neglected because
of the difficulty of combining it with parametric function approximation. In this paper, we have
begun to overcome these difficulties. First, we have shown that the WIS estimate can be viewed as
the solution to an empirical objective where the squared errors of individual samples are weighted
by the importance-sampling ratios. Second, we have introduced a new method for general supervised learning called WIS-LS by extending the error-weighted empirical objective to linear function
approximation and shown that the new method has similar properties as those of the WIS estimate.
Finally, we have introduced a new off-policy LSTD algorithm WIS-LSTD( ) that extends the benefits of WIS to reinforcement learning. Our empirical results show that the new WIS-LSTD( ) can
outperform Yu?s off-policy LSTD( ) in both tabular and function approximation tasks and shows
robustness in terms of its parameters. An interesting direction for future work is to extend these
ideas to off-policy linear-complexity methods.

Acknowledgement
This work was supported by grants from Alberta Innovates Technology Futures, National Science
and Engineering Research Council, and Alberta Innovates Centre for Machine Learning.

8

References
Andrad?ottir, S., Heyman, D. P., Ott, T. J. (1995). On the choice of alternative measures in importance sampling
with markov chains. Operations Research, 43(3):509?519.
Bertsekas, D. P., Yu, H. (2009). Projected equation methods for approximate solution of large linear systems.
Journal of Computational and Applied Mathematics, 227(1):27?50.
Boyan, J. A. (1999). Least-squares temporal difference learning. In Proceedings of the 17th International
Conference, pp. 49?56.
Casella, G., Robert, C. P. (1998). Post-processing accept-reject samples: recycling and rescaling. Journal of
Computational and Graphical Statistics, 7(2):139?157.
Dann, C., Neumann, G., Peters, J. (2014). Policy evaluation with temporal differences: a survey and comparison. Journal of Machine Learning Research, 15:809?883.
Geist, M., Scherrer, B. (2014). Off-policy learning with eligibility traces: A survey. Journal of Machine
Learning Research, 15:289?333.
Hachiya, H., Akiyama, T., Sugiayma, M., Peters, J. (2009). Adaptive importance sampling for value function
approximation in off-policy reinforcement learning. Neural Networks, 22(10):1399?1410.
Hachiya, H., Sugiyama, M., Ueda, N. (2012). Importance-weighted least-squares probabilistic classifier for
covariate shift adaptation with application to human activity recognition. Neurocomputing, 80:93?101.
Hesterberg, T. C. (1988), Advances in importance sampling, Ph.D. Dissertation, Statistics Department, Stanford
University.
Kahn, H., Marshall, A. W. (1953). Methods of reducing sample size in Monte Carlo computations. In Journal
of the Operations Research Society of America, 1(5):263?278.
Koller, D., Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques. MIT Press,
2009.
Liu, J. S. (2001). Monte Carlo strategies in scientific computing. Berlin, Springer-Verlag.
Maei, H. R., Sutton, R. S. (2010). GQ( ): A general gradient algorithm for temporal-difference prediction
learning with eligibility traces. In Proceedings of the Third Conference on Artificial General Intelligence,
pp. 91?96. Atlantis Press.
Maei, H. R. (2011). Gradient temporal-difference learning algorithms. PhD thesis, University of Alberta.
Precup, D., Sutton, R. S., Singh, S. (2000). Eligibility traces for off-policy policy evaluation. In Proceedings
of the 17th International Conference on Machine Learning, pp. 759?766. Morgan Kaufmann.
Precup, D., Sutton, R. S., Dasgupta, S. (2001). Off-policy temporal-difference learning with function approximation. In Proceedings of the 18th International Conference on Machine Learning.
Robert, C. P., and Casella, G., (2004). Monte Carlo Statistical Methods, New York, Springer-Verlag.
Rubinstein, R. Y. (1981). Simulation and the Monte Carlo Method, New York, Wiley.
Shelton, C. R. (2001). Importance Sampling for Reinforcement Learning with Multiple Objectives. PhD thesis,
Massachusetts Institute of Technology.
Shimodaira, H. (2000). Improving predictive inference under covariate shift by weighting the log-likelihood
function. Journal of Statistical Planning and Inference, 90(2):227?244.
Sutton, R. S., Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press.
Sutton, R. S., Mahmood, A. R., Precup, D., van Hasselt, H. (2014). A new Q( ) with interim forward view
and Monte Carlo equivalence. In Proceedings of the 31st International Conference on Machine Learning,
Beijing, China.
Yu, H. (2010). Convergence of least squares temporal difference methods under general conditions. In Proceedings of the 27th International Conference on Machine Learning, pp. 1207?1214.

9

"
3557,2011,Collective Graphical Models,"There are many settings in which we wish to fit a model of the behavior of individuals but where our data consist only of aggregate information (counts or low-dimensional contingency tables).  This paper introduces Collective Graphical Models---a framework for modeling and probabilistic inference that operates directly on the sufficient statistics of the individual model.  We derive a highly-efficient Gibbs sampling algorithm for sampling from the posterior distribution of the sufficient statistics conditioned on noisy aggregate observations, prove its correctness, and demonstrate its effectiveness experimentally.","Collective Graphical Models

Thomas G. Dietterich
Oregon State University
tgd@eecs.oregonstate.edu

Daniel Sheldon
Oregon State University
sheldon@eecs.oregonstate.edu

Abstract
There are many settings in which we wish to fit a model of the behavior of individuals but where our data consist only of aggregate information (counts or
low-dimensional contingency tables). This paper introduces Collective Graphical Models?a framework for modeling and probabilistic inference that operates
directly on the sufficient statistics of the individual model. We derive a highlyefficient Gibbs sampling algorithm for sampling from the posterior distribution
of the sufficient statistics conditioned on noisy aggregate observations, prove its
correctness, and demonstrate its effectiveness experimentally.

1

Introduction

In fields such as ecology, marketing, and the social sciences, data about identifiable individuals is
rarely available, either because of privacy issues or because of the difficulty of tracking individuals
over time. Far more readily available are aggregated data in the form of counts or low-dimensional
contingency tables. Despite the fact that only aggregated data are available, researchers often seek
to build models and test hypotheses about individual behavior. One way to build a model connecting
individual-level behavior to aggregate data is to explicitly model each individual in the population,
together with the aggregation mechanism that yields the observed data.
However, with large populations it is infeasible to reason about each individual. Luckily, for many
purposes it is also unnecessary. To fit a probabilistic model of individual behavior, we only need
the sufficient statistics of that model. This paper introduces a formalism in which one starts with a
graphical model describing the behavior of individuals, and then derives a new graphical model ?
the Collective Graphical Model (CGM) ? on the sufficient statistics of a population drawn from
that model. Remarkably, the CGM has a structure similar to that of the original model.
This paper is devoted to the problem of inference in CGMs, where the goal is to calculate conditional
probabilities over the sufficient statistics given partial observations made at the population level. We
consider both an exact observation model where subtables of the sufficient statistics are observed
directly, and a noisy observation model where these counts are corrupted. A primary application is
learning: for example, computing the expected value of the sufficient statistics comprises the ?E?
step of an EM algorithm for learning the individual model from aggregate data.
Main concepts. The ideas behind CGMs are best illustrated by an example. Figure 1(a) shows the
graphical model plate notation for the bird migration model from [1, 2], in which birds transition
stochastically among a discrete set of locations (say, grid cells on a map) according to a Markov
chain (the individual model). The variable Xtm denotes the location of the mth bird at time t, and
birds are independent and identically distributed. This model gives an explicit way to reason about
the interplay between individual-level behavior (inside the plate) and aggregate data. Suppose, for
example, that very accurate surveys reveal the number of birds nt (i) in each location i at each time
t, and these numbers are collected into a single vector nt for each time step. Then, for example, one
can compute the likelihood of the survey data given parameters of the individual model by summing
out the individual variables. However, this is highly impractical: if our map has L grid cells, then
the variable elimination algorithm run on this model would instantiate tabular potentials of size LM .
1

X1m

X2m

???

n1,2

XTm

...

n2,3

nT?1,T

m=1:M

n1

n2

n1

nT

n2

(a)
i

3
1

i

1
j
t=1

5

2
2

i

j
t=2

(c)

nT

(b)
3+?

i

1
5

...

n3

1
j

j

t=3

t=1

1??
5

i

2+?

j

1??
5

t=2

(d)

i

i

3+?
1??

i

j

j

t=3

t=1

5+?

i

1

1??

2

2
2

j
t=2

5

j
t=3

(e)

Figure 1: Collective graphical model of bird migation: (a) replicates of individual model connected to
population-level observations, (b) CGM after marginalizing away individuals, (c) trellis graph on locations
{i, j} for T = 3, M = 10; numbers on edges indicate flow amounts, (d) a degree-one cycle; flows remain
non-negative for ? ? {?3, . . . , 1}, (e) a degree-two cycle; flows remain non-negative for ? ? {?2, . . . , 1}.

Figure 1(b) shows the CGM for this model, which we obtain by analytically marginalizing away the
individual variables to get a new model on their sufficient statistics, which are the tables nt,t+1 with
entries nt,t+1 (i, j) equaling the number of birds that fly from i to j from time t to t + 1. A much
better inference approach would be to conduct variable elimination or message passing directly in
the CGM. However, this would still instantiate potentials that are much too big for realistic problems

2
2
?1
due to the huge state space: e.g., there are ML+L
= O(M L ?1 ) possible values for the table
2 ?1
nt,t+1 .
Instead, we will perform approximate inference using MCMC. Here, we are faced with yet another
challenge: the CGM has hard constraints encoded into its distribution, and our MCMC moves must
preserve these constraints yet still connect the state space. To understand this, observe that the
hidden variables in this example comprise a flow of M units through the trellis graph of the Markov
chain, with the interpretation that nt,t+1 (i, j) birds ?flow? along edge (i, j) at time t (see Figure
1(c) and [1]). The constraints are that (1) flow is conserved at each trellis node, and (2) the number
of birds that enter location i at time t equals the observed number nt (i). (In the case of noisy or
partial observations, the latter constraint may not be present.)
How can we design a set of moves that connect any two M -unit flows while preserving these constraints? The answer is to make moves that send flow around cycles. Cycles of the form illustrated
in Figure 1(d) preserve flow conservation but change the amount of flow through some trellis nodes.
Cycles of the form in Figure 1(e) preserve both constraints. One can show by graph-theoretic arguments that moves of these two general classes are enough to connect any two flows.
This gives us the skeleton of an ergodic MCMC sampler: starting with a feasible flow, select cycles
from these two classes uniformly at random and propose moves that send ? units of flow around
the cycle. There is one unassuming but crucially important final question: how to select ?? The
following is a form of Gibbs sampler: from all values that preserve non-negativity, select ? with
probability proportional to that of the new flow. Such moves are always accepted. Remarkably,
even though ? may take on as many as M different values, the resulting distribution over ? has an
extremely tractable form ? either binomial or hypergeometric ? and thus it is possible to select ?
in constant time, so we can make very large moves in time independent of the population size.
Contributions. This paper formally develops these concepts in a way that generalizes the construction of Figure 1 to allow arbitrary graphical models inside the plate, and a more general observation
model that includes both noisy observations and observations involving multiple variables. We develop an efficient Gibbs sampler to conduct inference in CGMs that builds on existing work for conducting exact tests in contingency tables and makes several novel technical contributions. Foremost
is the analysis of the distribution over the move size ?, which we show to be a discrete univariate
distribution that generalizes both the binomial and hypergeometric distributions. In particular, we
prove that it is always log-concave [3], so it can be sampled in constant expected running time. We
2

show empirically that resulting inference algorithm runs in time that is independent of the population
size, and is dramatically faster than alternate approaches.
Related Work. The bird migration model of [1, 2] is a special case of CGMs where the individual
model is a Markov chain and observations are made for single variables only. That work considered
only maximum a posteriori (MAP) inference; the method of this paper could be used for learning in
that application. Sampling methods for exact tests in contingency tables (e.g. [4]) generate tables
with the same sufficient statistics as an observed table. Our work differs in that our observations
are not sufficient, and we are sampling the sufficient statistics instead of the complete contingency
table. Diaconis and Sturmfels [5] broadly introduced the concept of Markov bases, which are sets
of moves that connect the state space when sampling from conditional distributions by MCMC. We
construct a Markov basis in Section 3.1 based on work of Dobra [6]. Lauritzen [7] discusses the
problem of exact tests in nested decomposable models, a setup that is similar to ours. Inference
in CGMs can be viewed as a form of lifted inference [8?12]. The counting arguments used to derive the CGM distribution (see below) are similar to the operations of counting elimination [9] and
counting conversion [10] used in exact lifted inference algorithms for first-order probabilistic models. However, those algorithms do not replicate the CGM construction when applied to a first-order
representation of the underlying population model. For example, when applied to the bird migration
model, the C-FOVE algorithm of Milch et al. [10] cannot introduce contingency tables over pairs
of variables (Xt , Xt+1 ) as required to represent the sufficient statistics; it can only introduce histograms over single variables Xt . Apsel and Brafman [13] have recently taken a step in this direction
by introducing a lifting operation to construct the Cartesian product of two first-order formulas. In
the applications we are considering, exact inference (even when lifted) is intractable.

2

Problem Setup

Let (X1 , X2 , . . . , X|V | ) be a set of discrete random variables indexed by the finite set V , where Xv
takes values in the set Xv . Let x = (x1 , . . . , x|V | ) denote a joint setting for these variables from the
set X = X1 ? . . . ? X|V | . For our individual model, we consider graphical models of the form:
1 Y
p(x) =
?C (xC ).
(1)
Z
C?C

Here, C is the set of cliques of the independence graph, the functions ?C : XC ? R+ are potentials,
and Z is a normalization constant. For A ? V , we use the notation xA to indicate the sub-vector
of variables with indices belonging to A, and use similar notation for the corresponding domain
XA . We also assume that p(x) > 0 for all x ? X , which is required for our sampler to be ergodic.
Models that fail this restriction can be modified by adding a small positive amount to each potential.
A collection A is a set of subsets of V . For collections A and B, define A  B to mean that
each A ? A is contained in some B ? B. A collection A is decomposable if there is a junction tree
T = (A, E(T )) on vertex set A [7]. Any collection A can be extended to a decomposable collection
B such that A  B; this corresponds to adding fill-in edges to a graphical model.
Consider a sample {x(1) , . . . , x(M ) } from the graphical model. A contingency table n = (n(i))i?X
PM
has entries n(i) = m=1 I{x(m) = i} that count the number of times each element i ? X appears in the sample. We use index variables such as i, j ? X (instead of x ? X ) to refer to
cells of the contingency table, where i = (i1 , . . . , iV ) is a vector of indices and iA is the subvector corresponding to A ? V . Let tbl(A) denote the set of all valid contingency tables on the
domain XA . A valid table is indexed by elements iA ? XA and has non-negative integer entries.
For a full table n ? tbl(V ) and A ? V , let the marginal table n ? A ? tbl(A) be defined as
P
PM
(m)
(n ? A)(iA ) = m=1 I{xA = iA } = iB ?XV \A n(iA , iB ). When A = ?, define n ? A to be
the scalar M , the grand total of the table. Write nA  nB to mean that nA is a marginal table of
nB (i.e., A ? B and nA = nB ? A)
Our observation model is as follows. We assume that a sample {x(1) , . . . , x(M ) } is drawn from the
individual model, resulting in a complete, but unobserved, contingency table nV . We then observe
the marginal tables nD = nV ? D for each set D in a collection of observed margins D, which
we require to be decomposable. Write this overall collection of tables as nD = {nD }D?D . We
consider noisy observations in Section 3.3.
3

Building the CGM. In a discrete graphical model, the sufficient statistics are the contingency tables
nC = {nC }C?C over cliques. Our approach relies on the ability to derive a tractable probabilistic
model for these statistics by marginalizing out the sample. If C is decomposable, this is possible, so
let us assume that C has a junction tree TC (if not, fill-in edges must be added to the original model).
Let ?C be the table of marginal probabilities for clique C (i.e. ?C (iC ) = Pr(XC = iC )). Let S
be the collection of separators of TC (with repetition if the same set appears as a separator multiple
times) and let nS and ?S be the tables of counts and marginal probabilities for the separator S ? S.
The distribution of nC was first derived by Sundberg [14]:
!
!?1
Y Y ?C (iC )nC (iC )
Y Y ?S (iS )nS (iS )
p(nC ) = M !
,
nC (iC )!
nS (iS )!
C?C iC ?XC

(2)

S?S iS ?XS

which can be understood as a product of multinomial distributions corresponding to a sampling
scheme for nC (details omitted). It is this distribution that we call the collective graphical model;
the parameters are the marginal probabilities of the individual model. To understand the conditional
distribution given the observations, let us further assume that D  C (if not, add additional fillin edges for variables that co-occur within D), so that each observed table is determined by some
clique table. Write nD  nC to express the condition that the tables nC produce observations nD :
formally, this means that D  C and that D ? C implies that nD  nC . Let I{?} be an indicator
variable. Then
p(nC | nD ) ? p(nC , nD ) = p(nC )I{nD  nC }.
(3)
In general, the number of contingency tables over small sets of variables leads to huge state spaces
that prohibit exact inference schemes using (2) and (3). Thus, our approach is based on Gibbs
sampling. However, there are two constraints that significanlty complicate sampling. First, the
clique tables must match the observations (i.e., nD  nC ). Second, implicit in (2) is the constraint
that the tables nC must be consistent in the sense that they are the sufficient statistics of some sample,
otherwise p(nC ) = 0.
Definition 1. Refer to the set of contingency tables nA = {nA }A?A as a configuration. A configuration is (globally) consistent if there exists nV ? tbl(V ) such that nA = nV ? A for all A ? A.
Consistency requires, for example, that any two tables must agree on their common marginal, which
yields the flow conservation constraints in the bird migration model. Table entries must be carefully
updated in concert to maintain these constraints. A full discussion follows.

3

Inference

Our goal is to develop a sampler for p(nC | nD ) given the observed tables nD . We assume that the
CGM specified in Equations (1) and (2) satisfies D  C, and that the configuration nD is consistent.
Initialization. The first step is to construct a valid initial value for nC , which must be a globally
consistent configuration satisfying nD  nC . Doing so without instantiating huge intermediate
tables requires a careful sequence of operations on the two junction trees TC and TD . We state one
key theorem, but defer the full algorithm, which is lengthy and technical, to the supplement.
Theorem 1. Let A be a decomposable collection with junction tree TA . Say that the configuration
nA is locally consistent if it agrees on edges of TA , i.e., if nA ? S = nB ? S for all (A, B) ? E(TA )
with S = A ? B. If nA is locally consistent, then it is also globally consistent.
In the bird migration example, Theorem 1 guarantees that preserving flow conservation is enough to
maintain consistency. It is structurally equivalent to the ?junction tree theorem? (e.g., [15]) which
asserts that marginal probability tables {?A }A?A that are locally consistent are realizable as the
marginals of some joint distribution p(x). Like that result, Theorem 1 also has a constructive proof,
which is the foundation for our initialization algorithm. However, the integrality requirements of
contingency tables necessitate a different style of construction.
3.1

Markov Basis

The first key challenge in designing the MCMC sampler is constructing a set of moves that preserve
the constraints mentioned above, yet still connect any two points in the support of the distribution.
Such a set of moves is called a Markov basis [5].
4

Definition 2. A set of moves M is a Markov basis for the set F if, for any two configurations
PL
n, n0 ? F, there is a sequence of moves z1 , . . . , zL ? M such that: (i) n0 = n + `=1 z` , and (ii)
PL0
n + `=1 z` ? F for all L0 = 1, . . . , L ? 1.
In our problem, the set we wish to connect is the support of p(nC | nD ). Our positivity assumption
on p(x) implies that any consistent configuration nC has positive probability, and thus the support
of p(nC | nD ) is exactly the set of consistent configurations that match the observations:
FnD = {nC : nC is consistent and nD  nC }
It is useful at this point to think of the configuration nC as a vector obtained by sorting the table
entries in any consistent fashion (e.g., lexicographically first by C ? C and then by iC ? XC ). A
move can be expressed as n0C = nC + z where z is an integer-valued vector of the same dimension
as nC that may have negative entries.
The Dobra Markov basis for complete tables. Dobra [6] showed how to construct a Markov
basis for moves in a complete contingency table given S
a decomposable set of margins. Specifically,
let A be decomposable and let nA be consistent with A = V , so that each variable is part of an
observed margin. Define Fn?A = {nV ? tbl(V ) : nA  nV }. Dobra gave a Markov basis for Fn?A
consisting of only degree-two moves:
Definition 3. Let (A, S, B) be a partition of V . A degree-two move z has two positive entries and
two negative entries:
z(i, j, k) = 1, z(i, j, k 0 ) = ?1, z(i0 , j, k) = ?1, z(i0 , j, k 0 ) = 1,

(4)

where i 6= i0 ? XA , j ? XS k 6= k 0 , ? XB . Let Md=2 (A, S, B) be the set of all degree-two moves
generated from this partition.
These are extensions of the well-known ?swap moves? for two-dimensional contingency tables (e.g. [5]) to the subtable n(?, j, ?), and they can be visualized as
k k0
shown at right. In this arrangement, it is clear that any such move preserves the i + ?
marginal table nA (row sums) and the marginal table nB (column sums); in other
i0 ? +
words, z ? A = 0 and z ? B = 0. Moreover, because j is fixed, it is straightforward to see that z ? A ? S = 0 and z ? B ? S = 0. The cycle in Figure 1(e) is a degree-two move
on the table n1,2 , with A = {X1 }, S = ?, C = {X2 }.
S
Theorem 2 (Dobra [6]). Let A be decomposable with A = V . Let M?A be the union of the sets of
degree-two moves Md=2 (A, S, B) where S is a separator of TA and (A, S, B) is the corresponding
decomposition of V . Then M?A is a Markov basis for Fn?A .
Adaptation of Dobra basis to FnD . We now adapt the Dobra basis to our setting. Consider a
complete table n ? tbl(V ) and the configuration nC = {n ? C}C?C . Because marginalization is a
linear operation, there is a linear operator A such that nC = AnV . Moreover, FnA is the image of
Fn?A under A. Thus, the image of the Dobra basis under A is a Markov basis for FnA .
Lemma 1. Let M?A be a Markov basis for Fn?A . Then MA = {Az : z ? M?A } is a Markov basis
for FnA . We call MA the projected Dobra basis.
Proof. Let nC , n0C ? FnA . By consistency, there exist nV , n0V ? Fn?A such that nC = AnV and
n0C = An0V . There is a sequence of moves z1 , . . . , zL ? M?A leading from n0V to nV , meaning
PL
that n0V = nV + `=1 z` . By appliyng the linear operator A to both sides of this equation, we
PL
PL0
have that n0C = nC + `=1 Az` . Furthermore, each intermediate configuration nC + `=1 Az` =
PL0
A(nV + `=1 z` ) ? FnA . Thus MA = {Az : z ? M?A } is a Markov basis for FnA .
Locality of moves. First consider the case where all variables are part of some observed table, as
in Dobra?s setting. The practical message so far is that to sample from p(nC | nD ), it suffices to
generate moves from the projected Dobra basis MD . This is done by first selecting a degree-two
move z ? M?D , and then marginalizing z onto each clique of C. Naively, it appears that a single
move may require us to update each clique. However, we will show that z ? C will be zero for many
cliques, a fact we can exploit to implement moves more efficiently. Let (A, S, B) be the partition
5

used to generate z. We deduce from the discussion following Definition 3 that z ? C = 0 unless C
has a nonempty intersection with both A and B, so we may restrict our attention to these cliques,
which form a connected subtree (Proposition S.1 in supplementary material). An implementation
can then exploit this by pre-computing the connected subtrees for each separator and only generating
the necessary components of the move. Algorithm 1 gives the details of generating moves.
Unobserved variables. Let us now consider
Algorithm 1: The projected Dobra basis MA
settings where some variables are not part of
Input: Junction tree TA with separators SA
any observed table, which may happen when
1 Before sampling
the individual model has hidden variables, or,
later, with noisy observations. Additional
2
For each S ? SA , find the associated
moves are needed to connect two configuradecomposition (A, S, B)
3
Find the cliques C ? C that have non-empty
tions that disagree on marginal tables involvintersection with both A and B. These form a
ing unobserved variables. Several approaches
subtreeSof TC . Denote these cliques by CS and let
are possible. All require the introduction
VS = CS .
d=1
of degree-one moves z ? M (A, B),
4
Let AS = A ? VS and BS = B ? VS
which partition the variables into two sets
5 During sampling: to generate a move for separator
(A, B) and have two nonzero entries z(i, j) =
S ? SA
1, z(i0 , j) = ?1 for i 6= i0 ? XA , j ? XB . In
the parlance of two-dimensional tables, these
6
Select z ? Md=2 (AS , S, BS )
7
For
each clique C ? CS , calculate z ? C
moves adjust two entries in a single column so
they preserve the column sums (nB ) but modify the row sums (nA ). The cycle in Figure 1(d) is a degree-one move which adjusts the marginal
table over A = {X2 }, but preserves the marginal table over B = {X1 , X3 }. We proceed once again
by constructing a basis for complete tables and then marginalizing the moves onto cliques.
S
Theorem 3. Let U be any decomposable collection on the set of unobserved variables U = V \ D,
and let D0 = D ? U. Let M? consist of the moves M?D0 together with the moves Md=1 (A, V \ A)
for each A ? U. Then M? is a Markov basis for Fn?D , and M = {Az : z ? M? } is a Markov
basis for FnD .
Theorem 3 is proved in the supplementary material. The degree-one moves also become local upon
marginalization: it is easy to check that z ? C is zero unless C ? A is nonempty. These cliques also
form a connected subtree. We recommend choosing U by restricting TC to the variables in U . This
has the effect of adding degree-one moves for each clique of C. By matching the structure of TC ,
many of the additional degree-two moves become zero upon marginalization.
3.2

Constructing an efficient MCMC sampler

The second key challenge in constructing the MCMC sampler is utilizing the moves from the
Markov basis in a way that efficiently explores the state space. A standard approach is to select
a random move z, a direction ? = ?1 (each with probability 1/2), and then propose the move
nC + ?z in a Metropolis Hastings sampler. Although these moves are enough to connect any two
configurations, we are particularly interested in problems where M is large, for which moving by
increments of ?1 will be prohibitively slow.
For general Markov bases, Diaconis and Sturmfels [5] suggest instead to construct a Gibbs sampler
that uses the moves as directions for longer steps, by choosing the value of ? from the following
distribution:
p(?) ? p(nC + ?z | nD ),
? ? {? : nC + ?z ? 0}.
(5)
Lemma 2 (Adapted from Diaconis and Sturmfels [5]). Let M be a Markov basis for FnD . Consider
the Markov chain with moves ?z generated by first choosing z uniformly at random from M and
then choosing ? according to (5). This is a connected, reversible, aperiodic Markov chain on FnD
with stationary distribution p(nC | nD ).
However, it is not obvious how to sample from p(?). They suggest running a Markov chain in ?,
again having the property of moving in increments of one (see also [16]). In our case, the support of
p(?) may be as big as the population size M , so this solution remains unsatisfactory.
Fortunately, p(?) has several properties that allow us to create a very efficient sampling algorithm.
For a separator S ? S, define zS as zC ? S for any clique C containing S. Now let C(z) be the
6

4

10

1

Calculate ?min and ?max using (8)

2

Extend the function f (?) := log p(?) to the real line using the
equality n! = ?(n + 1) in Equation (7) for each constituent
function fA (?) := log pA (?), A ? S(z) ? C(z).

3

Use the logarithm of Equation (6) to evaluate f (?) (for sampling)
and its derivatives (for Newton?s method):
X (q)
X (q)
fC (?) ?
fS (?).
q = 0, 1, 2.
f (q) (?) =
S?S(z)

C?C(z)

Evaluate the derivatives of fA (?) using the logarithm of Equation
d
(7) and the digamma and trigamma functions ?(n) = dn
?(n)
d2
and ?1 (n) = dn2 ?(n).
4

5

Find the mode ? ? by first using Newton?s method to find ? 0
maximizing f (?) over the real line, and then letting ? ? be the
value in {b? 0 c, d? 0 e, ?min , ?max } that attains the maximum.
Run the rejection sampling algorithm of Devroye [3].

VE
MCMC

2

10

0

10

1

10
Population size
Relative error

Input: move z and current configuration nC , with |C(z)| > 1

Seconds

Algorithm 2: Sampling from p(?) in constant time

2

10

exact?nodes
exact?chain
noisy?nodes
noisy?chain

0.4
0.3
0.2
0.1
0
0

50
Seconds

100

Figure 2: Top: running time
vs. M for a small CGM. Bottom:
convergence of MCMC for random Bayes nets.

set of cliques C for which zC is nonzero, and let S(z) be defined analogously. For A ? S ? C, let
I + (zA ) ? XA be the indices of +1 entries of zA and let I ? (zA ) be the indices of ?1 entries. By
ignoring constant terms in (2), we can write (5) as
Y
Y
p(?) ?
pC (?)
pS (?)?1 ,
(6)
C?C(z)

pA (?) :=

Y
i?I + (zA )

?

?A (i)
(nA (i) + ?)!

S?S(z)

Y
j?I ? (zA )

?A (j)??
,
(nA (j) ? ?)!

A ? S ? C.

To maintain the non-negativity of nC , ? is restricted to the support ?min , . . . , ?max with:
?min := ?
min +
nC (i),
?max :=
min ?
nC (j).
C?C(z),i?I (zC )

C?C(z),j?I (zC )

(7)

(8)

Notably, each move in our basis satisfies |I + (zA ) ? I + (zA )| ? 4, so p(?) can be evaluated by
examining at most four entries in each table for cliques in C(z). It is worth noting that Equation
(7) reduces to the binomial distribution for degree-one moves and the (noncentral) hypergeometric distribution for degree-two moves, so we may sample from these distributions directly when
|C(z)| = 1. More importantly, we will now show that p(?) is always a member of the log-concave
class of distributions, which are unimodal and can be sampled very efficiently.
Definition 4. A discrete distribution {pk } is log-concave if p2k ? pk?1 pk+1 for all k [3].
Theorem 4. For any degree-one or degree-two move z, the distribution p(?) is log-concave.
It is easy to show that both pC (?) and pS (?) are log-concave. The proof of Theorem 4, which is
found in the supplementary material, then pairs each separator S with a clique C and uses properties
of the moves to show that pC (?)/pS (?) is also log-concave. Then, by Equation (6), we see that p(?)
is a product of log-concave distributions, which is also log-concave.
We have implemented the rejection sampling algorithm of Devroye [3], which applies to any discrete
log-concave distribution and is simple to implement. The expected number of times it evaluates p(?)
(up to normalization) is fewer than 5. We must also provide the mode of the distribution, which we
find by Newton?s method, usually taking only a few steps. The running time for each move is thus
independent of the population size. Additional details are given in Algorithm 2.
3.3

Noisy Observations

Population-level counts from real survey data are rarely exact, and it is thus important to incorporate noisy observations into our model. In this section, we describe how to modify the sampler for
7

the case when all observations are noisy; it is a straightforward generalization to allow both noisy
and exact observations. Suppose that we make noisy observations yR = {yR : R ? R} corresponding to the true marginal tables nR for a collection R  C (that need not be decomposable).
For simplicity, we restrict our attention to models where each entry n in the true table is corrupted
independently according to a univariate noise model p(y | n).
We assume that the noise model is log-concave, meaning in this case that log p(y | n) is a concave
function of the parameter n. Most commonly-used univariate densities are log-concave with respect
to various parameters [17]. A canonical example from the bird migration model is p(y | n) =
Poisson(?n), so the survey count is Poisson with mean proportional to the true number of birds
present. This example and others are discussed in [2]. We also assume that the support of p(y | n)
does not depend on n, so that observations do not restrict the support of the sampling distribution.
For example, we must modify our Poisson noise model to be p(y | n) = Poisson(?n + ?0 ) with
small background rate ?0 to avoid the hard constraint that n must be positive if y is positive.
In analogy with (3), we can then write p(nC | yR ) ? p(nC )p(yR |nC ) (the hard constraint is now
replaced with the likelihood term p(yR |nC )). Given our assumption on p(y | n), the support of
p(nC | yR ) is the same as the support of p(nC ), and a Markov basis can be constructed using the
tools from Section 3.1, with all variables being unobserved. In the sampler, the expression for p(?)
must now be updated to incorporate the likelihood term p(yR |nC +?z). Following reasoning similar
to before, we let R(z)
Qbe the sets in R for which z ? R is nonzero and find that Equation (6) gains
the additional factor R?R(z) pR (?), where
Y
Y
pR (?) =
p(yR (i) | nR (i) + ?)
p(yR (j) | nR (j) ? ?).
(9)
j?I ? (zR )

i?I + (zR )

Each factor in (9) is log-concave in ? by our assumption on p(y | n), and hence the overall distribution p(?) remains log-concave. To update the sampler for p(?), modify line 3 of Algorithm 2 in the
obvious fashion to include these new factors when computing log p(?) and its derivatives.

4

Experiments

We implemented our sampler in MATLAB using Murphy?s Bayes net toolbox [18] for the underlying
operations on graphical models and junction trees. Figure 2 (top) compares the running time of our
method vs. exact inference in the CGM by variable elimination (VE) for a very small model. The
task was to estimate E[n2,3 | n1 , n3 ] in the bird migration model for L = 2, T = 3, and varying M .
2
The running time of VE is O(M L ?1 ), which is cubic in M (linear on a log-log plot), while the time
for our method to estimate the same quantity within 2% relative error actually decreases slightly with
population size. Figure 2 (bottom) shows convergence of the sampler for more complex models. We
generated 30 random Bayes nets on 10 binary variables, and generated two sets of observed tables
for a population of M = 100, 000: the set NODES has a table for each single variable, while the
set CHAIN has tables for pairs of variables that are adjacent in a random ordering. We repeated the
same process with the noise model p(y | n) = Poisson(0.2n + 0.1) to generate noisy observations.
We then ran our sampler to estimate E[nC | nD ] as would be done in the EM algorithm. The plots
show relative error in this estimate as a function of time, averaged over the 30 nets. For more details,
including how we derived the correct answer for comparison, see Section 4.1 in the supplementary
material. The sampler converged quickly in all cases with the more complex CHAIN observation
model taking longer than NODES, and noisy observations taking slightly longer than exact ones. We
found (not shown) that the biggest source of variability in convergence time was due to individual
Bayes nets, while repeat trials using the same net demonstrated very similar behavior.
Concluding Remarks. An important area of future research is to further explore the use of CGMs
within learning algorithms, as well as the limitations of that approach: when is it possible to learn individual models from aggregate data? We believe that the ability to model noisy observations will be
an indispensable tool in real applications. For complex models, convergence may be difficult to diagnose. Some mixing results are known for samplers in related problems with hard constraints [16];
any such results for our model would be a great advance. The use of distributional approximations
for the CGM model and other methods of approximate inference also hold promise.
Acknowledgments. We thank Lise Getoor for pointing out the connection between CGMs and lifted
inference. This research was supported in part by the grant DBI-0905885 from the NSF.
8

References
[1] D. Sheldon, M. A. S. Elmohamed, and D. Kozen. Collective inference on Markov models for
modeling bird migration. In Advances in Neural Information Processing Systems (NIPS 2007),
pages 1321?1328, Cambridge, MA, 2008. MIT Press.
[2] Daniel Sheldon. Manipulation of PageRank and Collective Hidden Markov Models. PhD
thesis, Cornell University, 2009.
[3] L. Devroye. A simple generator for discrete log-concave distributions. Computing, 39(1):
87?91, 1987.
[4] A. Agresti. A survey of exact inference for contingency tables. Statistical Science, 7(1):131?
153, 1992.
[5] P. Diaconis and B. Sturmfels. Algebraic algorithms for sampling from conditional distributions. The Annals of statistics, 26(1):363?397, 1998. ISSN 0090-5364.
[6] A. Dobra. Markov bases for decomposable graphical models. Bernoulli, 9(6):1093?1108,
2003. ISSN 1350-7265.
[7] S.L. Lauritzen. Graphical models. Oxford University Press, USA, 1996.
[8] D. Poole. First-order probabilistic inference. In Proc. IJCAI, volume 18, pages 985?991, 2003.
[9] R. de Salvo Braz, E. Amir, and D. Roth. Lifted first-order probabilistic inference. Introduction
to Statistical Relational Learning, page 433, 2007.
[10] B. Milch, L.S. Zettlemoyer, K. Kersting, M. Haimes, and L.P. Kaelbling. Lifted probabilistic
inference with counting formulas. Proc. 23rd AAAI, pages 1062?1068, 2008.
[11] P. Sen, A. Deshpande, and L. Getoor. Bisimulation-based approximate lifted inference. In
Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages
496?505. AUAI Press, 2009.
[12] J. Kisynski and D. Poole. Lifted aggregation in directed first-order probabilistic models. In
Proc. IJCAI, volume 9, pages 1922?1929, 2009.
[13] Udi Apsel and Ronen Brafman. Extended lifted inference with joint formulas. In Proceedings
of the Proceedings of the Twenty-Seventh Conference Annual Conference on Uncertainty in
Artificial Intelligence (UAI-11), pages 11?18, Corvallis, Oregon, 2011. AUAI Press.
[14] R. Sundberg. Some results about decomposable (or Markov-type) models for multidimensional
contingency tables: distribution of marginals and partitioning of tests. Scandinavian Journal
of Statistics, 2(2):71?79, 1975.
[15] M.J. Wainwright and M.I. Jordan. Graphical models, exponential families, and variational
inference. Foundations and Trends in Machine Learning, 1(1-2):1?305, 2008.
[16] P. Diaconis, S. Holmes, and R.M. Neal. Analysis of a nonreversible Markov chain sampler.
The Annals of Applied Probability, 10(3):726?752, 2000.
[17] W.R. Gilks and P. Wild. Adaptive Rejection sampling for Gibbs Sampling. Journal of the Royal
Statistical Society. Series C (Applied Statistics), 41(2):337?348, 1992. ISSN 0035-9254.
[18] K. Murphy. The Bayes net toolbox for MATLAB. Computing science and statistics, 33(2):
1024?1034, 2001.

9

"
4036,2012,Multiple Operator-valued Kernel Learning,"Positive definite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients. The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces.","Multiple Operator-valued Kernel Learning

Hachem Kadri
LIF - CNRS / INRIA Lille - Sequel Project
Universit?e Aix-Marseille
Marseille, France
hachem.kadri@lif.univ-mrs.fr

Alain Rakotomamonjy
LITIS EA 4108
Universit?e de Rouen
St Etienne du Rouvray, France
alain.rakotomamony@insa-rouen.fr
Philippe Preux
INRIA Lille - Sequel Project
LIFL - CNRS, Universit?e de Lille
Villeneuve d?Ascq, France
philippe.preux@inria.fr

Francis Bach
INRIA - Sierra Project
Ecole Normale Sup?erieure
Paris, France
francis.bach@inria.fr

Abstract
Positive definite operator-valued kernels generalize the well-known notion of
reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a finite linear combination
of infinite-dimensional operator-valued kernels which are suitable for extending
functional data analysis methods to nonlinear contexts. We study this problem in
the case of kernel ridge regression for functional responses with an `r -norm constraint on the combination coefficients (r ? 1). The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since
operator-valued kernels pose more technical and theoretical issues. We propose a
multiple operator-valued kernel learning algorithm based on solving a system of
linear operator equations by using a block coordinate-descent procedure. We experimentally validate our approach on a functional regression task in the context
of finger movement prediction in brain-computer interfaces.

1

Introduction

During the past decades, a large number of algorithms have been proposed to deal with learning
problems in the case of single-valued functions (e.g., binary-output function for classification or real
output for regression). Recently, there has been considerable interest in estimating vector-valued
functions [21, 5, 7]. Much of this interest has arisen from the need to learn tasks where the target is
a complex entity, not a scalar variable. Typical learning situations include multi-task learning [11],
functional regression [12], and structured output prediction [4].
In this paper, we are interested in the problem of functional regression with functional responses in
the context of brain-computer interface (BCI) design. More precisely, we are interested in finger
movement prediction from electrocorticographic signals [23]. Indeed, from a set of signals measuring brain surface electrical activity on d channels during a given period of time, we want to predict,
for any instant of that period whether a finger is moving or not and the amplitude of the finger flexion. Formally, the problem consists in learning a functional dependency between a set of d signals
and a sequence of labels (a step function indicating whether a finger is moving or not) and between
the same set of signals and vector of real values (the amplitude function). While, it is clear that this
problem can be formalized as functional regression problem, from our point of view, such problem
can benefit from the multiple operator-valued kernel learning framework. Indeed, for these problems, one of the difficulties arises from the unknown latency between the signal related to the finger
1

movement and the actual movement [23]. Hence, instead of fixing in advance some value for this
latency in the regression model, our framework allows to learn it from the data by means of several
operator-valued kernels.
If we wish to address functional regression problem in the principled framework of reproducing
kernel Hilbert spaces (RKHS), we have to consider RKHSs whose elements are operators that map
a function to another function space, possibly source and target function spaces being different.
Working in such RKHSs, we are able to draw on the important core of work that has been performed on scalar-valued and vector-valued RKHSs [28, 21]. Such a functional RKHS framework
and associated operator-valued kernels have been introduced recently [12, 13]. A basic question
with reproducing kernels is how to build these kernels and what is the optimal kernel choice for a
given application. In order to overcome the need for choosing a kernel before the learning process,
several works have tried to address the problem of learning the scalar-valued kernel jointly with
the decision function [18, 29]. Since these seminal works, many efforts have been carried out in
order to theoretically analyze the kernel learning framework [9, 3] or in order to provide efficient
algorithms [24, 1, 15]. While many works have been devoted to multiple scalar-valued kernel learning, this problem of kernel learning have been barely investigated for operator-valued kernels. One
motivation of this work is to bridge the gap between multiple kernel learning (MKL) and operatorvalued kernels by proposing a framework and an algorithm for learning a finite linear combination
of operator-valued kernels. While each step of the scalar-valued MKL framework can be extended
without major difficulties to operator-valued kernels, technical challenges arise at all stages because
we deal with infinite dimensional spaces. It should be pointed out that in a recent work [10], the
problem of learning the output kernel was formulated as an optimization problem over the cone
of positive semidefinite matrices, and a block-coordinate descent method was proposed to solve it.
However, they did not focus on learning the input kernel. In contrast, our multiple operator-valued
kernel learning formulation can be seen as a way of learning simultaneously input and output kernels, although we consider a linear combination of kernels that are fixed in advance.
In this paper, we make the following contributions: 1) we introduce a novel approach to infinitedimensional multiple operator-valued kernel learning (MovKL) suitable for learning the functional
dependencies and interactions between continuous data; 2) we extend the original formulation of
ridge regression in dual variables to the functional data analysis domain, showing how to perform
nonlinear functional regression with functional responses by constructing a linear regression operator in an operator-valued kernel feature space (Section 2); 3) we derive a dual form of the MovKL
problem with functional ridge regression, and show that a solution of the related optimization problem exists (Section 2); 4) we propose a block-coordinate descent algorithm to solve the MovKL
optimization problem which involves solving a challenging linear system with a sum of block operator matrices (Section 3); 5) we provide an empirical evaluation of MovKL performance which
demonstrates its effectiveness on a BCI dataset (Section 4).

2

Problem Setting

Before describing the multiple operator-valued kernel learning algorithm that we will study and experiment with in this paper, we first review notions and properties of reproducing kernel Hilbert
spaces with operator-valued kernels, show their connection to learning from multiple response
data (multiple outputs; see [21] for discrete data and [12] for continuous data), and describe the
optimization problem for learning kernels with functional response ridge regression.
2.1

Notations and Preliminaries

We start by some standard notations and definitions used all along the paper. Given a Hilbert
space H, h?, ?iH and k ? kH refer to its inner product and norm, respectively. We denote by Gx
and Gy the separable real Hilbert spaces of input and output functional data, respectively. In functional data analysis domain, continuous data are generally assumed to belong to the space of square
integrable functions L2 . In this work, we consider that Gx and Gy are the Hilbert space L2 (?) which
consists of all equivalence classes of square integrable functions on a finite set ?. ? being potentially different for Gx and Gy . We denote by F(Gx , Gy ) the vector space of functions f : Gx ?? Gy ,
and by L(G y ) the set of bounded linear operators from Gy to Gy .
2

We consider the problem of estimating a function f such that f (xi ) = yi when observed functional
data (xi , yi )i=1,...,n ? (Gx , Gy ). Since Gx and Gy are spaces of functions, the problem can be thought
of as an operator estimation problem, where the desired operator maps a Hilbert space of factors to
a Hilbert space of targets. We can define the regularized operator estimate of f ? F as:
n
1X
f? , arg min
kyi ? f (xi )k2Gy + ?kf k2F .
(1)
f ?F n
i=1
In this work, we are looking for a solution to this minimization problem in a function-valued reproducing kernel Hilbert space F. More precisely, we mainly focus on the RKHS F whose elements
are continuous linear operators on Gx with values in Gy . The continuity property is obtained by
considering a special class of reproducing kernels called Mercer kernels [7, Proposition 2.2]. Note
that in this case, F is separable since Gx and Gy are separable [6, Corollary 5.2].
We now introduce (function) Gy -valued reproducing kernel Hilbert spaces and show the correspondence between such spaces and positive definite (operator) L(G y )-valued kernels. These extend the
traditional properties of scalar-valued kernels.
Definition 1 (function-valued RKHS)
A Hilbert space F of functions from Gx to Gy is called a reproducing kernel Hilbert space if there is
a positive definite L(G y )-valued kernel KF (w, z) on Gx ? Gx such that:
i. the function z 7?? KF (w, z)g belongs to F, ?z ? Gx , w ? Gx , g ? Gy ,
ii. ?f ? F, w ? Gx , g ? Gy , hf, KF (w, ?)giF = hf (w), giGy (reproducing property).
Definition 2 (operator-valued kernel)
An L(G y )-valued kernel KF (w, z) on Gx is a function KF (?, ?) : Gx ? Gx ?? L(G y ); furthermore:
i. KF is Hermitian if KF (w, z) = KF (z, w)? , where ? denotes the adjoint operator,
ii. KF is positive definite on GxPif it is Hermitian and for every natural number r and all
{(wi , ui )i=1,...,r } ? Gx ? Gy , i,j hKF (wi , wj )uj , ui iGy ? 0.
Theorem 1 (bijection between function-valued RKHS and operator-valued kernel)
An L(G y )-valued kernel KF (w, z) on Gx is the reproducing kernel of some Hilbert space F, if and
only if it is positive definite.
The proof of Theorem 1 can be found in [21]. For further reading on operator-valued kernels and
their associated RKHSs, see, e.g., [5, 6, 7].
2.2

Functional Response Ridge Regression in Dual Variables

We can write the ridge regression with functional responses optimization problem (1) as follows:
n
1 X
1
min kf k2F +
k?i k2Gy
(2)
f ?F 2
2n? i=1
with ?i = yi ? f (xi ).
Now, we introduce the Lagrange multipliers ?i , i = 1, . . . , n which are functional variables since
the output space is the space of functions Gy . For the optimization problem (2), the Lagrangian
multipliers exist and the Lagrangian function is well defined. The method of Lagrange multipliers on
Banach spaces, which is a generalization of the classical (finite-dimensional) Lagrange multipliers
method suitable to solve certain infinite-dimensional constrained optimization problems, is applied
here. For more details, see [16]. Let ? = (?i )i=1,...,n ? Gyn the vector of functions containing the
Lagrange multipliers, the Lagrangian function is defined as
1
1
L(f, ?, ?) = kf k2F +
k?k2Gyn + h?, y ? f (x) ? ?iGyn ,
(3)
2
2n?
where ? = (?1 , . . . , ?n ) ? Gyn , y = (y1 , . . . , yn ) ? Gyn , f (x) = (f (x1 ), . . . , f (xn )) ? Gyn ,
n
P
? = (?1 , . . . , ?n ) ? Gyn , and ?a, b ? Gyn , ha, biGyn =
hai , bi iGy .
i=1

3

Differentiating (3) with respect to f ? F and setting to zero, we obtain
f (.) =

n
X

K(xi , .)?i ,

(4)

i=1

where K : Gx ? Gx ?? L(G y ) is the operator-valued kernel of F.
Substituting this into (3) and minimizing with respect to ?, we obtain the dual of the functional
response kernel ridge regression (KRR) problem
n?
1
k?k2Gyn ? hK?, ?iGyn + h?, yiGyn ,
(5)
2
2
where K = [K(xi , xj )]ni,j=1 is the block operator kernel matrix. The computational details regrading the dual formulation of functional KRR are derived in Appendix B of [14].
max ?
?

2.3

MovKL in Dual Variables

Let us now consider that the function f (?) is sum of M functions {fk (?)}M
k=1 where each fk belongs
to a Gy -valued RKHS with kernel Kk (?, ?). Similarly to scalar-valued multiple kernel learning, we
can cast the problem of learning these functions fk as
min min

M
X
kfk k2F

k

n

+

1 X
k?i k2Gy
2n? i=1

(6)
2dk
PM
with ?i = yi ? k=1 fk (xi ),
P
where d = [d1 , ? ? ? , dM ], D = {d : ?k, dk ? 0 and k drk ? 1} and 1 ? r ? ?. Note that this
problem can equivalently be rewritten as an unconstrained optimization problem. Before deriving
the dual of this problem, it can be shown by means of the generalized Weierstrass theorem [17] that
this problem admits a solution. We report the proof in Appendix A of [14].
d?D fk ?Fk

k=1

Now, following the lines of [24], a dualization of this problem leads to the following equivalent one
min max ?

d?D ??Gyn

where K =

M
P

1
n?
k?k2Gyn ? hK?, ?iGyn + h?, yiGyn ,
2
2

(7)

dk Kk and Kk is the block operator kernel matrix associated to the operator-valued

k=1

kernel Kk . The KKT conditions also state that at optimality we have fk (?) =

n
P

dk Kk (xi , ?)?i .

i=1

3

Solving the MovKL Problem

After having presented the framework, we now devise an algorithm for solving this MovKL problem.
3.1

Block-coordinate descent algorithm

Since the optimization problem (6) has the same structure as a multiple scalar-valued kernel learning
problem, we can build our MovKL algorithm upon the MKL literature. Hence, we propose to
borrow from [15], and consider a block-coordinate descent method. The convergence of a block
coordinate descent algorithm which is related closely to the Gauss-Seidel method was studied in
works of [30] and others. The difference here is that we have operators and block operator matrices
rather than matrices and block matrices, but this doesn?t increase the complexity if the inverse of
the operators are computable (typically analytically or by spectral decomposition). Our algorithm
iteratively solves the problem with respect to ? with d being fixed and then with respect to d with ?
being fixed (see Algorithm 1). After having initialized {dk } to non-zero values, this boils down to
the following steps :
1. with {dk } fixed, the resulting optimization problem with respect to ? has the following
form:
(K + ?I)? = y,
(8)
4

PM
where K = k=1 dk Kk . While the form of solution is rather simple, solving this linear
system is still challenging in the operator setting and we propose below an algorithm for its
resolution.
2. with {fk } fixed, according to problem (6), we can rewrite the problem as
min
d?D

M
X
kfk k2F

k

k=1

dk

(9)

which has a closed-form solution and for which optimality occurs at [20]:
2

kfk k r+1
.
dk = P
2r
( k kfk k r+1 )1/r

(10)

This algorithm is similar to that of [8] and [15] both being based on alternating optimization. The
difference here is that we have to solve a linear system involving a block-operator kernel matrix
which is a combination of basic kernel matrices associated to M operator-valued kernels. This
makes the system very challenging, and we present an algorithm for solving it in the next paragraph.
We also report in Appendix C of [14] a convergence proof of a modified version of the MovKL
algorithm that minimizes a perturbation of the objective function (6) with a small positive parameter
required to guarantee convergence [2].
3.2

Solving a linear system involving multiple operator-valued kernel matrices

One common way to construct operator-valued kernels is to build scalar-valued ones which are
carried over to the vector-valued (resp. function-valued) setting by a positive definite matrix (resp.
operator). In this setting an operator-valued kernel has the following form:
K(w, z) = G(w, z)T,
where G is a scalar-valued kernel and T is a positive operator in L(G y ). In multi-task learning,
T is a finite dimensional matrix that is expected to share information between tasks [11, 5]. More
recently and for supervised functional output learning problems, T is chosen to be a multiplication or an integral operator [12, 13]. This choice is motivated by the fact that functional linear
models for functional responses [25] are based on these operators and then such kernels provide
an interesting alternative to extend these models to nonlinear contexts. In addition, some works on
functional regression and structured-output learning consider operator-valued kernels constructed
from the identity operator as in [19] and [4]. In this work we adopt a functional data analysis point
of view and then we are interested in a finite combination of operator-valued kernels constructed
from the identity, multiplication and integral operators. A problem encountered when working with
operator-valued kernels in infinite-dimensional spaces is that of solving the system of linear operator
equations (8). In the following we show how to solve this problem for two cases of operator-valued
kernel combinations.
Case 1: multiple scalar-valued kernels and one operator. This is the simpler case where the
combination of operator-valued kernels has the following form
K(w, z) =

M
X

dk Gk (w, z)T,

(11)

k=1

where Gk is a scalar-valued kernel, T is a positive operator in L(G y ), and dk are the combination coefficients. In this setting, the block operator kernel matrix K can be expressed as a
PM
Kronecker product between the multiple scalar-valued kernel matrix G =
k=1 dk Gk , where
Gk = [Gk (xi , xj )]ni,j=1 , and the operator T . Thus we can compute an analytic solution of the
system of equations (8) by inverting K + ?I using the eigendecompositions of G and T as in [13].
Case 2: multiple scalar-valued kernels and multiple operators. This is the general case where
multiple operator-valued kernels are combined as follows
K(w, z) =

M
X

dk Gk (w, z)Tk ,

k=1

5

(12)

Algorithm 1 `r -norm MovKL

Algorithm 2 Gauss-Seidel Method

Input Kk for k = 1, . . . , M
1
for k = 1, . . . , M
d1k ??
M
? ?? 0
for t = 1, 2, . . . do
?0 ?? ?
P
K ?? k dtk Kk

choose an initial vector of functions ?(0)
repeat
for i = 1, 2, . . . , n
(t)

?? sol. of (13):
(t)
[K(xi , xi ) + ?I]?i = si
end for
until convergence
?i

? ?? solution of (K + ?I)? = y
if k? ? ?0 k <  then
break
end if
2
kfk k r+1
for k = 1, . . . , M
dt+1
??
P
2r
k
( k kfk k r+1 )1/r
end for

where Gk is a scalar-valued kernel, Tk is a positive operator in L(G y ), and dk are the combination
coefficients. Inverting the associated block operator kernel matrix K is not feasible in this case,
that is why we propose a Gauss-Seidel iterative procedure (see Algorithm 2) to solve the system
of linear operator equations (8). Starting from an initial vector of functions ?(0) , the idea is to
iteratively compute, until a convergence condition is satisfied, the functions ?i according to the
following expression
(t)

[K(xi , xi ) + ?I]?i = yi ?

i?1
X

(t)

K(xi , xj )?j ?

j=1

n
X

(t?1)

K(xi , xj )?j

,

(13)

j=i+1

where t is the iteration index. This problem is still challenging because the kernel K(?, ?) still
involves a positive combination of operator-valued kernels. Our algorithm is based on the idea
that instead of inverting the finite combination of operator-valued kernels [K(xi , xi ) + ?I], we can
consider the following variational formulation of this system
M +1
1 X
(t)
(t)
(t)
Kk (xi , xi )?i , ?i iGy ? hsi , ?i iGy ,
min h
(t) 2
?i
k=1

where si = yi ?

i?1
P
j=1

(t)

K(xi , xj )?j ?

n
P
j=i+1

(t?1)

K(xi , xj )?j

, Kk = dk Gk Tk , ?k ? {1, . . . , M },

and KM +1 = ?I.
Now, by means of a variable-splitting approach, we are able to decouple the role of the different
kernels. Indeed, the above problem is equivalent to the following one :
1 ?
(t)
(t)
(t)
(t)
(t)
min hK(x
i , xi )?i , ?i iGyM ? hsi , ?i iGyM with ?i,1 = ?i,k for k = 2, . . . , M + 1,
(t) 2
?i
? i , xi ) is the (M + 1) ? (M + 1) diagonal matrix [Kk (xi , xi )]M +1 . ?(t) is the vector
where K(x
i
k=1
(t)
(t)
(?i,1 , . . . , ?i,M +1 ) and the (M + 1)-dimensional vector si = (si , 0, . . . , 0). We now have to deal
with a quadratic optimization problem with equality constraints. Writing down the Lagrangian
of this optimization problem and then deriving its first-order optimality conditions leads us to the
following set of linear equations
?
P
? K1 (xi , xi )?i,1 ? si + M
= 0
k=1 ?k
(14)
Kk (xi , xi )?i,k ? ?k
= 0
?
?i,1 ? ?i,k
= 0
where k = 2, . . . , M + 1 and {?k } are the Lagrange multipliers related to the M equality constraints. Finally, in this set of equations, the operator-valued kernels have been decoupled and thus,
if their inversion can be easily computed (which is the case in our experiments), one can solve the
problem (14) with respect to {?i,k } and ?k by means of another Gauss-Seidel algorithm after simple
reorganization of the linear system.
6

Ch. 1

20

1.5

6

0
?20

0

20

40

60

80

100

120

140

160

180

200

0

20

40

60

80

100

120

140

160

180

200

5

1

4

?10

Ch. 3

5
0
?5

0

20

40

60

80

100

120

140

160

180

200

Ch. 4

5

0

?0.5

3
2
1

0
?5

0

20

40

60

80

100

120

140

160

180

200

5

Ch. 5

0.5

Finger Movement

0

Finger Movement State

Ch. 2

10

0

?1

?1

0
?5

0

20

40

60

80

100

120

140

160

180

200

?1.5
0

50

100
Time samples

Time samples

150

200

0

50

100
Time samples

150

200

Figure 1: Example of a couple of input-output signals in our BCI task. (left) Amplitude modulation features extracted from ECoG signals over 5 pre-defined channels. (middle) Signal of labels
denoting whether the finger is moving or not. (right) Real amplitude movement of the finger.

4

Experiments

In order to highlight the benefit of our multiple operator-valued kernel learning approach, we have
considered a series of experiments on a real dataset, involving functional output prediction in a
brain-computer interface framework. The problem we addressed is a sub-problem related to finger movement decoding from Electrocorticographic (ECoG) signals. We focus on the problem of
estimating if a finger is moving or not and also on the direct estimation of the finger movement
amplitude from the ECoG signals. The development of the full BCI application is beyond the scope
of this paper and our objective here is to prove that this problem of predicting finger movement can
benefit from multiple kernel learning.
To this aim, the fourth dataset from the BCI Competition IV [22] was used. The subjects were 3
epileptic patients who had platinium electrode grids placed on the surface of their brains. The number of electrodes varies between 48 to 64 depending on the subject, and their position on the cortex
was unknown. ECoG signals of the subject were recorded at a 1KHz sampling using BCI2000 [27].
A band-pass filter from 0.15 to 200Hz was applied to the ECoG signals. The finger flexion of the
subject was recorded at 25Hz and up-sampled to 1KHz by means of a data glove which measures
the finger movement amplitude. Due to the acquisition process, a delay appears between the finger
movement and the measured ECoG signal [22]. One of our hopes is that this time-lag can be properly learnt by means of multiple operator-valued kernels. Features from the ECoG signals are built
by computing some band-specific amplitude modulation features, which is defined as the sum of the
square of the band-specific filtered ECoG signal samples during a fixed time window.
For our finger movement prediction task, we have kept 5 channels that have been manually selected
and split ECoG signals in portions of 200 samples. For each of these time segments, we have the
label of whether at each time sample, the finger is moving or not as well as the real movement
amplitudes. The dataset is composed of 487 couples of input-output signals, the output signals
being either the binary movement labels or the real amplitude movement. An example of inputoutput signals are depicted in Figure 1. In a nutshell, the problem boils down to be a functional
regression task with functional responses.
To evaluate the performance of the multiple operator-valued kernel learning approach, we use both:
(1) the percentage of labels correctly recognized (LCR) defined by (Wr /Tn ) ? 100%, where Wr
is the number of well-recognized labels and Tn the total number of labels to be recognized; (2) the
residual sum of squares error (RSSE) as evaluation criterion for curve prediction
Z X
RSSE =
{yi (t) ? ybi (t)}2 dt,
(15)
i

where ybi (t) is the prediction of the function yi (t) corresponding to real finger movement or the
finger movement state.
For the multiple operator-valued kernels having the form (12), we have used a Gaussian kernel
with 5 different bandwidths and a polynomial kernel of degree 1 to 3 combined with three oper2
ators T : identity T y(t) = y(t), multiplication operator associated with the function e?t defined
2
by T y(t) = e?t y(t),
and the integral Hilbert-Schmidt operator with the kernel e?|t?s| proposed
R ?|t?s|
in [13], T y(t) = e
y(s)ds. The inverses of these operators can be computed analytically.
7

Table 1: (Left) Results for the movement state prediction. Residual Sum of Squares Error (RSSE)
and the percentage number of Labels Correctly Recognized (LCR) of : (1) baseline KRR with the
Gaussian kernel, (2) functional response KRR with the integral operator-valued kernel, (3) MovKL
with `? , `1 and `2 -norm constraint. (Right) Residual Sum of Squares Error (RSSE) results for
finger movement prediction.
Algorithm

RSSE

LCR(%)

Algorithm

RSSE

KRR - scalar-valued KRR - functional response MovKL - `? norm MovKL - `1 norm MovKL - `2 norm -

68.32
49.40
45.44
48.12
39.36

72.91
80.20
81.34
80.66
84.72

KRR - scalar-valued KRR - functional response MovKL - `? norm MovKL - `1 norm MovKL - `2 norm -

88.21
79.86
76.52
78.24
75.15

While the inverses of the identity and the multiplication operators are easily and directly computable
from the analytic expressions of the operators, the inverse of the integral operator is computed from
its spectral decomposition as in [13]. The number of eigenfunctions as well as the regularization
parameter ? are fixed using ?one-curve-leave-out cross-validation? [26] with the aim of minimizing
the residual sum of squares error.
Empirical results on the BCI dataset are summarized in Table 1. The dataset was randomly partitioned into 65% training and 35% test sets. We compare our approach in the case of `1 and `2 -norm
constraint on the combination coefficients with: (1) the baseline scalar-valued kernel ridge regression algorithm by considering each output independently of the others, (2) functional response ridge
regression using an integral operator-valued kernel [13], (3) kernel ridge regression with an evenlyweighted sum of operator-valued kernels, which we denote by `? -norm MovKL.
As in the scalar case, using multiple operator-valued kernels leads to better results. By directly combining kernels constructed from identity, multiplication and integral operators we could reduce the
residual sum of squares error and enhance the label classification accuracy. Best results are obtained
using the MovKL algorithm with `2 -norm constraint on the combination coefficients. RSSE and
LCR of the baseline kernel ridge regression are significantly outperformed by the operator-valued
kernel based functional response regression. These results confirm that by taking into account the
relationship between outputs we can improve performance. This is due to the fact that an operatorvalued kernel induces a similarity measure between two pairs of input/output.

5

Conclusion

In this paper we have presented a new method for learning simultaneously an operator and a finite linear combination of operator-valued kernels. We have extended the MKL framework to deal
with functional response kernel ridge regression and we have proposed a block coordinate descent
algorithm to solve the resulting optimization problem. The method is applied on a BCI dataset to
predict finger movement in a functional regression setting. Experimental results show that our algorithm achieves good performance outperforming existing methods. It would be interesting for future
work to thoroughly compare the proposed MKL method for operator estimation with previous related methods for multi-class and multi-label MKL in the contexts of structured-output learning and
collaborative filtering.
Acknowledgments
We would like to thank the anonymous reviewers for their valuable comments. This research was
funded by the Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council
and FEDER (Contrat de Projets Etat Region CPER 2007-2013), ANR projects LAMPADA (ANR09-EMER-007) and ASAP (ANR-09-EMER-001), and by the IST Program of the European Community under the PASCAL2 Network of Excellence (IST-216886). This publication only reflects the
authors? views. Francis Bach was partially supported by the European Research Council (SIERRA
Project).
8

References
[1] J. Aflalo, A. Ben-Tal, C. Bhattacharyya, J. Saketha Nath, and S. Raman. Variable sparsity kernel learning.
JMLR, 12:565?592, 2011.
[2] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning,
73(3):243?272, 2008.
[3] F. Bach. Consistency of the group Lasso and multiple kernel learning. JMLR, 9:1179?1225, 2008.
[4] C. Brouard, F. d?Alch?e-Buc, and M. Szafranski. Semi-supervised penalized output kernel regression for
link prediction. In Proc. ICML, 2011.
[5] A. Caponnetto, C. A. Micchelli, M. Pontil, and Y. Ying. Universal multi-task kernels. JMLR, 68:1615?
1646, 2008.
[6] C. Carmeli, E. De Vito, and A. Toigo. Vector valued reproducing kernel Hilbert spaces of integrable
functions and mercer theorem. Analysis and Applications, 4:377?408, 2006.
[7] C. Carmeli, E. De Vito, and A. Toigo. Vector valued reproducing kernel Hilbert spaces and universality.
Analysis and Applications, 8:19?61, 2010.
[8] C. Cortes, M. Mohri, and A. Rostamizadeh. L2 regularization for learning kernels. In Proc. UAI, 2009.
[9] C. Cortes, M. Mohri, and A. Rostamizadeh. Generalization bounds for learning kernels. In ICML, 2010.
[10] F. Dinuzzo, C. S. Ong, P. Gehler, and G. Pillonetto. Learning output kernels with block coordinate descent.
In Proc. ICML, 2011.
[11] T. Evgeniou, C. A. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods. JMLR,
6:615?637, 2005.
[12] H. Kadri, E. Duflos, P. Preux, S. Canu, and M. Davy. Nonlinear functional regression: a functional RKHS
approach. In Proc. AISTATS, pages 111?125, 2010.
[13] H. Kadri, A. Rabaoui, P. Preux, E. Duflos, and A. Rakotomamonjy. Functional regularized least squares
classification with operator-valued kernels. In Proc. ICML, 2011.
[14] H. Kadri, A. Rakotomamonjy, F. Bach, and P. Preux. Multiple operator-valued kernel learning. Technical
Report 00677012, INRIA, 2012.
[15] M. Kloft, U. Brefeld, S. Sonnenburg, and A. Zien. `p -norm multiple kernel learning. JMLR, 12:953?997,
2011.
[16] S. Kurcyusz. On the existence and nonexistence of lagrange multipliers in Banach spaces. Journal of
Optimization Theory and Applications, 20:81?110, 1976.
[17] A. Kurdila and M. Zabarankin. Convex Functional Analysis. Birkhauser Verlag, 2005.
[18] G. Lanckriet, N. Cristianini, L. El Ghaoui, P. Bartlett, and M. Jordan. Learning the kernel matrix with
semi-definite programming. JMLR, 5:27?72, 2004.
[19] H. Lian. Nonlinear functional models for functional responses in reproducing kernel Hilbert spaces. The
Canadian Journal of Statistics, 35:597?606, 2007.
[20] C. Micchelli and M. Pontil. Learning the kernel function via regularization. JMLR, 6:1099?1125, 2005.
[21] C. A. Micchelli and M. Pontil. On learning vector-valued functions. Neural Comput., 17:177?204, 2005.
[22] K. J. Miller and G. Schalk. Prediction of finger flexion: 4th brain-computer interface data competition.
BCI Competition IV, 2008.
[23] T. Pistohl, T. Ball, A. Schulze-Bonhage, A. Aertsen, and C. Mehring. Prediction of arm movement
trajectories from ECoG-recordings in humans. Journal of Neuroscience Methods, 167(1):105?114, 2008.
[24] A. Rakotomamonjy, F. Bach, Y. Grandvalet, and S. Canu. SimpleMKL. JMLR, 9:2491?2521, 2008.
[25] J. O. Ramsay and B. W. Silverman. Functional Data Analysis, 2nd ed. Springer Verlag, New York, 2005.
[26] John A. Rice and B. W. Silverman. Estimating the mean and covariance structure nonparametrically when
the data are curves. Journal of the Royal Statistical Society. Series B, 53(1):233?243, 1991.
[27] G. Schalk, D. J. McFarland, T. Hinterberger, N. Birbaumer, and J. R. Wolpaw. BCI2000: a generalpurpose brain-computer interface system. Biomedical Engineering, IEEE Trans. on, 51:1034?1043, 2004.
[28] B. Sch?olkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA, 2002.
[29] S. Sonnenburg, G. R?atsch, C. Sch?afer, and B. Sch?olkopf. Large scale multiple kernel learning. JMLR,
7:1531?1565, 2006.
[30] P. Tseng. Convergence of block coordinate descent method for nondifferentiable minimization. J. Optim.
Theory Appl., 109:475?494, 2001.

9

"
435,1997,Function Approximation with the Sweeping Hinge Algorithm,Abstract Missing,"Function Approximat.ion with the
Sweeping Hinge Algorithm

Don R. Hush, Fernando Lozano
Dept. of Elec. and Compo Engg.
University of New Mexico
Albuquerque, NM 87131

Bill Horne
MakeWaves, Inc.
832 Valley Road
Watchung, NJ 07060

Abstract
We present a computationally efficient algorithm for function approximation with piecewise linear sigmoidal nodes. A one hidden
layer network is constructed one node at a time using the method of
fitting the residual. The task of fitting individual nodes is accomplished using a new algorithm that searchs for the best fit by solving
a sequence of Quadratic Programming problems. This approach offers significant advantages over derivative-based search algorithms
(e.g. backpropagation and its extensions). Unique characteristics
of this algorithm include: finite step convergence, a simple stopping criterion, a deterministic methodology for seeking ""good"" local
minima, good scaling properties and a robust numerical implementation.

1

Introduction

The learning algorithm developed in this paper is quite different from the traditional family of derivative-based descent methods used to train Multilayer Perceptrons (MLPs) for function approximation. First, a constructive approach is used,
which builds the network one node at a time. Second, and more importantly, we
use piecewise linear sigmoidal nodes instead of the more popular (continuously differentiable) logistic nodes. These two differences change the nature of the learning
problem entirely. It becomes a combinatorial problem in the sense that the number
of feasible solutions that must be considered in the search is finite. We show that
this number is exponential in the input dimension, and that the problem of finding the global optimum admits no polynomial-time solution. We then proceed to
develop a heuristic algorithm that produces good approximations with reasonable
efficiency. This algorithm has a simple stopping criterion, and very few user specified parameters. In addition, it produces solutions that are comparable to (and
sometimes better than) those produced by local descent methods, and it does so

D. R. Hush, R Lozano and B. Horne

536

using a deterministic methodology, so that the results are independent of initial
conditions.

2

Background and Motivation

We wish to approximate an unknown continuous function f(x) over a compact set
with a one-hidden layer network described by
n

f~(x)

= ao + L

(1)

aiU(x, Wi)

i=l

where n is the number of hidden layer nodes (basis functions), x E ~d is the input
vector, and {u(x, w)} are sigmoidal functions parameterized by a weight vector w.
A set of example data, S = {Xi, Yi}, with a total of N samples is available for
training and test.
The models in (1) have been shown to be universal approximators. More importantly, (Barron, 1993) has shown that for a special class of continuous functions,
r c, the generalization error satisfies
~

E[lIf - fn,NII2]

IIf -

fnll 2 + E[lIfn - fn,NII2]

= 0 (*)

+0

( nd ~g N

)

where 11?11 is the appropriate two-norm, f n is the the best n-node approximation to
f, and fn,N is the approximation that best fits the samples in S. In this equation
IIf - fnll 2 and E[lIfn - fn,NII2] correspond to the approximation and estimation error
respectively. Of particular interest is the O(l/n) bound on approximation error,
which for fixed basis functions is of the form O(1/n 2 / d ) (Barron, 1993). Barron's
result tells us that the (tunable) sigmoidal bases are able to avoid the curse of
dimensionality (for functions in rc). Further, it has been shown that the O(l/n)
bound can be achieved constructively (Jones, 1992), that is by designing the basis
functions (nodes) one at a time. The proof of this result is itself constructive,
and thus provides a framework for the development of an algorithm which can (in
principle) achieve this bound. One manifestation of this algorithm is shown in
Figure 1. We call this the iterative approximation algorithm (I1A) because it builds
the approximation by iterating on the residual (Le. the unexplained portion of the
function) at each step. This is the same algorithmic strategy used to form bases in
numerous other settings, e.g. Grahm-Schmidt, Conjugate Gradient, and Projection
Pursuit. The difficult part of the I1A algorithm is in the determination of the best
fitting basis function Un in step 2. This is the focus of the remainder of this paper.

3

Algorithmic Development

We begin by defining the hinging sigmoid (HS) node on which our algorithms are
based. An HS node performs the function
w+,
Uh(X,

w) =

{

- Tw,
X,

w_,

-T>
w,
X _ w+

- Tw_ ~ w,
X ~ w+
T<
w, x _ w_

(2)

where w T = [WI w+ w_] and x is an augmented input vector with a 1 in the first
component. An example of the surface formed by an HS node on a two-dimensional
input is shown in Figure 2. It is comprised of three hyperplanes joined pairwise

Function Approximation with the Sweeping Hinge Algorithm

537

Initialization: fo(x) = 0
for n
1 to nma:c do
1. Compute Residual:
2. Fit Residual:

=

en(x) = f(x) - fn-l (x)
un(x) = argminO""EE lIen(x) - u(x)11
3. Update Estimate:
fn(x) = o:fn-l (x) + f3u n (x)
where 0: and f3 are chosen to minimize IIf(x) - fn(x)1I

endloop

Figure 1: Iterative Approximation Algorithm (rIA).

Figure 2: A Sigmoid Hinge function in two dimensions .
continuously at two hinge locations. The upper and middle hyperplanes are joined
at ""Hinge I"" and the lower and middle hyperplanes are joined at ""Hinge 2"". These
hinges induce linear partitions on the input space that divide the space into three
regions, and the samples in 5 into three subsets,
5+ = {(Xi,Yi): Wr-Xi ~ w+}
5, = {(Xi,Yi): w_ ~ WTXi ~ w+}
5_ = {(Xi,Yi): WTXi ~ w_}

(3)

These subsets, and the corresponding regions of the input space, are referred to as
the PLUS, LINEAR and MINUS subsets/regions respectively. We refer to this type
of partition as a sigmoidal partition. A sigmoidal partition of 5 will be denoted
P = {5+, 5"" 5_}, and the set of all such partitions will be denoted II = {Pd.
Input samples that fall on the boundary between two regions can be assigned to the
set on either side. These points are referred to as hinge samples and playa crucial
role in subsequent development. Note that once a weight vector w is specified, the
partition P is completely determined, but the reverse is not necessarily true. That
is, there are generally an infinite number of weight vectors that induce the same
partition.
We begin our quest for a learning algorithm with the development of an expression
for the empirical risk. The empirical risk (squared error over the sample set) is
defined
(4)

D. R Hush, F. Lozano and B. Horne

538

This expression can be expanded into three terms, one for each set in the partition,
Ep(w) =

~ :E(Yi -

W_)2

~

+ ~ :E(Yi -

W+)2

+ ~ 2)Yi -

~

WT Xi)2

~

After further expansion and rearrangement of terms we obtain
1
Ep(w) = 2wTRw - w T r

where

+

s;

""L:s, XiX; r, = ""L: s, XiYi
s; = ! ""L:s Y; st = ""L:s+ Yi Sy = ""L:s_ Yi
R, =

R=

(

R,

~

r=un

(5)
(6)
(7)
(8)

and N+ , N, and N_ are the number of samples in S+ , S, and S_ respectively. The
subscript P is used to emphasize that this criterion is dependent on the partition (i.e.
P is required to form Rand r). In fact, the nature of the partition plays a critical
role in determining the properties of the solution. When R is positive definite (i.e.
full rank), P is referred to as a stable partition, and when R has reduced rank P is
referred to as an unstable partition. A stable partition requires that R, > O. For
purposes of algorithm development we will assume that R, > 0 when ISti > Nmin,
where Nmin is a suitably chosen value greater than or equal to d + 1. With this, a
necessary condition for a stable partition is that there be at least one sample in S+
and S_ and N, ~ Nmin. When seeking a minimizing solution for Ep(w) we restrict
ourselves to stable partitions because of the potential nonuniqueness associated with
solutions to unstable partitions.
Determining a weight vector that simultaneously minimizes E p (w) and preserves
the current partition can be posed as a constrained optimization problem. This
problem takes on the form
- w Tr
min !wTRw
2
subject to Aw ~ 0

(9)

where the inequality constraints are designed to maintain the current partition defined by (3). This is a Quadratic Programming problem with inequality constraints,
and because R > 0 it has a unique global minimum. The general Quadratic Programming problem is N P-hard and also hard to approximate (Bellare and Rogaway,
1993). However, the convex case which we restrict ourselves to here (i.e. R > 0)
admits a polynomial time solution. In this paper we use the active set algorithm
(Luenberger, 1984) to solve (9). With the proper implementation, this algorithm
runs in O(k(~ + Nd)) time, where k is typically on the order of d or less.
The solution to the quadratic programming problem in (9) is only as good as the
current partition allows. The more challenging aspect of minimizing Ep(w) is in
the search for a good partition. Unfortunately there is no ordering or arrangement
of partitions that is convex in Ep(w), so the search for the optimal partition will
be a computationally challenging problem. An exhaustive search is usually out of
the question because of the prohibitively large number of partitions, as given by the
following lemma.
Lemma 1: Let S contain a total of N samples in Rd that lie in general position.
Then the number of sigmoidal partitions defined in (3) is 8(Nd+l).

Function Approximation with the Sweeping Hinge Algorithm

539

Proof: A detailed proof is beyond the scope of this paper, but an intuitive proof
follows. It is well-known that the number of linear dichotomies of N points in d
dimensions is 8(N d ) (Edelsbrunner, 1987). Each sigmoidal partition is comprised
of two linear dichotomies, one formed by Hinge 1 and the other by Hinge 2, and
these dichotomies are constrained to be simple translations of one another. Thus,
to enumerate all sigmoidal partitions we allow one of the hinges, say Hinge 1, can
take on 8(Nd) different positions. For each of these the other hinge can occupy
only'"" N unique positions. The total is therefore 8 (Nd+l ).
The search algorithm developed here employs a Quadratic Programming (QP) algorithm at each new partition to determine the optimal weight vector for that
partition (Le. the optimal orientation for the separating hyperplanes). Transitions
are made from one partition to the next by allowing hinge samples to flip from one
side of the hinge boundary to the next. The search is terminated when a minimum
value of Ep(w) is found (Le. it can no longer be reduced by flipping hinge samples).
Such an algorithm is shown in Figure 3. We call this the HingeDescent algorithm
because it allows the hinges to ""walk across"" the data in a manner that descends
the Ep(w) criterion. Note that provisions are made within the algorithm to avoid
unstable partitions. Note also that it is easy to modify this algorithm to descend
only one hinge at a time, simply by omitting one of the blocks of code that flips
samples across the corresponding hinge boundary.
{This routine is invoked with a stable feasible solution W = {w, R, r, A, S+, SI, S_ }.}
procedure HingeDescent (W)
{ Allow hinges to walk across the data until a minimizing partition is found. }
E_1wTRw-wTr
- 2
do
Emin = E
{Flip Hinge 1 Samples.}
for each ?Xi, Yi) on Hinge 1) do
if ?Xi, Yi) E S+ and N+ > 1) then
Move (Xi,Yi) from S+ to S"" and update R, r, and A
elseif ?Xi, Yi) E S, and N, > N min ) then
Move (Xi, Yi) from S, to S+, and update R, r, and A
endif
endloop
{Flip Hinge 2 Samples.}
for each ?Xi, Yi) on Hinge 2) do
if ?Xi,Yi) E S- and N_ > 1) then
Move (Xi,Yi) from S- to S"" and update R, r, and A
elseif ?Xi, Yi) E S, and N, > Nmin) then
Move (Xi,Yi) from S, to S-, and update R, r, and A
endif
endloop
{Compute optimal solution for new partition.}
W = QPSolve(W};
E= ~wTRw-wTr
while (E < Emin) j
return(W)j
end;
{HingeDescent}

Figure 3: The HingeDescent Algorithm.
Lemma 2: When started at a stable partition, the HingeDescent algorithm will

D. R Hush, R Lozano and B. Horne

540

converge to a stable partition of Ep(w) in a finite number of steps.

Proof: First note that when R> 0, a QP solution can always be found in a finite
number of steps. The proof of this result is beyond the scope of this paper, but can
easily be found in the literature (Luenberger, 1984). Now, by design, HingeDescent
always moves from one stable partition to the next, maintaining the R > 0 property
at each step so that all QP solutions can be produced in a finite number of steps.
In addition, Ep(w) is reduced at each step (except the last one) so no partitions
are revisited, and since there are a finite number of partitions (see Lemma 1) this
algorithm must terminate in a finite number of steps. QED.
Assume that QPSol ve runs in O(k( cP +N d)) time as previously stated. Then the run
time of HingeDescent is given by O(Np((k+Nh)cP+kNd)), where Nh is the number
of samples flipped at each step and Np is the total number of partitions explored.
Typical values for k and Nh are on the order of d, simplifying this expression to
O(Np(d 3 + NcP)). Np can vary widely, but is often substantially less than N.
HingeDescent seeks a local minimum over II, and may produce a poor solution,
depending on the starting partition. One way to remedy this is to start from
several different initial partitions, and then retain the best solution overall. We
take a different approach here, that always starts with the same initial condition,
visits several local minima along the way, and always ends up with the same final
solution each time.
The SweepingHinge algorithm works as follows. It starts by placing one of the
hinges, say Hinge 1, at the outer boundary of the data. It then sweeps this hinge
across the data, M samples at a time (e.g. M = 1), allowing the other hinge (Hinge
2) to descend to an optimal position at each step. The initial hinge locations are
determined as follows. A linear fit is formed to the entire data set and the hinges are
positioned at opposite ends of the data so that the PLUS and MINUS regions meet
the LINEAR region at the two data samples on either end. After the initial linear
fit, the hinges are allowed to descend to a local minimum using HingeDescent. Then
Hinge 1 is swept across the data M samples at a time. Mechanically this is achieved
by moving M additional samples from S, to S+ at each step. Hinge 2 is allowed
to descend to an optimal position at each of these steps using the Hinge2Descent
algorithm. This algorithm is identical to HingeDescent except that the code that
flips samples across Hinge 1 is omitted. The best overall solution from the sweep is
retained and ""fine-tuned"" with one final pass through the HingeDescent algorithm
to produce the final solution.
The run time of SweepingHinge is no worse than N j M times that of HingeDescent.
Given this, an upper bound on the (typical) run time for this algorithm (with
M = 1) is O(NNp(d 3 + NcP)). Consequently, SweepingHinge scales reasonably
well in both Nand d, considering the nature of the problem it is designed to solve.

4

Empirical Results

The following experiment was adapted from (Breiman, 1993). The function lex) =
e- lIx ll 2 is sampled at 100d points {xd such that IIxll ~ 3 and IIxll is uniform on [0,3].
The dimension d is varied from 4 to 10 (in steps of 2) and models of size 1 to 20
nodes are trained using the I1AjSweepingHinge algorithm. The number of samples
traversed at each step of the sweep in SweepingHinge was set to M = 10. Nmin
was set equal to 3d throughout. A refitting pass was employed after each new node
was added in the I1A. The refitting algorithm used HingeDescent to ""fine-tune""
each node each node before adding the next node. The average sum of squared

Function Approximation with the Sweeping Hinge Algorithm

3000
2500

2000

541

d=4
d=6

d=8
d=10

d=4
d=6

d=8
d=10

1500

1000
500

6

8

10

12

14

16

Number or Nodes

18

20

Figure 4: Upper (lower) curves are for training (test) data.
error, e-2 , was computed for both the training data and an independent set of test
data of size 200d. Plots of 1/e-2 versus the number of nodes are shown in Figure
4. The curves for the training data are clearly bounded below by a linear function
of n (as suggested by inverting the O(l/n) result of Barron's). More importantly
however, they show no significant dependence on the dimension d. The curves for
the test data show the effect of the estimation error as they start to ""bend over""
around n = 10 nodes. Again however, they show no dependence on dimension.

Acknowledgements
This work was inspired by the theoretical results of (Barron, 1993) for Sigmoidal
networks as well as the ""Hinging Hyperplanes"" work of (Breiman, 1993) , and the
""Ramps"" work of (Friedman and Breiman, 1994). This work was supported in part
by ONR grant number N00014-95-1-1315.
References

Barron, A.R. (1993) Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information Theory 39(3):930-945.
Bellare, M. & Rogaway, P. (1993) The complexity of approximating a nonlinear
program. In P.M. Pardalos (ed.), Complexity in numerical optimization, pp. 16-32,
World Scientific Pub. Co.
Breiman, L. (1993) Hinging hyperplanes for regression, classification and function
approximation. IEEE Transactions on Information Theory 39(3):999-1013.
Breiman, L. & Friedman, J.H. (1994) Function approximation using RAMPS. Snowbird Workshop on Machines that Learn.
Edelsbrunner, H. (1987) In EATCS Monographs on Theoretical Computer Science
V. 10, Algorithms in Combinatorial Geometry. Springer-Verlag.
Jones, L.K. (1992) A simple lemma on greedy approximation in Hilbert space and
convergence rates for projection pursuit regression and neural network training.
The Annals of Statistics, 20:608-613.
Luenberger, D.G. (1984) Introduction to Linear and Nonlinear Programming.
Addison-Wesley.

"
6160,2016,On Regularizing Rademacher Observation Losses,"It has recently been shown that supervised learning linear classifiers with two of the most popular losses, the logistic and square loss, is equivalent to optimizing an equivalent loss over sufficient statistics about the class: Rademacher observations (rados). It has also been shown that learning over rados brings solutions to two prominent problems for which the state of the art of learning from examples can be comparatively inferior and in fact less convenient: protecting and learning from private examples, learning from distributed datasets without entity resolution.   Bis repetita placent: the two proofs of equivalence are different and rely on specific properties of the corresponding losses, so whether these can be unified and generalized inevitably comes to mind. This is our first contribution: we show how they can be fit into the same theory for the equivalence between example and rado losses. As a second contribution, we show that the generalization unveils a surprising new connection to regularized learning, and in particular a sufficient condition under which regularizing the loss over examples is equivalent to regularizing the rados (i.e. the data) in the equivalent rado loss, in such a way that an efficient algorithm for one regularized rado loss may be as efficient when changing the regularizer. This is our third contribution: we give a formal boosting algorithm for the regularized exponential rado-loss which boost with any of the ridge, lasso, \slope, l_\infty, or elastic nets, using the same master routine for all. Because the regularized exponential rado-loss is the equivalent of the regularized logistic loss over examples we obtain the first efficient proxy to the minimisation of the regularized logistic loss over examples using such a wide spectrum of regularizers. Experiments with a readily available code display that regularization significantly improves rado-based learning and compares favourably with example-based learning.","On Regularizing Rademacher Observation Losses

Richard Nock
Data61, The Australian National University & The University of Sydney
richard.nock@data61.csiro.au

Abstract
It has recently been shown that supervised learning linear classifiers with two of
the most popular losses, the logistic and square loss, is equivalent to optimizing an
equivalent loss over sufficient statistics about the class: Rademacher observations
(rados). It has also been shown that learning over rados brings solutions to two
prominent problems for which the state of the art of learning from examples can be
comparatively inferior and in fact less convenient: (i) protecting and learning from
private examples, (ii) learning from distributed datasets without entity resolution.
Bis repetita placent: the two proofs of equivalence are different and rely on specific
properties of the corresponding losses, so whether these can be unified and generalized inevitably comes to mind. This is our first contribution: we show how they can
be fit into the same theory for the equivalence between example and rado losses. As
a second contribution, we show that the generalization unveils a surprising new connection to regularized learning, and in particular a sufficient condition under which
regularizing the loss over examples is equivalent to regularizing the rados (i.e. the
data) in the equivalent rado loss, in such a way that an efficient algorithm for one
regularized rado loss may be as efficient when changing the regularizer. This is our
third contribution: we give a formal boosting algorithm for the regularized exponential rado-loss which boost with any of the ridge, lasso, SLOPE, `? , or elastic
net regularizer, using the same master routine for all. Because the regularized exponential rado-loss is the equivalent of the regularized logistic loss over examples
we obtain the first efficient proxy to the minimization of the regularized logistic
loss over examples using such a wide spectrum of regularizers. Experiments with a
readily available code display that regularization significantly improves rado-based
learning and compares favourably with example-based learning.

1

Introduction

What kind of data should we use to train a supervised learner ? A recent result has shown that
minimising the popular logistic loss over examples with linear classifiers (in supervised learning) is
equivalent to the minimisation of the exponential loss over sufficient statistics about the class known
as Rademacher observations (rados, [Nock et al., 2015]), for the same classifier. In short, we fit a
classifier over data that is different from examples, and the same classifier generalizes well to new
observations. It has been shown that rados offer solutions for two problems for which the state of the
art involving examples can be comparatively significantly inferior:
? protection of the examples? privacy from various algebraic, geometric, statistical and computational standpoints, and learning from private data [Nock et al., 2015];
? learning from a large number of distributed datasets without having to perform entity
resolution between datasets [Patrini et al., 2016].
Quite remarkably, the training time of the algorithms involved can be smaller than it would be on
examples, by orders of magnitude [Patrini et al., 2016]. Two key problems remain however: the
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

accuracy of learning from rados can compete experimentally with that of learning from examples, yet
there is a gap to reduce for rados to be not just a good material to learn from in a privacy/distributed
setting, but also a serious alternative to learning from examples at large, yielding new avenues to
supervised learning. Second, theoretically speaking, it is now known that two widely popular losses
over examples admit an equivalent loss in the rado world: the logistic loss and the square loss [Nock
et al., 2015, Patrini et al., 2016]. This inevitably suggests that this property may hold for more losses,
yet barely anything displays patterns of generalizability in the existing proofs.
Our contributions: in this paper, we provide answers to these two questions, with three main
contributions. Our first contribution is to show that this generalization indeed holds: other example
losses admit equivalent losses in the rado world, meaning in particular that their minimiser classifier
is the same, regardless of the dataset of examples. The technique we use exploits a two-player zero
sum game representation of convex losses, that has been very useful to analyse boosting algorithms
[Schapire, 2003, Telgarsky, 2012], with one key difference: payoffs are non-linear convex, eventually
non-differentiable. These also resemble the entropic dual losses [Reid et al., 2015], with the difference
that we do not enforce conjugacy over the simplex. The conditions of the game are slightly different
for examples and rados. We provide necessary and sufficient conditions for the resulting losses over
examples and rados to be equivalent. Informally, equivalence happens iff the convex functions of the
games satisfy a symmetry relationship and the weights satisfy a linear system of equations. Some
popular losses fit in the equivalence [Nair and Hinton, 2010, Gentile and Warmuth, 1998, Nock and
Nielsen, 2008, Telgarsky, 2012, Vapnik, 1998, van Rooyen et al., 2015].
Our second contribution came unexpectedly through this equivalence. Regularizing a loss is standard
in machine learning [Bach et al., 2011]. We show a sufficient condition for the equivalence under
which regularizing the example loss is equivalent to regularizing the rados in the equivalent rado
loss, i.e. making a Minkowski sum of the rado set with a classifier-based set. This property is
independent of the regularizer, and incidentally happens to hold for all our cases of equivalence (Cf
first contribution). A regularizer added to a loss over examples thus transfers to data in the rado world,
in essentially the same way for all regularizers, and if one can solve the non-trivial computational and
optimization problem that poses this data modification for one regularized rado loss, then, basically,
""A good optimization algorithm for this regularized rado loss may fit to other regularizers as well?
Our third contribution exemplifies this. We propose an iterative boosting algorithm, ?-R.A DA B OOST,
that learns a classifier from rados using the exponential regularized rado loss, with regularization
choice belonging to the ridge, lasso, `? , or the recently coined SLOPE [Bogdan et al., 2015]. Since
rado regularization would theoretically require to modify data at each iteration, such schemes are
computationally non-trivial. We show that this modification can in fact be bypassed for the exponential rado loss, and the algorithm, ?-R.A DA B OOST, is as fast as A DA B OOST. ?-R.A DA B OOST has
however a key advantage over A DA B OOST that to our knowledge is new in the boosting world: for
any of these four regularizers, ?-R.A DA B OOST is a boosting algorithm ? thus, because of the
equivalence between the minimization of the logistic loss over examples and the minimization of the
exponential rado loss, ?-R.A DA B OOST is in fact an efficient proxy to boost the regularized logistic
loss over examples using whichever of the four regularizers, and by extension, linear combination of
them (e.g., elastic net regularization [Zou and Hastie, 2005]). We are not aware of any regularized
logistic loss formal boosting algorithm with such a wide spectrum of regularizers. Extensive experiments validate this property: ?-R.A DA B OOST is all the better vs A DA B OOST (unregularized or
regularized) as the domain gets larger, and is able to rapidly learn both accurate and sparse classifiers,
making it an especially good contender for supervised learning at large on big domains.
The rest of this paper is as follows. Sections ?2, 3 and 4 respectively present the equivalence
between example and rado losses, its extension to regularized learning and ?-R.A DA B OOST. ?5
and 6 respectively present experiments, and conclude. In order not to laden the paper?s body, a
Supplementary Material (SM) contains the proofs and additional theoretical and experimental results.

2

Games and equivalent example/rado losses

To avoid notational load, we briefly present our learning setting to point the key quantity in our
.
.
formulation of the general two players game. Let [m] = {1, 2, ..., m} and ?m = {?1, 1}m , for
m > 0. The classical (batch) supervised learner is example-based: it is given a set of examples
S = {(xi , yi ), i ? [m]} where xi ? Rd , yi ? ?1 , ?i ? [m]. It returns a classifier h : Rd ? R from
2

.

a predefined set H. Let zi (h) = yh(xi ) and abbreviate z(h) by z for short. The learner fits h to the
minimization of a loss. Table 1, column `e , presents some losses that can be used: we remark that h
appears only through z, so let us consider in this section that the learner rather fits vector z ? Rm .
We can now define our two players game setting. Let ?e : R ? R and ?r : R ? R two convex and
m
lower-semicontinuous generators. We define functions Le : Rm ?Rm ? R and Lr : R2 ?Rm ? R:
X
X
.
Le (p, z) =
pi zi + ?e
?e (pi ) ,
(1)
i?[m]
.

Lr (q, z) =

i?[m]

X

qI

I?[m]

X

zi + ?r

X

?r (qI ) ,

(2)

I?[m]

i?I

where ?e , ?r > 0 do not depend on z. For the notation to be meaningful, the coordinates in q are
assumed (wlog) to be in bijection with 2[m] . The dependence of both problems in their respective
generators is implicit and shall be clear from context. The adversary?s goal is to fit
.

p? (z) =
.

.

(3)

arg minm Lr (q, z) ,

(4)

p?R

q ? (z) =
m

arg minm Le (p, z) ,
q?H2

m

with H2 = {q ? R2 : 1> q = 1}, so as to attain
.

Le (z) = Le (p? (z), z) ,
.
Lr (z) = Lr (q ? (z), z) ,

(5)
(6)

and let ?Le (z) and ?Lr (z) denote their subdifferentials. We view the learner?s task as the problem
of maximising the corresponding problems in eq. (5) (with examples; this is already sketched above)
or (6) (with what we shall call Rademacher observations, or rados), or equivalently minimising
negative the corresponding function, and then resort to a loss function. The question of when these
two problems are equivalent from the learner?s standpoint motivates the following definition.
Definition 1 Two generators ?e , ?r are said proportionate iff ?m > 0, there exists (?e , ?r ) such that
Le (z)

= Lr (z) + b , ?z ? Rm .

(b does not depend on z) ?m ? N? , let

.
0>
2m?1
Gm =

Gm?1

1>
2m?1
Gm?1



(7)

m

(? {0, 1}m?2 )

(8)

.

if m > 1, and G1 = [0 1] otherwise (notation zd indicates a vector in Rd ).
Theorem 2 ?e , ?r are proportionate iff the optima p? (z) and q ? (z) to eqs (3) and (4) satisfy:
p? (z) ? ?Lr (z) ,
Gm q ? (z) ? ?Le (z) .

(9)
(10)

If ?e , ?r are differentiable and strictly convex, they are proportionate iff p? (z) = Gm q ? (z).
We can alleviate the fact that convexity is strict, which results in a set-valued identity for ?e , ?r to be
proportionate. This gives a necessary and sufficient condition for two generators to be proportionate.
It does not say how to construct one from the other, if possible. We now show that it is indeed possible
and prune the search space: if ?e is proportionate to some ?r , then it has to be a ?symmetrized?
version of ?r , according to the following definition.
.

Definition 3 Let ?r s.t. dom?r ? (0, 1). ?s(r) (z) = ?r (z) + ?r (1 ? z) is the symmetrisation of ?r .
Lemma 4 If ?e and ?r are proportionate, then ?e (z) = (?r /?e ) ? ?s(r) (z) + (b/?e ) (b is in (7)).
3

#
I
II
III
IV

P

`e (z, ?e )
log (1 + exp (zie ))
P
2
(1 + zie )
P i?[m]
max {0, zie }
i?[m]P
e
i zi

i?[m]

P `r (z, ?r ) r
I?[m] exp (zI )
?(EI [?zIr ] ? ?r ? VI [?zIr ])

	
max 0, maxI?[m] {zIr }
EI [zIr ]

?r (z)
z log z ? z
(1/2) ? z 2
?[0,1] (z)
?[ 1m , 1 ] (z)
2

2

ae
?e
?e /4
?e
?e

?e and ?r
??e = ?r
??e = ?r
??e , ?r
??e , ?r

Table 1: Examples of equivalent example and rado losses. Names of the rado-losses `r (z, ?r ) are
respectively the Exponential (I), Mean-variance (II), ReLU
P (III) and Unhinged (IV) rado loss. We
.
.
use shorthands zie = ?(1/?e ) ? zi and zIr = ?(1/?r ) ? i?I zi . Parameter ae appears in eq. (14).
Column ??e and ?r ? gives the constraints for the equivalence to hold. EI and VI are the expectation
and variance over uniform sampling in sets I ? [m] (see text for details).
To summarize, ?e and ?r are proportionate iff (i) they meet the structural property that ?e is
(proportional to) the symmetrized version of ?r (according to Definition 3), and (ii) the optimal
solutions p? (z) and q ? (z) to problems (1) and (2) satisfy the conditions of Theorem 2. Depending on
the direction, we have two cases to craft proportionate generators. First, if we have ?r , then necessarily
?e ? ?s(r) so we merely have to check Theorem 2. Second, if we have ?e , then it matches Definition
31 . In this case, we have to find ?r = f + g where g(z) = ?g(1 ? z) and ?e (z) = f (z) + f (1 ? z).
We now come back to Le (z), Lr (z) (Definition 1), and make the connection with example and rado
losses. In the next definition, an e-loss `e (z) is a function defined over the coordinates of z, and a
r-loss `r (z) is a function defined over the subsets of sums of coordinates. Functions can depend on
other parameters as well.
Definition 5 Suppose e-loss `e (z) and r-loss `r (z) are such that there exist (i) fe : R ? R and
fr (z) : R ? R both strictly increasing and such that ?z ? Rm ,
?Le (z) =
?Lr (z) =

fe (`e (z)) ,
fr (`r (z)) ,

(11)
(12)

where Le (z) and Lr (z) are defined via two proportionate generators ?e and ?r (Definition 1). Then
the couple (`e , `r ) is called a couple of equivalent example-rado losses.
Following is the main Theorem of this Section, which summarizes all the cases of equivalence
between example and rado losses, and shows that the theory developed on example / rado losses with
proportionate generators encompasses the specific proofs and cases already known [Nock et al., 2015,
Patrini et al., 2016]. Table 1 also displays generator ?r .
Theorem 6 In each row of Table 1, `e (z, ?e ) and `r (z, ?r ) are equivalent for ?e and ?r as indicated.
The proof (SM, Subsection 2.3) details for each case the proportionate generators ?e and ?r .

3

Learning with (rado) regularized losses
.

We now detail further the learning setting. In the preceeding Section, we have definef zi (h) = yh(xi ),
which we plug in the losses of Table 1 to obtain the corresponding example and rado losses. Losses
.
simplify conveniently when H consists of linear classifiers, h(x) = ? > x for some ? ? ? ? Rd . In
.
this case, the example loss can be described using edge vectors Se = {yi ? xi , i = 1, 2, ..., m} since
>
zi = ?P (yi ?xi ), and the rado loss can be described using rademacher observations [Nock
P et al., 2015],
.
since i?I zi = ? > ?? for ?i = yi iff i ? I (and ?yi otherwise) and ?? = (1/2) ? i (?i + yi ) ? xi .
.
Let us define S?r = {?? , ? ? ?m } the set of all rademacher observations. We rewrite any couple of
equivalent example and rado losses as `e (Se , ?) and `r (S?r , ?) respectively2 , omitting parameters ?e
and ?r , assumed to be fixed beforehand for the equivalence to hold (see Table 1). Let us regularize
the example loss, so that the learner?s goal is to minimize
`e (Se , ?, ?)
1
2

.

= `e (Se , ?) + ?(?) ,

(13)

Alternatively, ??e is permissible [Kearns and Mansour, 1999].
To prevent notational overload, we blend notions of (pointwise) loss and (samplewise) risk, as just ?losses?.

4

Algorithm 1 ?-R.A DA B OOST
.
Input set of rados Sr = {?1 , ?2 , ..., ?n }; T ? N? ; parameters ? ? (0, 1), ? ? R+ ;
Step 1 : let ?0 ? 0, w0 ? (1/n)1 ;
Step 2 : for t = 1, 2, ..., T
Step 2.1 : call the weak learner: (?(t), rt ) ? ?- WL(Sr , wt , ?, ?, ?t?1 );
.
Step 2.2 : compute update parameters ??(t) and ?t (here, ??k = maxj |?jk |):
??(t) ? (1/(2???(t) )) log((1 + rt )/(1 ? rt )) and

?t ? ? ? (?(?t ) ? ?(?t?1 )) ;

(16)

Step 2.3 : update and normalize weights: for j = 1, 2, ..., n,
wtj


? w(t?1)j ? exp ??t ?j?(t) + ?t /Zt ;

(17)

Return ?T ;

with ? a regularizer [Bach et al., 2011]. The following shows that when fe in eq. (11) is linear, there
is a rado-loss equivalent to this regularized loss, regardless of ?.
Theorem 7 Suppose H contains linear classifiers. Let (`e (Se , ?), `r (S?r , ?)) be any couple of equivalent example-rado losses such that fe in eq. (11) is linear:
fe (z)

= ae ? z + be ,

(14)

for some ae > 0, be ? R. Then for any regularizer ?(.) (assuming wlog ?(0) = 0), the regularized
example loss `e (Se , ?, ?) is equivalent to rado loss `r (S?,?,?
, ?) computed over regularized rados:
r
S?,?,?
r

.
?
= S?r ? {??(?)
? ?} ,

(15)

.
?
Here, ? is Minkowski sum and ?(?)
= ae ? ?(?)/k?k22 if ? 6= 0 (and 0 otherwise).

Theorem 7 applies to all rado losses (I-IV) in Table 1. The effect of regularization on rados is intuitive
from the margin standpoint: assume that a ?good? classifier ? is one that ensures lowerbounded inner
products ? > z ? ? for some margin P
threshold ? . Then any good classifier on a regularized rado ??
shall actually meet, over examples, i:yi =?i ? > (yi ? xi ) ? ? + ae ? ?(?). This inequality ties an
""accuracy"" of ? (edges, left hand-side) and its sparsity (right-hand side). Clearly, Theorem 7 has an
unfamiliar shape since regularisation modifies data in the rado world: a different ?, or a different
?, yields a different S?,?,?
, and therefore it may seem very tricky to minimize such a regularized
r
loss. Even more, iterative algorithms like boosting algorithms look at first glance a poor choice, since
any update on ? implies an update on the rados as well. What we show in the following Section
is essentially the opposite for the exponential rado loss, and a generalization of the R ADO B OOST
algorithm of Nock et al. [2015], which does not modify rados, is a formal boosting algorithm for a
broad set of regularizers. Also, remarkably, only the high-level code of the weak learner depends on
the regularizer; that of the strong learner is not affected.

4

Boosting with (rado) regularized losses

?-R.A DA B OOST presents our approach to learning with rados regularized
with regularizer ? to
. Pt
exp
minimise loss `r (Sr , ?, ?) in eq. (45). Classifier ?t is defined as ?t = t0 =1 ??(t0 ) ? 1?(t0 ) , where
1k is the k th canonical basis vector. The expected edge rt used to compute ?t in eq. (16) is based on
the following basis assignation:
r?(t)

?

1

n
X

???(t)

j=1

wtj ?j?(t) (? [?1, 1]) .

(19)

The computation of rt is eventually tweaked by the weak learner, as displayed in Algorithm ?WL . We investigate four choices for ?. For each of them, we prove the boosting ability of ?R.A DA B OOST (? is symmetric positive definite, Sd is the symmetric group of order d, |?| is the
5

Algorithm 2 ?- WL, for ? ? {k.k1 , k.k2? , k.k? , k.k? }
.
Input set of rados Sr = {?1 , ?2 , ..., ?n }; weights w ? 4n ; parameters ? ? (0, 1), ? ? R+ ;
classifier ? ? Rd ;
Step 1 : pick weak feature ?? ? [d];
Optional ? use preference order: ?  ?0 ? |r? | ? ?? ? |r?0 | ? ??0
.
// ?? = ? ? (?(? + ?? ? 1? ) ? ?(?)), r? is given in (19) and ?? is given in (16)
Step 2 : if ? = k.k2? then

r??
if r?? ? [??, ?]
r? ?
;
(18)
sign (r?? ) ? ? otherwise
else r? ? r?? ;
Return (?? , r? );

vector whose coordinates are the absolute values of the coordinates of ?):
?
.
k?k1 = |?|> 1
Lasso
?
?
.
2
k?k? = ? > ? ?
Ridge
.
?(?) =
k?k
=
max
|?
|
`?
?
?
k
k
?
.
k?k? = maxM?Sd (M|?|)> ? SLOPE

(20)

[Bach et al., 2011, Bogdan et al., 2015, Duchi and Singer, 2009, Su and Cand?s, 2015]. The
.
coordinates of ? in SLOPE are ?k = ??1 (1 ? kq/(2d)) where ??1 (.) is the quantile of the standard
normal distribution and q ? (0, 1); thus, the largest coordinates (in absolute value) of ? are more
penalized. We now establish the boosting ability of ?-R.A DA B OOST. We give no direction for Step
1 in ?- WL, which is consistent with the definition of a weak learner in the boosting theory: all we
require from the weak learner is |r. | no smaller than some weak learning threshold ?WL > 0.
Definition 8 Fix any constant ?WL ? (0, 1). ?- WL is said to be a ?WL -Weak Learner iff the feature
?(t) it picks at iteration t satisfies |r?(t) | ? ?WL , for any t = 1, 2, ..., T .
We also provide an optional step for the weak learner in ?- WL, which we exploit in the experimentations, which gives a total preference order on features to optimise further ?-R.A DA B OOST.
Theorem 9 (boosting with ridge). Take ?(.) = k.k2? . Fix any 0 < a < 1/5, and suppose that ?
and the number of iterations T of ?-R.A DA B OOST are chosen so that
?

<

(2a min max ?2jk )/(T ?? ) ,
j

k

(21)

where ?? > 0 is the largest eigenvalue of ?. Then there exists some ? > 0 (depending on a,
and given to ?- WL) such that for any fixed 0 < ?WL < ?, if ?- WL is a ?WL -Weak Learner, then
?-R.A DA B OOST returns at the end of the T boosting iterations a classifier ?T which meets:
`exp
(Sr , ?T , k.k2? ) ? exp(?a?2WL T /2) .
r

(22)

Furthermore, if we fix a = 1/7, then we can fix ? = 0.98, and if a = 1/10, then we can fix ? = 0.999.
Two remarks are in order. First, the cases a = 1/7, 1/10 show that ?- WL can still obtain large
edges in eq. (19), so even a ?strong? weak learner might fit in for ?- WL, without clamping edges.
Second, the right-hand side of ineq. (21) may be very large if we consider that mink maxj ?2jk may
be proportional to m2 . So the constraint on ? is in fact loose.
Theorem 10 (boosting with lasso or `? ). Take ?(.) ? {k.k1 , k.k? }. Suppose ?- WL is a ?WL -Weak
Learner for some ?WL > 0. Suppose ?0 < a < 3/11 s. t. ? satisfies:
?

= a?WL min max |?jk | .
k

j

(23)

Then ?-R.A DA B OOST returns at the end of the T boosting iterations a classifier ?T which meets:
`exp
(Sr , ?T , ?) ? exp(?T??2WL /2) ,
r
6

(24)

where T? = a?WL T if ? = k.k1 , and T? = (T ? T? ) + a?WL ? T? if ? = k.k? ; T? is the number of
iterations where the feature computing the `? norm was updated3 .
We finally investigate the SLOPE choice. The Theorem is proven for ? = 1 in ?-R.A DA B OOST, for
two reasons: it matches the original definition [Bogdan et al., 2015] and furthermore it unveils an
interesting connection between boosting and SLOPE properties.
.

Theorem 11 (boosting with SLOPE). Take ?(.) = k.k? . Let a = min{3?WL /11, ??1 (1 ?
q/(2d))/ mink maxj |?jk |}. Suppose wlog |?T k | ? |?T (k+1) |, ?k, and fix ? = 1. Suppose (i)
?- WL is a ?WL -Weak Learner for some ?WL > 0, and (ii) the q-value is chosen to meet:


  
3?WL
k
q ? 2 ? max
1??
? max |?jk |
.
j
k
11
d
Then classifier ?T returned by ?-R.A DA B OOST at the end of the T boosting iterations satisfies:
`exp
(Sr , ?T , k.k? ) ? exp(?a?2WL T /2) .
r

(25)

Constraint (ii) on q is interesting in the light of the properties of SLOPE [Su and Cand?s, 2015].
Modulo some assumptions, SLOPE yields a control the false discovery rate (FDR) ? i.e., negligible
coefficients in the ""true? linear model ? ? that are found significant in the learned ? ?. Constraint
(ii) links the ""small? achievable FDR (upperbounded by q) to the ""boostability? of the data: the fact
that each feature k can be chosen by the weak learner for a ""large? ?WL , or has maxj |?jk | large,
precisely flags potential significant features, thus reducing the risk of sparsity errors, and allowing
small q, which is constraint (ii). Using the second order approximation of normal quantiles [Su and
Cand?s, 2015], a sufficient condition for (ii) is that, for some K > 0,
p
?WL min max |?jk | ? K ? log d + log q ?1 ;
(26)
j

j

but minj maxj |?jk | is proportional to m, so ineq. (26), and thus (ii), may hold even for small
samples and q-values. An additional Theorem deferred to SM sor space considerations shows that
for any applicable choice of regularization (eq. 20), the regularized log-loss of ?T over examples
enjoys with high probability a monotonically decreasing upperbound with T as: `log
(Se , ?, ?) ?
e
log 2 ? ? ? T + ?(m), with ?(m) ? 0 when m ? ? (and ? does not depend on T ), and ? > 0 does
not depend on T . Hence, ?-R.A DA B OOST is an efficient proxy to boost the regularized log-loss over
examples, using whichever of the ridge, lasso, `? or SLOPE regularization ? establishing the first
boosting algorithm for this choice ?, or linear combinations of the choices, e.g. for elastic nets. If
we were to compare Theorems 9 ? 11 (eqs (22, 24, 25)), then the convergence looks best for ridge
? 2 )) while it looks slightly worse for `? and SLOPE (the unsigned
(the unsigned exponent is O(?
WL
3
?
exponent is now O(?WL )), the lasso being in between.

5

Experiments

We have implemented ?- WL4 using the order suggested to retrieve the topmost feature in the order.
Hence, the weak learner returns the feature maximising
|r? | ? ?? . The rationale for this comes from
Q
2
the proofs of Theorems 9 ? 11, showing that t exp(?(r?(t)
/2 ? ??(t) )) is an upperbound on the
exponential regularized rado-loss. We do not clamp the weak learner for ?(.) = k.k2? , so the weak
learner is restricted to Step 1 in ?- WL5 .
The objective of these experiments is to evaluate ?-R.A DA B OOST as a contender for supervised
learning per se. We compared ?-R.A DA B OOST to A DA B OOST/`1 regularized-A DA B OOST [Schapire
and Singer, 1999, Xi et al., 2009]. All algorithms are run for a total of T = 1000 iterations, and
at the end of the iterations, the classifier in the sequence that minimizes the empirical loss is kept.
Notice therefore that rado-based classifiers are evaluated on the training set which computes the
3

If several features match this criterion, T? is the total number of iterations for all these features.
Code available at: http://users.cecs.anu.edu.au/?rnock/
5
the values for ? that we test, in {10?u , u ? {0, 1, 2, 3, 4, 5}}, are small with respect to the upperbound in
ineq. (21) given the number of boosting steps (T = 1000), and would yield on most domains a maximal ? ? 1.
4

7

rados. To obtain very sparse solutions for regularized-A DA B OOST, we pick its ? (? in [Xi et al.,
2009]) in {10?4 , 1, 104 }. The complete results aggregate experiments on twenty (20) domains, all
but one coming from the UCI [Bache and Lichman, 2013] (plus the Kaggle competition domain
?Give me some credit?), with up to d =500+ features and m =100 000+ examples. Two tables, in
the SM (Tables 1 and 2 in Section 3) report respectively the test errors and sparsity of classifiers,
whose summary is given here in Table 2. The experimental setup is a ten-folds stratified cross
validation for all algorithms and each domain. A DA B OOST/regularized-A DA B OOST is trained
using the complete training fold. When the domain size m ? 40000, the number of rados n
used for ?-R.A DA B OOST is a random subset of rados of size equal to that of the training fold.
When the domain size exceeds 40000, a random set of n = 10000 rados is computed from the
training fold. Thus, (i) there is no optimisation of the examples chosen to compute rados, (ii) we
always keep a very small number of rados compared to the maximum available, and (iii) when the
domain size gets large, we keep a comparatively tiny number of rados. Hence, the performances
of ?-R.A DA B OOST do not stem from any optimization in the choice or size of the rado sample.
Ada

?
11

k.k2Id
10
3

k.k1
10
3
11

k.k?
8
2
9
7

k.k?
9
1
7
4
8

Experiments support several key observations.
First, regularization consistently reduces the
9
?
test error of ?-R.A DA B OOST, by more than
k.k2Id
10 17
15% on Magic, and 20% on Kaggle. In Table
k.k1
10 17
7
2, ?-R.A DA B OOST unregularized (""?"") is virk.k?
11 18
9
9
tually always beaten by its SLOPE regularized
k.k?
10 19
10
10
11
version. Second, ?-R.A DA B OOST is able to
obtain both very sparse and accurate classiTable 2: Number of domains for which algorithm in
fiers (Magic, Hardware, Marketing, Kaggle).
row beats algorithm in column (Ada = best result of A D Third, ?-R.A DA B OOST competes or beats
A B OOST , ? = ?-R.A DA B OOST not regularized, see text).
A DA B OOST on all domains, and is all the
better as the domain gets bigger. Even qualitatively as seen in Table 2, the best result
obtained by A DA B OOST (regularized or not) does not manage to beat any of the regularized versions
of ?-R.A DA B OOST on the majority of the domains. Fourth, it is important to have several choices
of regularizers at hand. On domain Statlog, the difference in test error between the worst and the
best regularization of ?-R.A DA B OOST exceeds 15%. Fifth, as already remarked [Nock et al., 2015],
significantly subsampling rados (e.g. Marketing, Kaggle) still yields very accurate classifiers. Sixth,
regularization in ?-R.A DA B OOST successfully reduces sparsity to learn more accurate classifiers on
several domains (Spectf, Transfusion, Hill-noise, Winered, Magic, Marketing), achieving efficient
adaptive sparsity control. Last, the comparatively extremely poor results of A DA B OOST on the
biggest domains seems to come from another advantage of rados that the theory developed so far does
not take into account: on domains for which some features are significantly correlated with the class
and for which we have a large number of examples, the concentration of the expected feature value in
rados seems to provide leveraging coefficients that tend to have much larger (absolute) value than in
A DA B OOST, making the convergence of ?-R.A DA B OOST significantly faster than A DA B OOST. For
example, we have checked that it takes much more than the T = 1000 iterations for A DA B OOST to
start converging to the results of regularized ?-R.A DA B OOST on Hardware or Kaggle.
Ada

6

Conclusion

We have shown that the recent equivalences between two example and rado losses can be unified
and generalized via a principled representation of a loss function in a two-player zero-sum game.
Furthermore, we have shown that this equivalence extends to regularized losses, where the regularization in the rado loss is performed over the rados themselves with Minkowski sums. Our theory and
experiments on ?-R.A DA B OOST with prominent regularizers (including ridge, lasso, `? , SLOPE)
indicate that when such a simple regularized form of the rado loss is available, it may help to devise
accurate and efficient workarounds to boost a regularized loss over examples via the rado loss, even
when the regularizer is significantly more involved like e.g. for group norms [Bach et al., 2011].

Acknowledgments
Thanks are due to Stephen Hardy and Giorgio Patrini for stimulating discussions around this material.
8

References
F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Optimization with sparsity-inducing penalties.
Foundations and Trends in Machine Learning, 4:1?106, 2011.
K. Bache and M. Lichman. UCI machine learning repository, 2013.
M Bogdan, E. van den Berg, C. Sabatti, W. Su, and E.-J. Cand?s. SLOPE ? adaptive variable selection
via convex optimization. Annals of Applied Statistics, 2015. Also arXiv:1310.1969v2.
J.-C. Duchi and Y. Singer. Efficient learning using forward-backward splitting. In NIPS*22, pages
495?503, 2009.
C. Gentile and M. Warmuth. Linear hinge loss and average margin. In NIPS*11, pages 225?231,
1998.
M.J. Kearns and Y. Mansour. On the boosting ability of top-down decision tree learning algorithms.
J. Comp. Syst. Sc., 58:109?128, 1999.
V. Nair and G. Hinton. Rectified linear units improve restricted boltzmann machines. In 27th ICML,
pages 807?814, 2010.
R. Nock and F. Nielsen. On the efficient minimization of classification-calibrated surrogates. In
NIPS*21, pages 1201?1208, 2008.
R. Nock, G. Patrini, and A Friedman. Rademacher observations, private data, and boosting. In 32nd
ICML, pages 948?956, 2015.
G. Patrini, R. Nock, S. Hardy, and T. Caetano. Fast learning from distributed datasets without entity
matching. In 26 th IJCAI, 2016.
M.-D. Reid, R.-M. Frongillo, R.-C. Williamson, and N.-A. Mehta. Generalized mixability via
entropic duality. In 28th COLT, pages 1501?1522, 2015.
R.-E. Schapire. The boosting approach to machine learning: An overview. In D.-D. Denison, M.-H.
Hansen, C.-C. Holmes, B. Mallick, and B. Yu, editors, Nonlinear Estimation and Classification,
volume 171 of Lecture Notes in Statistics, pages 149?171. Springer Verlag, 2003.
R. E. Schapire and Y. Singer. Improved boosting algorithms using confidence-rated predictions. MLJ,
37:297?336, 1999.
W. Su and E.-J. Cand?s. SLOPE is adaptive to unkown sparsity and asymptotically minimax. CoRR,
abs/1503.08393, 2015.
M. Telgarsky. A primal-dual convergence analysis of boosting. JMLR, 13:561?606, 2012.
B. van Rooyen, A. Menon, and R.-C. Williamson. Learning with symmetric label noise: The
importance of being unhinged. In NIPS*28, 2015.
V. Vapnik. Statistical Learning Theory. John Wiley, 1998.
Y.-T. Xi, Z.-J. Xiang, P.-J. Ramadge, and R.-E. Schapire. Speed and sparsity of regularized boosting.
In 12th AISTATS, pages 615?622, 2009.
H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal
Statistical Society B, 67:301?321, 2005.

9

"
5427,2015,Bounding errors of Expectation-Propagation,"Expectation Propagation is a very popular algorithm for variational inference, but comes with few theoretical guarantees. In this article, we prove that the approximation errors made by EP can be bounded. Our bounds have an asymptotic interpretation in the number n of datapoints, which allows us to study EP's convergence with respect to the true posterior. In particular, we show that EP converges at a rate of $O(n^{-2})$ for the mean, up to an order of magnitude faster than the traditional Gaussian approximation at the mode. We also give similar asymptotic expansions for moments of order 2 to 4, as well as excess Kullback-Leibler cost (defined as the additional KL cost incurred by using EP rather than the ideal Gaussian approximation). All these expansions highlight the superior convergence properties of EP. Our approach for deriving those results is likely applicable to many similar approximate inference methods. In addition, we introduce bounds on the moments of log-concave distributions that may be of independent interest.","Bounding errors of Expectation-Propagation

Simon Barthelm?
CNRS, Gipsa-lab
simon.barthelme@gipsa-lab.fr

Guillaume Dehaene
University of Geneva
guillaume.dehaene@gmail.com

Abstract
Expectation Propagation is a very popular algorithm for variational inference, but
comes with few theoretical guarantees. In this article, we prove that the approximation errors made by EP can be bounded. Our bounds have an asymptotic interpretation in the number n of datapoints, which allows us to study EP?s convergence with respect to the true posterior. In particular, we show that EP converges
at a rate of O(n?2 ) for the mean, up to an order of magnitude faster than the traditional Gaussian approximation at the mode. We also give similar asymptotic expansions for moments of order 2 to 4, as well as excess Kullback-Leibler cost (defined as the additional KL cost incurred by using EP rather than the ideal Gaussian
approximation). All these expansions highlight the superior convergence properties of EP. Our approach for deriving those results is likely applicable to many
similar approximate inference methods. In addition, we introduce bounds on the
moments of log-concave distributions that may be of independent interest.

Introduction
Expectation Propagation (EP, 1) is an efficient approximate inference algorithm that is known to give
good approximations, to the point of being almost exact in certain applications [2, 3]. It is surprising
that, while the method is empirically very successful, there are few theoretical guarantees on its
behavior. Indeed, most work on EP has focused on efficiently implementing the method in various
settings. Theoretical work on EP mostly represents new justifications of the method which, while
they offer intuitive insight, do not give mathematical proofs that the method behaves as expected.
One recent breakthrough is due to Dehaene and Barthelm? [4] who prove that, in the large datalimit, the EP iteration behaves like a Newton search and its approximation is asymptotically exact.
However, it remains unclear how good we can expect the approximation to be when we have only
finite data. In this article, we offer a characterization of the quality of the EP approximation in terms
of the worst-case distance between the true and approximate mean and variance.
When approximating a probability distribution p(x) that is, for some reason, close to being Gaussian,
a natural approximation to use is the Gaussian with mean equal to the mode (or argmax) of p(x) and
with variance the inverse log-Hessian at the mode. We call it the Canonical Gaussian Approximation
(CGA), and its use is usually justified by appealing to the Bernstein-von Mises theorem, which
shows that, in the limit of a large amount of independent observations, posterior distributions tend
towards their CGA. This powerful justification, and the ease with which the CGA is computed
(finding the mode can be done using Newton methods) makes it a good reference point for any
method like EP which aims to offer a better Gaussian approximation at a higher computational cost.
In section 1, we introduce the CGA and the EP approximation. In section 2, we give our theoretical
results bounding the quality of EP approximations.
1

1

Background

In this section, we present the CGA and give a short introduction to the EP algorithm. In-depth
descriptions of EP can be found in Minka [5], Seeger [6], Bishop [7], Raymond et al. [8].
1.1

The Canonical Gaussian Approximation

What we call here the CGA is perhaps the most common approximate inference method in the
machine learning cookbook. It is often called the ?Laplace approximation?,
but this is a misnomer:
?
the Laplace approximation refers to approximating the integral p from the integral of the CGA.
The reason the CGA is so often used is its compelling simplicity: given a target distribution p(x) =
exp (?? (x)), we find the mode x? and compute the second derivatives of ? at x? :
x?
??

=
=

argmin?(x)
?00 (x? )



to form a Gaussian approximation q(x) = N x|x? , ?1? ? p(x). The CGA is effectively just
a second-order Taylor expansion, and its use is justified by the Bernstein-von Mises theorem [9],
which essentially saysQthat the CGA becomes exact in the large-data (large-n) asymptotic limit.
n
Roughly, if pn (x) ?  i=1 p (yi|x) p0 (x), where y1 . . . yn represent independent datapoints, then
limn?? pn (x) = N x|x?n , ?1? in total variation.
n

1.2

CGA vs Gaussian EP

Gaussian EP, as its name indicates, provides an alternative way of computing a Gaussian approximation to a target distribution. There is broad overlap between the problems where EP can be applied
and the problems where the CGA can be used, with EP coming at a higher cost. Our contribution is
to show formally that the higher computational cost for EP may well be worth bearing, as EP approximations can outperform CGAs by an order of magnitude. To be specific, we focus on the moment
estimates (mean and covariance) computed by EP and CGA, and derive bounds on their distance to
the true mean and variance of the target distribution. Our bounds have an asymptotic interpretation,
and under that interpretation we show for example that the mean returned by EP is within an order
of O n?2 of the true mean, where n is the numberof datapoints. For the CGA, which uses the
mode as an estimate of the mean,
we exhibit a O n?1 upper bound, and we compute the error term

?1
responsible for this O n
behavior. This enables us to show that, in the situations in which this
error is indeed O n?1 , EP is better than the CGA.
1.3

The EP algorithm

We consider the task of approximating a probability distribution over a random-variable X : p(x),
which we call the target distribution. X can be high-dimensional, but for simplicity, we focus on
the one-dimensional case. One important hypothesis that makes EP feasible is that p(x) factorizes
into n simple factor terms:
Y
p(x) =
fi (x)
i

EP proposes to approximate each fi (x) (usually referred to as sites) by a Gaussian function qi (x)
(referred to as the site-approximations). It is convenient to use the parametrization of Gaussians in
terms of natural parameters:


x2
qi (x|ri , ?i ) ? exp ri x ? ?i
2
which makes some of the further computations easier to understand. Note that EP could also be
used with other exponential approximating families. These Gaussian approximations are computed
iteratively. Starting from a current approximation (qit (x|rit , ?it )), we select a site for update with
index i. We then:
2

Q
t
? Compute the cavity distribution q?i
(x) ? j6=1 qjt (x). This is very easy in natural parameters:
??
?
?
? ?
X
X
x2
q?i (x) ? exp ??
rjt ? x ? ?
?jt ? ?
2
j6=i

hti (x)

j6=i

t
q?i
(x)fi (x)

? Compute the hybrid distribution
?
and its mean and variance
? Compute the Gaussian which minimizes the Kullback-Leibler divergence to the hybrid, ie
the Gaussian with same mean and variance:

P(hti ) = argmin KL hti |q
q

? Finally, update the approximation of fi :
qit+1 =

P(hti )
t
q?i

where the division is simply computed as a subtraction between natural parameters
We iterate these operations
Q until a fixed point is reached, at which point we return a Gaussian approximation of p(x) ? qi (x).
1.4

The ?EP-approximation?

In this work, we will characterize the quality of an EP approximation of p(x). We define this to be
any fixed point of the iteration presented in section 1.3, which could all be returned by the algorithm.
It is known that EP will have at least one fixed-point [1], but it is unknown under which conditions
the fixed-point is unique. We conjecture that, when all sites are log-concave (one of our hypotheses
to control the behavior of EP), it is in fact unique but we can?t offer a proof yet. If p (x) isn?t logconcave, it is straightforward to construct examples in which EP has multiple fixed-points. These
open questions won?t matter for our result because we will show that all fixed-points of EP (should
there be more than one) produce a good approximation of p (x).
Fixed points of EP have a very interesting characterization. If we note qi? the site-approximations at
a given fixed-point, h?i the corresponding hybrid distributions, and q ? the global approximation of
p(x), then the mean and variance of all the hybrids and q ? is the same1 . As we will show in section
2.2, this leads to a very tight bound on the possible positions of these fixed-points.
1.5

Notation

Q
We will use repeatedly the following notation. p(x) = i fi (x) is the target distribution we want
to approximate. The sites fi (x) are each approximated
by a Gaussian site-approximation qi (x)
Q
yielding an approximation to p(x) ? q(x) = i qi (x). The hybrids hi (x) interpolate between q(x)
and p(x) by replacing one site approximation qi (x) with the true site fi (x).
Our results make heavy use of the log-functions of the P
sites and the target distribution. We note
?i (x) = ? log (fi (x)) and ?p (x) = ? log (p(x)) =
?i (x). We will introduce in section 2
hypotheses on these functions. Parameter ?m controls their minimum curvature and parameters Kd
control the maximum dth derivative.
We will always consider fixed-points of EP, where the mean and variance under all hybrids and q(x)
is identical. We will note these common values: ?EP and vEP . We will also refer to the third and
fourth centered moment of the hybrids, denoted by mi3 , mi4 and to the fourth moment of q(x) which
2
is simply 3vEP
. We will show how all these moments are related to the true moments of the target
distribution which we will note ?, v for the mean and variance, and mp3 , mp4 for the third and fourth
h 00
i?1
moment. We also investigate the quality of the CGA: ? ? x? and v ? ?p (x? )
where x? is the
the mode of p(x).
1

For non-Gaussian approximations, the expected values of all sufficient statistics of the exponential family
are equal.

3

2

Results

In this section, we will give tight bounds on the quality of the EP approximation (ie: of fixed-points
of the EP iteration). Our results lean on the properties of log-concave distributions [10]. In section
2.1, we introduce new bounds on the moments of log-concave distributions. The bounds show that
those distributions are in a certain sense close to being Gaussian. We then apply these results to
study fixed points of EP, where they enable us to compute bounds on the distance between the mean
and variance of the true distribution p(x) and of the approximation given by EP, which we do in
section 2.2.
Our bounds require us to assume that all sites fi (x) are ?m -strongly log-concave with slowlychanging log-function. That is, if we note ?i (x) = ? log (fi (x)):
00

?i ?x ?i (x) ? ?m > 0


 (d) 
?i ?d ? [3, 4, 5, 6] ?i (x) ? Kd

(1)
(2)

The target distribution
p(x) then inherits those properties from the sites. Noting ?p (x) =
P
? log (p(x)) =
i ?i (x), then ?p is n?m -strongly log-concave and its higher derivatives are
bounded:
00

?x, ?p (x) ?




?d ? [3, 4, 5, 6] ?(d)
p (x) ?

n?m

(3)

nKd

(4)

A natural concern here is whether or not our conditions on the sites are of practical interest. Indeed,
strongly-log-concave likelihoods are rare. We picked these strong regularity conditions because they
make the proofs relatively tractable (although still technical and long). The proof technique carries
over to more complicated, but more realistic, cases. One such interesting generalization consists
of the case in which p(x) and all hybrids at the fixed-point are log-concave with slowly changing
log-functions (with possibly differing constants). In such a case, while the math becomes more
unwieldy, similar bounds as ours can be found, greatly extending the scope of our results. The
results we present here should thus be understood as a stepping stone and not as the final word on
the quality of the EP approximation: we have focused on providing a rigorous but extensible proof.
2.1

Log-concave distributions are strongly constrained

Log-concave distributions have many interesting properties. They are of course unimodal, and the
family is closed under both marginalization and multiplication. For our purposes however, the most
important property is a result due to Brascamp and Lieb [11], which bounds their even moments. We
give here an extension in the case of log-concave distributions with slowly changing log-functions
(as quantified by eq. (2)). Our results show that these are close to being Gaussian.
The Brascamp-Lieb inequality states that, if LC(x) ? exp (??(x)) is ?m -strongly log-concave (ie:
00
? (x) ? ?m ), then centered even moments of LC are bounded by the corresponding moments of a
?1
Gaussian with variance ?m
. If we note these moments m2k and ?LC = ELC (x) the mean of LC:


2k
m2k = ELC (x ? ?LC )
m2k

?

?k
(2k ? 1)!!?m

(5)

where (2k ? 1)!! is the double factorial: the product of all odd terms from 1 to 2k ? 1. 3!! = 3,
5!! = 15, 7!! = 105, etc. This result can be understood as stating that a log-concave distribution
must have a small variance, but doesn?t generally need to be close to a Gaussian.
With our hypothesis of slowly changing log-functions, we were able to improve on this result. Our
improved results include a bound on odd moments, as well as first order expansions of even moments
(eqs. (6)-(9)).
Our extension to the Brascamp-Lieb inequality is as follows. If ? is slowly changing in the sense
0
that some of its higher derivatives are bounded, as per eq. 2, then we can give a bound on ? (?LC )
4

(showing that ?LC is close to the mode x? of LC, see eqs. (10) to (13)) and m3 (showing that LC
is mostly symmetric):

 0


? (?LC )

?

|m3 |

?

K3
2?m
2K3
3
?m

(6)
(7)

and we can compute the first order expansions of m2 and m4 , and bound the errors in terms of ?m
and the K?s :


00
 ?1

m2 ? ? (?LC )

?

 00



? (?LC )m4 ? 3m2 

?

With eq. (8) and (9), we see that m2 ?

K32
K4
+
2
?m
2?m
5 K4
19 K32
+
4
3
2 ?m
2 ?m

(8)
(9)

 00
?2
 00
?1
and m4 ? 3 ? (?LC )
and, in that
? (?LC )
00

sense, that LC(x) is close to the Gaussian with mean ?LC and inverse-variance ? (?LC ).
These expansions could be extended to further orders and similar formulas can be found for the other
?(k+1)
moments of LC(x): for example, any odd moments can be bounded by |m2k+1 | ? Ck K3 ?m
(with Ck some constant) and any even moment can be found to have first-order expansion:
 00
?k
m2k ? (2k ? 1)!! ? (?LC )
. The proof, as well as more detailed results, can be found in
the Supplement.
Note how our result relates to the Bernstein-von Mises theorem, which says that, in the limit of a
large amount of observations, a posterior p(x) tends towards its CGA. If we consider the posterior
obtained from n likelihood functions that are all log-concave and slowly changing, our results show
the slightly different result that the moments of that posterior are close to those of a Gaussian with
00
00
mean ?LC (instead of x?LC ) and inverse-variance ? (?LC ) (instead of ? (x?LC )) . This point is
critical. While the CGA still ends up capturing the limit behavior of p, as ?LC ? x? in the largedata limit (see eq. (13) below), an approximation that would return the Gaussian approximation at
?LC would be better. This is essentially what EP does, and this is how it improves on the CGA.

2.2

Computing bounds on EP approximations

?
In this section, we consider
a given
P
P EP fixed-point qk (x|ri , ?i ) and the corresponding approximation
of p(x): q ? (x|r =
ri , ? =
?i ). We will show that the expected value and variance of q ? (resp.
?EP and vEP ) are close to the true mean and variance of p (resp. ? and v), and also investigate the
h 00
i?1
quality of the CGA (? ? x? , v ? ?p (x? )
).

Under our assumptions on the sites (eq. (1) and (2)), we are able to derive bounds on the quality
of the EP approximation. The proof is quite involved and long, and we will only present it in the
Supplement. In the main text, we give a partial version: we detail the first step of the demonstration, which consists of computing a rough bound on the distance between the true mean ?, the EP
approximation ?EP and the mode x? , and give an outline of the rest of the proof.
Let?s show that ?, ?EP and x? are all close to one another. We start from eq. (6) applied to p(x):
 0

K3


?p (?) ?
2?m
5

(10)

0

which tells us that ?p (?) ? 0. ? must thus be close to x? . Indeed:
 0


 0
0




?p (?) = ?p (?) ? ?p (x? )
 00



= ?p (?) (? ? x? ) ? ? [?, x? ]

 00


? ?p (?) |? ? x? |
? n?m |? ? x? |

(11)

(12)

Combining eq. (10) and (12), we finally have:
|? ? x? | ? n?1

K3
2
2?m

(13)

Let?s now show that ?EP is also close to x? . We proceed similarly, starting from eq. (6) but applied
to all hybrids hi (x):
 0

K3


?i ?i (?EP ) + ??i ?EP ? r?i  ? n?1
(14)
2?m
which is not really equivalent to eq. (10) yet. Recall that q(x|r, ?) has mean ?EP : we thus have:
r = ??EP . Which gives:
!
X
??i ?EP = ((n ? 1)?) ?EP
i

(n ? 1)r
X
=
r?i

=

(15)

i

If we sum all terms in eq. (14), the ??i ?EP and r?i thus cancel, leaving us with:
 0

K3


?p (?EP ) ?
2?m

(16)

which is equivalent to eq. (10) but for ?EP instead of ?. This shows that ?EP is, like ?, close to x? :
|?EP ? x? | ? n?1

K3
2
2?m

(17)

At this point,
we can show that, since they are both close to x? (eq. (13) and (17)), ? = ?EP +

?1
O n
, which constitutes the first step of our computation of bounds on the quality of EP.
After computing this, the next step is evaluating
the quality
of the approximation of the variance,




00


?1 
via computing v ?1 ? vEP
for EP and v ?1 ? ?p (x? ) for the CGA, from eq. (8). In both cases,
we find:
v ?1

?1
vEP
+ O (1)

=

00

?

?p (x ) + O (1)

=

(18)
(19)

Since v ?1 is of order n, because of eq. (5) (Brascamp-Lieb upper bound on variance), this is a
decent approximation: the relative error is of order n?1 .
We can find similarly that both EP and CGA do a good job of finding a good approximation of the
fourth moment of p: m4 . For EP this means that the fourth moment of each hybrid and of q are a
close match:
?i m4

?
?

2
mi4 ? 3vEP
 00
?2
3 ?p (m)

(20)
(21)

In contrast, the third moment of the hybrids doesn?t match at all the third moment of p, but their sum
does !
X
m3 ?
mi3
(22)
i

6

Finally, we come back to the approximation of ? by ?EP . These obey two very similar relationships:
0

?p (?) + ?(3)
p (?)
0

?p (?EP ) + ?(3)
p (?EP )

v
2

vEP
2

 ?1 
= O n
 ?1 
= O n


Since v = vEP + O n?2 (a slight rephrasing of eq. (18)), we finally have:

? = ?EP + O n?2

(23)
(24)

(25)

We summarize the results in the following theorem:
Theorem 1. Characterizing fixed-points of EP
Under the assumptions given by eq. (1) and (2) (log-concave sites with slowly changing log), we
can bound the quality of the EP approximation and the CGA:
|? ? x? |

?

n?1

K3
2
2?m

|? ? ?EP | ? B1 (n) = O n?2


00
K4
2K32
 ?1

+
v ? ?p (x? ) ?
2
?m
2?m
 ?1

v ? v ?1  ? B2 (n) = O (1)
EP



We give the full expression for the bounds B1 and B2 in the Supplement
Note that the order of magnitude of the bound on |? ? x? | is the best possible, because it is attained for certain distributions. For example, consider a Gamma distribution with natural parameters
1
?1
(n?, n?) whose mean ?
by its mode ?
? is approximated at order n
? ? n? . More generally, from
eq. (23), we can compute the first order of the error:
(3)

(3)

??m??

1 ?p (?)
?p (?) v
? ?  00 2
?00p (?) 2
2 ? (?)

(26)

p

which is the term causing the order n?1 error. Whenever this term is significant, it is thus safe to
conclude that EP improves on the CGA.
Also note that, since v ?1 is of order n, the relative error for the v ?1 approximation is of order n?1
for both methods. Despite having a convergence rate of the same order, the EP approximation is
demonstrably better than the CGA, as we show next. Let us first see why the approximation for v ?1
is only of order 1 for both methods. The following relationship holds:
00

v ?1 = ?p (?) + ?(3)
p (?)


mp3
mp4
+ ?(4)
(?)
+ O n?1
p
2v
3!v

(27)

00

In this relationship, ?p (?) is an order n term while the rest are order 1. If we now compare this to
the CGA approximation of v ?1 , we find that it fails at multiple levels. First, it completely ignores
00
the two order 1 terms, and then, because it takes the value of ?p at x? which is at a distance of

(3)
O n?1 from ?, it adds another order 1 error term (since ?p = O (n)). The CGA is thus adding
quite a bit of error, even if each component is of order 1.
Meanwhile, vEP obeys a relationship similar to eq. (27):

2
X  (3)

00
mi
3vEP
?1
vEP
= ?p (?EP ) +
?i (?EP ) 3 + ?(4)
(?
)
+ O n?1
EP
p
2vEP
3!vEP
i
00

(28)

We can see where the EP approximation produces errors. The ?p term is well approximated: since


00
00
|? ? ?EP | = O n?2 , we have ?p (?) = ?p (?EP ) + O n?1 . The term involving m4 is also well
7

approximated, and we can see that the only term that fails is the m3 term. The order 1 error is thus
entirely coming from this term, which shows that EP performance suffers more from the skewness
of the target distribution than from its kurtosis.
Finally, note that, with our result, we can get some intuitions about the quality of the EP approximation using other metrics. For example, if the most interesting metric is the KL divergence KL (p, q),
the excess KL divergence from using the EP approximation q instead of the true minimizer qKL
(which has the same mean ? and variance v as p) is given by:

!
?
?
2
2
qKL
(x ? ?)
(x ? ?EP )
1
v
?KL = p log
=
p(x) ?
+
? log
(29)
q
2v
2vEP
2
vEP



2
1
v
v
(? ? ?EP )
=
? 1 ? log
+
(30)
2 vEP
vEP
2vEP

2
2
(? ? ?EP )
1 v ? vEP
+
(31)
?
4
vEP
2vEP
which we recognize as KL (qKL , q). A similar formula gives the excess KL divergence from using
the CGA instead of qKL . For both methods, the variance term is of order n?2 (though it should be
smaller for EP), but the mean term is of order n?3 for EP while it is of order n?1 for the CGA. Once
again, EP is found to be the better approximation.
Finally, note that our bounds are quite pessimistic: the true value might be a much better fit than we
have predicted here.
A first cause is the bounding of the derivatives of log(p) (eqs. (3),(4)): while those bounds are
correct, they might prove to be very pessimistic. For example, if the contributions from the sites to
the higher-derivatives cancel each other out, a much lower bound than nKd might apply. Similarly,
there might be another lower bound on the curvature much higher than n?m .
Another cause is the bounding of the variance from the curvature. While applying Brascamp-Lieb
requires the distribution to have high log-curvature everywhere, a distribution with high-curvature
close to the mode and low-curvature in the tails still has very low variance: in such a case, the
Brascamp-Lieb bound is very pessimistic.
In order to improve on our bounds, we will thus need to use tighter bounds on the log-derivatives of
the hybrids and of the target distribution, but we will also need an extension of the Brascamp-Lieb
result that can deal with those cases where a distribution is strongly log-concave around its mode
but, in the tails, the log-curvature is much lower.

3

Conclusion

EP has been used for now quite some time without any theoretical concrete guarantees on its performance. In this work, we provide explicit performance bounds and show that EP is superior to the
CGA, in the sense of giving provably better approximations of the mean and variance. There are
now theoretical arguments for substituting EP to the CGA in a number of practical problems where
the gain in precision is worth the increased computational cost. This work tackled the first steps in
proving that EP offers an appropriate approximation. Continuing in its tracks will most likely lead
to more general and less pessimistic bounds, but it remains an open question how to quantify the
quality of the approximation using other distance measures. For example, it would be highly useful for machine learning if one could show bounds on prediction error when using EP. We believe
that our approach should extend to more general performance measures and plan to investigate this
further in the future.

References
[1] Thomas P. Minka. Expectation Propagation for approximate Bayesian inference. In UAI ?01:
Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, pages 362?369,
San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc. ISBN 1-55860-800-1.
URL http://portal.acm.org/citation.cfm?id=720257.
8

[2] Malte Kuss and Carl E. Rasmussen. Assessing Approximate Inference for Binary Gaussian
Process Classification. J. Mach. Learn. Res., 6:1679?1704, December 2005. ISSN 1532-4435.
URL http://portal.acm.org/citation.cfm?id=1194901.
[3] Hannes Nickisch and Carl E. Rasmussen. Approximations for Binary Gaussian Process
Classification. Journal of Machine Learning Research, 9:2035?2078, October 2008. URL
http://www.jmlr.org/papers/volume9/nickisch08a/nickisch08a.pdf.
[4] Guillaume Dehaene and Simon Barthelm?. Expectation propagation in the large-data limit.
Technical report, March 2015. URL http://arxiv.org/abs/1503.08060.
[5] T. Minka. Divergence Measures and Message Passing. Technical report, 2005. URL
http://research.microsoft.com/en-us/um/people/minka/papers/
message-passing/minka-divergence.pdf.
[6] M. Seeger.
Expectation Propagation for Exponential Families.
Technical report,
2005. URL http://people.mmci.uni-saarland.de/~{}mseeger/papers/
epexpfam.pdf.
[7] Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science
and Statistics). Springer, 1st ed. 2006. corr. 2nd printing 2011 edition, October 2007.
ISBN 0387310738. URL http://www.amazon.com/exec/obidos/redirect?
tag=citeulike07-20&path=ASIN/0387310738.
[8] Jack Raymond, Andre Manoel, and Manfred Opper. Expectation propagation, September
2014. URL http://arxiv.org/abs/1409.6179.
[9] Anirban DasGupta.
Asymptotic Theory of Statistics and Probability (Springer
Texts in Statistics).
Springer, 1 edition, March 2008.
ISBN 0387759700.
URL
http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20&
path=ASIN/0387759700.
[10] Adrien Saumard and Jon A. Wellner. Log-concavity and strong log-concavity: A review.
Statist. Surv., 8:45?114, 2014. doi: 10.1214/14-SS107. URL http://dx.doi.org/10.
1214/14-SS107.
[11] Herm J. Brascamp and Elliott H. Lieb. Best constants in young?s inequality, its converse, and
its generalization to more than three functions. Advances in Mathematics, 20(2):151?173,
May 1976. ISSN 00018708. doi: 10.1016/0001-8708(76)90184-5. URL http://dx.doi.
org/10.1016/0001-8708(76)90184-5.

9

"
3191,2010,Getting lost in space: Large sample analysis of the resistance distance,"The commute distance between two vertices in a graph is the expected   time it takes a random walk to travel from the first to the second   vertex and back. We study the behavior of the commute distance as the size of the underlying graph   increases. We prove that the commute distance converges to an   expression that does not take into account the structure of the   graph at all and that is completely meaningless as a distance   function on the graph. Consequently, the use of the raw commute   distance for machine learning purposes is strongly discouraged for   large graphs and in high dimensions. As an alternative we introduce   the amplified commute distance that corrects for   the undesired large sample effects.","Getting lost in space: Large sample analysis of the
commute distance
Ulrike von Luxburg
Agnes Radl
Max Planck Institute for Biological Cybernetics, T?ubingen, Germany
{ulrike.luxburg,agnes.radl}@tuebingen.mpg.de
Matthias Hein
Saarland University, Saarbr?ucken, Germany
hein@cs.uni-sb.de

Abstract
The commute distance between two vertices in a graph is the expected time it takes
a random walk to travel from the first to the second vertex and back. We study the
behavior of the commute distance as the size of the underlying graph increases.
We prove that the commute distance converges to an expression that does not take
into account the structure of the graph at all and that is completely meaningless
as a distance function on the graph. Consequently, the use of the raw commute
distance for machine learning purposes is strongly discouraged for large graphs
and in high dimensions. As an alternative we introduce the amplified commute
distance that corrects for the undesired large sample effects.

1

Introduction

Given an undirected, weighted graph, the commute distance between two vertices u and v is defined
as the expected time it takes a random walk starting in vertex u to travel to vertex v and back to u.
As opposed to the shortest path distance, it takes into account all paths between u and v, not just the
shortest one. As a rule of thumb, the more paths connect u with v, the smaller the commute distance
becomes. As a consequence, it supposedly satisfies the following, highly desirable property:
Property (F): Vertices in the same cluster of the graph have a small commute
distance, whereas two vertices in different clusters of the graph have a ?large?
commute distance.
It is because of this property that the commute distance has become a popular choice and is widely
used, for example in clustering (Yen et al., 2005), semi-supervised learning (Zhou and Sch?olkopf,
2004), in social network analysis (Liben-Nowell and Kleinberg, 2003), for proximity search (Sarkar
et al., 2008), in image processing (Qiu and Hancock, 2005), for dimensionality reduction (Ham
et al., 2004), for graph embedding (Guattery, 1998, Saerens et al., 2004, Qiu and Hancock, 2006,
Wittmann et al., 2009) and even for deriving learning theoretic bounds for graph labeling (Herbster
and Pontil, 2006, Cesa-Bianchi et al., 2009). One of the main contributions of this paper is to
establish that property (F) does not hold in many relevant situations.
In this paper we study how the commute distance (up to a constant factor equivalent to the resistance
distance, see below for exact definitions) behaves when the size of the graph increases. We focus on
the case of random geometric graphs as this is most relevant to machine learning, but similar results
hold for very general classes of graphs under mild assumptions. Denoting by Hij the expected
hitting time, by Cij the commute distance between two vertices vi and vj and by di the degree of
1

vertex vi we prove that the hitting times and commute distances can be approximated (up to the
constant vol(G) that denotes the volume of the graph) by
1
1
Hij ?
vol(G)
dj

1
1
1
+ .
Cij ?
vol(G)
di
dj

and

The intuitive reason for this behavior is that if the graph is large, the random walk ?gets lost? in
the sheer size of the graph. It takes so long to travel through a substantial part of the graph that by
the time the random walk comes close to its goal it has already ?forgotten? where it started from.
For this reason, the hitting time Hij does not depend on the starting vertex vi any more. It only
depends on the inverse degree of the target vertex vj , which intuitively represents the likelihood that
the random walk exactly hits vj once it is in its neighborhood. In this respect it shows the same
behavior as the mean return time at j (the mean time it takes a random walk that starts at j to return
to its staring point) which is well-known to be vol(G) ? 1/dj as well.
Our findings have very strong implications:
The raw commute distance is not a useful distance function on large graphs. On the negative
side, our approximation result shows that contrary to popular belief, the commute distance does not
take into account any global properties of the data, at least if the graph is ?large enough?. It just
considers the local density (the degree of the vertex) at the two vertices, nothing else. The resulting
large sample commute distance dist(vi , vj ) = 1/di + 1/dj is completely meaningless as a distance
on a graph. For example, all data points have the same nearest neighbor (namely, the vertex with
the largest degree), the same second-nearest neighbor (the vertex with the second-largest degree),
and so on. In particular, the main motivation to use the commute distance, Property (F), no longer
holds when the graph becomes ?large enough?. Even more disappointingly, computer simulations
show that n does not even need to be very large before (F) breaks down. Often, n in the order of
1000 is already enough to make the commute distance very close to its approximation expression
(see Section 5 for details). This effect is even stronger if the dimensionality of the underlying data
space is large. Consequently, even on moderate-sized graphs, the use of the raw commute distance
as a basis for machine learning algorithms should be discouraged.
Correcting the commute distance. It has been reported in the literature that hitting times and commute times can be observed to be quite small if the vertices under consideration have a high degree,
and that the spread of the commute distance values can be quite large (Liben-Nowell and Kleinberg,
2003, Brand, 2005, Yen et al., 2009). Subsequently, the authors suggested several different methods
to correct for this unpleasant behavior. In the light of our theoretical results we can see immediately
why the undesired behavior of the commute distance occurs. Moreover, we are able to analyze the
suggested corrections and prove which ones are meaningful and which ones not (see Section 4).
Based on our theory we suggest a new correction, the amplified commute distance. This is a new
distance function that is derived from the commute distance, but avoids its artifacts. This distance
function is Euclidean, making it well-suited for machine learning purposes and kernel methods.
Efficient computation of approximate commute distances. In some applications the commute
distance is not used as a distance function, but for other reasons, for example in graph sparsification
(Spielman and Srivastava, 2008) or when computing bounds on mixing or cover times (Aleliunas
et al., 1979, Chandra et al., 1989, Avin and Ercal, 2007, Cooper and Frieze, 2009) or graph labeling
(Herbster and Pontil, 2006, Cesa-Bianchi et al., 2009). To obtain the commute distance between all
points in a graph one has to compute the pseudo-inverse of the graph Laplacian matrix, an operation
of time complexity O(n3 ). This is prohibitive in large graphs. To circumvent the matrix inversion,
several approximations of the commute distance have been suggested in the literature (Spielman and
Srivastava, 2008, Sarkar and Moore, 2007, Brand, 2005). Our results lead to a much simpler and
well-justified way of approximating the commute distance on large random geometric graphs.

2

General setup, definitions and notation

We consider undirected, weighted graphs G = (V, E) with n vertices. We always assume that G is
connected and not bipartite.PThe non-negative weight matrix (adjacency matrix) is denoted
W :=
Pby
n
n
(wij )i,j=1,...,n . By di := j=1 wij we denote the degree of vertex vi and vol(G) := j=1 dj is
the volume of the graph. D denotes the diagonal matrix with diagonal entries d1 , . . . , dn and is
called the degree matrix.
2

Our main focus in this paper is the class of random geometric graphs as it is most relevant to machine
learning. Here we are given a sequence of points X1 , . . . , Xn that has been drawn i.i.d. from some
underlying density p on Rd . These points form the vertices v1 , . . . , vn of the graph. The edges in
the graph are defined such that ?neighboring points? are connected: In the ?-graph we connect two
points whenever their Euclidean distance is less than or equal to ?. In the undirected, symmetric
k-nearest neighbor graph we connect vi to vj if Xi is among the k nearest neighbors of Xj or vice
versa. In the mutual k-nearest neighbor graph we connect vi to vj if Xi is among the k nearest
neighbors of Xj and vice versa. For space constraints we only discuss the case of unweighted
graphs in this paper. Our results can be carried over to weighted graphs, in particular to weighted
kNN-graphs and Gaussian similarity graphs.
Consider the natural random walk on G, that is the random walk with transition matrix P = D?1 W .
The hitting time Hij is defined as the expected time it takes a random walk starting in vertex vi to
travel to vertex vj (with Hii := 0 by definition). The commute distance (also called commute time)
between vi and vj is defined as Cij := Hij + Hji . Some readers might also know the commute
distance under the name resistance distance. Here one interprets the graph as an electrical network
where the edges represent resistors. The conductance of a resistor is given by the corresponding edge
weight. The resistance distance Rij between i and j is defined as the effective resistance between
the vertices i and j in the network. It is well known that the resistance distance coincides with the
commute distance up to a constant: Cij = vol(G)?Rij . For background reading see Doyle and Snell
(1984), Klein and Randic (1993), Xiao and Gutman (2003), Fouss et al. (2006), Bollob?as (1998),
Lyons and Peres (2010).
For the rest of the paper we consider a probability distribution with density p on Rd . We want to
study the behavior of the commute distance between two fixed points s and t. We will see that we
only need to study the density in a reasonably small region X ? Rd that contains s and t. For
convenience, let us make the following definition.
Definition 1 (Valid region) Let p be any density on Rd , and s, t ? Rd be two points with
p(s), p(t) > 0. We call a connected subset X ? Rd a valid region with respect to s, t and p if
the following properties are satisfied:
1. s and t are interior points of X .
2. The density on X is bounded away from 0, that is for all x ? X we have that p(x) ?
pmin > 0 for some constant pmin . Assume that pmax := maxx?X p(x) < ?.
3. X has ?bottleneck? larger than some value h > 0: the set {x ? X : dist(x, ?X ) > h/2}
is connected (here ?X denotes the topological boundary of X ).
4. The boundary of X is regular in the following sense. We assume that there exist positive
constants ? > 0 and ?0 > 0 such that if ? < ?0 , then for all points x ? ?X we have
vol(B? (x) ? X ) ? ? vol(B? (x)) (where vol denotes the Lebesgue volume). Essentially
this condition just excludes the situation where the boundary has arbitrarily thin spikes.
For readability reasons, we are going to state some of our main results using constants ci > 0. These
constants are independent of n and the graph connectivity parameter (? or k, respectively) but depend
on the dimension, the geometry of X , and p. The values of all constants are determined explicitly in
the proofs. They do not coincide across different propositions. For notational convenience, we will
formulate all the following results in terms of the resistance distance. To obtain the results for the
commute distance one just has to multiply by factor vol(G).

3

Convergence of the resistance distance on random geometric graphs

In this section we present our theoretical main results for random geometric graphs. We show that
on this type of graph, the resistance distance Rij converges to the trivial limit 1/di + 1/dj . For
space constraints we only formulate these results for unweighted kNN and ?-graphs. Similar results
also hold for weighted variants of these graphs and for Gaussian similarity graphs.
Theorem 2 (Resistance distance on kNN-graphs) Fix two points Xi and Xj . Consider a valid
region X with respect to Xi and Xj with bottleneck h and density bounds pmin and pmax . Assume
that Xi and Xj have distance at least h from the boundary of X and that (k/n)1/d /2pmax ? h.
3

Then there exist constants c1 , . . . , c5 > 0 such that with probability at least 1 ? c1 n exp(?c2 k) the
resistance distance on both the symmetric and the mutual kNN-graph satisfies



  1
1/3


if d = 3
kRij ? k + k  ? c4 k log(n/k) + (k/n) + 1

c5 k1
if d > 3
di
dj 
The probability converges to 1 if n ? ? and k/ log(n) ? ?. The rhs of the deviation bound
converges to 0 as n ? ?, if k ? ? and k/ log(n/k) ? ? in case d = 3, and if k ? ? in case
d > 3. Under these conditions, if the density p is continuous and if additionally k/n ? 0, then
kRij ? 2 in probability.
Theorem 3 (Resistance distance on ?-graphs) Fix two points Xi and Xj . Consider a valid region
X with respect to Xi and Xj with bottleneck h and density bounds pmin and pmax . Assume that Xi
and Xj have distance at least h from the boundary of X and that ? ? h. Then there exist constants
c1 , . . . , c6 > 0 such that with probability at least 1 ? c1 n exp(?c2 n?d ) ? c3 exp(?c4 n?d )/?d the
resistance distance on the ?-graph satisfies

 ( log(1/?)+?+1
 d
d 
 d
if d = 3
n?3
n? Rij ? n? + n?  ? c5
1


di
dj
if d > 3
c6 n?d
The probability converges to 1 if n ? ? and n?d / log(n) ? ?. The rhs of the deviation bound
converges to 0 as n ? ?, if n?3 / log(1/?) ? ? in case d = 3, and if n?d ? ? in case d > 3.
Under these conditions, if the density p is continuous and if additionally ? ? 0, then
1
1
n?d Rij ?
+
in probability.
?d p(Xi ) ?d p(Xj )
Let us discuss the theorems en bloc. We start with a couple of technical remarks. Note that to achieve
the convergence of the resistance distance we have to rescale it appropriately (for example, in the
?-graph we scale by a factor of n?d ). Our rescaling is exactly chosen such that the limit expressions
are finite, positive values. Scaling by any other factor in terms of n, ? or k either leads to divergence
or to convergence to zero.
The convergence conditions on n and ? (or k, respectively) are the ones to be expected for random
geometric graphs. They are satisfied as soon as the degrees are of the order log(n) (for smaller
degrees, the graphs are not connected anyway, see e.g. Penrose, 1999). Hence, our results hold for
sparse as well as for dense connected random geometric graphs.
The valid region X has been introduced for technical reasons. We need to operate in such a region
in order to be able to control the behavior of the graph, e.g. the average degrees. The assumptions
on X are the standard assumptions used in the random geometric graph literature. In our setting, we
have the freedom of choosing X ? Rd as we want. In order to obtain the tightest bounds one should
aim for a valid X that has a wide bottleneck and a high minimal density.
More generally, results about the convergence of the commute distance to 1/di + 1/dj can also be
proved for other kinds of graphs such as graphs with given expected degrees and even for power law
graphs, under the assumption that the minimal degree in the graph slowly increases with n. Details
are beyond the scope of this paper.
Proof outline of Theorems 2 and 3 (full proofs are presented in the supplementary material). Consider two fixed vertices s and t in a connected graph and consider the graph as an electrical network
where each edge has resistance 1. By the electrical laws, resistances in series add up, that is for two
resistances R1 and R2 in series we get the overall resistance R = R1 + R2 . Resistances in parallel
lines satisfy 1/R = 1/R1 + 1/R2 . Now consult the situation in Figure 1. Consider the vertex s and
all edges from s to its ds neighbors. The resistance ?spanned? by these ds parallel edges satisfies
Pds
1/R = i=1
1, that is R = 1/ds . Similarly for t. Between the neighbors of s and the ones of t there
are very many paths. It turns out that the contribution of these paths to the resistance is negligible
(essentially, we have so many wires between the two neighborhoods that electricity can flow nearly
freely). So the overall effective resistance between s and t is dominated by the edges adjacent to i
and j with contributions 1/ds + 1/dt .
Providing a clean mathematical proof for this argument is quite technical. Our proof is based on
Corollary 6 in Section IX.2 of Bollob?as (1998) that states that the resistance distance beween two
4

ds many outgoing
edges, each with s
resistance 1

d t many outgoing
t edges, each with
resistance 1
very many paths

Figure 1: Intuition for the proof of Theorems 2 and 3. See text for details.
fixed vertices s and t can be expressed as

nP
o
2 
Rst = inf
u
u
=
(u
)
unit
flow
from
s
to
t
.

e
e?E
e?E e
To apply this theorem one has to construct a flow that spreads ?as widely as possible? over the whole
graph. Counting edges and adding up resistances then leads to the desired results. Details are fiddly
and can be found in the supplementary material.
,

4

Correcting the resistance distance

Obviously, the large sample resistance distance Rij ? 1/di + 1/dj is completely meaningless as
a distance on a graph. The question we want to discuss in this section is whether there is a way to
correct the commute distance such that this unpleasant large sample effect does not occur. Let us
start with some references to the literature. It has been observed in several empirical studies that the
commute distances are quite small if the vertices under consideration have a high degree, and that
the spread of the commute distance values can be quite large. Our theoretical results immediately
explain this behavior: if the degrees are large, then 1/di + 1/dj is very small. And compared to the
?spread? of di , the spread of 1/di can be enormous.
Several heuristics have been suggested to solve this problem. Liben-Nowell and Kleinberg (2003)
suggest to correct the hitting times by simply multiplying by the degrees. For the commute distance,
this leads to the suggested correction of CLN K (i, j) := dj Hij + di Hji . Even though we did
not prove it explicitly in our paper, the convergence results for the commute time also hold for
the individual hitting times. Namely, hitting time Hij can be approximated by vol(G)/dj . These
theoretical results immediately show that the correction CLN K is not useful, at least if we consider
the absolute values. For large graphs, it simply has the effect of normalizing all hitting times to
? 1, leading to CLN K ? 2. However, we believe that the ranking introduced by this distance
function still contains useful information about the data. The reason is that while the first order
terms dominate the absolute value and converge to two, the second order terms introduce some
?variation around two?, and this variation might encode the cluster structure.
Yen et al. (2009) exploit the well-known fact that the commute distance is Euclidean and its kernel
matrix coincides with the Moore-Penrose inverse L+ of the graph Laplacian matrix. The authors
+
now apply a sigmoid transformation to L+ and consider KYen (i, j) = 1/(1 + exp(?lij
/?)) for
some contant ?. The idea is that the sigmoid transformation reduces the spread of the distance (or
similarity) values. However, this is an ad-hoc approach that has the disadvantage that the resulting
?kernel? KYen is not positive definite.
A third correction has been suggested in Brand (2005). As Yen et al. (2009) he considers the kernel
matrix that corresponds to the commute distance. But instead of applying a sigmoid transformation
he centers and normalizes the kernel matrix in the feature space. This leads to the corrected kernel
p
? ij / K
? ii K
? jj
? ij = ?Rij + 1 Pn (Rik + Rkj ) ? 12 Pn
KBrand (i, j) = K
with
2K
k=1
k,l=1 Rkl .
n
n
One the first glance it is surprising that using the centered and normalized kernel instead of the
commute distance should make any difference. However, whenever one takes a Euclidean distance
function of the form dist(i, j) = sij + ui + uj ? 2?ij ui and computes the corresponding centered
kernel matrix, one obtains
n
2
2 X
s
Kij = Kij
+ 2?ij ui ? (ui + uj ) + 2
ur ,
(1)
n
n r=1
5

where K s is the kernel matrix induced by s. Thus the off-diagonal terms are still influenced by ui
but with a decaying factor n1 compared to the diagonal. Even though this is no longer the case after
normalization (because for the normalization the diagonal terms are important, and these terms still
depend on the di ), we believe that this is the key to why Brand?s kernel is useful.
What would be a suitable correction based on our theoretical results? The proof of our main theorems shows that the edges adjacent to i and j completely dominate the behavior of the resistance
distance: they are the ?bottleneck? of the flow, and their contribution 1/di + 1/dj dominates all
the other terms. The interesting information about the global topology of the graph is contained
in the remainder terms Sij = Rij ? 1/di ? 1/dj , which summarize the flow contributions of all
other edges in the graph. We believe that the key to obtaining a good distance function is to remove
the influence of the 1/di terms and ?amplify? the influence of the general graph term Sij . This
can be achieved by either using the off-diagonal terms of the pseudo-inverse graph Laplacian L?
while ignoring its diagonal, or by building a distance function based on the remainder terms Sij
directly. We choose the second option and propose the following new distance function. We define
the amplified commute distance as Camp (i, j) = Sij + uij with Sij = Rij ? 1/di ? 1/dj and
uij = 2wij /di dj ? wii /d2i ? wjj /d2j . Of course we set Camp (i, i) = 0 for all i.
Proposition 4 (Amplified commute distance is Euclidean) The matrix D with entries dij =
Camp (i, j)1/2 is a Euclidean distance matrix.
Proof outline. In preliminary work we show that the remainder terms can be written as Sij =
h(ei ? ej ), B(ei ? ej )i ? uij where ei denotes the i-th unit vector and B is a positive definite matrix
(see the proof of Proposition 2 in von Luxburg et al., 2010). This implies the desired statement. ,

Additionally to being a Euclidean distance, the amplified commute distance has a nice limit behavior.
When n ? ? the terms uij are dominated by the terms Sij , hence all that is left are the ?interesting
terms? Sij . For all practical purposes, one should use the kernel induced by the amplified commute
distance and center and normalize it. In formulas, the amplified commute kernel is
q
? ij / K
? ii K
? jj
? = (I ? 1 110 )Camp (I ? 1 110 )
Kamp (i, j) := K
with
K
(2)
n
n
(where I is the identity matrix, 1 the vector of all ones, and Camp the amplified commute distance
matrix). The next section shows that the kernel Kamp works very nicely in practice.
Note that the correction by Brand and our amplified commute kernel are very similar, but not identical with each other. The off-diagonal terms of both kernels are very close to each other, see Equation
(1), that is if one is only interested in a ranking based on similarity values, both kernels behave similarly. However, an important difference is that the diagonal terms in the Brand kernel are way bigger
than the ones in the amplified kernel (using our convergence techniques one can show that the Brand
kernel converges to an identity matrix, that is the diagonal completely dominates the off-diagonal
terms). This might lead to the effect that the Brand kernel behaves worse than our kernel with
algorithms like the SVM that do not ignore the diagonal of the kernel.

5

Experiments

Our first set of experiments considers the question how fast the convergence of the commute distance
takes place in practice. We will see that already for relatively small data sets, a very good approximation takes place. This means that the problems of the raw commute distance already occur for
small sample size. Consider the plots in Figure 2. They report the maximal relative error defined as
maxij |Rij ?1/di ?1/dj |/Rij and the corresponding mean relative error on a log10 -scale. We show
the results for ?-graphs, unweighted kNN graphs and Gaussian similarity graphs (fully connected
weighted graphs with edge weights exp(kxi ? xj k2 /? 2 )). In order to be able to plot all results in
the same figure, we need to match the parameters of the different graphs. Given some value k for
the kNN-graph we thus set the values of ? for the ?-graph and ? for the Gaussian graph to be equal
to the maximal k-nearest neighbor distance in the data set.
Sample size. Consider a set of points drawn from the uniform distribution on the unit cube in R10 .
As can be seen in Figure 2 (first plot), the maximal relative error decreases very fast with increasing
sample size. Note that already for small sample sizes the maximal deviations get very small.
Dimension. A result that seems surprising at first glance is that the maximal deviation decreases
6

data uniform, dim=10, k= n/10

data uniform, n=2000, k=100
?0.5

Gaussian
epsilon
knn

?1.5

log (rel deviation)

?2
?2.5
?3

?1.5
?2
?2.5
?3
2

1000
2000
n
mixture of Gaussians,n=2000, dim=10, k=100
0
Gaussian
epsilon
?1
knn

200 500

5
dim
USPS data set

0
log (rel deviation)

?2

10

Gaussian
epsilon
knn

?1
?2
?3

10

log10(rel deviation)

Gaussian
epsilon
knn

?1

10

log10(rel deviation)

?1

?3
?4
1

2
separation

?4
?5
0

3

1

log10(k)

2

3

Figure 2: Relative deviations between true and approximate commute distances. Solid lines show
the maximal relative deviations, dashed lines the mean relative deviations. See text for details.

as we increase the dimension, see Figure 2 (second plot). The intuitive explanation is that in higher
dimensions, geometric graphs mix faster as there exist more ?shortcuts? between the two sides of
the point cloud. Thus, the random walk ?forgets faster? where it started from.
Clusteredness. The deviation gets worse if the data has a more pronounced cluster structure. Consider a mixture of two Gaussians in R10 with unit variances and the same weight on both components. We call the distance between the centers of the two components the separation. In Figure 2
(third plot) we show both the maximum relative errors (solid lines) and mean relative errors (dashed
lines). We can clearly see that with increasing separation, the deviation increases.
Sparsity. The last plot of Figure 2 shows the relative errors for increasingly dense graphs, namely
for increasing parameter k. Here we used the well-known USPS data set of handwritten digits (9298
points in 256 dimensions). We plot both the maximum relative errors (solid lines) and mean relative
errors (dashed lines). We can see that the errors decrease the denser the graph gets. Again this is
due to the fact that the random walk mixes faster on denser graphs. Note that the deviations are
extremely small on this real-world data set.
In a second set of experiments we compare the different corrections of the raw commute distance. To
this end, we built a kNN graph of the whole USPS data set (all 9298 points, k = 10), computed the
commute distance matrix and the various corrections. The resulting matrices are shown in Figure
3 (left part) as heat plots. In all cases, we only plot the off-diagonal terms. We can see that as
predicted by theory, the raw commute distance does not identify the cluster structure. However, the
cluster structure is still visible in the kernel corresponding to the commute distance, the pseudoinverse graph Laplacian L? . The reason is that the diagonal of this matrix can be approximated by
(1/d1 , ...., 1/dn ), whereas the off-diagonal terms encode the graph structure, but on a much smaller
scale than the diagonal. In our heat plots, all four corrections of the graph Laplacian show the cluster
structure to a certain extent (the correction by LNK to a small extent, the corrections by Brand, Yen
and us to a bigger extent).
A last experiment evaluates the performance of the different distances in a semi-supervised learning
task. On the whole USPS data set, we first chose some random points to be labeled. Then
we classified the unlabeled points by the k-nearest neighbor classifier based on the distances
to the labeled data points. For each classifier, k was chosen by 10-fold cross-validation among
k ? {1, ..., 10}. The experiment was repeated 10 times. The mean results can be seen in Figure 3
(right figure). As baseline we also report results based on the standard Euclidean distance between
the data points. As predicted by theory, we can see that the raw commute distance performs
extremely poor. The Euclidean distance behaves reasonably, but is outperformed by all corrections
of the commute distance. This shows first of all that using the graph structure does help over the
basic Euclidean distance. While the naive correction by LNK stays close to the Euclidean distance,
the three corrections by Brand, Yen and us virtually lie on top of each other and outperform the
7

Semi?supervised learning task
1

Raw commute dist
Euclidean dist
LNK dist
Amplified kernel
Brand kernel
Yen kernel

classification error

0.8
0.6
0.4
0.2
0

20 50

100
200
Number of labeled points

400

Figure 3: Figures on the left: Distances and kernels based on a kNN graph between all 9298 USPS
points (heat plots, off-diagonal terms only): exact resistance distance, pseudo-inverse graph Laplacian L? ; kernels corresponding to the corrections by LNK, Yen, Brand, and our amplified Kamp .
Figure on the right: Semi-supervised learning results based on the different distances and kernels.
The last three lines corresponding to the amplified, Brand and Yen kernel lie on top of each other.
other methods by a large margin.
We conclude with the following tentative statements. We believe that the correction by LNK is ?a bit
too naive?, whereas the corrections by Brand, Yen and us ?tend to work? in a ranking based setting.
Based on our simple experiments it is impossible to judge which out of these candidates is ?the best
one?. We are not too fond of Yen?s correction because it does not lead to a proper kernel. Both
Brand?s and our kernel converge to (different) limit functions. So far we do not know the theoretical
properties of these limit functions and thus cannot present any theoretical reason to prefer one over
the other. However, we think that the diagonal dominance of the Brand kernel can be problematic.

6

Discussion

In this paper we have proved that the commute distance on random geometric graphs can be
approximated by a very simple limit expression. Contrary to intuition, this limit expression no
longer takes into account the cluster structure of the graph, nor any other global property (such as
distances in the underlying Euclidean space). Both our theoretical bounds and our simulations tell
the same story: the approximation gets better if the data is high-dimensional and not extremely
clustered, both of which are standard situations in machine learning. This shows that the use of the
raw commute distance for machine learning purposes can be problematic. However, the structure
of the graph can be recovered by certain corrections of the commute distance. We suggest to use
either the correction by Brand (2005) or our own amplified commute kernel from Section 4. Both
corrections have a well-defined, non-trivial limit and perform well in experiments.
The intuitive explanation for our result is that as the sample size increases, the random walk on the
sample graph ?gets lost? in the sheer size of the graph. It takes so long to travel through a substantial
part of the graph that by the time the random walk comes close to its goal it has already ?forgotten?
where it started from. Stated differently: the random walk on the graph has mixed before it hits the
desired target vertex. On a higher level, we expect that the problem of ?getting lost? not only affects
the commute distance, but many other methods where random walks are used in a naive way to
explore global properties of a graph. For example, the results in Nadler et al. (2009), where artifacts
of semi-supervised learning in the context of many unlabeled points are studied, seem strongly
related to our results. In general, we believe that one has to be particularly careful when using
random walk based methods for extracting global properties of graphs in order to avoid getting lost
and converging to meaningless results.

8

References
R. Aleliunas, R. Karp, R. Lipton, L. Lov?asz, and C. Rackoff. Random walks, universal traversal
sequences, and the complexity of maze problems. In FOCS, 1979.
C. Avin and G. Ercal. On the cover time and mixing time of random geometric graphs. Theor.
Comput. Sci, 380(1-2):2?22, 2007.
B. Bollob?as. Modern Graph Theory. Springer, 1998.
M. Brand. A random walks perspective on maximizing satisfaction and profit. In SDM, 2005.
N. Cesa-Bianchi, C. Gentile, and F. Vitale. Fast and optimal prediction on a labeled tree. In COLT,
2009.
A. Chandra, P. Raghavan, W. Ruzzo, R. Smolensky, and P. Tiwari. The electrical resistance of a
graph captures its commute and cover times. In STOC, 1989.
C. Cooper and A. Frieze. The cover time of random geometric graphs. In SODA, 2009.
P. G. Doyle and J. L. Snell. Random walks and electric networks. Mathematical Association of
America, Washington, DC, 1984.
F. Fouss, A. Pirotte, J.-M. Renders, and M. Saerens. A novel way of computing dissimilarities
between nodes of a graph, with application to collaborative filtering and subspace projection of
the graph nodes. Technical Report IAG WP 06/08, Universit?e catholique de Louvain, 2006.
S. Guattery. Graph embeddings, symmetric real matrices, and generalized inverses. Technical report,
Institute for Computer Applications in Science and Engineering, NASA Research Center, 1998.
J. Ham, D. D. Lee, S. Mika, and B. Sch?olkopf. A kernel view of the dimensionality reduction of
manifolds. In ICML, 2004.
M. Herbster and M. Pontil. Prediction on a graph with a perceptron. In NIPS, 2006.
D. Klein and M. Randic. Resistance distance. Journal of Mathematical Chemistry, 12:81?95, 1993.
D. Liben-Nowell and J. Kleinberg. The link prediction problem for social networks. In CIKM, 2003.
R. Lyons and Y. Peres. Probability on trees and networks. Book in preparation, available online on
the webpage of Yuval Peres, 2010.
B. Nadler, N. Srebro, and X. Zhou. Statistical analysis of semi-supervised learning: The limit of
infinite unlabelled data. In NIPS, 2009.
M. Penrose. A strong law for the longest edge of the minimal spanning tree. Ann. of Prob., 27(1):
246 ? 260, 1999.
H. Qiu and E. R. Hancock. Image segmentation using commute times. In BMVC, 2005.
H. Qiu and E. R. Hancock. Graph embedding using commute time. S+SSPR 2006, pages 441?449,
2006.
M. Saerens, F. Fouss, L. Yen, and P. Dupont. The principal components analysis of a graph, and its
relationships to spectral clustering. In ECML, 2004.
P. Sarkar and A. Moore. A tractable approach to finding closest truncated-commute-time neighbors
in large graphs. In UAI, 2007.
P. Sarkar, A. Moore, and A. Prakash. Fast incremental proximity search in large graphs. In ICML,
2008.
D. Spielman and N. Srivastava. Graph sparsification by effective resistances. In STOC, 2008.
U. von Luxburg, A. Radl, and M. Hein. Hitting times, commute distances and the spectral gap in
large random geometric graphs. Preprint available at Arxiv, March 2010.
D. M. Wittmann, D. Schmidl, F. Bl?ochl, and F. J. Theis. Reconstruction of graphs based on random
walks. Theoretical Computer Science, 2009.
W. Xiao and I. Gutman. Resistance distance and Laplacian spectrum. Theoretical Chemistry Accounts, 110:284?298, 2003.
L. Yen, D. Vanvyve, F. Wouters, F. Fouss, M. Verleysen, and M. Saerens. Clustering using a random
walk based distance measure. In ESANN, 2005.
L. Yen, F. Fouss, C. Decaestecker, P. Francq, and M. Saerens. Graph nodes clustering based on the
commute-time kernel. Advances in Knowledge Discovery and Data Mining, pages 1037?1045,
2009.
D. Zhou and B. Sch?olkopf. Learning from Labeled and Unlabeled Data Using Random Walks. In
DAGM, 2004.

9

"
3781,2011,Inductive reasoning about chimeric creatures,"Given one feature of a novel animal, humans readily make inferences about other features of the animal. For example, winged creatures often fly, and creatures that eat fish often live in the water. We explore the knowledge that supports these inferences and compare two approaches. The first approach proposes that humans rely on abstract representations of dependency relationships between features, and is formalized here as a graphical model.  The second approach proposes that humans rely on specific knowledge of previously encountered animals, and is formalized here as a family of exemplar models. We evaluate these models using a task where participants reason about chimeras, or animals with pairs of features that have not previously been observed to co-occur. The results support the hypothesis that humans rely on explicit representations of relationships between features.","Inductive reasoning about chimeric creatures

Charles Kemp
Department of Psychology
Carnegie Mellon University
ckemp@cmu.edu

Abstract
Given one feature of a novel animal, humans readily make inferences about other
features of the animal. For example, winged creatures often fly, and creatures that
eat fish often live in the water. We explore the knowledge that supports these inferences and compare two approaches. The first approach proposes that humans rely
on abstract representations of dependency relationships between features, and is
formalized here as a graphical model. The second approach proposes that humans
rely on specific knowledge of previously encountered animals, and is formalized
here as a family of exemplar models. We evaluate these models using a task where
participants reason about chimeras, or animals with pairs of features that have not
previously been observed to co-occur. The results support the hypothesis that humans rely on explicit representations of relationships between features.
Suppose that an eighteenth-century naturalist learns about a new kind of animal that has fur and a
duck?s bill. Even though the naturalist has never encountered an animal with this pair of features,
he should be able to make predictions about other features of the animal?for example, the animal
could well live in water but probably does not have feathers. Although the platypus exists in reality,
from a eighteenth-century perspective it qualifies as a chimera, or an animal that combines two or
more features that have not previously been observed to co-occur. Here we describe a probabilistic
account of inductive reasoning and use it to account for human inferences about chimeras.
The inductive problems we consider are special cases of the more general problem in Figure 1a
where a reasoner is given a partially observed matrix of animals by features then asked to infer the
values of the missing entries. This general problem has been previously studied and is addressed
by computational models of property induction, categorization, and generalization [1?7]. A challenge faced by all of these models is to capture the background knowledge that guides inductive
inferences. Some accounts rely on similarity relationships between animals [6, 8], others rely on
causal relationships between features [9, 10], and others incorporate relationships between animals
and relationships between features [11]. We will evaluate graphical models that capture both kinds
of relationships (Figure 1a), but will focus in particular on relationships between features.
Psychologists have previously suggested that humans rely on explicit mental representations of relationships between features [12?16]. Often these representations are described as theories?for
example, theories that specify a causal relationship between having wings and flying, or living in
the sea and eating fish. Relationships between features may take several forms: for example, one
feature may cause, enable, prevent, be inconsistent with, or be a special case of another feature. For
simplicity, we will treat all of these relationships as instances of dependency relationships between
features, and will capture them using an undirected graphical model.
Previous studies have used graphical models to account for human inferences about features but
typically these studies consider toy problems involving a handful of novel features such as ?has
gene X14? or ?has enzyme Y132? [9, 11]. Participants might be told, for example, that gene X14
leads to the production of enzyme Y132, then asked to use this information when reasoning about
novel animals. Here we explore whether a graphical model approach can account for inferences
1

(a)

slow heavy

flies

wings

hippo

1

1

0

0

rhino

1

1

0

0

sparrow

0

0

1

1

robin

0

0

1

1

new

?

?

1

?

o

(b)

Figure 1: Inductive reasoning about animals and features. (a) Inferences about the features of a
new animal onew that flies may draw on similarity relationships between animals (the new animal is
similar to sparrows and robins but not hippos and rhinos), and on dependency relationships between
features (flying and having wings are linked). (b) A graph product produced by combining the two
graph structures in (a).
about familiar features. Working with familiar features raises a methodological challenge since
participants have a substantial amount of knowledge about these features and can reason about them
in multiple ways. Suppose, for example, that you learn that a novel animal can fly (Figure 1a). To
conclude that the animal probably has wings, you might consult a mental representation similar to
the graph at the top of Figure 1a that specifies a dependency relationship between flying and having
wings. On the other hand, you might reach the same conclusion by thinking about flying creatures
that you have previously encountered (e.g. sparrows and robins) and noticing that these creatures
have wings. Since the same conclusion can be reached in two different ways, judgments about
arguments of this kind provide little evidence about the mental representations involved.
The challenge of working with familiar features directly motivates our focus on chimeras. Inferences
about chimeras draw on rich background knowledge but require the reasoner to go beyond past
experience in a fundamental way. For example, if you learn that an animal flies and has no legs, you
cannot make predictions about the animal by thinking of flying, no-legged creatures that you have
previously encountered. You may, however, still be able to infer that the novel animal has wings
if you understand the relationship between flying and having wings. We propose that graphical
models over features can help to explain how humans make inferences of this kind, and evaluate our
approach by comparing it to a family of exemplar models. The next section introduces these models,
and we then describe two experiments designed to distinguish between the models.

1

Reasoning about objects and features

Our models make use of a binary matrix D where the rows {o1 , . . . , o129 } correspond to objects,
and the columns {f 1 , . . . , f 56 } correspond to features. A subset of the objects is shown in Figure 2a,
and the full set of features is shown in Figure 2b and its caption. Matrix D was extracted from the
Leuven natural concept database [17], which includes 129 animals and 757 features in total. We
chose a subset of these features that includes a mix of perceptual and behavioral features, and that
includes many pairs of features that depend on each other. For example, animals that ?live in water?
typically ?can swim,? and animals that have ?no legs? cannot ?jump far.?
Matrix D can be used to formulate problems where a reasoner observes one or two features of a
new object (i.e. animal o130 ) and must make inferences about the remaining features of the animal.
The next two sections describe graphical models that can be used to address this problem. The
first graphical model O captures relationships between objects, and the second model F captures
relationships between features. We then discuss how these models can be combined, and introduce
a family of exemplar-style models that will be compared with our graphical models.
A graphical model over objects
Many accounts of inductive reasoning focus on similarity relationships between objects [6, 8]. Here
we describe a tree-structured graphical model O that captures these relationships. The tree was
constructed from matrix D using average linkage clustering and the Jaccard similarity measure, and
part of the resulting structure is shown in Figure 2a. The subtree in Figure 2a includes clusters
2

alligator
caiman
crocodile
monitor lizard
dinosaur
blindworm
boa
cobra
python
snake
viper
chameleon
iguana
gecko
lizard
salamander
frog
toad
tortoise
turtle
anchovy
herring
sardine
cod
sole
salmon
trout
carp
pike
stickleback
eel
flatfish
ray
plaice
piranha
sperm whale
squid
swordfish
goldfish
dolphin
orca
whale
shark
bat
fox
wolf
beaver
hedgehog
hamster
squirrel
mouse
rabbit
bison
elephant
hippopotamus
rhinoceros
lion
tiger
polar bear
deer
dromedary
llama
giraffe
zebra
kangaroo
monkey
cat
dog
cow
horse
donkey
pig
sheep

(a)

(b)

can swim

lives in
water
eats
fish

eats nuts
eats grain
eats grass

has gills

can jump
far

has two
legs

has no
legs

has six
legs

has four
legs

can fly

can be
ridden

has sharp
teeth

nocturnal

has wings

strong

predator

can see
in dark

eats berries

lives in
the sea
lives
in the
desert

crawls

lives
in the
woods

has mane

lives
in trees

can climb
well

lives
underground

has
feathers

has scales
slow

has fur
heavy

Figure 2: Graph structures used to define graphical models O and F. (a) A tree that captures
similarity relationships between animals. The full tree includes 129 animals, and only part of the
tree is shown here. The grey points along the branches indicate locations where a novel animal o130
could be attached to the tree. (b) A network capturing pairwise dependency relationships between
features. The edges capture both positive and negative dependencies. All edges in the network are
shown, and the network also includes 20 isolated nodes for the following features: is black, is blue,
is green, is grey, is pink, is red, is white, is yellow, is a pet, has a beak, stings, stinks, has a long neck,
has feelers, sucks blood, lays eggs, makes a web, has a hump, has a trunk, and is cold-blooded.

corresponding to amphibians and reptiles, aquatic creatures, and land mammals, and the subtree
omitted for space includes clusters for insects and birds.
We assume that the features in matrix D (i.e. the columns) are generated independently over O:
Y
P (D|O, ?, ?) =
P (f i |O, ? i , ?i ).
i

i

i

i

The distribution P (f |O, ? , ? ) is based on the intuition that nearby nodes in O tend to have the
same value of f i . Previous researchers [8, 18] have used a directed graphical model where the
distribution at the root node is based on the baserate ? i , and any other node v with parent u has the
following conditional probability distribution:
(
i
? i + (1 ? ? i )e?? l , if u = 1
(1)
P (v = 1|u) =
i
? i ? ? i e?? l ,
if u = 0
where l is the length of the branch joining node u to node v. The variability parameter ?i captures the
extent to which feature f i is expected to vary over the tree. Note, for example, that any node v must
take the same value as its parent u when ? = 0. To avoid free parameters, the feature baserates ? i
and variability parameters ?i are set to their maximum likelihood values given the observed values
of the features {f i } in the data matrix D. The conditional distributions in Equation 1 induce a joint
distribution over all of the nodes in graph O, and the distribution P (f i |O, ? i , ?i ) is computed by
marginalizing out the values of the internal nodes. Although we described O as a directed graphical
model, the model can be converted into an equivalent undirected model with a potential for each
edge in the tree and a potential for the root node. Here we use the undirected version of the model,
which is a natural counterpart to the undirected model F described in the next section.
The full version of structure O in Figure 2a includes 129 familiar animals, and our task requires
inferences about a novel animal o130 that must be slotted into the structure. Let D? be an expanded
version of D that includes a row for o130 , and let O? be an expanded version of O that includes a
node for o130 . The edges in Figure 2a are marked with evenly spaced gray points, and we use a
3

uniform prior P (O? ) over all trees that can be created by attaching o130 to one of these points. Some
of these trees have identical topologies, since some edges in Figure 2a have multiple gray points.
Predictions about o130 can be computed using:
X
X
P (D? |D) =
P (D? |O? , D)P (O? |D) ?
P (D? |O? , D)P (D|O? )P (O? ).
(2)
O?

O?

Equation 2 captures the basic intuition that the distribution of features for o130 is expected to be
consistent with the distribution observed for previous animals. For example, if o130 is known to
fly then the trees with high posterior probability P (O? |D) will be those where o130 is near other
flying creatures (Figure 1a), and since these creatures have wings Equation 2 predicts that o130
probably also has wings. As this example suggests, model O captures dependency relationships
between features implicitly, and therefore stands in contrast to models like F that rely on explicit
representations of relationships between features.
A graphical model over features
Model F is an undirected graphical model defined over features. The graph shown in Figure 2b was
created by identifying pairs where one feature depends directly on another. The author and a research
assistant both independently identified candidate sets of pairwise dependencies, and Figure 2b was
created by merging these sets and reaching agreement about how to handle any discrepancies.
As previous researchers have suggested [13, 15], feature dependencies can capture several kinds of
relationships. For example, wings enable flying, living in the sea leads to eating fish, and having
no legs rules out jumping far. We work with an undirected graph because some pairs of features
depend on each other but there is no clear direction of causal influence. For example, there is clearly
a dependency relationship between being nocturnal and seeing in the dark, but no obvious sense in
which one of these features causes the other.
We assume that the rows of the object-feature matrix D are generated independently from an undirected graphical model F defined over the feature structure in Figure 2b:
Y
P (D|F) =
P (oi |F).
i

Model F includes potential functions for each node and for each edge in the graph. These potentials
were learned from matrix D using the UGM toolbox for undirected graphical models [19]. The
learned potentials capture both positive and negative relationships: for example, animals that live in
the sea tend to eat fish, and tend not to eat berries. Some pairs of feature values never occur together
in matrix D (there are no creatures that fly but do not have wings). We therefore chose to compute
maximum a posteriori values of the potential functions rather than maximum likelihood values, and
used a diffuse Gaussian prior with a variance of 100 on the entries in each potential.
After learning the potentials for model F, we can make predictions about a new object o130 using
the distribution P (o130 |F). For example, if o130 is known to fly (Figure 1a), model F predicts
that o130 probably has wings because the learned potentials capture a positive dependency between
flying and having wings.
Combining object and feature relationships
There are two simple ways to combine models O and F in order to develop an approach that incorporates both relationships between features and relationships between objects. The output combination
model computes the predictions of both models in isolation, then combines these predictions using
a weighted sum. The resulting model is similar to a mixture-of-experts model, and to avoid free
parameters we use a mixing weight of 0.5. The structure combination model combines the graph
structures used by the two models and relies on a set of potentials defined over the resulting graph
product. An example of a graph product is shown in Figure 1b, and the potential functions for this
graph are inherited from the component models in the natural way. Kemp et al. [11] use a similar
approach to combine a functional causal model with an object model O, but note that our structure
combination model uses an undirected model F rather than a functional causal model over features.
Both combination models capture the intuition that inductive inferences rely on relationships between features and relationships between objects. The output combination model has the virtue of
4

simplicity, and the structure combination model is appealing because it relies on a single integrated
representation that captures both relationships between features and relationships between objects.
To preview our results, our data suggest that the combination models perform better overall than
either O or F in isolation, and that both combination models perform about equally well.
Exemplar models
We will compare the family of graphical models already described with a family of exemplar models.
The key difference between these model families is that the exemplar models do not rely on explicit
representations of relationships between objects and relationships between features. Comparing the
model families can therefore help to establish whether human inferences rely on representations of
this sort.
Consider first a problem where a reasoner must predict whether object o130 has feature k after observing that it has feature i. An exemplar model addresses the problem by retrieving all previouslyobserved objects with feature i and computing the proportion that have feature k:
P (ok = 1|oi = 1) =

|f k & f i |
|f i |

(3)

where |f k | is the number of objects in matrix D that have feature k, and |f k & f i | is the number that
have both feature k and feature i. Note that we have streamlined our notation by using ok instead of
o130
to refer to the kth feature value for object o130 .
k
Suppose now that the reasoner observes that object o130 has features i and j. The natural generalization of Equation 3 is:
P (ok = 1|oi = 1, oj = 1) =

|f k & f i & f j |
|f i & f j |

(4)

Because we focus on chimeras, |f i & f j | = 0 and Equation 4 is not well defined. We therefore
evaluate an exemplar model that computes predictions for the two observed features separately then
computes the weighted sum of these predictions:
P (ok = 1|oi = 1, oj = 1) = wi

|f k & f i |
|f k & f j |
+ wj
.
i
|f |
|f j |

(5)

where the weights wi and wj must sum to one. We consider four ways in which the weights could
be set. The first strategy sets wi = wj = 0.5. The second strategy sets wi ? |f i |, and is consistent
with an approach where the reasoner retrieves all exemplars in D that are most similar to the novel
animal and reports the proportion of these exemplars that have feature k. The third strategy sets
wi ? |f1i | , and captures the idea that features should be weighted by their distinctiveness [20]. The
final strategy sets weights according to the coherence of each feature [21]. A feature is coherent if
objects with that feature tend to resemble each other overall, and we define the coherence of feature
i as the expected Jaccard similarity between two randomly chosen objects from matrix D that both
have feature i. Note that the final three strategies are all consistent with previous proposals from the
psychological literature, and each one might be expected to perform well.
Because exemplar models and prototype models are often compared, it is natural to consider a prototype model [22] as an additional baseline. A standard prototype model would partition the 129
animals into categories and would use summary statistics for these categories to make predictions
about the novel animal o130 . We will not evaluate this model because it corresponds to a coarser version of model O, which organizes the animals into a hierarchy of categories. The key characteristic
shared by both models is that they explicitly capture relationships between objects but not features.

2

Experiment 1: Chimeras

Our first experiment explores how people make inferences about chimeras, or novel animals with
features that have not previously been observed to co-occur. Inferences about chimeras raise challenges for exemplar models, and therefore help to establish whether humans rely on explicit representations of relationships between features. Each argument can be represented as f i , f j ? f k
5

exemplar
r = 0.42

7

feature F

exemplar

(wi = |f i |)

(wi = 0.5)

r = 0.44

7

object O

r = 0.69

7

output
combination

r = 0.31

7

structure
combination

r = 0.59

7

r = 0.60

7

5

5

5

5

5

3

3

3

3

3

3

all

5

1

1
0

1

r = 0.06

7

conflict

0.5

1

1
0

0.5

1

r = 0.71

7

1
0

0.5

1

r = ?0.02

7

1
0

0.5

1

r = 0.49

7

0

5

5

5

5

5

3

3

3

3

3

3

1
0.5

1

r = 0.51

7

1
0

0.5

1

r = 0.64

7

1
0

0.5

1

r = 0.83

7

1
0

0.5

1

r = 0.45

7

0.5

1

r = 0.76

7

0

5

5

5

5

3

3

3

3

3

3

1
0.5

1

r = 0.26

7

1
0

0.5

1

r = 0.25

7

1
0

0.5

1

r = 0.19

7

1
0

0.5

1

r = 0.25

7

0.5

1

r = 0.24

7

0

5

5

5

5

3

3

3

3

3

3

1
0.5

1

1
0

0.5

1

1
0

0.5

1

1
0

0.5

1

0.5

1

r = 0.33

7

5

0

1

1
0

5

1

0.5
r = 0.79

7

5

0

1

1
0

5

1

0.5
r = 0.57

7

5

0

edge

0.5
r = 0.17

7

1

other

1
0

1
0

0.5

1

0

0.5

1

Figure 3: Argument ratings for Experiment 1 plotted against the predictions of six models. The
y-axis in each panel shows human ratings on a seven point scale, and the x-axis shows probabilities
according to one of the models. Correlation coefficients are shown for each plot.
where f i and f k are the premises (e.g. ?has no legs? and ?can fly?) and f k is the conclusion (e.g.
?has wings?). We are especially interested in conflict cases where the premises f i and f j lead to
opposite conclusions when taken individually: for example, most animals with no legs do not have
wings, but most animals that fly do have wings. Our models that incorporate feature structure F can
resolve this conflict since F includes a dependency between ?wings? and ?can fly? but not between
?wings? and ?has no legs.? Our models that do not include F cannot resolve the conflict and predict
that humans will be uncertain about whether the novel animal has wings.
Materials. The object-feature matrix D includes 447 feature pairs {f i , f j } such that none of the
129 animals has both f i and f j . We selected 40 pairs (see the supporting material) and created
400 arguments in total by choosing 10 conclusion features for each pair. The arguments can be
assigned to three categories. Conflict cases are arguments f i , f j ? f k such that the single-premise
arguments f i ? f k and f j ? f k lead to incompatible predictions. For our purposes, two singlepremise arguments with the same conclusion are deemed incompatible if one leads to a probability
greater than 0.9 according to Equation 3, and the other leads to a probability less than 0.1. Edge cases
are arguments f i , f j ? f k such that the feature network in Figure 2b includes an edge between f k
and either f i or f j . Note that some arguments are both conflict cases and edge cases. All arguments
that do not fall into either one of these categories will be referred to as other cases.
The 400 arguments for the experiment include 154 conflict cases, 153 edge cases, and 120 other
cases. 34 arguments are both conflict cases and edge cases. We chose these arguments based on
three criteria. First, we avoided premise pairs that did not co-occur in matrix D but that co-occur in
familiar animals that do not belong to D. For example, ?is pink? and ?has wings? do not co-occur in
D but ?flamingo? is a familiar animal that has both features. Second, we avoided premise pairs that
specified two different numbers of legs?for example, {?has four legs,? ?has six legs?}. Finally, we
aimed to include roughly equal numbers of conflict cases, edge cases, and other cases.
Method. 16 undergraduates participated for course credit. The experiment was carried out using a
custom-built computer interface, and one argument was presented on screen at a time. Participants
6

rated the probability of the conclusion on seven point scale where the endpoints were labeled ?very
unlikely? and ?very likely.? The ten arguments for each pair of premises were presented in a block,
but the order of these blocks and the order of the arguments within these blocks were randomized
across participants.
Results. Figure 3 shows average human judgments plotted against the predictions of six models.
The plots in the first row include all 400 arguments in the experiment, and the remaining rows show
results for conflict cases, edge cases, and other cases. The previous section described four exemplar
models, and the two shown in Figure 3 are the best performers overall. Even though the graphical
models include more numerical parameters than the exemplar models, recall that these parameters
are learned from matrix D rather than fit to the experimental data. Matrix D also serves as the basis
for the exemplar models, which means that all of the models can be compared on equal terms.
The first row of Figure 3 suggests that the three models which include feature structure F perform
better than the alternatives. The output combination model is the worst of the three models that incorporate F, and the correlation achieved by this model is significantly greater than the correlation
achieved by the best exemplar model (p < 0.001, using the Fisher transformation to convert correlation coefficients to z scores). Our data therefore suggest that explicit representations of relationships
between features are needed to account for inductive inferences about chimeras. The model that
includes the feature structure F alone performs better than the two models that combine F with the
object structure O, which may not be surprising since Experiment 1 focuses specifically on novel
animals that do not slot naturally into structure O.
Rows two through four suggest that the conflict arguments in particular raise challenges for the
models which do not include feature structure F. Since these conflict cases are arguments f i , f j ?
f k where f i ? f k has strength greater than 0.9 and f j ? f k has strength less than 0.1, the
first exemplar model averages these strengths and assigns an overall strength of around 0.5 to each
argument. The second exemplar model is better able to differentiate between the conflict arguments,
but still performs substantially worse than the three models that include structure F. The exemplar
models perform better on the edge arguments, but are outperformed by the models that include F.
Finally, all models achieve roughly the same level of performance on the other arguments.
Although the feature model F performs best overall, the predictions of this model still leave room for
improvement. The two most obvious outliers in the third plot in the top row represent the arguments
{is blue, lives in desert ? lives in woods} and {is pink, lives in desert ? lives in woods}. Our
participants sensibly infer that any animal which lives in the desert cannot simultaneously live in
the woods. In contrast, the Leuven database indicates that eight of the twelve animals that live in
the desert also live in the woods, and the edge in Figure 2b between ?lives in the desert? and ?lives
in the woods? therefore represents a positive dependency relationship according to model F. This
discrepancy between model and participants reflects the fact that participants made inferences about
individual animals but the Leuven database is based on features of animal categories. Note, for
example, that any individual animal is unlikely to live in the desert and the woods, but that some
animal categories (including snakes, salamanders, and lizards) are found in both environments.

3

Experiment 2: Single-premise arguments

Our results so far suggest that inferences about chimeras rely on explicit representations of relationships between features but provide no evidence that relationships between objects are important. It
would be a mistake, however, to conclude that relationships between objects play no role in inductive reasoning. Previous studies have used object structures like the example in Figure 2a to account
for inferences about novel features [11]?for example, given that alligators have enzyme Y132 in
their blood, it seems likely that crocodiles also have this enzyme. Inferences about novel objects can
also draw on relationships between objects rather than relationships between features. For example,
given that a novel animal has a beak you will probably predict that it has feathers, not because there
is any direct dependency between these two features, but because the beaked animals that you know
tend to have feathers. Our second experiment explores inferences of this kind.
Materials and Method. 32 undergraduates participated for course credit. The task was identical
to Experiment 1 with the following exceptions. Each two-premise argument f i , f j ? f k from
Experiment 1 was converted into two one-premise arguments f i ? f k and f j ? f k , and these
7

feature F

exemplar

r = 0.78

all

7

output
combination

r = 0.75

7

structure
combination

r = 0.75

7

5

5

5

5

3

3

3

3

3

1
0.5

1

r = 0.87

7

1
0

0.5

1

r = 0.87

7

1
0

0.5

1

r = 0.84

7

1
0

0.5

1

r = 0.86

7

0

5

5

5

5

3

3

3

3

3

1
0

0.5

1

r = 0.79

7

1
0

0.5

1

r = 0.21

7

1
0

0.5

1

r = 0.74

7

0.5

1

r = 0.66

7

0

5

5

5

3

3

3

3

3

1
0.5

1

1
0

0.5

1

1
0

0.5

1

0.5

1

r = 0.73

7

5

0

1

1
0

5

1

0.5
r = 0.85

7

5

1

r = 0.77

7

5

0

edge

r = 0.54

7

1

other

object O

1
0

0.5

1

0

0.5

1

Figure 4: Argument ratings and model predictions for Experiment 2.
one-premise arguments were randomly assigned to two sets. 16 participants rated the 400 arguments
in the first set, and the other 16 rated the 400 arguments in the second set.
Results. Figure 4 shows average human ratings for the 800 arguments plotted against the predictions
of five models. Unlike Figure 3, Figure 4 includes a single exemplar model since there is no need
to consider different feature weightings in this case. Unlike Experiment 1, the feature model F
performs worse than the other alternatives (p < 0.001 in all cases). Not surprisingly, this model
performs relatively well for edge cases f j ? f k where f j and f k are linked in Figure 2b, but the
final row shows that the model performs poorly across the remaining set of arguments.
Taken together, Experiments 1 and 2 suggest that relationships between objects and relationships
between features are both needed to account for human inferences. Experiment 1 rules out an
exemplar approach but models that combine graph structures over objects and features perform
relatively well in both experiments. We considered two methods for combining these structures and
both performed equally well. Combining the knowledge captured by these structures appears to be
important, and future studies can explore in detail how humans achieve this combination.

4

Conclusion

This paper proposed that graphical models are useful for capturing knowledge about animals and
their features and showed that a graphical model over features can account for human inferences
about chimeras. A family of exemplar models and a graphical model defined over objects were
unable to account for our data, which suggests that humans rely on mental representations that
explicitly capture dependency relationships between features. Psychologists have previously used
graphical models to capture relationships between features, but our work is the first to focus on
chimeras and to explore models defined over a large set of familiar features.
Although a simple undirected model accounted relatively well for our data, this model is only a
starting point. The model incorporates dependency relationships between features, but people know
about many specific kinds of dependencies, including cases where one feature causes, enables, prevents, or is inconsistent with another. An undirected graph with only one class of edges cannot
capture this knowledge in full, and richer representations will ultimately be needed in order to provide a more complete account of human reasoning.
Acknowledgments I thank Madeleine Clute for assisting with this research. This work was supported in
part by the Pittsburgh Life Sciences Greenhouse Opportunity Fund and by NSF grant CDI-0835797.

8

References
[1] R. N. Shepard. Towards a universal law of generalization for psychological science. Science, 237:1317?
1323, 1987.
[2] J. R. Anderson. The adaptive nature of human categorization. Psychological Review, 98(3):409?429,
1991.
[3] E. Heit. A Bayesian analysis of some forms of inductive reasoning. In M. Oaksford and N. Chater, editors,
Rational models of cognition, pages 248?274. Oxford University Press, Oxford, 1998.
[4] J. B. Tenenbaum and T. L. Griffiths. Generalization, similarity, and Bayesian inference. Behavioral and
Brain Sciences, 24:629?641, 2001.
[5] C. Kemp and J. B. Tenenbaum. Structured statistical models of inductive reasoning. Psychological
Review, 116(1):20?58, 2009.
[6] D. N. Osherson, E. E. Smith, O. Wilkie, A. Lopez, and E. Shafir. Category-based induction. Psychological
Review, 97(2):185?200, 1990.
[7] D. J. Navarro. Learning the context of a category. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S.
Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 1795?1803.
2010.
[8] C. Kemp, T. L. Griffiths, S. Stromsten, and J. B. Tenenbaum. Semi-supervised learning with trees. In
Advances in Neural Information Processing Systems 16, pages 257?264. MIT Press, Cambridge, MA,
2004.
[9] B. Rehder. A causal-model theory of conceptual representation and categorization. Journal of Experimental Psychology: Learning, Memory, and Cognition, 29:1141?1159, 2003.
[10] B. Rehder and R. Burnett. Feature inference and the causal structure of categories. Cognitive Psychology,
50:264?314, 2005.
[11] C. Kemp, P. Shafto, and J. B. Tenenbaum. An integrated account of generalization across objects and
features. Cognitive Psychology, in press.
[12] S. E. Barrett, H. Abdi, G. L. Murphy, and J. McCarthy Gallagher. Theory-based correlations and their
role in children?s concepts. Child Development, 64:1595?1616, 1993.
[13] S. A. Sloman, B. C. Love, and W. Ahn. Feature centrality and conceptual coherence. Cognitive Science,
22(2):189?228, 1998.
[14] D. Yarlett and M. Ramscar. A quantitative model of counterfactual reasoning. In T. G. Dietterich,
S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages
123?130. MIT Press, Cambridge, MA, 2002.
[15] W. Ahn, J. K. Marsh, C. C. Luhmann, and K. Lee. Effect of theory-based feature correlations on typicality
judgments. Memory and Cognition, 30(1):107?118, 2002.
[16] D. C. Meehan C. McNorgan, R. A. Kotack and K. McRae. Feature-feature causal relations and statistical
co-occurrences in object concepts. Memory and Cognition, 35(3):418?431, 2007.
[17] S. De Deyne, S. Verheyen, E. Ameel, W. Vanpaemel, M. J. Dry, W. Voorspoels, and G. Storms. Exemplar by feature applicability matrices and other Dutch normative data for semantic concepts. Behavior
Research Methods, 40(4):1030?1048, 2008.
[18] J. P. Huelsenbeck and F. Ronquist. MRBAYES: Bayesian inference of phylogenetic trees. Bioinformatics,
17(8):754?755, 2001.
[19] M. Schmidt. UGM: A Matlab toolbox for probabilistic undirected graphical models. 2007. Available at
http://people.cs.ubc.ca/?schmidtm/Software/UGM.html.
[20] L. J. Nelson and D. T. Miller. The distinctiveness effect in social categorization: you are what makes you
unusual. Psychological Science, 6:246?249, 1995.
[21] A. L. Patalano, S. Chin-Parker, and B. H. Ross. The importance of being coherent: category coherence,
cross-classification and reasoning. Journal of memory and language, 54:407?424, 2006.
[22] S. K. Reed. Pattern recognition and categorization. Cognitive Psychology, 3:393?407, 1972.

9

"
2026,2005,Active Bidirectional Coupling in a Cochlear Chip,Abstract Missing,"Active Bidirectional Coupling in a Cochlear Chip

Bo Wen and Kwabena Boahen
Department of Bioengineering
University of Pennsylvania
Philadelphia, PA 19104
{wenbo,boahen}@seas.upenn.edu

Abstract
We present a novel cochlear model implemented in analog very large
scale integration (VLSI) technology that emulates nonlinear active
cochlear behavior. This silicon cochlea includes outer hair cell (OHC)
electromotility through active bidirectional coupling (ABC), a mechanism we proposed in which OHC motile forces, through the microanatomical organization of the organ of Corti, realize the cochlear
amplifier. Our chip measurements demonstrate that frequency responses
become larger and more sharply tuned when ABC is turned on; the degree of the enhancement decreases with input intensity as ABC includes
saturation of OHC forces.

1

Silicon Cochleae

Cochlear models, mathematical and physical, with the shared goal of emulating nonlinear
active cochlear behavior, shed light on how the cochlea works if based on cochlear micromechanics. Among the modeling efforts, silicon cochleae have promise in meeting the
need for real-time performance and low power consumption. Lyon and Mead developed
the first analog electronic cochlea [1], which employed a cascade of second-order filters
with exponentially decreasing resonant frequencies. However, the cascade structure suffers from delay and noise accumulation and lacks fault-tolerance. Modeling the cochlea
more faithfully, Watts built a two-dimensional (2D) passive cochlea that addressed these
shortcomings by incorporating the cochlear fluid using a resistive network [2]. This parallel structure, however, has its own problem: response gain is diminished by interference
among the second-order sections? outputs due to the large phase change at resonance [3].
Listening more to biology, our silicon cochlea aims to overcome the shortcomings of existing architectures by mimicking the cochlear micromechanics while including outer hair cell
(OHC) electromotility. Although how exactly OHC motile forces boost the basilar membrane?s (BM) vibration remains a mystery, cochlear microanatomy provides clues. Based
on these clues, we previously proposed a novel mechanism, active bidirectional coupling
(ABC), for the cochlear amplifier [4]. Here, we report an analog VLSI chip that implements
this mechanism. In essence, our implementation is the first silicon cochlea that employs
stimulus enhancement (i.e., active behavior) instead of undamping (i.e., high filter Q [5]).
The paper is organized as follows. In Section 2, we present the hypothesized mechanism
(ABC), first described in [4]. In Section 3, we provide a mathematical formulation of the

Oval
window

organ
of Corti

BM

Round
window

IHC
RL

A

OHC

PhP

DC

BM

Basal

d

Stereocilia

i -1

i

i+1

Apical

B

Figure 1: The inner ear. A Cutaway showing cochlear ducts (adapted from [6]). B Longitudinal view of cochlear partition (CP) (modified from [7]-[8]). Each outer hair cell (OHC)
tilts toward the base while the Deiter?s cell (DC) on which it sits extends a phalangeal process (PhP) toward the apex. The OHCs? stereocilia and the PhPs? apical ends form the
reticular lamina (RL). d is the tilt distance, and the segment size. IHC: inner hair cell.

model as the basis of cochlear circuit design. Then we proceed in Section 4 to synthesize
the circuit for the cochlear chip. Last, we present chip measurements in Section 5 that
demonstrate nonlinear active cochlear behavior.

2

Active Bidirectional Coupling

The cochlea actively amplifies acoustic signals as it performs spectral analysis. The movement of the stapes sets the cochlear fluid into motion, which passes the stimulus energy
onto a certain region of the BM, the main vibrating organ in the cochlea (Figure 1A). From
the base to the apex, BM fibers increase in width and decrease in thickness, resulting in an
exponential decrease in stiffness which, in turn, gives rise to the passive frequency tuning
of the cochlea. The OHCs? electromotility is widely thought to account for the cochlea?s
exquisite sensitivity and discriminability. The exact way that OHC motile forces enhance
the BM?s motion, however, remains unresolved.
We propose that the triangular mechanical unit formed by an OHC, a phalangeal process
(PhP) extended from the Deiter?s cell (DC) on which the OHC sits, and a portion of the
reticular lamina (RL), between the OHC?s stereocilia end and the PhP?s apical tip, plays
an active role in enhancing the BM?s responses (Figure 1B). The cochlear partition (CP)
is divided into a number of segments longitudinally. Each segment includes one DC, one
PhP?s apical tip and one OHC?s stereocilia end, both attached to the RL. Approximating
the anatomy, we assume that when an OHC?s stereocilia end lies in segment i ? 1, its
basolateral end lies in the immediately apical segment i. Furthermore, the DC in segment
i extends a PhP that angles toward the apex of the cochlea, with its apical end inserted just
behind the stereocilia end of the OHC in segment i + 1.
Our hypothesis (ABC) includes both feedforward and feedbackward interactions. On one
hand, the feedforward mechanism, proposed in [9], hypothesized that the force resulting
from OHC contraction or elongation is exerted onto an adjacent downstream BM segment
due to the OHC?s basal tilt. On the other hand, the novel insight of the feedbackward
mechanism is that the OHC force is delivered onto an adjacent upstream BM segment due
to the apical tilt of the PhP extending from the DC?s main trunk.
In a nutshell, the OHC motile forces, through the microanatomy of the CP, feed forward
and backward, in harmony with each other, resulting in bidirectional coupling between
BM segments in the longitudinal direction. Specifically, due to the opposite action of OHC

?!!!!!!!!!!!!!!!!!!!!!!
ReHZmL? S HxL M HxL

1

0.5

0
-0.2
0

A

5
10
15
20
Distance from stapes HmmL

25

B

Figure 2: Wave propagation (WP) and basilar membrane (BM) impedance in the active
cochlear model with a 2kHz pure tone (? = 0.15, ? = 0.3). A WPp
in fluid and BM. B BM
impedance Zm (i.e., pressure divided by velocity), normalized by S(x)M (x). Only the
resistive component is shown; dot marks peak location.
forces on the BM and the RL, the motion of BM segment i ? 1 reinforces that of segment i
while the motion of segment i + 1 opposes that of segment i, as described in detail in [4].

3

The 2D Nonlinear Active Model

To provide a blueprint for the cochlear circuit design, we formulate a 2D model of the
cochlea that includes ABC. Both the cochlea?s length (BM) and height (cochlear ducts)
are discretized into a number of segments, with the original aspect ratio of the cochlea
maintained. In the following expressions, x represents the distance from the stapes along
the CP, with x = 0 at the base (or the stapes) and x = L (uncoiled cochlear duct length) at
the apex; y represents the vertical distance from the BM, with y = 0 at the BM and y = ?h
(cochlear duct radius) at the bottom/top wall.
Providing that the assumption of fluid incompressibility holds, the velocity potential ?
of the fluids is required to satisfy 52 ?(x, y, t) = 0, where 52 denotes the Laplacian
operator. By definition, this potential is related to fluid velocities in the x and y directions:
Vx = ???/?x and Vy = ???/?y.
The BM is driven by the fluid pressure difference across it. Hence, the BM?s vertical motion
(with downward displacement being positive) can be described as follows.
?
?
Pd (x) + FOHC (x) = S(x)?(x) + ?(x)?(x)
+ M (x)?(x),

(1)

where S(x) is the stiffness, ?(x) is the damping, and M (x) is the mass, per unit area, of
the BM; ? is the BM?s downward displacement. Pd = ? ?(?SV (x, y, t) ? ?ST (x, y, t))/?t
is the pressure difference between the two fluid ducts (the scala vestibuli (SV) and the scala
tympani (ST)), evaluated at the BM (y = 0); ? is the fluid density.
The FOHC(x) term combines feedforward and feedbackward OHC forces, described by

FOHC (x) = s0 tanh(??S(x)?(x ? d)/s0 ) ? tanh(?S(x)?(x + d)/s0 ) ,
(2)
where ? denotes the OHC motility, expressed as a fraction of the BM stiffness, and ? is
the ratio of feedforward to feedbackward coupling, representing relative strengths of the
OHC forces exerted on the BM segment through the DC, directly and via the tilted PhP. d
denotes the tilt distance, which is the horizontal displacement between the source and the
recipient of the OHC force, assumed to be equal for the forward and backward cases. We
use the hyperbolic tangent function to model saturation of the OHC forces, the nonlinearity
that is evident in physiological measurements [8]; s0 determines the saturation level.

We observed wave propagation in the model and computed the BM?s impedance (i.e., the
ratio of driving pressure to velocity). Following the semi-analytical approach in [2], we
simulated a linear version of the model (without saturation). The traveling wave transitions
from long-wave to short-wave before the BM vibration peaks; the wavelength around the
characteristic place is comparable to the tilt distance (Figure 2A). The BM impedance?s
real part (i.e., the resistive component) becomes negative before the peak (Figure 2B). On
the whole, inclusion of OHC motility through ABC boosts the traveling wave by pumping
energy onto the BM when the wavelength matches the tilt of the OHC and PhP.

4

Analog VLSI Design and Implementation

Based on our mathematical model, which produces realistic responses, we implemented a
2D nonlinear active cochlear circuit in analog VLSI, taking advantage of the 2D nature of
silicon chips. We first synthesize a circuit analog of the mathematical model, and then we
implement the circuit in the log-domain. We start by synthesizing a passive model, and
then extend it to a nonlinear active one by including ABC with saturation.
4.1 Synthesizing the BM Circuit
The model consists of two fundamental parts: the cochlear fluid and the BM. First, we
design the fluid element and thus the fluid network. In discrete form, the fluids can be
viewed as a grid of elements with a specific resistance that corresponds to the fluid density
or mass. Since charge is conserved for a small sheet of resistance and so are particles for
a small volume of fluid, we use current to simulate fluid velocity. At the transistor level,
the current flowing through the channel of a MOS transistor, operating subthreshold as a
diffusive element, can be used for this purpose. Therefore, following the approach in [10],
we implement the cochlear fluid network using a diffusor network formed by a 2D grid of
nMOS transistors.
Second, we design the BM element and thus the BM. As current represents velocity, we
rewrite the BM boundary condition (Equation 1, without the FOHC term):
R
(3)
I?in = S(x) Imem dt + ?(x)Imem + M (x)I?mem ,
where Iin , obtained by applying the voltage from the diffusor network to the gate of a
pMOS transistor, represents the velocity potential scaled by the fluid density. In turn, Imem
? The FOHC
drives the diffusor network to match the fluid velocity with the BM velocity, ?.
term is dealt with in Section 4.2.
Implementing this second-order system requires two state-space variables, which we name
Is and Io . And with s = j?, our synthesized BM design (passive) is
?1 Is s + Is
?2 Io s + Io
Imem

= ?Iin + Io ,
= Iin ? bIs ,
= Iin + Is ? Io ,

(4)
(5)
(6)

where the two first-order systems are both low-pass filters (LPFs), with time constants ?1
and ?2 , respectively; b is a gain factor. Thus, Iin can be expressed in terms of Imem as:

Iin s2 = (b + 1)/?1 ?2 + ((?1 + ?2 )/?1 ?2)s + s2 Imem .
Comparing this expression with the design target (Equation 3) yields the circuit analogs:
S(x) = (b + 1)/?1?2 ,

?(x) = (?1 + ?2 )/?1 ?2 ,

and M (x) = 1.

Note that the mass M (x) is a constant (i.e., 1), which was also the case in our mathematical model simulation. These analogies require that ?1 and ?2 increase exponentially to

Half
LPF
( )

+

Iout-

Iin+

Iout+ Iout

Vq

Iin+

Iin-

C+

B Iin-

A
Iin+

+

-

-

Iin-

+

+

+

C

To neighbors

Is-

Is+

b

+

b

+

IT+

IT-

+
-

+

From neighbors

Io-

Io+

+
+

+

+

- - +

+

LPF
Iout+

Iout-

BM

Imem+

Imem-

Figure 3: Low-pass filter (LPF) and second-order section circuit design. A Half-LPF circuit. B Complete LPF circuit formed by two half-LPF circuits. C Basilar membrane (BM)
circuit. It consists of two LPFs and connects to its neighbors through Is and IT .

simulate the exponentially decreasing BM stiffness (and damping); b allows us to achieve
a reasonable stiffness for a practical choice of ?1 and ?2 (capacitor size is limited by silicon
area).

4.2 Adding Active Bidirectional Coupling
R
To include ABC in the BM boundary condition, we replace ? in Equation 2 with Imem dt
to obtain
R

R

FOHC = rff S(x)T Imem (x ? d)dt ? rfb S(x)T Imem (x + d)dt ,
where rff = ?? and rfb = ? denote the feedforward and feedbackward OHC motility
factors, and T denotes saturation. The saturation is applied to the displacement, instead
of the force, as this simplifies the implementation. We obtain the integrals
by observing
R
that, in the passive
design,
the
state
variable
I
=
?I
/s?
.
Thus,
I
(x ? d)dt =
s
mem
1
mem
R
??1f Isf and Imem (x + d)dt = ??1b Isb . Here, Isf and Isb represent the outputs of the first
LPF in the upstream and downstream BM segments, respectively; ?1f and ?1b represent
their respective time constants. To reduce complexity in implementation, we use ?1 to
approximate both ?1f and ?1b as the longitudinal span is small.
We obtain the active BM design by replacing Equation 5 with the synthesis result:
?2 Ios + Io = Iin ? bIs + rfb (b + 1)T (?Isb ) ? rff (b + 1)T (?Isf ).
Note that, to implement ABC, we only need to add two currents to the second LPF in
the passive system. These currents, Isf and Isb , come from the upstream and downstream
neighbors of each segment.

ISV

Fluid

Base

BM

IST

Apex

Fluid

A

IT +
IT Is+
Is-

+
Vsat Imem

Iin+

Imem-

Iin-

Is+ Is+
Is- IsBM
IT + IT +
IT IT

Vsat

IT +
IT Is+
Is-

B

Figure 4: Cochlear chip. A Architecture: Two diffusive grids with embedded BM circuits
model the cochlea. B Detail. BM circuits exchange currents with their neighbors.

4.3 Class AB Log-domain Implementation
We employ the log-domain filtering technique [11] to realize current-mode operation. In
addition, following the approach proposed in [12], we implement the circuit in Class AB to
increase dynamic range, reduce the effect of mismatch and lower power consumption. This
differential signaling is inspired by the way the biological cochlea works?the vibration of
BM is driven by the pressure difference across it.
Taking a bottom-up strategy, we start by designing a Class AB LPF, a building block for
the BM circuit. It is described by
+
?
+
?
+
?
+ ?
+
?
? (Iout
? Iout
)s + (Iout
? Iout
) = Iin
? Iin
and ? Iout
Iout s + Iout
Iout
= Iq2 ,

where Iq sets the geometric mean of the positive and negative components of the output
current, and ? sets the time constant.
Combining the common-mode constraint with the differential design equation yields the
nodal equation for the positive path (the negative path has superscripts + and ? swapped):

+
+
?
+
+
+
?
C V? out
= I? (Iin
? Iin
) + (Iq2 /Iout
? Iout
) /(Iout
+ Iout
).
+
This nodal equation suggests the half-LPF circuit shown in Figure 3A. Vout
, the voltage on
+
the positive capacitor (C ), gates a pMOS transistor to produce the corresponding current
+
?
?
signal, Iout
(Vout
and Iout
are similarly related). The bias Vq sets the quiescent current Iq
while V? determines the current I? , which is related to the time constant by ? = CuT/?I?
(? is the subthreshold slope coefficient and uT is the thermal voltage). Two of these subcircuits, connected in push?pull, form a complete LPF (Figure 3B).

The BM circuit is implemented using two LPFs interacting in accordance with the synthesized design equations (Figure 3C). Imem is the combination of three currents, Iin , Is , and
Io . Each BM sends out Is and receives IT , a saturated version of its neighbor?s Is . The
saturation is accomplished by a current-limiting transistor (see Figure 4B), which yields
IT = T (Is ) = Is Isat /(Is + Isat ), where Isat is set by a bias voltage Vsat.
4.4 Chip Architecture
We fabricated a version of our cochlear chip architecture (Figure 4) with 360 BM circuits
and two 4680-element fluid grids (360 ?13). This chip occupies 10.9mm2 of silicon area in
0.25?m CMOS technology. Differential input signals are applied at the base while the two
fluid grids are connected at the apex through a fluid element that represents the helicotrema.

5

Chip Measurements

We carried out two measurements that demonstrate the desired amplification by ABC, and
the compressive growth of BM responses due to saturation. To obtain sinusoidal current as
the input to the BM subcircuits, we set the voltages applied at the base to be the logarithm
of a half-wave rectified sinusoid.
We first investigated BM-velocity frequency responses at six linearly spaced cochlear positions (Figure 5). The frequency that maximally excites the first position (Stage 30), defined
as its characteristic frequency (CF), is 12.1kHz. The remaining five CFs, from early to later
stages, are 8.2k, 1.7k, 905, 366, and 218Hz, respectively. Phase accumulation at the CFs
ranges from 0.56 to 2.67? radians, comparable to 1.67? radians in the mammalian cochlea
[13]. Q10 factor (the ratio of the CF to the bandwidth 10dB below the peak) ranges from
1.25 to 2.73, comparable to 2.55 at mid-sound intensity in biology (computed from [13]).
The cutoff slope ranges from -20 to -54dB/octave, as compared to -85dB/octave in biology
(computed from [13]).

40

Stage

0

230 190

150

110

70

30

30

BM Velocity
Phase H? radiansL

BM Velocity
Amplitude HdBL

50

-2

20
10

-4

0

-10
0.1 0.2

0.5 1 2
5
Frequency HkHzL

A

10 20

0.1 0.2

0.5 1 2
5 10 20
Frequency HkHzL

B

Figure 5: Measured BM-velocity frequency responses at six locations. A Amplitude.
B Phase. Dashed lines: Biological data (adapted from [13]). Dots mark peaks.
We then explored the longitudinal pattern of BM-velocity responses and the effect of ABC.
Stimulating the chip using four different pure tones, we obtained responses in which a
4kHz input elicits a peak around Stage 85 while 500Hz sound travels all the way to Stage
178 and peaks there (Figure 6A). We varied the input voltage level and obtained frequency
responses at Stage 100 (Figure 6B). Input voltage level increases linearly such that the
current increases exponentially; the input current level (in dB) was estimated based on
the measured ? for this chip. As expected, we observed linearly increasing responses at
low frequencies in the logarithmic plot. In contrast, the responses around the CF increase
less and become broader with increasing input level as saturation takes effect in that region
(resembling a passive cochlea). We observed 24dB compression as compared to 27 to 47dB
in biology [13]. At the highest intensities, compression also occurs at low frequencies.
These chip measurements demonstrate that inclusion of ABC, simply through coupling
neighboring BM elements, transforms a passive cochlea into an active one. This active
cochlear model?s nonlinear responses are qualitatively comparable to physiological data.

6

Conclusions

We presented an analog VLSI implementation of a 2D nonlinear cochlear model that utilizes a novel active mechanism, ABC, which we proposed to account for the cochlear amplifier. ABC was shown to pump energy into the traveling wave. Rather than detecting
the wave?s amplitude and implementing an automatic-gain-control loop, our biomorphic
model accomplishes this simply by nonlinear interactions between adjacent neighbors. Im-

60
Frequency

4k

2k

1k

500 Hz

BM Velocity
Amplitude HdBL

BM Velocity
Amplitude HdBL

20
10
0

Input Level

40

48 dB

20

32 dB

Stage 100

16 dB

0

0 dB

-10
0

50

100
150
Stage Number

A

200

0.2

0.5 1 2
5 10 20
Frequency HkHzL

B

Figure 6: Measured BM-velocity responses (cont?d). A Longitudinal responses (20-stage
moving average). Peak shifts to earlier (basal) stages as input frequency increases from
500 to 4kHz. B Effects of increasing input intensity. Responses become broader and show
compressive growth.
plemented in the log-domain, with Class AB operation, our silicon cochlea shows enhanced
frequency responses, with compressive behavior around the CF, when ABC is turned on.
These features are desirable in prosthetic applications and automatic speech recognition
systems as they capture the properties of the biological cochlea.

References
[1] Lyon, R.F. & Mead, C.A. (1988) An analog electronic cochlea. IEEE Trans. Acoust. Speech
and Signal Proc., 36: 1119-1134.
[2] Watts, L. (1993) Cochlear Mechanics: Analysis and Analog VLSI . Ph.D. thesis, Pasadena, CA:
California Institute of Technology.
[3] Fragni`ere, E. (2005) A 100-Channel analog CMOS auditory filter bank for speech recognition.
IEEE International Solid-State Circuits Conference (ISSCC 2005) , pp. 140-141.
[4] Wen, B. & Boahen, K. (2003) A linear cochlear model with active bi-directional coupling.
The 25th Annual International Conference of the IEEE Engineering in Medicine and Biology
Society (EMBC 2003), pp. 2013-2016.
[5] Sarpeshkar, R., Lyon, R.F., & Mead, C.A. (1996) An analog VLSI cochlear model with new
transconductance amplifier and nonlinear gain control. Proceedings of the IEEE Symposium on
Circuits and Systems (ISCAS 1996) , 3: 292-295.
[6] Mead, C.A. (1989) Analog VLSI and Neural Systems . Reading, MA: Addison-Wesley.
[7] Russell, I.J. & Nilsen, K.E. (1997) The location of the cochlear amplifier: Spatial representation
of a single tone on the guinea pig basilar membrane. Proc. Natl. Acad. Sci. USA, 94: 2660-2664.
[8] Geisler, C.D. (1998) From sound to synapse: physiology of the mammalian ear . Oxford University Press.
[9] Geisler, C.D. & Sang, C. (1995) A cochlear model using feed-forward outer-hair-cell forces.
Hearing Research , 86: 132-146.
[10] Boahen, K.A. & Andreou, A.G. (1992) A contrast sensitive silicon retina with reciprocal
synapses. In Moody, J.E. and Lippmann, R.P. (eds.), Advances in Neural Information Processing Systems 4 (NIPS 1992) , pp. 764-772, Morgan Kaufmann, San Mateo, CA.
[11] Frey, D.R. (1993) Log-domain filtering: an approach to current-mode filtering. IEE Proc. G,
Circuits Devices Syst., 140 (6): 406-416.
[12] Zaghloul, K. & Boahen, K.A. (2005) An On-Off log-domain circuit that recreates adaptive
filtering in the retina. IEEE Transactions on Circuits and Systems I: Regular Papers , 52 (1):
99-107.
[13] Ruggero, M.A., Rich, N.C., Narayan, S.S., & Robles, L. (1997) Basilar membrane responses
to tones at the base of the chinchilla cochlea. J. Acoust. Soc. Am., 101 (4): 2151-2163.

"
1123,2001,Incorporating Invariances in Non-Linear Support Vector Machines,Abstract Missing,"Incorporating Invariances in Nonlinear
Support Vector Machines

Olivier Chapelle

Bernhard Scholkopf

olivier.chapelle@lip6.fr
LIP6, Paris, France
Biowulf Technologies

bernhard.schoelkopf@tuebingen.mpg.de
Max-Planck-Institute, Tiibingen, Germany
Biowulf Technologies

Abstract
The choice of an SVM kernel corresponds to the choice of a representation of the data in a feature space and, to improve performance , it should therefore incorporate prior knowledge such as
known transformation invariances. We propose a technique which
extends earlier work and aims at incorporating invariances in nonlinear kernels. We show on a digit recognition task that the proposed approach is superior to the Virtual Support Vector method,
which previously had been the method of choice.

1

Introduction

In some classification tasks, an a priori knowledge is known about the invariances
related to the task. For instance, in image classification, we know that the label of
a given image should not change after a small translation or rotation.
More generally, we assume we know a local transformation Lt depending on a
parameter t (for instance, a vertical translation of t pixels) such that any point x
should be considered equivalent to LtX, the transformed point. Ideally, the output
of the learned function should be constant when its inputs are transformed by the
desired invariance.
It has been shown [1] that one can not find a non-trivial kernel which is globally
invariant. For this reason, we consider here local invariances and for this purpose
we associate at each training point X i a tangent vector dXi,
dXi =

lim -1 (LtXi t--+o

t

Xi)

81t=o LtXi

= -

8t

In practice dXi can be either computed by finite difference or by differentiation.
Note that generally one can consider more than one invariance transformation.
A common way of introducing invariances in a learning system is to add the perturbed examples LtXi in the training set [7]. Those points are often called virtual
examples. In the SVM framework , when applied only to the SVs, it leads to the
Virtual Support Vector (VSV) method [10]. An alternative to this is to modify
directly the cost function in order to take into account the tangent vectors. This

has been successfully applied to neural networks [13] and linear Support Vector
Machines [11]. The aim of the present work is to extend these methods to the case
of nonlinear SVMs which will be achieved mainly by using the kernel peA trick

[12].
The paper is organized as follows. After introducing the basics of Support Vector
Machines in section 2, we recall the method proposed in [11] to train invariant linear
SVMs (section 3). In section 4, we show how to extend it to the nonlinear case and
finally experimental results are provided in section 5.

2

Support Vector Learning

We introduce some standard notations for SVMs; for a complete description, see
[15]. Let {(Xi, Yi) h<i<n be a set of training examples, Xi E IRd , belonging to classes
labeled by Yi E {-1,1}. In kernel methods, we map these vectors into a feature
space using a kernel function K(Xi' Xj) that defines an inner product in this feature
space. The decision function given by an SVM is the maximal margin hyperplane
in this space,

g(X) = sign(f(x)), where f(x) =

(~a?YiK(Xi'X) + b) .

(1)

The coefficients a? are obtained by maximizing the functional
1

n

W(o:) = Lai

-""2

i=l

n

L aiajYiyjK(Xi,Xj)
i,j=l

under the constraints L:~= 1 aiYi = 0 and

ai

(2)

~ O.

This formulation of the SVM optimization problem is called the hard margin formulation since no training errors are allowed. In the rest of the paper, we will
not consider the so called soft-margin SVM algorithm [4], where training errors are
allowed.

3

Invariances for Linear SVMs

For linear SVMs, one wants to find a hyperplane whose normal vector w is as
orthogonal as possible to the tangent vectors. This can be easily understood from
the equality
f(Xi + dXi) - f(Xi) = w . dXi'
For this purpose, it has been suggested [11] to minimize the functional
n

(1 - ')')w 2

+ ')' L(w, dXi)2

(3)

i=l

subject to the constraints Yi(W . Xi + b) ~ 1. The parameter,), trades off between
normal SVM training (')' = 0) and full enforcement of the orthogonality between
the hyperplane and the invariance directions (')' ---+ 1).
Let us introduce

c, ~

((1-0)[ +0

~dx'dxi) 'i',

the square root of the regularized covariance matrix of the tangent vectors.

(4)

It was shown in [11] that training a linear invariant SVM, i.e. minimizing (3), is
equivalent to a standard SVM training after the following linear transformation of
the input space
-1
X --+ C, x.

This method led to significant improvements in linear SVMs, and to small improvements when used as a linear preprocessing step in nonlinear SVMs. The latter,
however, was a hybrid system with unclear theoretical foundations. In the next
section we show how to deal with the nonlinear case in a principled way.

4

Extension to the nonlinear case

In the nonlinear case, the data are first mapped into a high-dimensional feature
space where a linear decision boundary is computed. To extend directly the previous
analysis to the nonlinear case, one would need to compute the matrix C, in feature
space,

C,

=

(

(1 - '""Y)I + '""Y ~ dlJ> (Xi) dlJ> (Xi) T

and the new kernel function
K(x , y) = C~ llJ>(x) . C~ llJ>(y)

)

1~

(5)

= lJ>(x) T C~ 21J>(y)

(6)

However, due to the high dimension of the feature space, it is impossible to do it
directly. We propose two different ways for overcoming this difficulty.
4.1

Decomposition of the tangent Gram matrix

In order to be able to compute the new kernel (6) , we propose to diagonalize the
matrix C, (eq 5) using a similar approach as the kernel PCA trick [12]. In that
article, they showed how it was possible to diagonalize the feature space covariance
matrix by computing the eigendecomposition of the Gram matrix of those points.
Presently, instead of having a set of training points {1J>(Xi)} , we have a set of tangent
vectors {dlJ> (Xi)} and a tangent covariance matrix (the right term of the sum in (5))
Let us introduce the Gram matrix Kt of the tangent vectors:
Kij = dlJ>(Xi )? dlJ>(xj)
K(Xi +dXi, Xj +dxj) - K(Xi +dXi, Xj) - K(Xi ' Xj +dxj)

+ K(Xi' Xj) (7)
(8)

.
dxiT02K(Xi,Xj)d
~
~
XJ
UXiUXj

This matrix Kt can be computed either by finite differences (equation 7) or with the
analytical derivative expression given by equation (8) . Note that for a linear kernel,
K(x,y) = x T y, and (8) reads Kfj = dxi dXj, which is a standard dot product
between the tangent vectors.
Writing the eigendecomposition of Kt as Kt = U AUT , and using the kernel PCA
tools [12], one can show after some algebra (details in [2]) that the new kernel matrix
reads

K(x,y)

1 Kx y
-I - '""Y

(,)

+

n1(

~ -Ap

U.dx,TOK(Xi' X))
(~
~1
U~
~

'p

~

1

'""Y Ap

+1-

(~U.
~ 'p
~1

'""Y

- -1
-)
1 - '""Y

T

d OK(Xi'
x,
~
U~

y))

4.2

The kernel PCA map

A drawback of the previous approach appears when one wants to deal with multiple
invariances (i.e. more than one tangent vector per training point). Indeed, it
requires to diagonalize the matrix Kt (cf eq 7), whose size is equal to the number of
different tangent vectors. For this reason, we propose an alternative method. The
idea is to use directly the so-called kernel peA map, first introduced in [12] and
extended in [14].
This map is based on the fact that even in a high dimensional feature space 1i, a
training set {Xl , .. . , x n } of size n when mapped to this feature space spans a subspace E C 1i whose dimension is at most n . More precisely, if (VI""'"" Vn ) E En is
an orthonormal basis of E with each Vi being a principal axis of {<I>(xd, ... , <I> (x n )} ,
the kernel peA map 'i/J : X -+ ~n is defined coordinatewise as

'i/Jp (x) = <I>(x) . v P '

1:S p:S n.

Each principal direction has a linear expansion on the training points {<I>(Xi)} and
the coefficients of this expansion are obtained using kernel peA [12]. Writing the
eigendecompostion of K as K = U AUT, with U an orthonormal matrix and A a
diagonal one, it turns out that the the kernel peA map reads

'i/J(x) = A-1/2U T k(x),

(9)

where k (x) = (K(x, Xl)""'"" K(x, xn)) T .
Note that by definition , for all i and j , <I>(Xi) and <I>(Xj) lie in E and thus K(Xi ' Xj) =
<I>(Xi) . <I>(Xj) = 'i/J(Xi) . 'i/J(Xj). This reflects the fact that if we retain all principal
components, kernel peA is just a basis transform in E, leaving the dot product of
training points invariant.
As a consequence, training a nonlinear SVM on {Xl , ... , xn} is equivalent to training
a linear SVM on {'i/J(xd, . . . ,'i/J(xn )} and thus, thanks to the nonlinear mapping 'i/J,
we can work directly in the linear space E and use exactly the technique described
for invariant linear SVMs (section 3) . However the invariance directions d<I>(Xi) do
not necessarily belong to E. By projecting them onto E, some information might
be lost. The hope is that this approximation will give a similar decision function to
the exact one obtained in section 4.l.
Finally, the proposed algorithm consists in training an invariant linear SVM as
described in section 3 with training set { 'i/J(XI) , ... ,'i/J(xn)} and with invariance
directions {d'i/J(XI) , ... , d'i/J (x n)}, where d'i/J (Xi) = 'i/J (Xi + dXi ) - 'i/J(Xi), which can
be expressed from equation (9) as

4.3

Comparisons with the VSV method

One might wonder what is the difference between enforcing an invariance and just
adding the virtual examples LtXi in the training set. Indeed the two approaches
are related and some equivalence can be shown [6] .
So why not just add virtual examples? This is the idea of the Virtual Support
Vector (VSV) method [10] . The reason is the following: if a training point Xi is
far from the margin, adding the virtual example LtXi will not change the decision
boundary since neither of the points can become a support vector. Hence adding

virtual examples in the SVM framework enforces invariance only around the decision
boundary, which, as an aside, is the main reason why the virtual SV method only
adds virtual examples generated from points that were support vectors in the earlier
iteration.
One might argue that the points which are far from the decision boundary do not
provide any information anyway. On the other hand, there is some merit in not
only keeping the output label invariant under the transformation Lt, but also the
real-valued output. This can be justified by seeing the distance of a given point
to the margin as an indication of its class-conditional probability [8]. It appears
reasonable that an invariance transformation should not affect this probability too
much.

5

Experiments

In our experiments, we compared a standard SVM with several methods taking into
account invariances: standard SVM with virtual examples (cf. the VSV method [10])
[VSV], invariant SVM as described in section 4.1 [ISVM] and invariant hyperplane
in kernel peA coordinates as described in section 4.2 [ IHKPcA ].
The hybrid method described in [11] (see end of section 3) did not perform better
than the VSV method and is not included in our experiments for this reason.
Note that in the following experiments, each tangent vector d<I>(Xi) has been normalized by the average length
Ild<I>(xi)W/n in order to be scale independent.

vI:

5.1

Toy problem

The toy problem we considered is the following: the training data has been generated uniformly from [-1 , 1]2. The true decision boundary is a circle centered at the
origin: f(x) = sign(x2 - 0.7).
The a priori knowledge we want to encode in this toy problem is local invariance
under rotations. Therefore, the output of the decision function on a given training
point Xi and on its image R(Xi,C:) obtained by a small rotation should be as similar
as possible. To each training point, we associate a tangent vector dXi which is
actually orthogonal to Xi.
A training set of 30 points was generated and the experiments were repeated 100
times. A Gaussian kernel K(x,y) = exp (_ II X2~~ 1I 2) was chosen.
The results are summarized in figure 1. Adding virtual examples (VSV method)
is already very useful since it made the test error decrease from 6.25% to 3.87%
(with the best choice of a). But the use of ISVM or IHKPcA yields even better
performance. On this toy problem, the more the invariances are enforced b -+ 1),
the better the performances are (see right side of figure 1), reaching a test error of
1.11%.
When comparing log a = 1.4 and log a = 0 (right side of of figure 1), one notices
that the decrease in the test error does not have the same speed. This is actually
the dual of the phenomenon observed on the left side of this figure: for a same value
of gamma, the test error tends to increase, when a is larger. This analysis suggests
that 'Y needs to be adapted in function of a. This can be done automatically by the
gradient descent technique described in [3].

0.12

-

0. 14

- -

log sigma=-O.8
log sigma=O
10 si ma= 1,4

0 .12

0.1

0.06

0 .08

0.04

0.02

O.02 '----_~
, ------:
-0~
.5--~
0 --0~
.5,------~------'c-""
.5
Log sigma

%'------~-~-~
6 -~
8--,~
0 -~
, 2~

- Log (1-gamma)

Figure 1: Left: test error for different learning algorithms plotted against the width
of a RBF kernel and ""( fixed to 0.9. Right: test error of IHKPcA across ""( and for
different values of (5. The test errors are averaged over the 100 splits and the error
bars correspond to the standard deviation of the means.

5.2

Handwritten digit recognition

As a real world experiment, we tried to incorporate invariances for a handwritten
digit recognition task. The USPS dataset have been used extensively in the past
for this purpose, especially in the SVM community. It consists of 7291 training and
2007 test examples.
According to [9], the best performance has been obtained for a polynomial kernel
of degree 3, and all the results described in this section were performed using this
kernel. The local transformations we considered are translations (horizontal and
vertical). All the tangent vectors have been computed by a finite difference between
the original digit and its I-pixel translated.
We split the training set into 23 subsets of 317 training examples after a random
permutation of the training and test set. Also we concentrated on a binary classification problem, namely separating digits a to 4 against 5 to 9. The gain in
performance should also be valid for the multiclass case.
Figure 2 compares ISVM, IHKPcA and VSV for different values of ""(. From those
figures, it can be seen that the difference between ISVM (the original method) and
IHKPcA (the approximation) is much larger than in the toy example. The difference
to the toy example is probably due to the input dimensionality. In 2 dimensions,
with an RBF kernel, the 30 examples of the toy problem ""almost span"" the whole
feature space, whereas with 256 dimensions , this is no longer the case.
What is noteworthy in these experiments is that our proposed method is much
better than the standard VSV. As explained in section 4.3, the reason for this
might be that invariance is enforced around all training points and not only around
support vectors. Note that what we call VSV here is a standard SVM with a double
size training set containing the original data points and their translates.
The horizontal invariance yields larger improvements than the vertical one. One
of the reason might be that the digits in the USPS database are already centered
vertically.

-

0 .068

- -

IHKPCA
ISVM
VSV

- -

0 .066

0.066

0 .064

0 .064

0 .062

0.062

0.06

0.06

0 .058

0 .058

0.056

0.056

0 .054
0

0.5

1.5

2

2.5

3.5

-Log (1-gamma)

Vertical translation (to the top)

-

0 .068

0 .054
0

0.5

1.5

2

2.5

IHKPCA
ISVM
VSV

3.5

-Log (1-gamma)

Horizontal translation (to the right)

Figure 2: Comparison of ISVM, IHKPcA and VSV on the USPS dataset. The left
of the plot (""( = 0) corresponds to standard SVM whereas the right part of the plot
h -+ 1) means that a lot of emphasis is put on the enforcement of the constraints.
The test errors are averaged over the 23 splits and the error bars correspond to the
standard deviation of the means.

6

Conclusion

We have extended a method for constructing invariant hyperplanes to the nonlinear
case. We have shown results that are superior to the virtual SV method. The latter
has recently broken the record on the NIST database which is the ""gold standard""
of handwritten digit benchmarks [5], therefore it appears promising to also try the
new system on that task. For this propose, a large scale version of this method
needs to be derived. The first idea we tried is to compute the kernel PCA map
using only a subset of the training points. Encouraging results have been obtained
on the lO-class USPS database (with the whole training set), but other methods
are also currently under study.

References
[1] C. J. C. Burges. Geometry and invariance in kernel based methods. In
B. Sch6lkopf, C. J . C. Burges, and A. J . Smola, editors, Advances in Kernel Methods - Support Vector Learning. MIT Press, 1999.
[2] O. Chapelle and B. Sch6lkopf. Incorporating invariances in nonlinear Support
Vector Machines, 2001. Availabe at: www-connex.lip6.frrchapelle.
[3] O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple
parameters for support vector machines. Machine Learning, 46:131- 159, 2002.
[4] C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:273 297,1995.
[5] D. DeCoste and B. Sch6lkopf. Training invariant support vector machines.
Machine Learning, 2001. In press.
[6] Todd K. Leen. From data distributions to regularization in invariant learning.
In Nips, volume 7. The MIT Press, 1995.
[7] P. Niyogi, T. Poggio, and F. Girosi. Incorporating prior information in machine
learning by creating virtual examples. IEEE Proceedings on Intelligent Signal
Processing, 86(11):2196-2209, November 1998.

[8] John Platt. Probabilities for support vector machines. In A. Smola, P. Bartlett,
B. Sch6lkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers. MIT Press, Cambridge, MA, 2000.
[9] B. Sch6lkopf, C. Burges, and V. Vapnik. Extracting support data for a given
task. In U. M. Fayyad and R. Uthurusamy, editors, First International Conference on Knowledge Discovery fj Data Mining. AAAI Press, 1995.
[10] B. Sch6lkopf, C. Burges, and V. Vapnik. Incorporating invariances in support
vector learning machines. In Artificial Neural Networks - ICANN'96, volume
1112, pages 47- 52, Berlin, 1996. Springer Lecture Notes in Computer Science.
[11] B. Sch6lkopf, P. Y. Simard, A. J. Smola, and V. N. Vapnik. Prior knowledge
in support vector kernels. In MIT Press, editor, NIPS, volume 10, 1998.
[12] B. Sch6lkopf, A. Smola, and K.-R. Muller. Nonlinear component analysis as a
kernel eigenvalue problem. Neural Computation, 10:1299- 1310, 1998.
[13] P. Simard, Y. LeCun, J. Denker, and B. Victorri. Transformation invariance
in pattern recognition, tangent distance and tangent propagation. In G. Orr
and K. Muller, editors, Neural Networks: Tricks of the trade. Springer, 1998.
[14] K. Tsuda. Support vector classifier with asymmetric kernel function. In M. Verleysen, editor, Proceedings of ESANN'99, pages 183- 188,1999.
[15] V. Vapnik. Statistical Learning Theory. John Wiley & Sons, 1998.

"
905,2000,Learning Winner-take-all Competition Between Groups of Neurons in Lateral Inhibitory Networks,Abstract Missing,"Learning winner-take-all competition between
groups of neurons in lateral inhibitory networks

Xiaohui Xie, Richard Hahnloser and H. Sebastian Seung
E25-21O, MIT, Cambridge, MA 02139
{xhxielrhlseung}@mit.edu

Abstract
It has long been known that lateral inhibition in neural networks can lead
to a winner-take-all competition, so that only a single neuron is active at
a steady state. Here we show how to organize lateral inhibition so that
groups of neurons compete to be active. Given a collection of potentially overlapping groups, the inhibitory connectivity is set by a formula
that can be interpreted as arising from a simple learning rule. Our analysis demonstrates that such inhibition generally results in winner-take-all
competition between the given groups, with the exception of some degenerate cases. In a broader context, the network serves as a particular
illustration of the general distinction between permitted and forbidden
sets, which was introduced recently. From this viewpoint, the computational function of our network is to store and retrieve memories as permitted sets of coactive neurons.

In traditional winner-take-all networks, lateral inhibition is used to enforce a localized,
or ""grandmother cell"" representation in which only a single neuron is active [1, 2, 3, 4].
When used for unsupervised learning, winner-take-all networks discover representations
similar to those learned by vector quantization [5]. Recently many research efforts have
focused on unsupervised learning algorithms for sparsely distributed representations [6, 7].
These algorithms lead to networks in which groups of multiple neurons are coactivated to
represent an object. Therefore, it is of great interest to find ways of using lateral inhibition
to mediate winner-take-all competition between groups of neurons, as this could be useful
for learning sparsely distributed representations.
In this paper, we show how winner-take-all competition between groups of neurons can be
learned. Given a collection of potentially overlapping groups, the inhibitory connectivity
is set by a simple formula that can be interpreted as arising from an online learning rule.
To show that the resulting network functions as advertised, we perform a stability analysis.
If the strength of inhibition is sufficiently great, and the group organization satisfies certain
conditions, we show that the only sets of neurons that can be coactivated at a stable steady
state are the given groups and their subsets. Because of the competition between groups,
only one group can be activated at a time. In general, the identity of the winning group
depends on the initial conditions of the network dynamics. If the groups are ordered by the
aggregate input that each receives, the possible winners are those above a cutoff that is set
by inequalities to be specified.

1 Basic definitions
Let m groups of neurons be given, where group membership is specified by the matrix

, {I?

fl

=

if the ith neuron is in the ath group
otherwise

(1)

We will assume that every neuron belongs to at least one group l, and every group contains
at least one neuron. A neuron is allowed to belong to more than one group, so that the
groups are potentially overlapping. The inhibitory synaptic connectivity of the network is
defined in terms of the group membership,
Ji '
J

= lIm (1 _ ~a ~'!) =
,

a=I

J

{o

if i and. j both belong to a group
1 otherwIse

(2)

One can imagine this pattern of connectivity arising by a simple learning mechanism. Suppose that all elements of J are initialized to be unity, and the groups are presented sequentially as binary vectors
,~m. The ath group is learned through the update

e, ...

(3)

In other words, if neurons i and j both belong to group a, then the connection between
them is removed. After presentation of all m groups, this leads to Eq. (2). At the start
of the learning process, the initial state of J corresponds to uniform inhibition, which is
known to implement winner-take-all competition between individual neurons. It will be
seen that, as inhibitory connections are removed during learning, the competition evolves
to mediate competition between groups of neurons rather than individual neurons.
The dynamics of the network is given by

dx'- + xdt
'

= [b-, + ax,- -

(3 'L...J
"" J-'J-x-J ] +

(4)

j

where [z]+ = max{z,O} denotes rectification, a
(3 > the strength of lateral inhibition.

?

>

?

the strength of self-excitation, and

Equivalently, the dynamics can be written in matrix-vector form as :i; + x = [b + W x]+,
where W = aI - (3J includes both self-excitation and lateral inhibition. The state of the
network is specified by the vector x, and the external input by the vector b. A vector v is
said to be nonnegative, v 2: 0, if all of its components are nonnegative. The nonnegative
orthant is the set of all nonnegative vectors. It can be shown that any trajectory of Eq. (4)
starting in the nonnegative orthant remains there. Therefore, for simplicity we will consider
trajectories that are confined to the nonnegative orthant x 2: 0. However, we will consider
input vectors b whose components are of arbitrary sign.

2 Global stability
The goal of this paper is to characterize the steady state response of the dynamics Eq. (4)
to an input b that is constant in time. For this to be a sensible goal, we need some guarantee
that the dynamics converges to a steady state, and does not diverge to infinity. This is
provided by the following theorem.
Theorem 1 Consider the network Eq. (4). The following statements are equivalent:
lThis condition can be relaxed, but is kept for simplicity_

1. For any input b, there is a nonempty set of steady states that is globally asymptotically stable, exceptfor initial conditions in a set of measure zero.
2. The strength a of self-excitation is less than one.
Proof sketch:
? (2) => (1): Ifa < 1, the function Hl-a)xTx+~xT Jx-bTxis bounded below
and radially unbounded in the nonnegative orthant. Furthermore it is nonincreasing under the dynamics Eq. (4), and constant only at steady states. Therefore it is
a Lyapunov function, and its local minima are globally asymptotically stable .

? (1) => (2): Suppose that (2) is false. If a ~ 1, it is possible to choose b and an
initial condition for x so that only one neuron is active, and the activity of this
neuron diverges, so that (1) is contradicted . ?

3 Relationship between groups and permitted sets
In this section we characterize the conditions under which the lateral inhibition of Eq. (4)
enforces winner-take-all competition between the groups of neurons. That is, the only sets
of neurons that can be coactivated at a stable steady state are the groups and their subsets.
This is done by performing a linear stability analysis, which allows us to classify active
sets using the following definition.
Definition 1 If a set of neurons can be coactivated by some input at an asymptotically
stable steady state, it is called permitted. Otherwise, it is forbidden
Elsewhere we have shown that whether a set is permitted or forbidden depends on the
submatrix of synaptic connections between neurons in that set[l]. If the largest eigenvalue
of the sub-matrix is less than unity, then the set is permitted. Otherwise, it is forbidden.
We have also proved that any superset of a forbidden set is forbidden, while any subset of
a permitted set is also permitted.
Our goal in constructing the network (4) is to make the groups and their subsets the only
permitted sets of the network. To determine whether this is the case, we must answer two
questions. First, are all groups and their subsets permitted? Second, are all permitted sets
contained in groups? The first question is answered by the following Lemma.
Lemma 1 All groups and their subsets are permitted.
Proof: If a set is contained in a group, then there is no lateral inhibition between the
neurons in the set. Provided that a < 1, all eigenvalues of the sub-matrix are less than
unity, and the set is permitted. ?
The answer to the second question, whether all permitted sets are contained in groups, is
not necessarily affirmative. For example, consider the network defined by the group membership matrix ~ = {(I, 1,0), (0, 1, 1), (1,0,1)}. Since every pair of neurons belongs to
some group, there is no lateral inhibition (J = 0), which means that there are no forbidden
sets. As a result, (1,1,1) is a permitted set, but obviously it is not contained in any group.
Let's define a spurious permitted set to be one that is not contained in any group. For
example, {I, 1, I} is a spurious permitted set in the above example. To eliminate all the
spurious permitted sets in the network, certain conditions on the group membership matrix
~ have to be satisfied.
Definition 2 The membership ~ is degenerate if there exists a set of n ~ 3 neurons that is
not contained in any group, but all of its subsets with n - 1 neurons belong to some group.

Otherwise, ~ is called nondegenerate. For example, ~
degenerate.

= {(I, 1, 0), (0, 1, 1), (1,0, I)} is

Using this definition, we can formulate the following theorem.
Theorem 2 The neural dynamics Eq. (4) with a
permitted set if and only if ~ is degenerate.

<

1 and (3

> 1- a

has a spurious

Before we prove this theorem, we will need the following lemma.
Lemma 2 If (3 > 1- a, any set containing two neurons not in the same group isforbidden
under the neural dynamics Eq. (4).
Proof sketch: We will start by analyzing a very simple case, where there are two neurons belonging to two different groups. Let the group membership be {(I, 0), (0, I)}. In
this case, W = {(a, -(3), (-(3, a)}. This matrix has eigenvectors (1,1) and (1, -1) and
eigenvalues a - (3 and a + (3. Since a < 1 for global stability and (3 > 0 by definition, the
(1,1) mode is always stable. But if (3 > 1 - a, the (1, -1) mode is unstable. This means
that it is impossible for the two neurons to be coactivated at a stable steady state. Since any
superset of a forbidden set is also forbidden, the general result of the lemma follows .?.
Proof of Theorem 2 (sketch):
?

?::: If ~ is degenerate, there must exist a set n ~ 3 neurons that is not contained in
any group, but all of its subsets with n - 1 neurons belong to some group. There is
no lateral inhibition between these n neurons, since every pair of neurons belongs
to some group. Thus the set containing all n neurons is permitted and spurious .

? =>: If there exists a spurious permitted set P, we need to prove that ~ must be
degenerate. We will prove this by contradiction and induction. Let's assume ~ is
nondegenerate.
P must contain at least 2 neurons since anyone neuron subset is permitted and
not spurious. By Lemma 2, these 2 neurons must be contained in some group, or
else it is forbidden. Thus P must contain at least 3 neurons to be spurious, and
any pair of neurons in P belongs to some group by Lemma 2.
If P contains at least n neurons and all of its subsets with n - 1 neurons belong
to some group, then the set with these n neurons must belong to some group,
otherwise ~ is degenerate. Thus n must contain at least n + 1 neurons to be
spurious, and all its n subsets belong to some group.
By induction, this implies that P must contain all neurons in the network, in which
case, P is either forbidden or nonspurious. This contradicts with the assumption
P is a spurious permitted set. ?
From Theorem 2, we can easily have the following result.
Corollary 1 If every group contains some neuron that does not belong to any other group,
then there is no any spurious permitted set.

4

The potential winners

We have seen that if ~ is nondegenerate, the active set must be contained in a group, provided that lateral inhibition is strong ?(3 > 1 - a). The group that contains the active set
will be called the ""winner"" of the competition between groups. The identity of the winner
depends on the input b, and also on the initial conditions of the dynamics. For a given input,
we need to characterize which pattern could potentially be the winner.

Suppose that the group inputs B a = Li [biJ + ~i are distinct. Without loss of generality,
we order the group inputs as Bl > ... > Bm. Let's denote the largest input as bmax =
maxi{bi} and assume bmax > 0.
Theorem 3 For nonoverlapping groups, the top c groups with the largest group input could
end up the winner depending on the initial conditions of the dynamics, where c is determined by the equation BC 2': (1 - a)(3-1b max > B C+!
Proof sketch: Suppose the ath group is the winner. For all neurons not in this group to be
inactive, the self-consistent condition should read

""'[
~ bi J+a
~i 2':
i

I-a max{ [J+
-(3bj }
J~a

(5)

If a group containing the neuron with the largest input, this condition can always be satisfied. Moreover, this group is always in the top c groups. For groups not containing the
neuron with the largest input, this condition can be satisfied if and only if they are in the
top c groups .?

The winner-take-all competition described above holds only for the case of strong inhibition (3 > 1 - a. On the other hand, if (3 is small, the competition will be weak and may
not result in group-winner-take-all. In particular, if (3 < (1 - a) / Amax, where Am ax is
the largest eigenvalue of -J, then the set of all neurons is permitted. Since every subset
of a permitted set is permitted, that means there are no forbidden sets and the network is
monostable. Hence, group-winner-take-all does not hold. If (1 - a) / Amax < (3 < 1 - a,
the network has forbidden sets, but the possibility of spurious permitted sets cannot be
excluded.

5 Examples
Traditional winner-take-all network This is a special case of our network with N
groups, each containing one of the N neurons. Therefore, the group membership matrix ~
is the identity matrix, and J = 11 T - I, where 1 denotes the vector of all ones. According
to Corollary 1, only one neuron is permitted to be active at a stable steady state, provided
that (3 > 1 - a. We refer to the active neuron as the ""winner"" of the competition mediated
by the lateral inhibition.
If we assume that the inputs bi have distinct values, they can be ordered as b1 > b2 > ... >
bN , without loss of generality. According to Theorem 3, any of the neurons 1 to k can be
the winner, where k is defined by bk 2': (1 - a)(3-1b1 > bk+!. The winner depends on
the initial condition of the network dynamics. In other words, any neuron whose input is
greater than (1 - a) / (3 times the largest input can end up the winner.

Topographic organization Let the N neurons be organized into a ring, and let every
set of d contiguous neurons be a group. d will be called the width. For example, in a
network with N = 4 neurons and group width d = 2, then the membership matrix is
~ = {(I, 1,0,0), (0,1,1,0), (0,0,1,1), (1,0,0, I)}. This ring network is similar to the
one proposed by Ben-Yishai et al in the modeling of orientation tuning of visual cortex[9].
Unlike the WTA network where all groups are non-overlapping which implies that ~ is
always nondegenerate, in the ring network neurons are shared among different groups, ~
will become degenerate when the width of the group is large. To guarantee all permitted
sets are the subsets of some group, we have the following corollary, which can be derived
from Theorem 2.

A

D

10
15 L--_~_~

50

100

C

150

200

100

200

F

300

400

10
15L--_~_~

10

15

Figure 1: Permitted sets of the ring network. The ring network is comprised of 15 neurons with
Q = 0.4 and /3 = 1. In panels A and D, the 15 groups are represented by columns. Black refers to
active neurons and white refers to inactive neurons. (A) 15 groups of width d = 5. (B) All permitted
sets corresponding to the groups in A. (C) The 15 permitted sets in B that have no permitted supersets.
They are the same as the groups in A. (D) 15 groups with width d = 6. (E) All permitted set
corresponding to groups in D. (F) There are 20 permitted sets in E that have no permitted supersets.
Note that there are 5 spurious permitted sets.

Corollary 2 In the ring network with N neurons,
no spurious permitted set.

if the width d < N /3 + 1, then there is

Fig. (1) shows the permitted sets of a ring network with 15 neurons. From Corollary 2, we
know that if the group width is no larger than 5 neurons, there will not exist any spurious
permitted set. In the left three panels of Fig. (1), the group width is 5 and all permitted sets
are subsets of these groups. However, when the group width is 6 (right three panels), there
exists 5 spurious permitted sets as shown in panel F.
As we have mentioned earlier, the lateral inhibition strength (3 plays a critical role in determining the dynamics of the network. Fig. (2) shows four types of steady states of a ring
network corresponding to different values of (3.

6

Discussion

We have shown that it is possible to organize lateral inhibition to mediate a winner-take-all
competition between potentially overlapping groups of neurons. Our construction utilizes
the distinction between permitted and forbidden sets of neurons.
If there is strong lateral inhibition between two neurons, then any set that contains

them is forbidden (Lemma 2). Neurons that belong to the same group do not have
any mutual inhibition, and so they form a permitted set. Because the synaptic connections between neurons in the same group are only composed of self-excitation, their
outputs equal their rectified inputs, amplified by the gain factor of 1/(1 - a). Hence
the neurons in the winning group operate in a purely analog regime. The coexistence of analog filtering with logical constraints on neural activation represents a form
of hybrid analog-digital computation that may be especially appropriate for perceptual tasks. It might be possible to apply a similar method to the problem of data re-

construction using a constrained set of basis vectors.
combination of basis vectors could for example imA
plement sparsity or nonnegativity constraints.
1.
As we have shown in Theorem 2, there are some degenerate cases of overlapping groups, to which our
method does not apply. It is
an interesting open question
whether there exists a general way of how to translate
arbitrary groups of coactive
neurons into permitted sets
without involving spurious
permitted sets.

In the past, a great deal of

The constraints on the linear
B

o.
o.

c

D

15

5

10

15

research has been inspired
Figure 2: Lateral inhibition strength f3 determines the behavior
by the idea of storing memof the network. The network is a ring network of 15 neurons with
ories as dynamical attracwidth d = 5 and where 0: = 0.4 and input bi = 1, Vi. These
tors in neural networks [10].
panels show the steady state activities of the 15 neurons. (A)
Our theory suggests an alThere are no forbidden sets. (B) The marginal state f3 = (1 ternative viewpoint, which
O:)/Amax = 0.874, in which the network forms a continuous
is to regard permitted sets
attractor. (C) Forbidden sets exist, and so do spurious permitted
sets. (D) Group-winner-take-a11 case, no spurious permitted sets.
as memories latent in the
synaptic connections. From
this viewpoint, the contribution of the present paper is a method of storing and retrieving memories as permitted sets
in neural networks.

References
[1] R. Hahnloser, R. Sarpeshkar, M. Mahowald, Douglas R., and H.S. Seung. Digital selection and

analog amplification coexist in an electronic circuit inspired by neocortex. Nature, 3:609- 616,
2000.
[2] Shun-Ichi Amari and Michael A. Arbib. Competition and Cooperation in Neural Nets, pages
119- 165. Systems Neuroscience. Academic Press, 1977. J. Metzler (ed).
[3] 1. Feng and K.P. Hadeler. Qualitative behaviour of some simple networks. 1. Phys. A:, 29:50195033, 1996.
[4] Richard H.R. Hahnloser. About the piecewise analysis of networks of linear threshold neurons .
Neural Networks, 11:691- 697, 1998.
[5] T. Kohonen . Self-Organization and Associative Memory. Springer-Verlag, Berlin, 3 edition,
1989.
[6] D. D. Lee and H. S. Seung. Learning the parts of objects by nonnegative matrix factorization.
Nature, 401:788- 91, 1999.
[7] B. A. Olshausen and D. 1. Field. Emergence of simple-cell receptive field properties by learning
a sparse code for natural images. Nature, 381:607-609, 1996.
[8] R. Ben-Yishai, R. Lev Bar-Or, and H. Sompolinsky. Theory of orientation tuning in visual cortex.
Proc. Natl. Acad. Sci. USA , 92:3844-3848, 1995.
[9] 1. J. Hopfield. Neurons with graded response have collective properties like those of two-state
neurons . Proc. Natl. Acad. Sci. USA, 81:3088- 3092, 1984.

"
5884,2016,Learnable Visual Markers,"We propose a new approach to designing visual markers (analogous to QR-codes, markers for augmented reality, and robotic fiducial tags) based on the advances in deep generative networks. In our approach, the markers are obtained as color images synthesized by a deep network from input bit strings, whereas another deep network is trained to recover the bit strings back from the photos of these markers. The two networks are trained simultaneously in a joint backpropagation process that takes characteristic photometric and geometric distortions associated with marker fabrication and capture into account. Additionally, a stylization loss based on statistics of activations in a pretrained classification network can be inserted into the learning in order to shift the marker appearance towards some texture prototype. In the experiments, we demonstrate that the markers obtained using our approach are capable of retaining bit strings that are long enough to be practical. The ability to automatically adapt markers according to the usage scenario and the desired capacity as well as the ability to combine information encoding with artistic stylization are the unique properties of our approach. As a byproduct, our approach provides an insight on the structure of patterns that are most suitable for recognition by ConvNets and on their ability to distinguish composite patterns.","Learnable Visual Markers
Oleg Grinchuk1 , Vadim Lebedev1,2 , and Victor Lempitsky1
1

Skolkovo Institute of Science and Technology, Moscow, Russia
2
Yandex, Moscow, Russia

Abstract
We propose a new approach to designing visual markers (analogous to QR-codes,
markers for augmented reality, and robotic fiducial tags) based on the advances
in deep generative networks. In our approach, the markers are obtained as color
images synthesized by a deep network from input bit strings, whereas another
deep network is trained to recover the bit strings back from the photos of these
markers. The two networks are trained simultaneously in a joint backpropagation
process that takes characteristic photometric and geometric distortions associated
with marker fabrication and marker scanning into account. Additionally, a stylization loss based on statistics of activations in a pretrained classification network
can be inserted into the learning in order to shift the marker appearance towards
some texture prototype. In the experiments, we demonstrate that the markers obtained using our approach are capable of retaining bit strings that are long enough
to be practical. The ability to automatically adapt markers according to the usage
scenario and the desired capacity as well as the ability to combine information
encoding with artistic stylization are the unique properties of our approach. As
a byproduct, our approach provides an insight on the structure of patterns that
are most suitable for recognition by ConvNets and on their ability to distinguish
composite patterns.

1

Introduction

Visual markers (also known as visual fiducials or visual codes) are used to facilitate humanenvironment and robot-environment interaction, and to aid computer vision in resource-constrained
and/or accuracy-critical scenarios. Examples of such markers include simple 1D (linear) bar
codes [31] and their 2D (matrix) counterparts such as QR-codes [9] or Aztec codes [18], which
are used to embed chunks of information into objects and scenes. In robotics, AprilTags [23] and
similar methods [3, 4, 26] are a popular way to make locations, objects, and agents easily identifiable for robots. Within the realm of augmented reality (AR), ARCodes [6] and similar marker
systems [13, 21] are used to enable real-time camera pose estimation with high accuracy, low latency, and on low-end devices. Overall, such markers can embed information into the environment
in a more compact and language-independent way as compared to traditional human text signatures,
and they can also be recognized and used by autonomous and human-operated devices in a robust
way.
Existing visual markers are designed ?manually? based on the considerations of the ease of processing by computer vision algorithms, the information capacity, and, less frequently, aesthetics.
Once marker family is designed, a computer vision-based approach (a marker recognizer) has to be
engineered and tuned in order to achieve reliable marker localization and interpretation [1, 17, 25].
The two processes of the visual marker design on one hand and the marker recognizer design on the
other hand are thus separated into two subsequent steps, and we argue that such separation makes
the corresponding design choices inherently suboptimal. In particular, the third aspect (aesthetics)
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

is usually overlooked, which leads to visually-intrusive markers that in many circumstances might
not fit the style of a certain environment and make this environment ?computer-friendly? at the cost
of ?human-friendliness?.
In this work, we propose a new general approach to constructing use visual markers that leverages
recent advances in deep generative learning. To this end, we suggest to embed the two tasks of the
visual marker design and the marker recognizer design into a single end-to-end learning framework.
Within our approach, the learning process produces markers and marker recognizers that are adapted
to each other ?by design?. While our idea is more general, we investigate the case where the markers
are synthesized by a deep neural network (the synthesizer network), and when they are recognized
by another deep network (the recognizer network). In this case, we demonstrate how these two
networks can be both learned by a joint stochastic optimization process.
The benefits of the new approach are thus several-fold:
1. As we demonstrate, the learning process can take into account the adversarial effects that
complicate recognition of the markers, such as perspective distortion, confusion with background, low-resolution, motion blur, etc. All such effects can be modeled at training time
as piecewise-differentiable transforms. In this way they can be embedded into the learning
process that will adapt the synthesizer and the recognizer to be robust with respect to such
effect.
2. It is easy to control the trade-offs between the complexity of the recognizer network, the
information capacity of the codes, and the robustness of the recognition towards different
adversarial effects. In particular, one can set the recognizer to have a certain architecture,
fix the variability and the strength of the adversarial effects that need to be handled, and
then the synthesizer will adapt so that the most ?legible? codes for such circumstances can
be computed.
3. Last but not least, the aesthetics of the neural codes can be brought into the optimization.
Towards this end we show that we can augment the learning objective with a special stylization loss inspired by [7, 8, 29]. Including such loss facilitates the emergence of stylized
neural markers that look as instances of a designer-provided stochastic texture. While such
modification of the learning process can reduce the information capacity of the markers, it
can greatly increase the ?human-friendliness? of the resulting markers.
Below, we introduce our approach and then briefly discuss the relation of this approach to prior art.
We then demonstrate several examples of learned marker families.

2

Learnable visual markers

We now detail our approach (Figure 1). Our goal is to build a synthesizer network S(b; ?S ) with
learnable parameters ?S that can encode a bit sequence b = {b1 , b2 , . . . bn } containing n bits into
an image M of the size m-by-m (a marker). For notational simplicity in further derivations, we
assume that bi ? {?1, +1}.
To recognize the markers produced by the synthesizer, a recognizer network R(I; ?R ) with learnable
parameters ?R is created. The recognizer takes an image I containing a marker and infers the realvalued sequence r = {r1 , r2 , . . . , rn }. The recognizer is paired to the synthesizer to ensure that
sign ri = bi , i.e. that the signs of the numbers inferred by the recognizers correspond to the bits
encoded by the synthesizer. In particular, we can measure the success of the recognition using a
simple loss function based on element-wise sigmoid:
n

L(b, r) = ?

n

1X
1X
1
?(bi ri ) = ?
n i=1
n i=1 1 + exp(?bi ri )

(1)

where the loss is distributed between ?1 (perfect recognition) and 0.
In real life, the recognizer network does not get to work with the direct outputs of the synthesizer.
Instead, the markers produced by the synthesizer network are somehow embedded into an environment (e.g. via printing or using electronic displays), and later their images are captioned by some
camera controlled by a human or by a robot. During learning, we model the transformation between
2

input bit string

decoding
loss

pretrained
ConvNet

synthesizer network

texture
sample

rendering
network

backpropagation
Gram
matrix

recognizer network
decoded input

texture
loss

Gram
matrix

Figure 1: The outline of our approach and the joint learning process. Our core architecture consists of the synthesizer network that converts input bit sequences into visual markers, the rendering
network that simulates photometric and geometric distortions associated with marker printing and
capturing, and the recognizer network that is designed to recover the input bit sequence from the
distorted markers. The whole architecture is trained end-to-end by backpropagation, after which
the synthesizer network can be used to generate markers, and the recognizer network to recover the
information from the markers placed in the environment. Additionally, we can enforce the visual
similarity of markers to a given texture sample using the mismatch in deep Gram matrix statistics in
a pretrained network [7] as the second loss term during learning (right part).
a marker produced by the synthesizer and the image of that marker using a special feed-forward network (the renderer network) T (M ; ?), where the parameters of the renderer network ? are sampled
during learning and correspond to background variability, lighting variability, perspective slant, blur
kernel, color shift/white balance of the camera, etc. In some scenarios, the non-learnable parameters ? can be called nuisance parameters, although in others we might be interested in recovering
some of them (e.g. the perspective transform parameters). During learning ? is sampled from some
distribution ? which should model the variability of the above-mentioned effects in the conditions
under which the markers are meant to be used.
When our only objective is robust marker recognition, the learning process can be framed as the
minimization of the following functional:




f (?S , ?R ) = E b?U (n) L b, R T S(b; ?S ); ? ; ?R
.
(2)
???

Here, the bit sequences b are sampled uniformly from U (n) = {?1; +1}n , passed through the
synthesizer, the renderer, and the recognizer, with the (minus) loss (1) being used to measure the
success of the recognition. The parameters of the synthesizer and the recognizer are thus optimized
to maximize the success rate.
The minimization of (2) can then be accomplished using a stochastic gradient descent algorithm,
e.g. ADAM [14]. Each iteration of the algorithm samples a mini-batch of different bit sequences as
well as different rendering layer parameter sets and updates the parameters of the synthesizer and
the recognizer networks in order to minimize the loss (1) for these samples.
Practical implementation. As mentioned above, the components of the architecture, namely the
synthesizer, the renderer, and the recognizer can be implemented as feed-forward networks. The
recognizer network can be implemented as a feedforward convolutional network [16] with n output
units. The synthesizer can use multiplicative and up-convolutional [5, 34] layers, as well as elementwise non-linearities.
Implementing the renderer T (M ; ?) (Figure 2) requires non-standard layers. We have implemented
the renderer as a chain of layers, each introducing some ?nuisance? transformation. We have implemented a special layer that superimposes an input over a bigger background patch drawn from
a random pool of images. We use the spatial transformer layer [11] to implement the geometric
distortion in a differentiable manner. Color shifts and intensity changes can be implemented using differentiable elementwise transformations (linear, multiplicative, gamma). Blurring associated
with lens effect or motion can be simply implemented using a convolutional layer. The nuisance
transformation layers can be chained resulting in a renderer layer that can model complex geometric
and photometric transformations (Figure 2).
3

Marker

Superimpose

Spatial Transform

Color Transform

Blur

Figure 2: Visualizations of the rendering network T (M ; ?). For the input marker M on the left the
output of the network is obtained through several stages (which are all piecewise-differentiable w.r.t.
inputs); on the right the outputs T (M ; ?) for several random nuisance parameters ? are shown. The
use of piecewise-differentiable transforms within T allows to backpropagate through T .
Controlling the visual appearance. Interestingly, we observed that under variable conditions, the
optimization of (2) results in markers that have a consistent and interesting visual texture (Figure 3).
Despite such style consistency, it might be desirable to control the appearance of the resulting markers more explicitly e.g. using some artistic prototypes. Recently, [7] have achieved remarkable
results in texture generation by measuring the statistics of textures using Gram matrices of convolutional maps inside deep convolutional networks trained to classify natural images. Texture synthesis
can then be achieved by minimizing the deviation between such statistics of generated images and
of style prototypes. Based on their approach, [12, 29] have suggested to include such deviation as
a loss into the training process for deep feedforward generative neural networks. In particular, the
feed-forward networks in [29] are trained to convert noise vectors into textures.
We follow this line of work and augment our learning objective (2) with the texture loss of [7]. Thus,
we consider a feed-forward network C(M ; ?) that computes the result of the t-th convolutional
layers of a network trained for large-scale natural image classification such as the VGGNet [28].
For an image M , the output C(M ; ?) thus contains k 2D channels (maps). The network C uses
the parameters ? that are pre-trained on a large-scale dataset and that are not part of our learning
process. The style of an image M is then defined using the following k-by-k Gram matrix G(M ; ?)
with each element defined as:
Gij (M ; ?) = h Ci (M ; ?), Cj (M ; ?) i ,

(3)

where Ci and Cj are the i-th and the j-th maps and the inner product is taken over all spatial locations.
Given a prototype texture M 0 , the learning objective can be augmented with the term:

2

fstyle (?S ) = E b?U (n) 
 G(S(b; ?S ); ?) ? G(M 0 ; ?) 
 .
(4)
The incorporation of the term (4) forces the markers S(b; ?S ) produced by the synthesizer to have
the visual appearance similar to instances of the texture defined by the prototype M0 [7].

3

Related Work

We now discuss the classes of deep learning methods that to the best of our understanding are most
related to our approach.
Our work is partially motivated by the recent approaches that analyze and visualize pretrained deep
networks by synthesizing color images evoking certain responses in these networks. Towards this
end [27] generate examples that maximize probabilities of certain classes according to the network,
[33] generate visual illusions that maximize such probabilities while retaining similarity to a predefined image of a potentially different class, [22] also investigate ways of generating highly-abstract
and structured color images that maximize probabilities of a certain class. Finally, [20] synthesize
color images that evoke a predefined vector of responses at a certain level of the network for the
purpose of network inversion. Our approach is related to these approaches, since our markers can be
regarded as stimuli invoking certain responses in the recognizer network. Unlike these approaches,
our recognizer network is not kept fixed but is updated together with the synthesizer network that
generates the marker images.
Another obvious connection are autoencoders [2], which are models trained to (1) encode inputs into
a compact intermediate representation through the encoder network and (2) recover the original input
4

64 bits, default params, C=59.9, p=99.3%

96 bits, low affine, C=90.2, p=99.3%

64 bits, low affine ? = 0.05, C=61.2, p=99.5%

8 bits, high blur, C=7.91, p=99.9%

32 bits, grayscale, C=27.9, p=98.3%

64 bits, nonlinear encoder, C=58.4, p=98.9%

64 bits, thin network, C=40.1, p=93.2%

64 bits, 16 pixel marker, C=56.8, p=98.5%

Figure 3: Visualization of the markers learned by our approach under different circumstances shown
in captions (see text for details). The captions also show the bit length, the capacity of the resulting encoding (in bits), as well as the accuracy achieved during training. In each case we show six
markers: (1) ? the marker corresponding to a bit sequence consisting of ?1, (2) ? the marker corresponding to a bit sequence consisting of +1, (3) and (4) ? markers for two random bit sequences that
differ by a single bit, (5) and (6) ? two markers corresponding to two more random bit sequences.
Under many conditions a characteristic grid pattern emerges.

by passing the compact representation through the decoder network. Our system can be regarded
as a special kind of autoencoder with the certain format of the intermediate representation (a color
image). Our decoder is trained to be robust to certain class of transformations of the intermediate
representations that are modeled by the rendering network. In this respect, our approach is related
to variational autoencoders [15] that are trained with stochastic intermediate representations and to
denoising autoencoders [30] that are trained to be robust to noise.
Finally, our approach for creating textured markers can be related to steganography [24], which aims
at hiding a signal in a carrier image. Unlike steganography, we do not aim to conceal information,
but just to minimize its ?intrusiveness?, while keeping the information machine-readable in the
presence of distortions associated with printing and scanning.

4

Experiments

Below, we present qualitative and quantitative evaluation of our approach. For longer bit sequences,
the approach might not be able to train a perfect pair of a synthesizer and a recognizer, and therefore,
similarly to other visual marker systems, it makes sense to use error-correcting encoding of the
signal. Since the recognizer network returns the odds for each bit in the recovered signal, our
approach is suitable for any probabilistic error-correction coding [19].
Synthesizer architectures. For the experiments without texture loss, we use the simplest synthesizer network, which consists of a single linear layer (with a 3m2 ? n matrix and a bias vector)
that is followed by an element-wise sigmoid. For the experiments with texture loss, we started with
the synthesizer used in [29], but found out that it can be greatly simplified for our task. Our final
architecture takes a binary code as input, transforms it with single fully connected layer and series
of 3 ? 3 convolutions with 2? upsamplings in between.
Recognizer architectures. Unless reported otherwise, the recognizer network was implemented as
a ConvNet with three convolutional layers (96 5 ? 5 filters followed by max-pooling and ReLU),
and two fully-connected layer with 192 and n output units respectively (where n is the length of
the code). We find this architecture sufficient to successfully deal with marker encoding. In some
experiments we have also considered a much smaller networks with 24 maps in convolutional layers,
and 48 units in the penultimate layer (?thin network?). In general, the convergence on the training
stage greatly benefits from adding Batch Normalization [10] after every convolutional layer. During
5

prototype

all ?1

all +1

half

random

random +
1 bit diff.

Figure 4: Examples of textured 64-bit marker families. The texture protototype is shown in the first
column, while five remaining columns show markers for the following sequences: all ?1, all +1,
32 consecutive ?1 followed by 32 ?1, and, finally, two random bit sequences that differ by a single
bit.

our experiments with texture loss, we used VGGNet-like architecture with 3 blocks, each consisting
of two 3 ? 3 convolutions and maxpooling, followed by two dense layers.
Rendering settings. We perform a spatial transform as an affine transformation, where the 6 affine
parameters are sampled from [1, 0, 0, 0, 1, 0]+N (0, ?) (assuming origin at the center of the marker).
The example for ? = 0.1 is shown in Fig. 2. We leave more complex spatial transforms (e.g. thin
plate spline [11]) that can make markers more robust to bending for future work. Some resilience to
bending can still be observed in our qualitative results.
Given an image x, we implement the color transformation layer as c1 xc2 + c3 , where the parameters
are sampled from the uniform distribution U [??, ?]. As we find that printed markers tend to reduce
the color contrast, we add a contrast reduction layer that transforms each value to kx + (1 ? k)[0.5]
for a random k.
Quantitative measurements. To quantify the performance of our markers under different circumstances, we report the accuracy p to which our system converges during the learning under different
settings (to evaluate accuracy, we threshold recognizer predictions at zero). Whenever we vary the
signal length n, we also report the capacity of the code, which is defined as C = n(1?H(p)), where
H(p) = ?p log p ? (1 ? p) log(1 ? p) is the coding entropy. Unless specified otherwise, we use
the rendering network settings visualized in Figure 2, which gives the impression of the variability
and the difficulty of the recovery problem, as the recognizer network is applied to the outputs of this
rendering network.
Experiments without texture loss. The bulk of experiments without the texture loss has been
performed with m = 32 i.e. 32 ? 32 patches (we used bilinear interpolation when printing or visualizing). The learned marker families with the base architectures as well as with its variations are
shown in Figure 3. It is curious to see the emergence of lattice structures (even though our synthesizer network in this case was a simple single-layer multiplicative network). Apparently, such
6

64/64

126/128

32/32

64/64

64/64

63/64

124/128

32/32

64/64

64/64

62/64

122/128

31/32

56/64

59/64

59/64

115/128

31/32

60/64

56/64

Figure 5: Screenshots of marker recognition process (black box is a part of the user interface and
corresponds to perfect alignment). The captions are in (number of correctly recovered bits/total
sequence length) format. The rightmost two columns correspond to stylized markers. These marker
families were trained with spatial variances ? = 0.1, 0.05, 0.1, 0.05, 0.05 respectively. Larger ?
leads to code recovery robustness with respect to affine transformation.

lattices are most efficient in terms of storing information for later recovery with a ConvNet. It can
also be seen how the system can adapt the markers to varying bit lengths or to varying robustness demands (e.g. to increasing blur or geometric distortions). We have further plotted how the quantitative
performance depends on the bit length and and on the marker size in Figure 6.
Experiments with texture loss. An interesting effect we have encountered while training synthesizer with texture loss and small output marker size is that it often ended up producing very similar
patterns. We tried to tweak architecture to handle this problem but eventually found out that it goes
away for larger markers.
Performance of real markers. We also show some qualitative results that include printing (on a
laser printer using various backgrounds) and capturing (with a webcam) of the markers. Characteristic results in Figure 4 demonstrate that our system can successfully recover encoded signals with
small amount of mistakes. The amount of mistakes can be further reduced by applying the system
with jitter and averaging the odds (not implemented here).
Here, we aid the system by roughly aligning the marker with a pre-defined square (shown as part
of the user interface). As can be seen the degradation of the results with the increasing alignment
error is graceful (due to the use of affine transforms inside the rendering network at train time). In
a more advanced system, such alignment can be bypassed altogether, using a pipeline that detects
marker instances in a video stream and localizes their corners. Here, one can either use existing
quad detection algorithms as in [23] or make the localization process a deep feed-forward network
and include it into the joint learning in our system. In the latter case, the synthesizer would adapt to
produce markers that are distinguishable from backgrounds and have easily identifiable corners. In
7

100

Accuracy, %

Accuracy, %

100

99

98

97

less affine
default
thin network
0

50

100
150
Number of bits

95

90
200

0

20
40
Marker size, pixels

60

Figure 6: Left ? dependence of the recognition accuracy on the size of the bit string for two variants
with the default networks, and one with the reduced number of maps in each convolutional layer.
Reducing the capacity of the network hurts the performance a lot, while reducing spatial variation in
the rendering network (to ? = 0.05) increases the capacity very considerably. Right ? dependence
of the recognition accuracy on the marker size (with otherwise default settings). The capacity of the
coding quickly saturates as markers grow bigger.

such qualitative experiments (Figure 4), we observe the error rates that are roughly comparable with
our quantitative experiments.
Recognizer networks for QR-codes. We have also experimented with replacing the synthesizer
network with a standard QR-encoder. While we tried different settings (such as error-correction
level, input bit sequence representation), the highest recognition rate we could achieve with our
architecture of the recognizer network was only 85%. Apparently, the recognizer network cannot
reverse the combination of error-correction encoding and rendering transformations well. We also
tried to replace both the synthesizer and the recognizer with a QR-encoder and a QR-decoder. Here
we found that standard QR-decoders cannot decode QR-markers processed by our renderer network
at the typical level of blur in our experiments (though special-purpose blind deblurring algorithms
such as [32] are likely to succeed).

5

Discussion

In this work, we have proposed a new approach to marker design, where marker design and their recognizer are learned jointly. Additionally, an aesthetics-related term can be added into the objective.
To the best of our knowledge, we are the first to approach visual marker design using optimization.
One curious side aspect of our work is the fact that the learned markers can provide an insight into
the architecture of ConvNets (or whatever architecture is used in the recognizer network). In more
details, they represent patterns that are most suitable for recognition with ConvNets. Unlike other
approaches that e.g. visualize patterns for networks trained to classify natural images, our method
decouples geometric and topological factors on one hand from the natural image statistics on the
other, as we obtain these markers in a ?content-free? manner1 .
As discussed above, one further extension to the system might be including marker localizer into
the learning as another deep feedforward network. We note that in some scenarios (e.g. generating
augmented reality tags for real-time camera localization), one can train the recognizer to estimate
the parameters of the geometric transformation in addition or even instead of the recovering the
input bit string. This would allow to create visual markers particularly suitable for accurate pose
estimation.
1

The only exception are the background images used by the rendering layer. In our experience, their statistics have negligible influence on the emerging patterns.

8

References
[1] L. F. Belussi and N. S. Hirata. Fast component-based qr code detection in arbitrarily acquired images.
Journal of mathematical imaging and vision, 45(3):277?292, 2013.
[2] Y. Bengio. Learning deep architectures for AI. Foundations and trends in Machine Learning, 2(1):1?127,
2009.
[3] F. Bergamasco, A. Albarelli, and A. Torsello. Pi-tag: a fast image-space marker design based on projective
invariants. Machine vision and applications, 24(6):1295?1310, 2013.
[4] D. Claus and A. W. Fitzgibbon. Reliable fiducial detection in natural scenes. Computer Vision-ECCV
2004, pp. 469?480. Springer, 2004.
[5] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural
networks. Conf. on Computer Vision and Pattern Recognition (CVPR), 2015.
[6] M. Fiala. ARTag, a fiducial marker system using digital techniques. Conf. Computer Vision and Pattern
Recognition (CVPR), v. 2, pp. 590?596, 2005.
[7] L. Gatys, A. S. Ecker, and M. Bethge. Texture synthesis using convolutional neural networks. Advances
in Neural Information Processing Systems, NIPS, pp. 262?270, 2015.
[8] L. A. Gatys, A. S. Ecker, and M. Bethge. A neural algorithm of artistic style. Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,CVPR, 2016.
[9] M. Hara, M. Watabe, T. Nojiri, T. Nagaya, and Y. Uchiyama. Optically readable two-dimensional code
and method and apparatus using the same, 1998. US Patent 5,726,435.
[10] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal
covariate shift. Proc. International Conference on Machine Learning, ICML, pp. 448?456, 2015.
[11] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial transformer networks. Advances in Neural
Information Processing Systems, pp. 2008?2016, 2015.
[12] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution.
European Conference on Computer Vision (ECCV), pp. 694?711, 2016.
[13] M. Kaltenbrunner and R. Bencina. Reactivision: a computer-vision framework for table-based tangible
interaction. Proc. of the 1st international conf. on tangible and embedded interaction, pp. 69?74, 2007.
[14] D. P. Kingma and J. B. Adam. A method for stochastic optimization. International Conference on
Learning Representation, 2015.
[15] D. P. Kingma and M. Welling. Auto-encoding variational bayes. International Conference on Learning
Representations, 2014.
[16] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541?551, 1989.
[17] C.-C. Lo and C. A. Chang. Neural networks for bar code positioning in automated material handling.
Industrial Automation and Control: Emerging Technologies, pp. 485?491. IEEE, 1995.
[18] A. Longacre Jr and R. Hussey. Two dimensional data encoding structure and symbology for use with
optical readers, 1997. US Patent 5,591,956.
[19] D. J. MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003.
[20] A. Mahendran and A. Vedaldi. Understanding deep image representations by inverting them. Conf.
Computer Vision and Pattern Recognition (CVPR), 2015.
[21] J. Mooser, S. You, and U. Neumann. Tricodes: A barcode-like fiducial design for augmented reality
media. IEEE Multimedia and Expo, pp. 1301?1304, 2006.
[22] A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence predictions
for unrecognizable images. Conf. on Computer Vision and Pattern Recognition (CVPR), 2015.
[23] E. Olson. Apriltag: A robust and flexible visual fiducial system. Robotics and Automation (ICRA), 2011
IEEE International Conference on, pp. 3400?3407. IEEE, 2011.
[24] F. A. Petitcolas, R. J. Anderson, and M. G. Kuhn. Information hiding-a survey. Proceedings of the IEEE,
87(7):1062?1078, 1999.
[25] A. Richardson and E. Olson. Learning convolutional filters for interest point detection. Conf. on Robotics
and Automation (ICRA), pp. 631?637, 2013.
[26] D. Scharstein and A. J. Briggs. Real-time recognition of self-similar landmarks. Image and Vision
Computing, 19(11):763?772, 2001.
[27] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising image
classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.
[28] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.
[29] D. Ulyanov, V. Lebedev, A. Vedaldi, and V. Lempitsky. Texture networks: Feed-forward synthesis of
textures and stylized images. Int. Conf. on Machine Learning (ICML), 2016.
[30] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with
denoising autoencoders. Int. Conf. on Machine learning (ICML), 2008.
[31] N. J. Woodland and S. Bernard. Classifying apparatus and method, 1952. US Patent 2,612,994.
[32] S. Yahyanejad and J. Str?om. Removing motion blur from barcode images. 2010 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition-Workshops, pp. 41?46. IEEE, 2010.
[33] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. Computer vision?
ECCV 2014, pp. 818?833. Springer, 2014.
[34] M. D. Zeiler, G. W. Taylor, and R. Fergus. Adaptive deconvolutional networks for mid and high level
feature learning. Int. Conf. on Computer Vision (ICCV), pp. 2018?2025, 2011.

9

"
3177,2009,Statistical Consistency of Top-k Ranking,"This paper is concerned with the consistency analysis on listwise ranking methods. Among various ranking methods, the listwise methods have competitive performances on benchmark datasets and are regarded as one of the state-of-the-art approaches. Most listwise ranking methods manage to optimize ranking on the whole list (permutation) of objects, however, in practical applications such as information retrieval, correct ranking at the top k positions is much more important. This paper aims to analyze whether existing listwise ranking methods are statistically consistent in the top-k setting. For this purpose, we define a top-k ranking framework, where the true loss (and thus the risks) are defined on the basis of top-k subgroup of permutations. This framework can include the permutation-level ranking framework proposed in previous work as a special case. Based on the new framework, we derive sufficient conditions for a listwise ranking method to be consistent with the top-k true loss, and show an effective way of modifying the surrogate loss functions in existing methods to satisfy these conditions. Experimental results show that after the modifications, the methods can work significantly better than their original versions.","Statistical Consistency of Top-k Ranking

Fen Xia
Institute of Automation
Chinese Academy of Sciences
fen.xia@ia.ac.cn

Tie-Yan Liu
Microsoft Research Asia
tyliu@microsoft.com

Hang Li
Microsoft Research Asia
hanglig@microsoft.com

Abstract
This paper is concerned with the consistency analysis on listwise ranking methods. Among various ranking methods, the listwise methods have competitive performances on benchmark datasets and are regarded as one of the state-of-the-art
approaches. Most listwise ranking methods manage to optimize ranking on the
whole list (permutation) of objects, however, in practical applications such as information retrieval, correct ranking at the top k positions is much more important.
This paper aims to analyze whether existing listwise ranking methods are statistically consistent in the top-k setting. For this purpose, we define a top-k ranking
framework, where the true loss (and thus the risks) are defined on the basis of
top-k subgroup of permutations. This framework can include the permutationlevel ranking framework proposed in previous work as a special case. Based on
the new framework, we derive sufficient conditions for a listwise ranking method
to be consistent with the top-k true loss, and show an effective way of modifying the surrogate loss functions in existing methods to satisfy these conditions.
Experimental results show that after the modifications, the methods can work significantly better than their original versions.

1

Introduction

Ranking is the central problem in many applications including information retrieval (IR). In recent
years, machine learning technologies have been successfully applied to ranking, and many learning
to rank methods have been proposed, including the pointwise [12] [9] [6], pairwise [8] [7] [2], and
listwise methods [13] [3] [16]. Empirical results on benchmark datasets have demonstrated that the
listwise ranking methods have very competitive ranking performances [10].
To explain the high ranking performances of the listwise ranking methods, a theoretical framework
was proposed in [16]. In the framework, existing listwise ranking methods are interpreted as making
use of different surrogate loss functions of the permutation-level 0-1 loss. Theoretical analysis shows
that these surrogate loss functions are all statistically consistent in the sense that minimization of the
conditional expectation of them will lead to obtaining the Bayes ranker, i.e., the optimal ranked list
of the objects.
Here we point out that there is a gap between the analysis in [16] and many real ranking problems,
where the correct ranking of the entire permutation is not needed. For example, in IR, users usually
care much more about the top ranking results and thus only correct ranking at the top positions is
important. In this new situation, it is no longer clear whether existing listwise ranking methods are
still statistically consistent. The motivation of this work is to perform formal study on the issue.
For this purpose, we propose a new ranking framework, in which the ?true loss? is defined on the
top-k subgroup of permutations instead of on the entire permutation. The new true loss only measures errors occurring at the top k positions of a ranked list, therefore we refer to it as the top-k true
loss (Note that when k equals the length of the ranked list, the top-k true loss will become exactly
1

the permutation-level 0-1 loss). We prove a new theorem which gives sufficient conditions for a
surrogate loss function to be consistent with the top-k true loss. We also investigate the change of
the conditions with respect to different k?s. Our analysis shows that, as k decreases, to guarantee the
consistency of a surrogate loss function, the requirement on the probability space becomes weaker
while the requirement on the surrogate loss function itself becomes stronger. As a result, a surrogate loss function that is consistent with the permutation-level 0-1 loss might not be consistent with
the top-k true loss any more. Therefore, the surrogate loss functions in existing listwise ranking
methods, which have been proved to be consistent with the permutation-level 0-1 loss, are not theoretically guaranteed to have good performances in the top-k setting. Modifications to these surrogate
loss functions are needed to further make them consistent with the top-k true loss. We show how
to make such modifications, and empirically verify that such modifications can lead to significant
performance improvement. This validates the correctness of our theoretical analysis.

2

Permutation-level ranking framework

We review the permutation-level ranking framework proposed in [16].
Let X be the input space whose elements are groups of objects to be ranked, Y be the output space
whose elements are permutations of objects, and PXY be an unknown but fixed joint probability
distribution of X and Y . Let h ? H : X ? Y be a ranking function. Let x ? X and y ? Y , and
let y(i) be the index of the object that is ranked at position i in y. The task of learning to rank is to
learn a function that can minimize the expected risk R(h), defined as,
Z
R(h) =
l(h(x), y)dP (x, y),
(1)
X?Y

where l(h(x), y) is the true loss such that
l(h(x), y) =



1,
0,

if h(x) 6= y
if h(x) = y.

(2)

The above true loss indicates that if the permutation of the predicted result is exactly the same as
the permutation in the ground truth, then the loss is zero; otherwise the loss is one. For ease of
reference, we call it permutation-level 0-1 loss. The optimal ranking function which can minimize
the expected true risk R(h? ) = inf R(h) is referred to as the permutation-level Bayes ranker.
h? (x) = arg max P (y|x).
y?Y

(3)

In practice, for efficiency consideration, the ranking function is usually defined as h(x) =
sort(g(x1 ), . . . , g(xn )), where g(?) denotes the scoring function, and sort(?) denotes the sorting
function. Since the risk is non-continuous and non-differentiable with respect to the scoring function
g, a continuous and differentiable surrogate loss function ?(g(x), y) is usually used as an approximation of the true loss. In this way, the expected risk becomes
Z
R? (g) =
?(g(x), y)dP (x, y),
(4)
X?Y

where g(x) = (g(x1 ), . . . , g(xn )) is a vector-valued function induced by g.
It has been shown in [16] that many existing listwise ranking methods fall into the above framework,
with different surrogate loss functions used. Furthermore, their surrogate loss functions are statistically consistent under certain conditions with respect to the permutation-level 0-1 loss. However,
as shown in the next section, the permutation-level 0-1 loss is not suitable to describe the ranking
problem in many real applications.

3

Top-k ranking framework

We next describe the real ranking problem, and then propose the top-k ranking framework.
2

3.1

Top-k ranking problem

In real ranking applications like IR, people pay more attention to the top-ranked objects. Therefore
the correct ranking on the top positions is critically important. For example, modern web search
engines only return top 1, 000 results and 10 results in each page. According to a user study1 , 62%
of search engine users only click on the results within the first page, and 90% of users click on the
results within the first three pages. It means that two ranked lists of documents will likely provide
the same experience to the users (and thus suffer the same loss), if they have the same ranking results
for the top positions. This, however, cannot be reflected in the permutation-level 0-1 loss in Eq.(2).
This characteristic of ranking problems has also been explored in earlier studies in different settings
[4, 5, 14]. We refer to it as the top-k ranking problem.
3.2

Top-k true loss

To better describe the top-k ranking problem, we propose defining the true loss based on the top k
positions in a ranked list, referred to as the top-k true loss.

0, if y?(i) = y(i) ?i ? {1, . . . , k}, where y? = h(x),
(5)
lk (h(x), y) =
1,
otherwise .
The actual value of k is determined by application. When k equals the length of the entire ranked
list, the top-k true loss will become exactly the permutation-level 0-1 loss. In this regard, the top-k
true loss is more general than the permutation-level 0-1 loss.
With Eq.(5), the expected risk becomes
Z
Rk (h) =

lk (h(x), y)dP (x, y).

(6)

X?Y

It can be proved that the optimal ranking function with respect to the top-k true loss (i.e., the top-k
Bayes ranker) is any permutation in the top-k subgroup having the highest probability2 , i.e.,
h?k (x) ? arg maxGk (j1 ,j2 ,...,jk )?Gk P (Gk (j1 , j2 , ..., jk )|x),

(7)

where Gk (j1 , j2 , ..., jk ) = {y ? Y |y(t) = jt , ?t = 1, 2, . . . k} denotes a top-k subgroup in which
all the permutations have the same top-k true loss; Gk denotes the collection of all top-k subgroups.
With the above setting, we will analyze the consistency of the surrogate loss functions in existing
ranking methods with the top-k true loss in the next section.

4

Theoretical analysis

In this section, we first give the sufficient conditions of consistency for the top-k ranking problem.
Next, we show how these conditions change with respect to k. Last, we discuss whether the surrogate
loss functions in existing methods are consistent, and how to make them consistent if not.
4.1

Statistical consistency

We investigate what kinds of surrogate loss functions ?(g(x), y) are statistically consistent with
the top-k true loss. For this purpose, we study whether the ranking function that minimizes the
conditional expectation of the surrogate loss function defined as follows coincides with the top-k
Bayes ranker as defined in Eq.(7).
X
Q(P (y|x), g(x)) =
P (y|x)?(g(x), y).
(8)
y?Y

1

iProspect Search Engine User Behavior Study, April 2006, http://www.iprospect.com/
Note that the probability of a top-k subgroup is defined as the sum of the probabilities of the permutations
in the subgroup (cf., Definitions 6 and 7 in [3]).
2

3

According to [1], the above condition is the weakest condition to guarantee that optimizing a surrogate loss function will lead to obtaining a model achieving the Bayes risk (in our case, the top-k
Bayes ranker), when the training sample size approaches infinity.
We denote Q(P (y|x), g(x)) as Q(p, g), g(x) as g and P (y|x) as py . Hence, Q(p, g) is the loss
of g at x with respect to the conditional probability distribution py . The key idea is to decompose
the sorting of g into pairwise relationship between scores of objects. To this end, we denote Yi,j as
a permutation set in which each permutation ranks object i before object j, i.e., Yi,j , {y ? Y :
y ?1 (i) < y ?1 (j)} (here y ?1 (j) denotes the position of object j in permutation y), and introduce
the following definitions.
Definition 1. ?Gk is the a top-k subgroup probability space, such that ?Gk , {p ? R|Gk | :
P
Gk (j1 ,j2 ,...,jk )?Gk pGk (j1 ,j2 ,...,jk ) = 1, pGk (j1 ,j2 ,...,jk ) ? 0}.

Definition 2. A top-k subgroup probability space ?Gk is order preserving with respect to objects
?1
?1
?1
i and j, if ?y ? Yi,j and Gk (y(1), y(2), ..., y(k)) 6= Gk (?i,j
y(1), ?i,j
y(2), ..., ?i,j
y(k)), we
?1
have pGk (y(1),y(2),...,y(k)) > pGk (??1 y(1),??1 y(2),...,??1 y(k)) . Here ?i,j y denotes the permutation
i,j
i,j
i,j
in which the positions of objects i and j are exchanged while those of the other objects remain the
same as in y.
Definition 3. A surrogate loss function ? is top-k subgroup order sensitive on a set ? ? Rn , if ?
is a non-negative differentiable function and the following three conditions hold for ? objects i and
?1
?1
j: (1) ?(g, y) = ?(?i,j
g, ?i,j
y); (2)Assume gi < gj , ?y ? Yi,j . If Gk (y(1), y(2), ..., y(k)) 6=
?1
?1
?1
?1
Gk (?i,j y(1), ?i,j y(2), ..., ?i,j y(k)), then ?(g, y) ? ?(g, ?i,j
y) and for at least one y, the strict
?1
inequality holds; otherwise, ?(g, y) = ?(g, ?i,j y). (3) Assume gi = gj . ?y ? Yi,j with
?1
?1
?1
Gk (y(1), y(2), ..., y(k)) 6= Gk (?i,j
y(1), ?i,j
y(2), ..., ?i,j
y(k)) satisfying

?1
??(g,?i,j
y)
?gi

>

??(g,y)
?gi .

The order preserving property of a top-k subgroup probability space (see Definition 2) indicates
that if the top-k subgroup probability on a permutation y ? Yi,j is larger than that on permutation
?1
?1 0
?i,j
y, then the relation holds for any other permutation y 0 in Yi,j and and the corresponding ?i,j
y
provided that the top-k subgroup of the former is different from that of the latter. The order sensitive
property of a surrogate loss function (see Definition 3) indicates that (i) ?(g, y) exhibits a symmetry
in the sense that simultaneously exchanging the positions of objects i and j in the ground truth
and their scores in the predicted score list will not make the surrogate loss change. (ii) When a
permutation is transformed to another permutation by exchanging the positions of two objects of it,
if the two permutations do not belong to the same top-k subgroup, the loss on the permutation that
ranks the two objects in the decreasing order of their scores will not be greater than the loss on its
counterpart. (iii) There exists a permutation, for which the speed of change in loss with respect to
the score of an object will become faster if exchanging its position with another object with the same
score but ranked lower. A top-k subgroup order sensitive surrogate loss function has several nice
properties as shown below.
Proposition 4. Let ?(g, y) be a top-k subgroup order sensitive loss function. ?y, ?? ?
Gk (y(1), y(2), . . . , y(k)), we have ?(g, ?) = ?(g, y).
Proposition 5. Let ?(g, y) be a top-k subgroup order sensitive surrogate loss function. ? objects i
?1
?1
?1
and j with gi = gj , ?y ? Yi,j , if Gk (y(1), y(2), ..., y(k)) 6= Gk (?i,j
y(1), ?i,j
y(2), ..., ?i,j
y(k)),
then

?1
??(g,?i,j
y)
?gi

?

??(g,y)
?gi .

Otherwise,

?1
??(g,?i,j
y)
?gi

=

??(g,y)
?gi .

Proposition 4 shows that all permutations in the same top-k subgroup share the same loss ?(g, y)
and thus share the same partial difference with respect to the score of a given object. Proposition 5
indicates that the partial difference of ?(g, y) also has a similar property to ?(g, y) (see the second
condition in Definition 3). Due to space restriction, we omit the proofs (see [15] for more details).
Based on the above definitions and propositions, we give the main theorem (Theorem 6), which
states the sufficient conditions for a surrogate loss function to be consistent with the top-k true loss.
Theorem 6. Let ? be a top-k subgroup order sensitive loss function on ? ? Rn . For ?n objects, if its top-k subgroup probability space is order preserving with respect to n ? 1 object pairs
{(ji , ji+1 )}ki=1 and {(jk+si , jk+i : 0 ? si < i)}n?k
i=2 , then the loss ?(g, y) is consistent with the
top-k true loss as defined in Eq.(5).
4

The proof of the main theorem is mostly based on Theorem 7, which specifies the score relation
between two objects for the minimizer of Q(p, g). Due to space restriction, we only give Theorem
7 and its detailed proof. For the detailed proof of Theorem 6, please refer to [15].
Theorem 7. Let ?(g, y) be a top-k subgroup order sensitive loss function. ?i and j, if the topk subgroup probability space is order preserving with respect to them, and g is a vector which
minimizes Q(p, g) in Eq.(8), then gi > gj .
Proof. Without loss of generality, we assume i = 1, j = 2, g10 = g2 , g20 = g1 , and gk0 = gk (k > 2).
First, we prove g1 ? g2 by contradiction. Assume g1 < g2 , we have
X
X
?1
Q(p, g0 ) ? Q(p, g) =
(p??1 y ? py )?(g, y) =
(p??1 y ? py )(?(g, y) ? ?(g, ?1,2
y)).
y?Y

1,2

y?Y1,2

1,2

?1
The first equation is based on the fact g0 = ?1,2
g, and the second equation is based on the fact
?1 ?1
?1,2
?1,2 y = y. After some algebra, by using Proposition 4, we have,
X
?1
Q(p, g0 ) ? Q(p, g) =
(pGk (??1 y) ? pGk (y) )(?(g, y) ? ?(g, ?1,2
y)),
1,2

?1
Gk (y)?{Gk :Gk (y)6=Gk (?1,2
y)}:y?Y1,2

where Gk (y) denotes the subgroup that y belongs to.
?1
Since g1 < g2 , we have ?(g, y) ? ?(g, ?1,2
y). Meanwhile, pGk (??1 y) < pGk (y) due to the order
1,2
preserving of the top-k subgroup probability space. Thus each component in the sum is non-positive
and at least one of them is negative, which means Q(p, g0 ) < Q(p, g). This is a contradiction to
the optimality of g. Therefore, we must have g1 ? g2 .

Second, we prove g1 6= g2 , again by contradiction. Assume g1 = g2 . By setting the derivative of
Q(p, g) with respect to g1 and g2 to zero and compare them3 , we have,
?1
X
??(g, y) ??(g, ?1,2 y)
(py ? p??1 y )(
?
) = 0.
1,2
?g1
?g1
y?Y1,2

After some algebra, we obtain,
X

(pGk (y) ? pGk (??1 y) )(
1,2

?1
Gk (y)?{Gk :Gk (y)6=Gk (?1,2
y)}:y?Y1,2

?1
??(g, y) ??(g, ?1,2 y)
?
) = 0.
?g1
?g1

??(g,? ?1 y)

1,2
According to Proposition 5, we have ??(g,y)
?
. Meanwhile, pGk (??1 y) < pGk (y) due to
?g1
?g1
1,2
the order preserving of the top-k subgroup probability space. Thus, the above equation cannot hold
since at least one of components in the sum is negative according to Definition 3.

Consistency with respect to k

4.2

We discuss the change of the consistency conditions with respect to various k values.
First, we have the following proposition for the top-k subgroup probability space.
Proposition 8. If the top-k subgroup probability space is order preserving with respect to object i
and j, the top-(k ? 1) subgroup probability space is also order preserving with respect to i and j.
The proposition can be proved by decomposing a top-(k ? 1) subgroup into the sum of top-k subgroups. One can find the detailed proof in [15]. Here we give an example to illustrate the basic idea.
Suppose there are three objects {1, 2, 3} to be ranked. If the top-2 subgroup probability space is order preserving with respect to objects 1 and 2, then we have pG2 (1,2) > pG2 (2,1) , pG2 (1,3) > pG2 (2,3)
and pG2 (3,1) > pG2 (3,2) . On the other hand, for top-1, we have pG1 (1) > pG1 (2) . Note that
pG1 (1) = pG2 (1,2) + pG2 (1,3) and pG1 (2) = pG2 (2,1) + pG2 (2,3) . Thus, it is easy to verify that
Proposition 8 holds for this case while the opposite does not.
Second, we obtain the following proposition for the surrogate loss function ?.
3

By trivial modifications, one can handle the case that g1 or g2 is infinite (cf. [17]).

5

Proposition 9. If the surrogate loss function ? is top-k subgroup order sensitive on a set ? ? Rn ,
then it is also top-(k + 1) subgroup order sensitive on the same set.
Again, one can refer to [15] for the detailed proof of the proposition, and here we only provide an example. Let us consider the same setting in the previous example. Assume that
g1 < g2 . If ? is top-1 subgroup order sensitive, then we have ?(g, (1, 2, 3)) ? ?(g, (2, 1, 3)),
?(g, (1, 3, 2)) ? ?(g, (2, 3, 1)), and ?(g, (3, 1, 2)) = ?(g, (3, 2, 1)). From Proposition 4, we know
that the two inequalities are strict. On the other hand, if ? is top-2 subgroup order sensitive, the
following inequalities hold with at least one of them being strict: ?(g, (1, 2, 3)) ? ?(g, (2, 1, 3)),
?(g, (1, 3, 2)) ? ?(g, (2, 3, 1)), and ?(g, (3, 1, 2)) ? ?(g, (3, 2, 1)). Therefore top-1 subgroup
order sensitive is a special case of top-2 subgroup order sensitive.
According to the above propositions, we can come to the following conclusions.
? For the consistency with the top-k true loss, when k becomes smaller, the requirement on
the probability space becomes weaker but the requirement on the surrogate loss function
becomes stronger. Since we never know the real property of the (unknown) probability
space, it is more likely the requirement on the probability space for the consistency with
the top-k true loss can be satisfied than that for the top-l (l > k) true loss. Specifically, it is
risky to assume the requirement for the permutation-level 0-1 loss to hold.
? If we fix the true loss to be top-k and the probability space to be top-k subgroup order
preserving, the surrogate loss function should be at most top-l (l ? k) subgroup order
sensitive in order to meet the consistency conditions. It is not guaranteed that a top-l (l > k)
subgroup order sensitive surrogate loss function can be consistent with the top-k true loss.
For example, a top-1 subgroup order sensitive surrogate loss function may be consistent
with any top-k true loss, but a permutation-level order sensitive surrogate loss function
may not be consistent with any top-k true loss, if k is smaller than the length of the list.
For ease of understanding the above discussions, let us see an example shown in the following
proposition (the proof of this proposition can be found in [15]). It basically says that given a probability space that is top-1 subgroup order preserving, a top-3 subgroup order sensitive surrogate loss
function may not be consistent with the top-1 true loss.
Proposition 10. Suppose there are three objects to be ranked. ? is a top-3 subgroup order sensitive
loss function and the strict inequality ?(g, (3, 1, 2)) < ?(g, (3, 2, 1)) holds when g1 > g2 . The
probabilities of permutations are p123 = p1 , p132 = 0, p213 = p2 , p231 = 0, p312 = 0, p321 = p2
respectively, where p1 > p2 . Then ? is not consistent with the top-1 true loss.
The above discussions imply that although the surrogate loss functions in existing listwise ranking
methods are consistent with the permutation-level 0-1 loss (under a rigid condition), they may not
be consistent with the top-k true loss (under a mild condition). Therefore, it is necessary to modify
these surrogate loss functions. We will make discussions on this in the next subsection.
4.3

Consistent surrogate loss functions

In [16], the surrogate loss functions in ListNet, RankCosine, and ListMLE have been proved to be
permutation-level order sensitive. According to the discussion in the previous subsection, however,
they may not be top-k subgroup order sensitive, and therefore not consistent with the top-k true loss.
Even for the consistency with the permutation-level 0-1 loss, in order to guarantee these surrogate
loss functions to be consistent, the requirement on the probability space may be too strong in some
real scenarios. To tackle the challenge, it is desirable to modify these surrogate loss functions to
make them top-k subgroup order sensitive. Actually this is doable, and the modifications to the
aforementioned surrogate loss functions are given as follows.
4.3.1

Likelihood loss

The likelihood loss is the loss function used in ListMLE [16], which is defined as below,
?(g(x), y) = ? log P (y|x; g),

where P (y|x; g) =

6

n
Y

exp(g(xy(i) ))
Pn
.
t=i exp(g(xy(t) ))
i=1

(9)

We propose replacing the permutation probability with the top-k subgroup probability (which is also
defined with the Luce model [11]) in the above definition:
P (y|x; g) =

k
Y

exp(g(xy(i) ))
Pn
.
t=i exp(g(xy(t) ))
i=1

(10)

It can be proved that the modified loss is top-k subgroup order sensitive (see [15]).
4.3.2

Cosine loss

The cosine loss is the loss function used in RankCosine [13], which is defined as follows,
?(g(x), y) =

?y (x)T g(x)
1
(1 ?
),
2
k?y (x)kkg(x)k

(11)

where the score vector of the ground truth is produced by a mapping function ?y (?) : Rd ? R,
which retains the order in a permutation, i.e., ?y (xy(1) ) > ? ? ? > ?y (xy(n) ).
We propose changing the mapping function as follows. Let the mapping function retain the order
for the top k positions of the ground truth permutation and assigns to all the remaining positions
a small value (which is smaller than the score of any object ranked at the top-k positions), i.e.,
?y (xy(1) ) > ? ? ? > ?y (xy(k) ) > ?y (xy(k+1) ) = ? ? ? = ?y (xy(n) ) = . It can be proved that after
the modification, the cosine loss becomes top-k subgroup order sensitive (see [15]).
4.3.3

Cross entropy loss

The cross entropy loss is the loss function used in ListNet [3], defined as follows,
?(g(x), y) = D(P (?|x; ?y )||P (?|x; g)),

(12)

where ? is a mapping function whose definition is similar to that in RankCosine, and P (?|x; ?y )
and P (?|x; g) are the permutation probabilities in the Luce model.
We propose using a mapping function to modify the cross entropy loss in a similar way as in the case
of the cosine loss4 It can be proved that such a modification can make the surrogate loss function
top-k subgroup order sensitive (see [15]).

5

Experimental results

In order to validate the theoretical analysis in this work, we conducted some empirical study. Specifically, we used OHSUMED, TD2003, and TD2004 in the LETOR benchmark dataset [10] to perform
some experiments. As evaluation measure, we adopted Normalized Discounted Cumulative Gain
(N) at positions 1, 3, and 10, and Precision (P) at positions 1, 3, and 10.5 It is obvious that these
measures are top-k related and are suitable to evaluate the ranking performance in top-k ranking
problems.
We chose ListMLE as example method since the likelihood loss has nice properties such as convexity, soundness, and linear computational complexity [16]. We refer to the new method that we
obtained by applying the modifications mentioned in Section 4.3 as top-k ListMLE. We tried different values of k (i.e., k=1, 3, 10, and the exact length of the ranked list). Obviously the last case
corresponds to the original likelihood loss in ListMLE.
Since the training data in LETOR is given in the form of multi-level ratings, we adopted the methods
proposed in [16] to produce the ground truth ranked list. We then used stochastic gradient descent
as the algorithm for optimization of the likelihood loss. As for the ranking model, we chose linear
Neural Network, since the model has been widely used [3, 13, 16].
4

Note that in [3], a top-k cross entropy loss was also proposed, by using the top-k Luce model. However,
it can be verified that the so-defined top-k cross entropy loss is still permutation-level order sensitive, but not
top-k subgroup order sensitive. In other words, the proposed modification here is still needed.
5
On datasets with only two ratings such as TD2003 and TD2004, N@1 equals P@1.

7

The experimental results are summarized in Tables 1-3.
Methods

N@1

N@3

N@10

P@1

P@3

P@10

Methods

N/P@1

N@3

N@10

P@3

P@10

ListMLE

0.548

0.473

0.446

0.642

0.582

0.495

ListMLE

0.24

0.253

0.261

0.22

0.146

Top-1 ListMLE

0.529

0.482

0.447

0.652

0.595

0.499

Top-1 ListMLE

0.4

0.329

0.314

0.3

0.176

Top-3 ListMLE

0.535

0.484

0.445

0.671

0.608

0.504

Top-3 ListMLE

0.44

0.382

0.343

0.34

0.204

Top-10 ListMLE

0.558

0.473

0.444

0.672

0.601

0.509

Top-10 ListMLE

0.5

0.410

0.378

0.38

0.22

Table 1: Ranking accuracies on OHSUMED
Methods
ListMLE
Top-1 ListMLE

N/P@1
0.4
0.52

N@3
0.351
0.469

N@10
0.356
0.451

P@3
0.284
0.413

Table 2: Ranking accuracies on TD2003
Methods

N@1

N@3

N@10

P@1

P@3

P@10

RankBoost

0.497

0.472

0.435

0.604

0.586

0.495

Ranking SVM

0.495

0.464

0.441

0.633

0.592

0.507

ListNet

0.523

0.477

0.448

0.642

0.602

0.509

RankCosine

0.523

0.475

0.437

0.642

0.589

0.493

Top-10 ListMLE

0.558

0.473

0.444

0.672

0.601

0.509

P@10
0.188
0.248

Top-3 ListMLE

0.506

0.456

0.458

0.417

0.261

Top-10 ListMLE

0.52

0.469

0.472

0.413

0.269

Table 3: Ranking accuracies on TD2004

Table 4: Ranking accuracies on OHSUMED

From the tables, we can see that with the modifications the ranking accuracies of ListMLE can be
significantly boosted, in terms of all measures, on both TD2003 and TD2004. This clearly validates
our theoretical analysis. On OHSUMED, all the loss functions achieve comparable performances.
The possible explanation is that the probability space in OHSUMED is well formed such that it is
order preserving for many different k values.
Next, we take Top-10 ListMLE as an example to make comparison with some other baseline methods such as Ranking SVM [8], RankBoost [7], ListNet [3], and RankCosine [13]. The results are
listed in Tables 4-6. We can see from the tables, Top-10 ListMLE achieves the best performance
among all the methods on the TD2003 and TD2004 datasets in terms of almost all measures. On the
OHSUMED dataset, it also performs fairly well as compared to the other methods. Especially for
N@1 and P@1, it significantly outperforms all the other methods on all the datasets.
Methods

N/P@1

N@3

N@10

P@3

P@10

Methods

N/P@1

N@3

N@10

P@3

P@10

RankBoost

0.26

0.270

0.285

0.24

0.178

RankBoost

0.48

0.463

0.471

0.404

0.253

Ranking SVM

0.42

0.378

0.341

0.34

0.206

Ranking SVM

0.44

0.409

0.420

0.351

0.225

ListNet

0.46

0.408

0.374

0.36

0.222

ListNet

0.439

0.437

0.457

0.399

0.257

RankCosine

0.36

0.346

0.322

0.3

0.182

RankCosine

0.439

0.397

0.405

0.328

0.209

Top-10 ListMLE

0.5

0.410

0.378

0.38

0.22

Top-10 ListMLE

0.52

0.469

0.472

0.413

0.269

Table 5: Ranking accuracies on TD2003

Table 6: Ranking accuracies on TD2004

From the above experimental results, we can come to the conclusion that for real ranking applications like IR (where top-k evaluation measures are widely used), it is better to use the top-k true loss
than the permutation-level 0-1 loss, and is better to use the modified surrogate loss functions than
the original surrogate loss functions.

6

Conclusion

In this paper we have proposed a top-k ranking framework, which can better describe real ranking applications like information retrieval. In the framework, the true loss is defined on the top-k
subgroup of permutations. We have derived the sufficient conditions for a surrogate loss function
to be statistically consistent with the top-k true loss. We have also discussed how to modify the
loss functions in existing listwise ranking methods to make them consistent with the top-k true loss.
Our experiments have shown that with the proposed modifications, algorithms like ListMLE can
significantly outperform their original version, and also many other ranking methods.
As future work, we plan to investigate the following issues. (1) we will empirically study the modified ListNet and RankCosine, to see whether their performances can also be significantly boosted in
the top-k setting. (2) We will also study the consistency of the pointwise and pairwise loss functions
with the top-k true loss.
8

References
[1] P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classification, and risk bounds.
Journal of the American Statistical Association, 101:138?156, 2006.
[2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender.
Learning to rank using gradient descent. In Proc. of ICML?05, pages 89?96, 2005.
[3] Z. Cao, T. Qin, T. Y. Liu, M. F. Tsai, and H. Li. Learning to rank: From pairwise approach to
listwise approach. In Proc. of ICML?07, pages 129?136, 2007.
[4] S. Clemencon and N. Vayatis. Ranking the best instances. Journal of Machine Learning
Research, 8:2671?2699, 2007.
[5] D. Cossock and T. Zhang. Subset ranking using regression. In Proc. of COLT, pages 605?619,
2006.
[6] D. Cossock and T. Zhang. Statistical analysis of bayes optimal subset ranking. Information
Theory, 54:5140?5154, 2008.
[7] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efficient boosting algorithm for combining
preferences. In Proc. of ICML?98, pages 170?178, 1998.
[8] R. Herbrich, T. Graepel, and K. Obermayer. Support vector vector learning for ordinal regression. In Proc. of ICANN?99, pages 97?102, 1999.
[9] P. Li, C. Burges, and Q. Wu. Mcrank: Learning to rank using multiple classification and
gradient boosting. In Advances in Neural Information Processing Systems 20(NIPS 07), pages
897?904, Cambridge, MA, 2008. MIT Press.
[10] T. Y. Liu, T. Qin, J. Xu, W. Y. Xiong, and H. Li. Letor: Benchmark dataset for research on
learning to rank for information retrieval. In LR4IR 2007, in conjunction with SIGIR 2007,
2007.
[11] J. I. Marden, editor. Analyzing and Modeling Rank Data. Chapman and Hall, London, 1995.
[12] R. Nallapati. Discriminative models for information retrieval. In Proc. of SIGIR?04, pages
64?71, 2004.
[13] T. Qin, X.-D. Zhang, M.-F. Tsai, D.-S. Wang, T.-Y. Liu, and H. Li. Query-level loss functions
for information retrieval. Information processing and management, 44:838?855, 2008.
[14] C. Rudin. Ranking with a p-norm push. In Proc. of COLT, pages 589?604, 2006.
[15] F. Xia, T. Y. Liu, and H. Li. Top-k consistency of learning to rank methods. Technical report,
Microsoft Research, MSR-TR-2009-139, 2009.
[16] F. Xia, T. Y. Liu, J. Wang, W. S. Zhang, and H. Li. Listwise approach to learning to rank theory and algorithm. In Proc. of ICML?08, pages 1192?1199, 2008.
[17] T. Zhang. Statistical analysis of some multi-category large margin classification methods.
Journal of Machine Learning Research, 5:1225?1251, 2004.

9

"
462,1997,How to Dynamically Merge Markov Decision Processes,Abstract Missing,"How to Dynamically Merge Markov
Decision Processes
Satinder Singh
Department of Computer Science
University of Colorado
Boulder, CO 80309-0430
baveja@cs.colorado.edu

David Cohn
Adaptive Systems Group
Harlequin, Inc.
Menlo Park, CA 94025
cohn@harlequin.com

Abstract

We are frequently called upon to perform multiple tasks that compete for our attention and resource. Often we know the optimal
solution to each task in isolation; in this paper, we describe how
this knowledge can be exploited to efficiently find good solutions
for doing the tasks in parallel. We formulate this problem as that of
dynamically merging multiple Markov decision processes (MDPs)
into a composite MDP, and present a new theoretically-sound dynamic programming algorithm for finding an optimal policy for the
composite MDP. We analyze various aspects of our algorithm and
illustrate its use on a simple merging problem.

Every day, we are faced with the problem of doing mUltiple tasks in parallel, each
of which competes for our attention and resource. If we are running a job shop,
we must decide which machines to allocate to which jobs, and in what order, so
that no jobs miss their deadlines. If we are a mail delivery robot, we must find the
intended recipients of the mail while simultaneously avoiding fixed obstacles (such
as walls) and mobile obstacles (such as people), and still manage to keep ourselves
sufficiently charged up.
Frequently we know how to perform each task in isolation; this paper considers how
we can take the information we have about the individual tasks and combine it to
efficiently find an optimal solution for doing the entire set of tasks in parallel. More
importantly, we describe a theoretically-sound algorithm for doing this merging
dynamically; new tasks (such as a new job arrival at a job shop) can be assimilated
online into the solution being found for the ongoing set of simultaneous tasks.

1058

1

S. Singh and D. Cohn

The Merging Framework

Many decision-making tasks in control and operations research are naturally formulated as Markov decision processes (MDPs) (e.g., Bertsekas & Tsitsikiis, 1996). Here
we define MDPs and then formulate what it means to have multiple simultanous
MDPs.

1.1

Markov decision processes (MDPs)

An MDP is defined via its state set 8, action set A, transition probability matrices
P, and payoff matrices R. On executing action a in state s the probability of
transiting to state s' is denoted pa(ss') and the expected payoff associated with
that transition is denoted Ra (ss'). We assume throughout that the payoffs are
non-negative for all transitions. A policy assigns an action to each state of the
MDP. The value of a state under a policy is the expected value of the discounted
sum of payoffs obtained when the policy is followed on starting in that state. The
objective is to find an optimal policy, one that maximizes the value of every state.
The optimal value of state s, V* (s), is its value under the optimal policy.
The optimal value function is the solution to the Bellman optimality equations: for
all s E 8 , V(s) = maxaEA(E s pa(ss') [Ra (ss') +/V(s'))), where the discount factor
o ~ / < 1 makes future payoffs less valuable than more immediate payoffs (e.g.,
Bertsekas & Tsitsiklis, 1996). It is known that the optimal policy 7r* can be determined from V* as follows: 7r*(s) = argmaxaE A(E s pa(ss')[Ra(ss') +/V*(s'))).
Therefore solving an MDP is tantamount to computing its optimal value function.
l

l

1.2

Solving MDPs via Value Iteration

Given a model (8, A, P, R) of an MDP value iteration (e.g., Bertsekas & Tsitsikiis,
1996) can be used to determine the optimal value function. Starting with an initial
guess, Vo, iterate for all s Vk+1(S) = maxaEA(E sIES pa(ss')[Ra(ss') + /Vk(S'))). It
is known that maxsES 1Vk+1 (s) - V*(s)1 ~ / maxsES IVk(S) - V*(s)1 and therefore
Vk converges to V* as k goes to infinity. Note that a Q-value (Watkins, 1989) based
version of value iteration and our algorithm presented below is also easily defined.

1.3

Multiple Simultaneous MDPs

The notion of an optimal policy is well defined for a single task represented as
an MDP. If, however, we have multiple tasks to do in parallel, each with its own
state, action, transition probability, and payoff spaces, optimal behavior is not
automatically defined. We will assume that payoffs sum across the MDPs, which
means we want to select actions for each MDP at every time step so as to maximize
the expected discounted value of this summed payoff over time. If actions can be
chosen independently for each MDP, then the solution to this ""composite"" MDP
is obvious - do what's optimal for each MDP. More typically, choosing an action
for one MDP constrains what actions can be chosen for the others. In a job shop
for example, actions correspond to assignment of resources, and the same physical
resource may not be assigned to more than one job simultaneously.
Formally, we can define a composite MDP as a set of N MDPs {Mi}f. We will use
superscripts to distinguish the component MDPs, e.g., 8i , Ai, pi, and Ri are the
state, action, transition probability and payoff parameters of MDP Mi. The state
space of the composite MDP, 8, is the cross product of the state spaces of the component MDPs, i.e., 8 = 8 1 X 8 2 X ... X 8 N . The constraints on actions implies that

1059

How to Dynamically Merge Markov Decision Processes

the action set of the composite MDP, A, is some proper subset of the cross product
of the N component action spaces. The transition probabilities and the payoffs of
the composite MDP are factorial because the following decompositions hold: for
all s, s' E S and a E A, pa(ss') = nf:lpai (SiSi') and Ra(ss') = l:~l ~i (SiSi').
Singh (1997) has previously studied such factorial MDPs but only for the case of a
fixed set of components.
The optimal value function of a composite MDP is well defined, and satisfies the
following Bellman equation: for all s E S,
N

V(s)

=

~a:L (nf:lpa'(sisi')[LRa\sisi')+'YV(s')]).
~ES

(1)

i=l

Note that the Bellman equation for a composite MDP assumes an identical discount
factor across component MDPs and is not defined otherwise.
1.4

The Dynamic Merging Problem

Given a composite MDP, and the optimal solution (e.g. the optimal value function)
for each of its component MDPs, we would like to efficiently compute the optimal
solution for the composite MDP. More generally, we would like to compute the
optimal composite policy given only bounds on the value functions of the component
MDPs (the motivation for this more general version will become clear in the next
section). To the best of our knowledge, the dynamic merging question has not been
studied before.
Note that the traditional treatment of problems such as job-shop scheduling would
formulate them as nonstationary MDPs (however, see Zhang and Dietterich, 1995
for another learning approach). This normally requires augmenting the state space
to include a ""time"" component which indexes all possible state spaces that could
arise (e.g., Bertsekas, 1995). This is inefficient, and potentially infeasible unless we
know in advance all combinations of possible tasks we will be required to solve. One
contribution of this paper is the observation that this type of nonstationary problem
can be reformulated as one of dynamically merging (individually) stationary MDPs.
1.4.1

The naive greedy policy is suboptimal

Given bounds on the value functions of the component MDPs, one heuristic composite policy is that of selecting actions according to a one-step greedy rule:
N

7I""(s) = argmax(l: nf:,lpa i (Si si')[l:(Rai (si, ai ) + 'YXi(Si'))]),
a

8'

i=l

where Xi is the upper or lower bound of the value function, or the mean of the
bounds. It is fairly easy however, to demonstrate that these policies are substantially
suboptimal in many common situations (see Section 3).

2

Dynamic Merging Algorithm

Consider merging N MDPs; job-shop scheduling presents a special case of merging
a new single MDP with an old composite MDP consisting of several factor MDPs.
One obvious approach to finding the optimal composite policy would be to directly
perform value iteration in the composite state and action space. A more efficient
approach would make use of the solutions (bounds on optimal value functions) of
the existing components; below we describe an algorithm for doing this.

S. SinghandD. Cohn

1060

Our algorithm will assume that we know the optimal values, or more generally,
upper and lower bounds to the optimal values of the states in each component
MDP. We use the symbols Land U for the lower and upper bounds; if the optimal
value function for the ith factor MDP is available then Li = U i = V?,i.l
Our algorithm uses the bounds for the component MDPs to compute bounds on
the values of composite states as needed and then incrementally updates and narrows these initial bounds using a form of value iteration that allows pruning of
actions that are not competitive, that is, actions whose bounded values are strictly
dominated by the bounded value of some other action.

Initial State: The initial composite state So is composed from the start state of
all the factor MOPs. In practice (e.g. in job-shop scheduling) the initial composite
state is composed of the start state of the new job and whatever the current state
of the set of old jobs is. Our algorithm exploits the initial state by only updating
states that can occur from the initial state under competitive actions.
Initial Value Step: When we need the value of a composite state S for the first
time. we compute upper and lower bounds to its optimal value as follows: L(s) =
max!1 Li(Si), and U(s) = E~1 Ui(S).
Initial Update Step: We dynamically allocate upper and lower bound storage
space for composite states as we first update them. We also create the initial set of
competitive actions for S when we first update its value as A(s) = A. As successive
backups narrow the upper and lower bounds of successor states, some actions will
no longer be competitive, and will be eliminated from further consideration.
Modified Value Iteration Algorithm:
At step t if the state to be updated is St:

Lt+l(St)
Ut+l(St)
At+l (St)

=

(L pa(sts')[Ra(st. s') + -yLt(s')])
s
max (L pa(sts')[Ra(st, s') + -yUt(s')])
Ua E At(st) AND L pa(sts')[Ra(st, s') + -yUt(s')]
s'
;::: argmax L pb(sts')[Rb(st, s') + -yLt(s')]
max

aEAt{st}

aEAt(st}

J

s'

bEAt(St)

St+l

{

8'

So if s~ is terminal for all Si E s
s' E S such that 3a E A t+1 (St), pa(StS')

> 0 otherwise

The algorithm terminates when only one competitive action remains for each state,
or when the range of all competitive actions for any state are bounded by an indifference parameter ?.
To elaborate, the upper and lower bounds on the value of a composite state are
backed up using a form of Equation 1. The set of actions that are considered
competitive in that state are culled by eliminating any action whose bounded values
is strictly dominated by the bounded value of some other action in At(st). The
next state to be updated is chosen randomly from all the states that have non-zero
1 Recall that unsuperscripted quantities refer to the composite MDP while superscripted
quantities refer to component MDPs. Also, A is the set of actions that are available to the
composite MDP after taking into account the constraints on picking actions simultaneously
for the factor MDPs.

How to Dynamically Merge Markov Decision Processes

1061

pro babili ty of occuring from any action in At+! (St) or, if St is the terminal state of
all component MDPs, then StH is the start state again.
A significant advantage of using these bounds is that we can prune actions whose
upper bounds are worse than the best lower bound. Only states resulting from
remaining competitive actions are backed up. When only one competitive action
remains, the optimal policy for that state is known, regardless of whether its upper
and lower bounds have converged.
Another important aspect of our algorithm is that it focuses the backups on states
that are reachable on currently competitive actions from the start state. The combined effect of only updating states that are reachable from the start state and
further only those that are reachable under currently competitive actions can lead
to significant computational savings. This is particularly critical in scheduling,
where jobs proceed in a more or less feedforward fashion and the composite start
state when a new job comes in can eliminate a large portion of the composite state
space. Ideas based on Kaelbling's (1990) interval-estimation algorithm and Moore
& Atkeson's (1993) prioritized sweeping algorithm could also be combined into our
algorithm.
The algorithm has a number of desirable ""anytime"" characteristics: if we have to
pick an action in state So before the algorithm has converged (while multiple competitive actions remain), we pick the action with the highest lower bound. If a new
MDP arrives before the algorithm converges, it can be accommodated dynamically
using whatever lower and upper bounds exist at the time it arrives.
2.1

Theoretical Analysis

In this section we analyze various aspects of our algorithm.
UpperBound Calculation: For any composite state, the sum of the optimal
values of the component states is an upper bound to the optimal value of the
composite state, i.e., V*(s = SI, S2, .. . , SN) ~ 2:~1 V*,i(Si).
If there were no constraints among the actions of the factor MDPs then V* (s) would
equal L~l V*,i(Si) because of the additive payoffs across MDPs. The presence of
constraints implies that the sum is an upper bound. Because V*,i(S') ~ Ut(Si) the
result follows.

LowerBound Calculation: For any composite state, the maximum of the optimal
values of the component states is a lower bound to the optimal value of the composite
states, i.e., V*(s = SI, S2, . .. ,SN) ~ max~1 V*,i(Si).
To see this for an arbitrary composite state s, let the MDP that has the largest component optimal value for state s always choose its component-optimal action first
and then assign actions to the other MDPs so as to respect the action constraints
encoded in set A. This guarantees at least the value promised by that MDP because
the payoffs are all non-negative. Because V*,i(Si) ~ Lt(Si) the result follows.
Pruning of Actions: For any composite state, if the upper bound for any composite action, a, is lower than the lower bound for some other composite action,
then action a cannot be optimal - action a can then safely be discarded from the
max in value iteration. Once discarded from the competitive set, an action never
needs to be reconsidered.
Our algorithm maintains the upper and lower bound status of U and L as it updates
them. The result follows.

S. Singh and D. Cohn

1062

Convergence: Given enough time our algorithm converges to the optimal policy
and optimal value function for the set of composite states reachable from the start
state under the optimal policy.
If every state were updated infinitely often, value iteration converges to the optimal

solution for the composite problem independent of the intial guess Vo. The difference
between standard value iteration and our algorithm is that we discard actions and
do not update states not on the path from the start state under the continually
pruned competitive actions. The actions we discard in a state are guaranteed not
to be optimal and therefore cannot have any effect on the value of that state. Also
states that are reachable only under discarded actions are automatically irrelevant
to performing optimally from the start state.

3

An Example: Avoiding Predators and Eating Food

We illustrate the use of the merging algorithm on a simple avoid-predator-andeat-food problem, depicted in Figure 1a. The component MDPs are the avoidpredator task and eat-food task; the composite MDP must solve these problems
simultaneously. In isolation, the tasks avoid-predator and eat-food are fairly easy
to learn. The state space of each task is of size n\ 625 states in the case illustrated.
Using value iteration, the optimal solutions to both component tasks can be learned
in approximately 1000 backups. Directly solving the composite problem requires
n 6 states (15625 in our case), and requires roughly 1 million backups to converge.
Figure 1b compares the performance of several solutions to the avoid-predatorand-eat-food task. The opt-predator and opt-food curves shows the performance
of value iteration on the two component tasks in isolation; both converge qUickly
to their optima. While it requires no further backups, the greedy algorithm of
Section 1.4.1 falls short of optimal performance. Our merging algorithm, when
initialized with solutions for the component tasks (5000 backups each) converges
quickly to the optimal solution. Value iteration directly on the composite state space
also finds the optimal solutions, but requires 4-5 times as many backups. Note that
value iteration in composite state space also updated states on trajectories (as in
Barto etal.'s, 1995 RTDP algorithm) through the state space just as in our merging
algorithm, only without the benefit of the value function bounds and the pruning
of non-competitive actions.

4

Conclusion

The ability to perform multiple decision-making tasks simultaneously, and even
to incorporate new tasks dynamically into ongoing previous tasks, is of obvious
interest to both cognitive science and engineering. Using the framework of MDPs
for individual decision-making tasks, we have reformulated the above problem as
that of dynamically merging MDPs. We have presented a modified value iteration
algorithm for dynamically merging MDPs, proved its convergence, and illustrated
its use on a simple merging task.
As future work we intend to apply our merging algorithm to a real-world jobshop scheduling problem, extend the algorithm into the framework of semi-Markov
decision processes, and explore the performance of the algorithm in the case where
a model of the MDPs is not available.

How to Dynamically Merge Markov Decision Processes

a)

b)
0.80

1063

I
f

0.70

*'~

f

P

Q.

A

& 0.60

j
opt-predator

0.40
0.0

,
I --

- - - - : : - :- . . . . , .- : - - -- - - ' - . , . . . , . . , . --

500000.0

1000000.0

-

---'

1500000.0

Number 01 Backups

Figure 1: a) Our agent (A) roams an n by n grid. It gets a payoff of 0.5 for every time
step it avoids predator (P), and earns a payoff of 1.0 for every piece of food (f) it finds.
The agent moves two steps for every step P makes, and P always moves directly toward
A. When food is found , it reappears at a random location on the next time step. On every
time step, A has a 10% chance of ignoring its policy and making a random move. b) The
mean payoff of different learning strategies vs. number of backups. The bottom two lines
show that when trained on either task in isolation, a learner reaches the optimal payoff for
that task in fewer than 5000 backups. The greedy approach makes no further backups, but
performs well below optimal. The optimal composite solution, trained ab initio, requires
requires nearly 1 million backups. Our algorithm begins with the 5000-backup solutions
for the individual tasks, and converges to the optimum 4-5 times more quickly than the
ab initio solution.

Acknowledgements
Satinder Singh was supported by NSF grant IIS-9711753.

References
Barto, A. G., Bradtke, S. J., & Singh, S. (1995) . Learning to act using real-time dynamic
programming. Artificial Intelligence, 72, 81-138.
Bertsekas, D. P. (1995). Dynamic Programming and Optimal Control. Belmont, MA:
Athena Scientific.
Bertsekas, D. P. & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Belmont, MA:
Athena Scientific.
Kaelbling, L. P. (1990) . Learning in Embedded Systems. PhD thesis, Stanford University,
Department of Computer Science, Stanford, CA. Technical Report TR-90-04.
Moore, A. W . & Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning with
less data and less real time. Machine Learning, 19(1).
Singh , S. (1997). Reinforcement learning in factorial environments. submitted.
Watkins, C. J. C. H. (1989). Learning from Delayed Rewards. PhD thesis, Cambridge
Univ., Cambridge, England.
Zhang, W. & Dietterich, T . G. (1995). High-performance job-shop scheduling with a time
delay TD(lambda) network. In NIPSystems 8. MIT Press.

"
6215,2017,Decoding with Value Networks for Neural Machine Translation,"Neural Machine Translation (NMT) has become a popular technology in recent years, and beam search is its de facto decoding method due to the shrunk search space and reduced computational complexity. However, since it only searches for local optima at each time step through one-step forward looking, it usually cannot output the best target sentence. Inspired by the success and methodology of AlphaGo, in this paper we propose using a prediction network to improve beam search, which takes the source sentence $x$, the currently available decoding output $y_1,\cdots, y_{t-1}$ and a candidate word $w$ at step $t$ as inputs and predicts the long-term value (e.g., BLEU score) of the partial target sentence if it is completed by the NMT model. Following the practice in reinforcement learning, we call this prediction network \emph{value network}. Specifically, we propose a recurrent structure for the value network, and train its parameters from bilingual data. During the test time, when  choosing a word $w$ for decoding, we consider both its conditional probability given by the NMT model and its long-term value predicted by the value network. Experiments show that such an approach can significantly improve the translation accuracy on several translation tasks.","Decoding with Value Networks for Neural Machine
Translation
Di He1
di_he@pku.edu.cn
Tao Qin4
taoqin@microsoft.com

Hanqing Lu2
hanqinglu@cmu.edu
Liwei Wang1,5
wanglw@cis.pku.edu.cn

Yingce Xia3
xiayingc@mail.ustc.edu.cn
Tie-Yan Liu4
tie-yan.liu@microsoft.com

1

Key Laboratory of Machine Perception, MOE, School of EECS, Peking University
2
Carnegie Mellon University 3 University of Science and Technology of China
4
Microsoft Research
5
Center for Data Science, Peking University, Beijing Institute of Big Data Research

Abstract
Neural Machine Translation (NMT) has become a popular technology in recent
years, and beam search is its de facto decoding method due to the shrunk search
space and reduced computational complexity. However, since it only searches
for local optima at each time step through one-step forward looking, it usually
cannot output the best target sentence. Inspired by the success and methodology of
AlphaGo, in this paper we propose using a prediction network to improve beam
search, which takes the source sentence x, the currently available decoding output
y1 , ? ? ? , yt?1 and a candidate word w at step t as inputs and predicts the long-term
value (e.g., BLEU score) of the partial target sentence if it is completed by the NMT
model. Following the practice in reinforcement learning, we call this prediction
network value network. Specifically, we propose a recurrent structure for the value
network, and train its parameters from bilingual data. During the test time, when
choosing a word w for decoding, we consider both its conditional probability
given by the NMT model and its long-term value predicted by the value network.
Experiments show that such an approach can significantly improve the translation
accuracy on several translation tasks.

1

Introduction

Neural Machine Translation (NMT), which is based on deep neural networks and provides an endto-end solution to machine translation, has attracted much attention from the research community
[2, 6, 12, 20] and gradually been adopted by industry in past several years [18, 22]. NMT uses
an RNN-based encoder-decoder framework to model the entire translation process. In training, it
maximizes the likelihood of a target sentence given a source sentence. In testing, given a source
sentence x, it tries to find a sentence y ? in the target language that maximizes the conditional
probability P (y|x). Since the number of possible target sentences is exponentially large, finding the
optimal y ? is NP-hard. Thus beam search is commonly employed to find a reasonably good y.
Beam search is a heuristic search algorithm that maintains the top-scoring partial sequences expanded
in a left-to-right fashion. In particular, it keeps a pool of candidates each of which is a partial sequence.
At each time step, the algorithm expands each candidate by appending a new word, and then keeps
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

the top-ranked new candidates scored by the NMT model. The algorithm terminates if it meets the
maximum decoding depth or all sentences are completely generated, i.e., all sentences are ended with
the end-of-sentence (EOS) symbol.
While NMT with beam search has been proved to be successful, it has several obvious issues,
including exposure bias [9], loss-evaluation mismatch [9] and label bias [16], which have been studied.
However, we observe that there is still an important issue associated with beam search of NMT, the
myopic bias, which unfortunately is largely ignored, to the best of our knowledge. Beam search tends
to focus more on short-term reward. At iteration t, for a candidate y1 , ? ? ? , yt?1 (refers to y<t ) and
two words w and w0 , we denote y<t + w if we append w to y<t . If P (y<t + w|x) > P (y<t + w0 |x),
new candidate y<t + w is more likely to be kept, even if w0 is the ground truth translation at step t or
can offer a better score in future decodings. Such search errors coming from short sighted actions
sometimes provide a bad translation even if the translation model is good.
To address the myopic bias, for each word w and each candidate y<t , we propose to design a
prediction model to estimate the long-term reward if we append w to y<t and follow the current
NMT model until the decoding finishes. Then we can leverage the predicted score from this model
during each decoding step to help find a better w that can contribute to the long-term translation
performance. This prediction model, which predicts long-term reward we will receive in the future, is
exactly the concept of value function in Reinforcement Learning (RL).
In this work, we develop a neural network-based prediction model, which is called value network
for NMT. The value network takes the source sentence and any partial target sequence as input, and
outputs a predicted value to estimate the expected total reward (e.g. BLEU) generated from this
partial sequence by the NMT model. In any decoding step, we select the best candidates not only
based on the conditional probability of the partial sequence outputted by the NMT model, but also
based on the estimated long-term reward outputted by the value network.
The main contributions of this work are summarized as follows. First, we develop a decoding scheme
that considers long-term reward while generating words one by one for machine translation, which is
new in NMT literature. At each step, the new decoding scheme not only considers the probability of
the word sequence conditioned on the source sentence, but also relies on the predicted future reward.
We believe that considering the two aspects can lead to better final translation.
Second, we design a novel structure for the value network. On the top of the encoder-decoder layer
of NMT, we develop another two modules for the value network, a semantic matching module and a
context-coverage module. The semantic matching module aims at estimating the similarity between
the source and target sentences, which can contribute to the quality of the translation. It is often
observed that the more context used in the attention mechanism, the better translation we will generate
[14, 15]. Thus we build a context-coverage module to measure the coverage of context used in the
encoder-decoder layer. With the outputs of the two modules, the value prediction is done via fully
connected layers.
We conduct a set of experiments on several translation tasks. All the results demonstrate the
effectiveness and robustness of the new decoding mechanism compared to several baseline algorithms.
The remaining parts of the paper are organized as follows. In Section 2, we briefly review the
literature of neural machine translation. After that, we describe the myopic bias problem of NMT in
Section 3 and introduce our method for value network learning in Section 4. Experimental results are
provided and analyzed in Section 5. We discuss future directions in the last section.

2

Neural Machine Translation

Neural machine translation systems are typically implemented with a Recurrent Neural Network
(RNN)-based encoder-decoder framework. Such a framework directly models the probability P (y|x)
of a target sentence y = {y1 , y2 , ..., yTy } conditioned on the source sentence x = {x1 , x2 , ..., xTx },
where Tx and Ty are the length of sentence x and y.
The encoder of NMT reads the source sentence x word by word and generates a hidden representation
for each word xi :
hi = f (hi?1 , xi ),
2

(1)

Decoder

y1

y2

y3

r1

r2

r3

c1

c2

c3

Semantic Matching (SM) Module

???

?

Mean pooling

Mean pooling

SM Module

?

c1

Attention

c3

c2

?

r1

r3

?

?

htx

r2

CC Module
Context-Coverage (CC) Module

Encoder

h1

h2

h3

x1

x2

x3

?

???

htx
Mean pooling

xtx

c1

c2

Mean pooling

c3

?

h1

h2

Figure 1: Architecture of Value Network
in which function f is the recurrent unit such as Long Short-Term Memory (LSTM) unit [12] or Gated
Recurrent Unit (GRU) [4]. Afterwards, the decoder of NMT computes the conditional probability of
each target word yt conditioned on its proceeding words y<t as well as the source sentence:
P (yt |y<t , x) ? exp(yt ; rt , ct ),
rt = g(rt?1 , yt?1 , ct ),
ct = q(rt?1 , h1 , ? ? ? , hTx ),

(2)
(3)
(4)

where rt is the decoder RNN hidden representation at step t, similarly computed by an LSTM or GRU,
and ct denotes the weighted contextual information summarizing the source sentence x using some
attention mechanism [4]. Denote all the parameters to be learned in the encoder-decoder framework
as ?. For ease of reference, we also use ?? to represent the translation model with parameter ?.
Denote D as the training dataset that contains source-target sentence pairs. The training process
aims at seeking the optimal parameters ?? to correctly encode source sentence and decode it into the
target sentence. While there are different objectives to achieve this [2, 10, 1, 5, 19, 17], maximum
likelihood estimation is the most popular one [2]:
Y
?? = argmax
P (y|x; ?)
?

=

argmax
?

3

(x,y)?D

Y

Ty
Y

P (yt |y<t , x; ?).

(5)

(x,y)?D t=1

The Myopic Bias

Since during training, one aims to find the conditional probability P (y|x), ideally in testing, the
translation of a source sentence x should be the target sentence y with the maximum conditional
probability P (y|x). However, as there are exponentially many candidates in the target language, one
cannot compute the probability for every candidate and find the maximum one. Thus, beam search is
widely used to find a reasonable good target sentence [4, 14, 12].
Note that the training objective of NMT is usually defined on the full target sentence y instead of
partial sentences. One issue with beam search is that a locally good word might not lead to a good
complete sentence. From the example mentioned in the introduction, we can see that such search
errors from short sighted actions can provide a bad translation even if we hold a perfect translation
model. We call such errors the myopic bias. To reduce myopic bias, we hope to predict the long-term
value of each action and use the value in decoding, which is the exact motivation of our work.
There exist several works weakly related to this issue. [3] develops the scheduled sampling approach,
which takes the generated outputs from the model as well as the golden truth sentence in training,
to help the model learn from its own errors. Although it can (to some extent) handle the negative
3

impact of choosing an incorrect word at middle steps, it still follows beam search during testing,
which cannot avoid the myopic bias. Another related work is [16]. It learns a predictor to predict
the ranking score of a certain word at step t, and use this score to replace the conditional probability
outputted by the NMT model for beam search during testing. Unfortunately, this work still looks
only one step forward and cannot address the problem.

4

Value Network for NMT

As discussed in the previous section, it is not reasonable to fully rely on the conditional probability
in beam search. This motivates us to estimate the expected performance of any sequence during
decoding, which is exactly the concept of value function in reinforcement learning.
4.1

Value Network Structure

In conventional reinforcement learning, a value function describes how much cumulated reward could
be collected from state s by following certain policy ?. In machine translation, we can consider any
input sentence x paired with partial output sentence y<t as the state, and consider the translation
model ?? as policy which can generate a word (action) given any state. Given policy ?? , the value
function characterizes what the expected translation performance (e.g. BLEU score) is if we use ??
to translate x with the first t ? 1 words being y<t . Denote
and y ? (x)
P v(x, y<t ) as the value? function
0
as the ground truth translation, and then v(x, y<t ) = y0 ?Y:y0 =y<t BLEU(y (x), y )P (y 0 |x; ?),
<t
where Y is the space of complete sentences.
The first important problem is how to design the input and the parametric form of the value function.
As the translation model is built up on an encoder-decoder framework, we also build up our value
network on the top of this architecture. To fully exploit the information in the encoder-decoder
framework, we develop a value network with two new modules, the semantic matching module and
the context-coverage module.
Semantic Matching (SM) Module: In the semanticP
matching module, at time step t, we use mean
t
pooling over the decoder RNN hidden states r?t = 1t l=1 rl as a summarization of the partial target
Pt
sentence, and use mean pooling over context states c?t = 1t l=1 cl as a summarization of the context
in source language. We concatenate r?t and c?t , and use a feed-forward network ?SM = fSM ([?
rt , c?t ])
to evaluate semantic information between the source sentence and the target sentence.
Context-Coverage (CC) Module: It is often observed that the more context covered in the attention
model, the better translation we will generate [14, 15]. Thus we build a context-coverage module to
measure the coverage of information used in the encoder-decoder framework. We argue that using
mean pooling over the context layer and the encoding states should give some effective knowledge.
? = 1 PTx hl , we use another feed-forward network ?CC = fCC ([?
? to
Similarly, denote h
ct , h])
l=1
Tx
process such information.
In the end, we concatenate both ?SM and ?CC and then use another fully connected layer with
sigmoid activation function to output a scalar as the value prediction. The whole architecture is shown
in Figure 1.
4.2

Training Data Generation

Based on the designed value network structure, we aim at finding a model that can correctly predict the
performance after the decoding ends. Popular value function learning algorithms include Monte-Carlo
methods and Temple-Difference methods, and both of them have been adopted in many challenging
tasks [13, 7, 11]. In this paper, we adopt the Monte-Carlo method to learn the value function. Given
a well-learnt NMT model ?? , the training of the value network for ?? is shown in Algorithm 1. For
randomly picked source sentence x in the training corpus, we generate a partial target sentence yp
using ?? with random early stop, i.e., we randomly terminate the decoding process before its end.
Then for the pair (x, yp ), we use ?? to finish the translation starting from yp and obtain a set S(yp )
of K complete target sentences, e.g., using beam search. In the end, we compute the BLEU score of
4

each complete target sentence and calculate the averaged BLEU score of (x, yp ):
avg_bleu(x, yp ) =

1
K

X

BLEU(y ? (x), y).

(6)

y?S(yp )

avg_bleu(x, yp ) can be considered as an estimation of the long-term reward of state (x, yp ) used in
value network training.
4.3

Learning

Algorithm 1 Value network training
1: Input: Bilingual corpus, a trained neural machine translation model ?? , hyperparameter K.
2: repeat
3:
t = t + 1.
4:
Randomly pick a source sentence x from the training dataset.
5:
Generate two partial translations yp,1 , yp,2 for x using ?? with random early stop.
6:
Generate K complete translations for each partial translation using ?? and beam search.
Denote this set of complete target sentences as S(yp,1 ) and S(yp,2 ).
7:
Compute the BLEU score for each sentence in S(yp,1 ) and S(yp,2 ).
8:
Calculate the average BLEU score for each partial translation according to Eqn.(6)
9:
Gradient Decent on stochastic loss defined in Eqn.(7).
10: until converge
11: Output: Value network with parameter ?
In conventional Monte-Carlo method for value function estimation, people usually use a regression
model to approximate the value function, i.e., learn a mapping from (x, yp ) ? avg_bleu(x, yp ) by
minimizing the mean square error (MSE). In this paper, we take an alternative objective function
which is shown to be more effective in experiments. We hope the value network we learn is accurate
and useful in differentiating good and bad examples. Thus we use pairwise ranking loss instead of
MSE loss.
To be concrete, we sample two partial sentences yp,1 and yp,2 for each x. We hope the predicted
score of (x, yp,1 ) can be larger than that of (x, yp,2 ) by certain margin if avg_bleu(x, yp,1 ) >
avg_bleu(x, yp,2 ). Denote ? as the parameter of the value function described in Section 4.1. We
design the loss function as follows:
X
L(?) =
ev? (x,yp,2 )?v? (x,yp,1 ) ,
(7)
(x,yp,1 ,yp,2 )

where avg_bleu(x, yp,1 ) > avg_bleu(x, yp,2 ).
Algorithm 2 Beam search with value network in NMT
1: Input: Testing example x, neural machine translation model P (y|x) with target vocabulary V ,
value network model v(x, y), beam search size K, maximum search depth L, weight ?.
2: Set S = ?, U = ? as candidate sets.
3: repeat
4:
t = t + 1.
5:
Uexpand ? {yi + {w}|yi ? U, w ? V }.
6:
U ? {top (K ? |S|) candidates that maximize
? ? 1t log P (y|x) + (1 ? ?) ? log v(x, y)|y ? Uexpand }
7:
Ucomplete ? {y|y ? U, yt = EOS}
8:
U ? U \ Ucomplete
9:
S ? S ? Ucomplete
10: until |S| = K or t = L
1
11: Output: y = argmaxy?S?U ? ? |y|
log P (y|x) + (1 ? ?) ? log v(x, y)

5

4.4

Inference

Since the value network estimates the long-term reward of a state, it will be helpful to enhance the
decoding process of NMT. For example, in a certain decoding step, the NMT model prefers word
w1 over w2 according to the conditional probability, but it does not know that picking w2 will be a
better choice for future decoding. As the value network provides sufficient information on the future
reward, if the value network outputs show that picking w2 is better than picking w1 , we can take both
NMT probability and future reward into consideration to choose a better action.
In this paper, we simply linearly combine the outputs of the NMT model and the value network,
which is motivated by the success of AlphaGo [11]. We first compute the normalized log probability
of each candidate, and then linearly combine it with the logarithmic value of the reward. In detail,
given a translation model P (y|x), a value network v(x, y) and a hyperparameter ? ? (0, 1), the score
of partial sequence y for x is computed by
??

1
log P (y|x) + (1 ? ?) ? log v(x, y),
|y|

(8)

where |y| is the length of y. The details of the decoding process are presented in Algorithm 2, and we
call our neural network-based decoding algorithm NMT-VNN for short.

5

Experiments

5.1

Settings

We compare our proposed NMT-VNN with two baselines. The first one is classic NMT with beam
search [2] (NMT-BS). The second one [16] trains a predictor that can evaluate the quality of any
partial sequence, e.g., partial BLEU score 1 , and then it uses the predictor to select words instead of
the probability. The main difference between [16] and ours is that they predict the local improvement
of BLEU for any single word, while ours aims at predicting the final BLEU score and use the predicted
score to select words. We refer their work as beam search optimization (we call it NMT-BSO). For
NMT-BS, we directly used the open source code [2]. NMT-BSO was implemented by ourselves
based on the open source code [2].
We tested our proposed algorithms and the baselines on three pairs of languages: English?French
(En?Fr), English?German (En?De), and Chinese?English (Zh?En). In detail, we used the same
bilingual corpora from WMT? 14 as used in [2] , which contains 12M, 4.5M and 10M training data
for each task. Following common practices, for En?Fr and En?De, we concatenated newstest2012
and newstest2013 as the validation set, and used newstest2014 as the testing set. For Zh?En, we
used NIST 2006 and NIST 2008 datasets for testing, and use NIST 2004 dataset for validation. For
all datasets in Chinese, we used a public tool for word segmentation. In all experiments, validation
sets were only used for early-stopping and hyperparameter tuning.
For NMT-VNN and NMT-BS, we need to train an NMT model first. We followed [2] to set
experimental parameters to train the NMT model. For each language, we constructed the vocabulary
with the most common 30K words in the parallel corpora, and out-of-vocabulary words were replaced
with a special token ?UNK"". Each word was embedded into a vector space of 620 dimensions, and
the dimension of the recurrent unit was 1000. We removed sentences with more than 50 words from
the training set. Batch size was set as 80 with 20 batches pre-fetched and sorted by sentence lengths.
The NMT model was trained with asynchronized SGD on four K40m GPUs for about seven days.
For NMT-BSO, we implemented the algorithm and the model was trained in the same environment.
For the value network used in NMT-VNN, we set the same parameters for the encoder-decoder
layers as the NMT model. Additionally, in the SM module and CC module, we set function ?SM
and ?CC as single-layer feed forward networks with 1000 output nodes. In Algorithm 1, we set
the hyperparameter K = 20 to estimate the value of any partial sequence. We adapted mini-batch
training with batch size to be 80, and the value network model was trained with AdaDelta [21] on
one K40m GPU for about three days.
If the ground truth is y ? , the partial bleu on the partial sequence y<t at step t is defined as the BLEU score
?
on y<t and y<t
.
1

6

During testing, the hyperparameter ? for NMT-VNN was set by cross validation. For En?Fr, En?De
and Zh?En tasks, we found setting ? to be 0.85, 0.9 and 0.8 respectively are the best choices. We
used the BLEU score [8] as the evaluation metric, which is computed by the multi-bleu.perl script2 .
We set the beam search size to be 12 for all the algorithms following the common practice [12].

(a) En?Fr

(b) En?De

(c) Zh?En NIST 2006

(d) Zh?En NIST 2008

Figure 2: Translation results on the test sets of three tasks
5.2

Overall Results

We report the experimental results in this subsection. From Table 1 we can see that our NMT-VNN
algorithm outperforms the baseline algorithms on all tasks.
For English?French task and English?German task, NMT-VNN outperforms the baseline NMT-BS
by about 1.03/1.3 points. As the only difference between the two algorithms is that our NMT-VNN
additionally uses the outputs of value network to enhance decoding, we can conclude that such
additional knowledge provides useful information to help the NMT model. Our method outperforms
NMT-BSO by about 0.31/0.33 points. Since NMT-BSO only uses a local BLEU predictor to estimate
the partial BLEU score while ours predicts the future performance, our proposed value network which
considers long-term benefit is more powerful.
The performance of our NMT-VNN is much better than NMT-BS and NMT-BSO for
Chinese?English tasks. NMT-VNN outperforms the baseline NMT-BS by about 1.4/1.82 points on
NIST 2006 and NIST 2008, and outperforms NMT-BSO by about 1.01/0.72 points. We plot BLEU
scores with respect to the length of source sentences in Figure 2 for all the tasks. From the figures, we
can see that our NMT-VNN algorithm outperforms the baseline algorithms in almost all the ranges of
length.
Furthermore, we also test our value network on a deep NMT model in which the encoder and decoder
are both stacked 4-layer LSTMs. The result also shows that we can get 0.33 points improvement on
English?French task. These results demonstrate the effectiveness and robustness of our NMT-VNN
algorithm.
2
https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl. For final evaluation we use corpus-level BLEU, while for the value network training we use sentence-level BLEU as in
[1].

7

Table 1: Overall Performance
En?Fr En?De Zh?En NIST06 Zh?En NIST08 En?Fr Deep
NMT-BS 30.51
NMT-BSO 31.23
NMT-VNN 31.54
5.3

15.67
16.64
16.97

36.2
36.59
37.6

29.4
30.5
31.22

37.86
?
38.19

Analysis on Value Network

We further look into the learnt value network and conduct some analysis to better understand it.
First, as we use an additional component during decoding, it will affect the efficiency of the translation process. As the designed value network architecture is similar to the basic NMT model, the
computational complexity is similar to the NMT model and the two processes can be run in parallel.
Second, it has been observed that the accuracy of NMT is sometimes very sensitive to the size of
beam search on certain tasks. As the beam size grows, the accuracy will drop drastically. [14] argues
this is because the training of NMT favors short but inadequate translation candidates. We also
observe this phenomenon on English?German translation. However, we show that by using value
network, such shortage can be largely avoided. We tested the accuracy of our algorithm with different
beam sizes, as shown in Figure 3.(a). It can be seen that NMT-VNN is much more stable than the
original NMT without value network: its accuracy only differs a little for different beam sizes while
NMT-BS drops more than 0.5 point when the beam size is large.

(a)

(b)

Figure 3: (a). BLEU scores of En?De task w.r.t different beam size. (b). BLEU scores of En?De
task w.r.t different hyperparameter ?.
Third, we tested the performances of NMT-VNN using different hyperparameter ? during decoding
for English?German task. As can be seen from the figure, the performance is stable for the ? ranging
from 0.7 to 0.95, and slightly drops for a smaller ?. This shows that our proposed algorithm is robust
to the hyperparameter.

6

Conclusions and Future Work

In this work we developed a new decoding scheme that incorporates value networks for neural
machine translation. By introducing the value network, the new decoding scheme considers not
only the local conditional probability of a candidate word, but also its long-term reward for future
decoding. Experiments on three translation tasks verify the effectiveness of the new scheme. We plan
to explore the following directions in the future. First, it is interesting to investigate how to design
better structures for the value network. Second, the idea of using value networks is quite general, and
we will extend it to other sequence-to-sequence learning tasks, such as image captioning and dialog
systems.
8

Acknowledgments
This work was partially supported by National Basic Research Program of China (973 Program)
(grant no. 2015CB352502), NSFC (61573026). We would like to thank the anonymous reviewers for
their valuable comments on our paper.

References
[1] D. Bahdanau, P. Brakel, K. Xu, A. Goyal, R. Lowe, J. Pineau, A. Courville, and Y. Bengio. An
actor-critic algorithm for sequence prediction. ICLR, 2017.
[2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align
and translate. ICLR, 2015.
[3] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for sequence prediction
with recurrent neural networks. In Advances in Neural Information Processing Systems, pages
1171?1179, 2015.
[4] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and
Y. Bengio. Learning phrase representations using rnn encoder?decoder for statistical machine
translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724?1734, Doha, Qatar, October 2014. Association for
Computational Linguistics.
[5] D. He, Y. Xia, T. Qin, L. Wang, N. Yu, T. Liu, and W.-Y. Ma. Dual learning for machine
translation. In Advances in Neural Information Processing Systems, pages 820?828, 2016.
[6] S. Jean, K. Cho, R. Memisevic, and Y. Bengio. On using very large target vocabulary for
neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for
Computational Linguistics and the 7th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 1?10, Beijing, China, July 2015. Association for
Computational Linguistics.
[7] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,
M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529?533, 2015.
[8] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of the 40th annual meeting on association for computational
linguistics, pages 311?318. Association for Computational Linguistics, 2002.
[9] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural
networks. ICLR, 2016.
[10] S. Shen, Y. Cheng, Z. He, W. He, H. Wu, M. Sun, and Y. Liu. Minimum risk training for neural
machine translation. In ACL, 2016.
[11] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser,
I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural
networks and tree search. Nature, 529(7587):484?489, 2016.
[12] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In
Advances in neural information processing systems, pages 3104?3112, 2014.
[13] G. Tesauro. Td-gammon, a self-teaching backgammon program, achieves master-level play.
Neural computation, 6(2):215?219, 1994.
[14] Z. Tu, Y. Liu, L. Shang, X. Liu, and H. Li. Neural machine translation with reconstruction. In
AAAI, pages 3097?3103, 2017.
[15] Z. Tu, Z. Lu, Y. Liu, X. Liu, and H. Li. Coverage-based neural machine translation. CoRR,
abs/1601.04811, 2016.
9

[16] S. Wiseman and A. M. Rush. Sequence-to-sequence learning as beam-search optimization. In
EMNLP, 2016.
[17] L. Wu, Y. Xia, L. Zhao, F. Tian, T. Qin, J. Lai, and T.-Y. Liu. Adversarial neural machine
translation. arXiv preprint arXiv:1704.06933, 2017.
[18] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao,
K. Macherey, et al. Google?s neural machine translation system: Bridging the gap between
human and machine translation. arXiv preprint arXiv:1609.08144, 2016.
[19] Y. Xia, T. Qin, W. Chen, J. Bian, N. Yu, and T.-Y. Liu. Dual supervised learning. In ICML,
2017.
[20] Y. Xia, F. Tian, L. Wu, J. Lin, T. Qin, N. Yu, and T. Liu. Deliberation networks: Sequence
generation beyond one-pass decoding. In 31st Annual Conference on Neural Information
Processing Systems (NIPS), 2017.
[21] M. D. Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
[22] J. Zhou, Y. Cao, X. Wang, P. Li, and W. Xu. Deep recurrent models with fast-forward
connections for neural machine translation. arXiv preprint arXiv:1606.04199, 2016.

10

"
5947,2016,Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula,"Factorizing low-rank matrices has many applications in machine learning and statistics. For probabilistic models in the Bayes optimal setting, a general expression for the mutual information has been proposed using heuristic statistical physics computations, and proven in few specific cases. Here, we show how to rigorously prove the conjectured formula for the symmetric rank-one case. This allows to express the minimal mean-square-error and to characterize the detectability phase transitions in a large set of estimation problems ranging from community detection to sparse PCA. We also show that for a large set of parameters, an iterative algorithm called approximate message-passing is Bayes optimal. There exists, however, a gap between what currently known polynomial algorithms can do and what is expected information theoretically. Additionally, the proof technique has an interest of its own and exploits three essential ingredients: the interpolation method introduced in statistical physics by Guerra, the analysis of the approximate message-passing algorithm and the theory of spatial coupling and threshold saturation in coding. Our approach is generic and applicable to other open problems in statistical estimation where heuristic statistical physics predictions are available.","Mutual information for symmetric rank-one matrix
estimation: A proof of the replica formula

Jean Barbier, Mohamad Dia and Nicolas Macris
Laboratoire de Th?orie des Communications, Facult? Informatique et Communications,
Ecole Polytechnique F?d?rale de Lausanne, 1015, Suisse.
firstname.lastname@epfl.ch
Florent Krzakala
Laboratoire de Physique Statistique, CNRS, PSL Universit?s et Ecole Normale Sup?rieure,
Sorbonne Universit?s et Universit? Pierre & Marie Curie, 75005, Paris, France.
florent.krzakala@ens.fr
Thibault Lesieur and Lenka Zdeborov?
Institut de Physique Th?orique, CNRS, CEA, Universit? Paris-Saclay, F-91191, Gif-sur-Yvette, France.
lesieur.thibault@gmail.com,lenka.zdeborova@gmail.com

Abstract
Factorizing low-rank matrices has many applications in machine learning and statistics. For probabilistic models in the Bayes optimal setting, a general expression
for the mutual information has been proposed using heuristic statistical physics
computations, and proven in few specific cases. Here, we show how to rigorously
prove the conjectured formula for the symmetric rank-one case. This allows to
express the minimal mean-square-error and to characterize the detectability phase
transitions in a large set of estimation problems ranging from community detection to sparse PCA. We also show that for a large set of parameters, an iterative
algorithm called approximate message-passing is Bayes optimal. There exists,
however, a gap between what currently known polynomial algorithms can do and
what is expected information theoretically. Additionally, the proof technique has
an interest of its own and exploits three essential ingredients: the interpolation
method introduced in statistical physics by Guerra, the analysis of the approximate
message-passing algorithm and the theory of spatial coupling and threshold saturation in coding. Our approach is generic and applicable to other open problems in
statistical estimation where heuristic statistical physics predictions are available.
Consider the following probabilistic rank-one matrix estimation problem: one has access to
noisy observations w = (wij )ni,j=1 of the pair-wise product of the components of a vector
s = (s1 , . . . , sn )| ? Rn with i.i.d components distributed as Si ? P0 , i = 1, . . . , n. The entries
of w are observed
? through a noisy element-wise (possibly non-linear) output probabilistic channel
Pout (wij |si sj / n). The goal is to estimate the vector s from w assuming that both P0 and Pout are
known and independent of n (noise is symmetric so that wij = wji ). Many important problems in
statistics and machine learning can be expressed in this way, such as sparse PCA [1], the Wigner
spiked model [2, 3], community detection [4] or matrix completion [5].
Proving a result initially derived by a heuristic method from statistical physics, we give an explicit
expression for the mutual information (MI) and the information theoretic minimal mean-square-error
(MMSE) in the asymptotic n ? ? limit. Our results imply that for a large region of parameters,
the posterior marginal expectations of the underlying signal components (often assumed intractable
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

to compute) can be obtained in the leading order in n using a polynomial-time algorithm called
approximate message-passing (AMP) [6, 3, 4, 7]. We also demonstrate the existence of a region
where both AMP and spectral methods [8] fail to provide a good answer to the estimation problem,
while it is nevertheless information theoretically possible to do so. We illustrate our theorems with
examples and also briefly discuss the implications in terms of computational complexity.

1

Setting and main results

The additive white Gaussian noise setting: A standard and natural setting
? is the case of additive
?
white Gaussian noise (AWGN) of known variance ?, wij = si sj / n+zij ?, where z = (zij )ni,j=1
is a symmetric matrix with i.i.d entries Zij ? N (0, 1), 1 ? i ? j ? n. Perhaps surprisingly, it turns
out that this Gaussian setting is sufficient to completely characterize all the problems discussed in
the introduction, even if these have more complicated output channels. This is made possible by a
theorem of channel universality [9] (already proven for community detection in [4] and conjectured in
[10]). This theorem states that given an output channel Pout (w|y), such that (s.t) log Pout (w|y = 0) is
three times differentiable
with bounded second and third derivatives, then the MI satisfies I(S; W) =
?
?
| ?
I(S; SS / n+Z ?)+O( n), where ? is the inverse Fisher information (evaluated at y = 0) of
?1 :=
the output channel: ?
EPout (w|0) [(?y log Pout (W |y)|y=0 )2 ]. Informally, this means that we
only have to compute the MI for an AWGN channel to take care of a wide range of problems, which
can be expressed in terms of their Fisher information. In this paper we derive rigorously, for a large
class of signal distributions P0 , an explicit one-letter formula for the MI per variable I(S; W)/n in
the asymptotic limit n ? ?.
Main result: Our central result is a proof of the expression for the asymptotic n ? ? MI per variable
via the so-called replica symmetric (RS) potential iRS (E; ?) defined as
 Z

x2
S
Z
(v ? E)2 + v 2
? 2?(E;?)
2 +x ?(E;?)2 + ?(E;?)
iRS (E; ?) :=
? ES,Z ln
, (1)
dx P0 (x)e
4?
with Z ? N (0, 1), S ? P0 , E[S 2 ] = v and ?(E; ?)2 := ?/(v?E), E ? [0, P
v]. Here we will assume
?
that P0 is a discrete distribution over a finite bounded real alphabet P0 (s) = ?=1 p? ?(s?a? ). Thus
the only continuous integral in (1) is the Gaussian over z. Our results can be extended to mixtures of
discrete and continuous signal distributions at the expense of technical complications in some proofs.
It turns out that both the information theoretic and algorithmic AMP thresholds are determined by the
set of stationary points of (1) (w.r.t E). It is possible to show that for all ? > 0 there always exist at
least one stationary minimum. Note E = 0 is never a stationary point (except for P0 a single Dirac
mass) and E = v is stationary only if E[S] = 0. In this contribution we suppose that at most three
stationary points exist, corresponding to situations with at most one phase transition. We believe that
situations with multiple transitions can also be covered by our techniques.
Theorem 1.1 (RS formula for the mutual information) Fix ? > 0 and let P0 be a discrete distribution s.t (1) has at most three stationary points. Then limn?? I(S; W)/n = minE?[0,v] iRS (E; ?).
The proof of the existence of the limit does not require the above hypothesis on P0 . Also, it was first
shown in [9] that for all n, I(S; W)/n ? minE?[0,v] iRS (E; ?), an inequality that we will use in the
proof section. It is conceptually useful to define the following threshold:
Definition 1.2 (Information theoretic threshold) Define ?Opt as the first non-analyticity point of
the MI as ? increases: ?Opt := sup{?| limn?? I(S; W)/n is analytic in ]0, ?[}.
When P0 is s.t (1) has at most three stationary points, as discussed below, then minE?[0,v] iRS (E; ?)
has at most one non-analyticity point denoted ?RS (if minE?[0,v] iRS (E; ?) is analytic over all R+
we set ?RS = ?). Theorem 1.1 gives us a mean to compute the information theoretic threshold
?Opt = ?RS . A basic application of theorem 1.1 is the expression of the MMSE:
Corollary 1.3 (Exact formula for the MMSE) For all ? 6= ?RS , the matrix-MMSE Mmmsen :=
ES,W [kSS| ? E[XX| |W]k2F ]/n2 (k ? kF being the Frobenius norm) is asymptotically
limn?? Mmmsen (??1 ) = v 2 ?(v ?argminE?[0,v] iRS (E; ?))2 . Moreover, if ? < ?AMP (where
?AMP is the algorithmic threshold, see definition 1.4) or ? > ?RS , then the usual vector-MMSE
Vmmsen := ES,W [kS?E[X|W]k22 ]/n satisfies limn?? Vmmsen = argminE?[0,v] iRS (E; ?).
2

It is natural to conjecture that the vector-MMSE is given by argminE?[0,v] iRS (E; ?) for all ? 6= ?RS ,
but our proof does not quite yield the full statement.
A fundamental consequence concerns the performance of the AMP algorithm [6] for estimating s.
AMP has been analysed rigorously in [11, 12, 4] where it is shown that its asymptotic performance
is tracked by state evolution (SE). Let E t := limn?? ES,Z [kS??st k22 ]/n be the asymptotic average
vector-MSE of the AMP estimate ?st at time t. Define mmse(??2 ) := ES,Z [(S ?E[X|S +?Z])2 ] as
the usual scalar mmse function associated to a scalar AWGN channel of noise variance ?2 , with
S ? P0 and Z ? N (0, 1). Then
E t+1 = mmse(?(E t ; ?)?2 ),

E 0 = v,

(2)

is the SE recursion. Monotonicity properties of the mmse function imply that E t is a decreasing
sequence s.t limt?? E t = E ? exists. Note that when E[S] = 0 and v is an unstable fixed point, as
such, SE ?does not start?. While this is not really a problem when one runs AMP in practice, for
analysis purposes one can slightly bias P0 and remove the bias at the end of the proofs.
Definition 1.4 (AMP algorithmic threshold) For ? > 0 small enough, the fixed point equation
corresponding to (2) has a unique solution for all noise values in ]0, ?[. We define ?AMP as the
supremum of all such ?.
Corollary 1.5 (Performance of AMP) In the limit n ? ?, AMP initialized without any knowledge
other than P0 yields upon convergence the asymptotic matrix-MMSE as well as the asymptotic
vector-MMSE iff ? < ?AMP or ? > ?RS , namely E ? = argminE?[0,v] iRS (E; ?).
?AMP can be read off the replica potential (1): by differentiation of (1) one finds a fixed point
equation that corresponds to (2). Thus ?AMP is the smallest solution of ?iRS /?E = ? 2 iRS /?E 2 = 0;
in other words it is the ?first? horizontal inflexion point appearing in iRS (E; ?) when ? increases.
Discussion: With our hypothesis on P0 there are only three possible scenarios: ?AMP < ?RS
(one ?first order? phase transition); ?AMP = ?RS < ? (one ?higher order? phase transition);
?AMP = ?RS = ? (no phase transition). In the sequel we will have in mind the most interesting
case, namely one first order phase transition, where we determine the gap between the algorithmic
AMP and information theoretic performance. The cases of no phase transition or higher order phase
transition, which present no algorithmic gap, are basically covered by the analysis of [3] and follow
as a special case from our proof. The only cases that would require more work are those where P0 is
s.t (1) develops more than three stationary points and more than one phase transition is present.
For ?AMP < ?RS the structure of stationary points of (1) is as follows1 (figure 1). There exist three
branches Egood (?), Eunstable (?) and Ebad (?) s.t: 1) For 0 < ? < ?AMP there is a single stationary
point Egood (?) which is a global minimum; 2) At ?AMP a horizontal inflexion point appears, for
? ? [?AMP , ?RS ] there are three stationary points satisfying Egood (?AMP ) < Eunstable (?AMP ) =
Ebad (?AMP ), Egood (?) < Eunstable (?) < Ebad (?) otherwise, and moreover iRS (Egood ; ?) ?
iRS (Ebad ; ?) with equality only at ?RS ; 3) for ? > ?RS there is at least the stationary point
Ebad (?) which is always the global minimum, i.e. iRS (Ebad ; ?) < iRS (Egood ; ?). (For higher ?
the Egood (?) and Eunstable (?) branches may merge and disappear); 4) Egood (?) is analytic for
? ?]0, ?0 [, ?0 > ?RS , and Ebad (?) is analytic for ? > ?AMP .
We note for further use in the proof section that E ? = Egood (?) for ? < ?AMP and E ? = Ebad (?)
for ? > ?AMP . Definition 1.4 is equivalent to ?AMP = sup{?|E ? = Egood (?)}. Moreover we
will also use that iRS (Egood ; ?) is analytic on ]0, ?0 [, iRS (Ebad ; ?) is analytic on ]?AMP , ?[, and
the only non-analyticity point of minE?[0,v] iRS (E; ?) is at ?RS .
Relation to other works: Explicit single-letter characterization of the MI in the rank-one problem
has attracted a lot of attention recently. Particular cases of theorem 1.1 have been shown rigorously
in a number of situations. A special case when si = ?1 ? Ber(1/2) already appeared in [13] where
an equivalent spin glass model is analysed. Very recently, [9] has generalized the results of [13]
and, notably, obtained a generic matching upper bound. The same formula has been also rigorously
computed following the study of AMP in [3] for spiked models (provided, however, that the signal
was not too sparse) and in [4] for strictly symmetric community detection.
1

We take E[S] 6= 0. Once theorem 1.1 is proven for this case a limiting argument allows to extend it to
E[S] = 0.

3

iRS(E)

0.125
0.12
0.115
0.11
0.105
0.1
0.095

0.086
0.085
0.084
0.083
?=0.0008
0

0.005

0.01

0.082
0.08
0

0.005

0.01
E

0.015

?=0.0012

0.082
0.02

?=0.00125

0.084
iRS(E)

0.015

0.02

0.08
0.078
0.076
0.074
0.072
0.07
0.068
0.066

0

0.005

0.01

0.015

0.02

?=0.0015

0

0.005

0.01
E

0.015

0.02

Figure 1: The replica symmetric potential iRS (E) for four values of ? in the Wigner spiked model. The MI
is min iRS (E) (the black dot, while the black cross corresponds to the local minimum) and the asymptotic
matrix-MMSE is v 2 ?(v?argminE iRS (E))2 , where v = ? in this case with ? = 0.02 as in the inset of figure 2.
From top left to bottom right: (1) For low noise values, here ? = 0.0008 < ?AMP , there exists a unique ?good?
minimum corresponding to the MMSE and AMP is Bayes optimal. (2) As the noise increases, a second local
?bad? minimum appears: this is the situation at ?AMP < ? = 0.0012 < ?RS . (3) For ? = 0.00125 > ?RS , the
?bad? minimum becomes the global one and the MMSE suddenly deteriorates. (4) For larger values of ?, only
the ?bad? minimum exists. AMP can be seen as a naive minimizer of this curve starting from E = v = 0.02. It
reaches the global minimum in situations (1), (3) and (4), but in (2), when ?AMP < ? < ?RS , it is trapped by
the local minimum with large MSE instead of reaching the global one corresponding to the MMSE.

For rank-one symmetric matrix estimation problems, AMP has been introduced by [6], who also
computed the SE formula to analyse its performance, generalizing techniques developed by [11] and
[12]. SE was further studied by [3] and [4]. In [7, 10], the generalization to larger rank was also
considered. The general formula proposed by [10] for the conditional entropy and the MMSE on the
basis of the heuristic cavity method from statistical physics was not demonstrated in full generality.
Worst, all existing proofs could not reach the more interesting regime where a gap between the
algorithmic and information theoretic perfomances appears, leaving a gap with the statistical physics
conjectured formula (and rigorous upper bound from [9]). Our result closes this conjecture and has
interesting non-trivial implications on the computational complexity of these tasks.
Our proof technique combines recent rigorous results in coding theory along the study of capacityachieving spatially coupled codes [14, 15, 16, 17] with other progress, coming from developments
in mathematical physics putting on a rigorous basis predictions of spin glass theory [18]. From
this point of view, the theorem proved in this paper is relevant in a broader context going beyond
low-rank matrix estimation. Hundreds of papers have been published in statistics, machine learning
or information theory using the non-rigorous statistical physics approach. We believe that our result
helps setting a rigorous foundation of a broad line of work. While we focus on rank-one symmetric
matrix estimation, our proof technique is readily extendable to more generic low-rank symmetric
matrix or low-rank symmetric tensor estimation. We also believe that it can be extended to other
problems of interest in machine learning and signal processing, such as generalized linear regression,
features/dictionary learning, compressed sensing or multi-layer neural networks.

2

Two examples: Wigner spiked model and community detection

In order to illustrate the consequences of our results we shall present two examples.
Wigner spiked model: In this model, the vector s is a Bernoulli random vector, Si ? Ber(?). For
large enough densities (i.e. ? > 0.041(1)), [3] computed the matrix-MMSE and proved that AMP is a
computationally efficient algorithm that asymptotically achieves the matrix-MMSE for any value of
the noise ?. Our results allow to close the gap left open by [3]: on one hand we now obtain rigorously
the MMSE for ? ? 0.041(1), and on the other one we observe that for such values of ?, and as ?
decreases, there is a small region where two local minima coexist in iRS (E; ?). In particular for
?AMP < ? < ?Opt = ?RS the global minimum corresponding to the MMSE differs from the local
one that traps AMP, and a computational gap appears (see figure 1). While the region where AMP
is Bayes optimal is quite large, the region where is it not, however, is perhaps the most interesting
one. While this is by no means evident, statistical physics analogies with physical phase transitions
in nature suggest that this region should be hard for a very broad class of algorithms. For small ? our
4

Wigner Spike model
0.005

Asymmetric Community Detection

matrix-MSE(?) at ?=0.02
0.0004

0.004

MMSE
AMP
?AMP

0.003

0.0003
?opt

0.0002
0.0001

?
0.002

0

0.001

?

0
0.002

0.001

?AMP
?Opt
?spectral

0
0

0.01

0.02

0.03

0.04

0.05

3
2.8
2.6
2.4
2.2
2
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0

matrix-MSE(?) at ?=0.05
1
0.8
MMSE
0.6 AMP
0.4
?AMP
0.2
0
0

0.5

1

1.5

2

2.5

?AMP
?Opt
?spectral
0

?

?opt

0.1

0.2

0.3

0.4

0.5

?

Figure 2: Phase diagram in the noise variance ? versus density ? plane for the rank-one spiked Wigner model
(left) and the asymmetric community detection (right). Left: [3] proved that AMP achieves the matrix-MMSE
for all ? as long as ? > 0.041(1). Here we show that AMP is actually achieving the optimal reconstruction in
the whole phase diagram except in the small region between the blue and red lines. Notice the large gap with
spectral methods (dashed black line). Inset: matrix-MMSE (blue) at ? = 0.02 as a function of ?. AMP (dashed
red) provably achieves the matrix-MMSE except in the region ?AMP < ? < ?Opt = ?RS . We conjecture
that no polynomial-time algorithm will do better than p
AMP in this region. Right: Asymmetric community
detection problem with two communities. For ? > 1/2? 1/12 (black point) and when ? > 1, it is information
theoretically impossible to find any overlap with the true communities and the matrix-MMSE is 1, while it
becomes possible for ? < 1. In this region, AMP is always achieving the matrix-MMSEp
and spectral methods
can find a non-trivial overlap with the truth as well, starting from ? < 1. For ? < 1/2? 1/12, however, it is
information theoretically possible to find an overlap with the hidden communities for ? > 1 (below the blue line)
but both AMP and spectral methods miss this information. Inset: matrix-MMSE (blue) at ? = 0.05 as a function
of ?. AMP (dashed red) again provably achieves the matrix-MMSE except in the region ?AMP < ? < ?Opt .

results are consistent with the known optimal and algorithmic thresholds predicted in sparse PCA
[19, 20], that treats the case of sub-extensive ? = O(1) values. Another interesting line of work for
such probabilistic models appeared in the context of random matrix theory (see [8] and references
therein) and predicts that a sharp phase transition occurs at a critical value of the noise ?spectral = ?2
below which an outlier eigenvalue (and its principal eigenvector) has a positive correlation with the
hidden signal. For larger noise values the spectral distribution of the observation is indistinguishable
from that of the pure random noise.
Asymmetric balanced community detection: We now consider the problem of detecting two communities (groups) with different sizes ?n and (1 ? ?)n, that generalizes the one considered in
[4]. One is given ?
a graph where the probability to have a link between?
nodes in the first group
is p + ?(1 ? ?)/(? n), between those in the
? second group is p + ??/( n(1 ? ?)), while interconnections appear with probability p ? ?/ n. With this peculiar ?balanced? setting, the nodes
in each group have the same degree distribution with mean pn, making them harder to distinguish. According to the universality property described in the first section, this is equivalent to a
2
model with AWGN
according to
p of variance ? = p(1p? p)/? where each variable si is chosen
2
P0 (s) = ??(s? (1??)/?)+(1??)?(s+ ?/(1??)). Our
results
for
this
problem
are
summarized
p
on the right hand side of figure 2. For ? > ?c = 1/2 ? 1/12 (black point), it is asymptotically
information theoretically possible to get an estimation better than chance if and only if ? < 1. When
? < ?c , however, it becomes possible for much larger values of the noise. Interestingly, AMP and
spectral methods have the same transition and can find a positive correlation with the hidden communities for ? < 1, regardless of the value of ?. Again, a region [?AMP , ?Opt = ?RS ] exists where a
computational gap appears when ? < ?c . One can investigate the very low ? regime where we find
that the information theoretic transition goes as ?Opt (? ? 0) = 1/(4?| log ?|). Now if we assume
that this result
? stays true even for ? = O(1) (which is a speculation at this point), we can choose
? ? (1?p)? n such that the small group is a clique. Then the problem corresponds to a ?balanced?
version of the famous planted clique problem [21]. We find that the AMP/spectral approach finds the
2

Note that here since E = v = 1 is an extremum of iRS (E; ?), one must introduce a small bias in P0 and let
it then tend to zero at the end of the proofs.

5

p
hidden clique when it is larger than np/(1?p), while the information theoretic transition translates
into size of the clique 4p log(n)/(1?p). This is indeed reminiscent of the more
pclassical planted
clique
problem
at
p
=
1/2
with
its
gap
between
log(n)
(information
theoretic),
n/e (AMP [22])
?
and n (spectral [21]). Since in our balanced case the spectral and AMP limits match, this suggests
that the small gain of AMP in the standard clique problem is simply due to the information provided
by the distribution of local degrees in the two groups (which is absent in our balanced case). We
believe this correspondence strengthens the claim that the AMP gap is actually a fundamental one.

3

Proofs

The crux of our proof rests on an auxiliary ?spatially coupled system?. The hallmark of spatially
coupled models is that one can tune them so that the gap between the algorithmic and information
theoretic limits is eliminated, while at the same time the MI is maintained unchanged for the coupled
and original models. Roughly speaking, this means that it is possible to algorithmically compute
the information theoretic limit of the original model because a suitable algorithm is optimal on
the coupled system. The present spatially coupled construction is similar to the one used for the
coupled Curie-Weiss model [14]. Consider a ring of length L+1 (L even) with blocks positioned at
? ? {0, . . . , L} and coupled to neighboring blocks {??w, . . . , ?+w}. Positions ? are taken modulo
L+1 and the integer w ? {0, . . . , L/2} equals the size of the coupling window. The coupled model is
r
?
???
wi? j? = si? sj?
(3)
+ zi? j? ?,
n
where the index i? ? {1, . . . , n} (resp. j? ) belongs to the block ? (resp. ?) along the ring, ? is an
(L+1)?(L+1) matrix which describes the strength of the coupling between blocks, and Zi? j? ? N (0, 1)
are i.i.d. For the proof to work, the matrix elements have to be chosen appropriately. We assume
that: i) ? is a doubly stochastic matrix; ii) ??? depends on |???|; iii) ??? is not vanishing for
|???| ? w and vanishes for |???| > w; iv) ? is smooth in the sense |??? ???+1? | = O(w?2 ); v)
? has a non-negative Fourier transform. All these conditions can easily be met, the simplest example
being a triangle of base 2w +1 and height 1/(w +1). The construction of the coupled system is
completed by introducing a seed in the ring: we assume perfect knowledge of the signal components
{si? } for ? ? B := {?w?1, . . . , w?1} mod L+1. This seed is what allows to close the gap between
the algorithmic and information theoretic limits and therefore plays a crucial role. Note it can also be
viewed as an ?opening? of the chain with fixed boundary conditions. Our first crucial result states
that the MI Iw,L (S; W) of the coupled and original systems are the same in a suitable limit.
Lemma 3.1 (Equality of mutual informations) For any fixed w the following limits exist and are
equal: limL?? limn?? Iw,L (S; W)/(n(L+1)) = limn?? I(S; W)/n.
An immediate corollary is that non-analyticity points (w.r.t ?) of the MIs are the same
in the coupled and original models.
In particular, defining ?Opt,coup := sup{? |
limL?? limn?? Iw,L (S; W)/(n(L+1)) is analytic in ]0, ?[}, we have ?Opt,coup = ?Opt .
The second crucial result states that the AMP threshold of the spatially coupled system is at least
as good as ?RS . The analysis of AMP applies to the coupled system as well [11, 12] and it can be
shown that the performance of AMP is assessed by SE. Let E?t := limn?? ES,Z [kS? ??st? k22 ]/n be
the asymptotic average vector-MSE of the AMP estimate ?st? at time t for the ?-th ?block? of S. We
associate to each position ? ? {0, . . . , L} an independent scalar system with AWGN of the form
PL
Y = S +?? (E; ?)Z, with ?? (E; ?)2 := ?/(v ? ?=0 ??? E? ) and S ? P0 , Z ? N (0, 1). Taking
into account knowledge of the signal components in B, SE reads:
E?t+1 = mmse(?? (Et ; ?)?2 ), E?0 = v for ? ? {0, . . . , L} \ B, E?t = 0 for ? ? B, t ? 0,

(4)

where the mmse function is defined as in section 1. From the monotonicity of the mmse function we
have E?t+1 ? E?t for all ? ? {0, . . . , L}, a partial order which implies that limt?? Et = E? exists.
This allows to define an algorithmic threshold for the coupled system: ?AMP,w,L := sup{?|E?? ?
Egood (?) ? ?}. We show (equality holds but is not directly needed):
Lemma 3.2 (Threshold saturation) Let ?AMP,coup := lim inf w?? lim inf L?? ?AMP,w,L . We
have ?AMP,coup ? ?RS .
6

Proof sketch of theorem 1.1: First we prove the RS formula for ? ? ?Opt . It is known [3] that the
matrix-MSE of AMP when n ? ? is equal to v 2 ?(v?E t )2 . This cannot improve the matrix-MMSE,
hence (v 2 ?(v ?E ? )2 )/4 ? lim supn?? Mmmsen /4. For ? ? ?AMP we have E ? = Egood (?)
which is the global minimum of (1) so the left hand side of the last inequality equals the derivative of
minE?[0,v] iRS (E; ?) w.r.t ??1 . Thus using the matrix version of the I-MMSE relation [23] we get
d
1 dI(S; W)
min iRS (E; ?) ? lim sup
.
(5)
?1
d??1 E?[0,v]
n?? n d?
Integrating this relation on [0, ?] ? [0, ?AMP ] and checking that minE?[0,v] iRS (E; 0) = H(S)
(the Shannon entropy of P0 ) we obtain minE?[0,v] iRS (E; ?) ? lim inf n?? I(S; W)/n. But we
know I(S; W)/n ? minE?[0,v] iRS (E; ?) [9], thus we already get theorem 1.1 for ? ? ?AMP . We
notice that ?AMP ? ?Opt . While this might seem intuitively clear, it follows from ?RS ? ?AMP
(by their definitions) which together with ?AMP > ?Opt would imply from theorem 1.1 that
limn?? I(S; W)/n is analytic at ?Opt , a contradiction. The next step is to extend theorem 1.1 to
the range [?AMP , ?Opt ]. Suppose for a moment ?RS ? ?Opt . Then both functions on each side of
the RS formula are analytic on the whole range ]0, ?Opt [ and since they are equal for ? ? ?AMP ,
they must be equal on their whole analyticity range and by continuity, they must also be equal at
?Opt (that the functions are continuous follows from independent arguments on the existence of
the n ? ? limit of concave functions). It remains to show that ?RS ? ]?AMP , ?Opt [ is impossible.
We proceed by contradiction, so suppose this is true. Then both functions on each side of the RS
formula are analytic on ]0, ?RS [ and since they are equal for ]0, ?AMP [?]0, ?RS [ they must be equal
on the whole range ]0, ?RS [ and also at ?RS by continuity. For ? > ?RS the fixed point of SE is
E ? = Ebad (?) which is also the global minimum of iRS (E; ?), hence (5) is verified. Integrating
this inequality on ]?RS , ?[?]?RS , ?Opt [ and using I(S; W)/n ? minE?[0,v] iRS (E; ?) again, we
find that the RS formula holds for all ? ? [0, ?Opt ]. But this implies that minE?[0,v] iRS (E; ?) is
analytic at ?RS , a contradiction.
We now prove the RS formula for ? ? ?Opt . Note that the previous arguments showed that necessarily
?Opt ? ?RS . Thus by lemmas 3.1 and 3.2 (and the sub-optimality of AMP as shown as before) we
obtain ?RS ? ?AMP,coup ? ?Opt,coup = ?Opt ? ?RS . This shows that ?Opt = ?RS (this is the
point where spatial coupling came in the game and we do not know of other means to prove such
an equality). For ? > ?RS we have E ? = Ebad (?) which is the global minimum of iRS (E; ?).
Therefore we again have (5) in this range and the proof can be completed by using once more the
integration argument, this time over the range [?RS , ?] = [?Opt , ?].
Proof sketch of corollaries 1.3 and 1.5: Let E? (?) = argminE iRS (E; ?) for ? 6= ?RS . By explicit
calculation one checks that diRS (E? , ?)/d??1 = (v 2 ?(v?E? (?))2 )/4, so from theorem 1.1 and
the matrix form of the I-MMSE relation we find Mmmsen ? v 2 ?(v?E? (?))2 as n ? ? which is
the first part of the statement of corollary 1.3. Let us now turn to corollary 1.5. For n ? ? the vectorMSE of the AMP estimator at time t equals E t , and since the fixed point equation corresponding to
SE is precisely the stationarity equation for iRS (E; ?), we conclude that for ? ?
/ [?AMP , ?RS ] we
must have E ? = E? (?). It remains to prove that E? (?) = limn?? Vmmsen (?) at least for ? ?
/
[?AMP , ?RS ] (we believe this is in fact true for all ?). This will settle the second part of corollary 1.3
as well as 1.5. Using (Nishimori) identities ES,W [Si Sj E[Xi Xj |W]] = ES,W [E[Xi Xj |W]2 ] (see e.g.
[9]) and using the law of large numbers we can show limn?? Mmmsen ? limn?? (v 2 ? (v ?
Vmmsen (?))2 ). Concentration techniques similar to [13] suggest that the equality in fact holds
(for ? 6= ?RS ) but there are technicalities that prevent us from completing the proof of equality.
However it is interesting to note that this equality would imply E? (?) = limn?? Vmmsen (?) for
all ? 6= ?RS . Nevertheless, another argument can be used when AMP is optimal. On one hand the
right hand side of the inequality is necessarily smaller than v 2 ?(v?E ? )2 . On the other hand the left
hand side of the inequality is equal to v 2?(v?E? (?))2 . Since E? (?) = E ? when ? ?
/ [?AMP , ?RS ],
we can conclude limn?? Vmmsen (?) = argminE iRS (E; ?) for this range of ?.
Proof sketch of lemma 3.1: Here we prove the lemma for a ring that is not seeded. An easy argument
shows that a seed of size w does not change the MI per variable when L ? ?. The statistical physics
formulation is convenient: up to the trivial additive
term n(L+1)v 2 /4, the MI Iw,L (S; W) equals the
R
:=
free energy ?ES,Z [ln Zw,L ], where Zw,L
dxP0 (x) exp(?H(x, z, ?)) and
?+w
L 

X
X
X
X
1
H(x, z, ?) =
???
Ai? j? (x, z, ?) +
???
Ai? j? (x, z, ?) ,
(6)
? ?=0
?=?+1
i ,j
i? ?j?

?

7

?

?
p
with Ai? j? (x, z, ?) := (x2i? x2j? )/(2n)?(si? sj? xi? xj? )/n?(xi? xj? zi? j? ?)/ n??? . Consider a
pair of systems with coupling matrices ? and ?0 and i.i.d noize realizations z, z0 , an interpolated
Hamiltonian H(x, z, t?)+H(x, z0 , (1?t)?0 ), t ? [0, 1], and the corresponding partition function Zt .
d
ES,Z,Z0 [ln Zt ] ? 0 for all
The main idea of the proof is to show that for suitable choices of matrices, ? dt
t ? [0, 1] (up to negligible terms), so that by the fundamental theorem of calculus, we get a comparison
between the free energies of H(x, z, ?) and H(x, z0 , ?0 ). Performing the t-derivative brings down a
Gibbs average of a polynomial in all variables si? , xi? , zi? j? and zi0? j? . This expectation over S, Z,
Z0 of this Gibbs average is simplified using integration by parts over the Gaussian noise zi? j? , zi0? j?
and Nishimori identities (see e.g. proof of corollary 1.3 for one of them). This algebra leads to
?

1
d
1
ES,Z,Z0 [ln Zt ] =
ES,Z,Z0 [hq| ?q ? q| ?0 qit ] + O(1/(nL)),
n(L + 1) dt
4?(L + 1)

(7)

where P
h?it is the Gibbs average w.r.t the interpolated Hamiltonian, q is the vector of overlaps
n
q? := i? =1 si? xi? /n. If we can choose matrices s.t ?0 > ?, the difference of quadratic forms
in the Gibbs bracket is negative and we obtain an inequality in the large size limit. We use this
scheme to interpolate between the fully decoupled system w = 0 and the coupled one 1 ? w < L/2
and then between 1 ? w < L/2 and the fully connected system w = L/2. The w = 0 system has
??? = ??? with eigenvalues (1, 1, . . . , 1). For the 1 ? w < L/2 system, we take any stochastic
translation invariant matrix with non-negative discrete Fourier transform (of its rows): such matrices
have an eigenvalue equal to 1 and all others in [0, 1[ (the eigenvalues are precisely equal to the
discrete Fourier transform). For w = L/2 we choose ??? = 1/(L + 1) which is a projector with
eigenvalues (0, 0, . . . , 1). With these choices we deduce that the free energies and MIs are ordered as
Iw=0,L + O(1) ? Iw,L + O(1) ? Iw=L/2,L + O(1). To conclude the proof we divide by n(L+1) and
note that the limits of the leftmost and rightmost MIs are equal, provided the limit exists. Indeed the
leftmost term equals L times I(S; W) and the rightmost term is the same MI for a system of n(L+1)
variables. Existence of the limit follows by subadditivity, proven by a similar interpolation [18].
Proof sketch of lemma 3.2: Fix ? < ?RS . We show that, for w large enough, the coupled SE
recursion (4) must converge to a fixed point E?? ? Egood (?) for all ?. The main intuition behind
the proof is to use a ?potential function? whose ?energy? can be lowered by small perturbation of
a fixed point that would go above Egood (?) [16, 17]. The relevant potential function iw,L (E, ?)
is in fact the replica potential of the coupled system (a generalization of (1)). The stationarity
condition for this potential is precisely (4) (without the seeding condition). Monotonicity properties
of SE ensure that any fixed point has a ?unimodal? shape (and recall that it vanishes for ? ? B =
{0, . . . , w?1} ? {L?w, . . . , L}). Consider a position ?max ? {w, . . . , L?w?1} where it is maximal
and suppose that E??max > Egood (?). We associate to the fixed point E? a so-called saturated
profile Es defined on the whole of Z as follows: E?s = Egood (?) for all ? ? ?? where ?? +1 is the
smallest position s.t E?? > Egood (?); E?s = E?? for ? ? {?? +1, . . . , ?max ?1}; E?s = E??max for all
? ? ?max . We show that Es cannot exist for w large enough. To this end define a shift operator by
s
[S(Es )]? := E??1
. On one hand the shifted profile is a small perturbation of Es which matches a
fixed point, except where it is constant, so if we Taylor expand, the first order vanishes and the second
order and higher orders can be estimated as |iw,L (S(Es ); ?)?iw,L (Es ; ?)| = O(1/w) uniformly in
L. On the other hand, by explicit cancellation of telescopic sums iw,L (S(Es ); ?)?iw,L (Es ; ?) =
iRS (Egood ; ?)?iRS (E??max ; ?). Now one can show from monotonicity properties of SE that if E?
is a non trivial fixed point of the coupled SE then E??max cannot be in the basin of attraction of
Egood (?) for the uncoupled SE recursion. Consequently as can be seen on the plot of iRS (E; ?) (e.g.
figure 1) we must have iRS (E??max ; ?) ? iRS (Ebad ; ?). Therefore iw,L (S(Es ); ?)?iw,L (Es ; ?) ?
?|iRS (Ebad ; ?)?iRS (Egood ; ?)| which is an energy gain independent of w, and for large enough
w we get a contradiction with the previous estimate coming from the Taylor expansion.

Acknowledgments
J.B and M.D acknowledge funding from the SNSF (grant 200021-156672). Part of this research
received funding from the ERC under the EU?s 7th Framework Programme (FP/2007-2013/ERC
Grant Agreement 307087-SPARCS). F.K and L.Z thank the Simons Institute for its hospitality.

8

References
[1] H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. Journal of computational and graphical statistics, 15(2):265?286, 2006.
[2] I.M. Johnstone and A.Y. Lu. On consistency and sparsity for principal components analysis in
high dimensions. Journal of the American Statistical Association, 2012.
[3] Y. Deshpande and A. Montanari. Information-theoretically optimal sparse pca. In IEEE Int.
Symp. on Inf. Theory, pages 2197?2201, 2014.
[4] Y. Deshpande, E. Abbe, and A. Montanari. Asymptotic mutual information for the two-groups
stochastic block model. arXiv:1507.08685, 2015.
[5] E.J. Cand?s and B. Recht. Exact matrix completion via convex optimization. Foundations of
Computational mathematics, 9(6):717?772, 2009.
[6] S. Rangan and A.K. Fletcher. Iterative estimation of constrained rank-one matrices in noise. In
IEEE Int. Symp. on Inf. Theory, pages 1246?1250, 2012.
[7] T. Lesieur, F. Krzakala, and L. Zdeborov?. Phase transitions in sparse pca. In IEEE Int. Symp.
on Inf. Theory, page 1635, 2015.
[8] J. Baik, G. Ben Arous, and S. P?ch?. Phase transition of the largest eigenvalue for nonnull
complex sample covariance matrices. Annals of Probability, page 1643, 2005.
[9] F. Krzakala, J. Xu, and L. Zdeborov?. Mutual information in rank-one matrix estimation.
arXiv:1603.08447, 2016.
[10] T. Lesieur, F. Krzakala, and L. Zdeborov?. Mmse of probabilistic low-rank matrix estimation:
Universality with respect to the output channel. In Annual Allerton Conference, 2015.
[11] M. Bayati and A. Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Trans. on Inf. Theory, 57(2):764 ?785, 2011.
[12] A. Javanmard and A. Montanari. State evolution for general approximate message passing
algorithms, with applications to spatial coupling. J. Infor. & Inference, 2:115, 2013.
[13] S.B. Korada and N. Macris. Exact solution of the gauge symmetric p-spin glass model on a
complete graph. Journal of Statistical Physics, 136(2):205?230, 2009.
[14] S.H. Hassani, N. Macris, and R. Urbanke. Coupled graphical models and their thresholds. In
IEEE Information Theory Workshop (ITW), 2010.
[15] S. Kudekar, T.J. Richardson, and R. Urbanke. Threshold saturation via spatial coupling: Why
convolutional ldpc ensembles perform so well over the bec. IEEE Trans. on Inf. Th., 57, 2011.
[16] A. Yedla, Y.Y. Jian, P.S. Nguyen, and H.D. Pfister. A simple proof of maxwell saturation for
coupled scalar recursions. IEEE Trans. on Inf. Theory, 60(11):6943?6965, 2014.
[17] J. Barbier, M. Dia, and N. Macris. Threshold saturation of spatially coupled sparse superposition
codes for all memoryless channels. CoRR, abs/1603.04591, 2016.
[18] F. Guerra. An introduction to mean field spin glass theory: methods and results. Mathematical
Statistical Physics, pages 243?271, 2005.
[19] A.A. Amini and M.J. Wainwright. High-dimensional analysis of semidefinite relaxations for
sparse principal components. In IEEE Int. Symp. on Inf. Theory, page 2454, 2008.
[20] Q. Berthet and P. Rigollet. Computational lower bounds for sparse pca. arXiv:1304.0828, 2013.
[21] A. d?Aspremont, L. El Ghaoui, M.I. Jordan, and G.RG. Lanckriet. A direct formulation for
sparse pca using semidefinite programming. SIAM review, 49(3):434, 2007.
p
[22] Y. Deshpande and A. Montanari. Finding hidden cliques of size N/e in nearly linear time.
Foundations of Computational Mathematics, 15(4):1069?1128, 2015.
[23] D. Guo, S. Shamai, and S. Verd?. Mutual information and minimum mean-square error in
gaussian channels. IEEE Trans. on Inf. Theory, 51, 2005.
9

"
5031,2014,Message Passing Inference for Large Scale Graphical Models with High Order Potentials,"To keep up with the Big Data challenge, parallelized algorithms based on dual decomposition have been proposed to perform inference in Markov random fields. Despite this parallelization, current algorithms struggle when the energy has high order terms and the graph is densely connected. In this paper we propose a partitioning strategy followed by a message passing algorithm which is able to exploit pre-computations. It only updates the high-order factors when passing messages across machines. We demonstrate the effectiveness of our approach on the task of joint layout and semantic segmentation estimation from single images, and show that our approach is orders of magnitude faster than current methods.","Message Passing Inference for Large Scale Graphical
Models with High Order Potentials

Jian Zhang
ETH Zurich

Alexander G. Schwing
University of Toronto

Raquel Urtasun
University of Toronto

jizhang@ethz.ch

aschwing@cs.toronto.edu

urtasun@cs.toronto.edu

Abstract
To keep up with the Big Data challenge, parallelized algorithms based on dual decomposition have been proposed to perform inference in Markov random fields.
Despite this parallelization, current algorithms struggle when the energy has high
order terms and the graph is densely connected. In this paper we propose a partitioning strategy followed by a message passing algorithm which is able to exploit
pre-computations. It only updates the high-order factors when passing messages
across machines. We demonstrate the effectiveness of our approach on the task of
joint layout and semantic segmentation estimation from single images, and show
that our approach is orders of magnitude faster than current methods.

1

Introduction

Graphical models are a very useful tool to capture the dependencies between the variables of interest. In domains such as computer vision, natural language processing and computational biology
they have been very widely used to solve problems such as semantic segmentation [37], depth reconstruction [21], dependency parsing [4, 25] and protein folding [36].
Despite decades of research, finding the maximum a-posteriori (MAP) assignment or the minimimum energy configuration remains an open problem, as it is NP-hard in general. Notable exceptions
are specialized solvers such as graph-cuts [7, 3] and dynamic programming [19, 1], which retrieve
the global optima for sub-modular energies and tree-shaped graphs. Algorithms based on message
passing [18, 9], a series of graph cut moves [16] or branch-and-bound techniques [5] are common
choices to perform approximate inference in the more general case. A task closely related to MAP
inference but typically harder is computation of the probability for a given configuration. It requires
computing the partition function, which is typically done via message passing [18], sampling or by
repeatedly using MAP inference to solve tasks perturbed via Gumbel distributions [8].
Of particular difficulty is the case where the involved potentials depend on more than two variables,
i.e., they are high-order, or the graph is densely connected. Several techniques have been developed
to allow current algorithms to handle high-order potentials, but they are typically restricted to potentials of a specific form, e.g., a function of the cardinality [17] or piece-wise linear potentials [11, 10].
For densely connected graphs with Gaussian potentials efficient inference methods based on filtering
have been proposed [14, 33].
Alternating minimization approaches, which iterate between solving for subsets of variables have
also been studied [32, 38, 29]. However, most approaches loose their guarantees since related subproblems are solved independently. Another method to improve computational efficiency is to
divide the model into smaller tasks, which are solved in parallel using dual decomposition techniques [13, 20, 22]. Contrasting alternating minimization, convergence properties are ensured.
However, these techniques are computationally expensive despite the division of computation, since
global and dense interactions are still present.
1

In this work we show that for many graphical models it is possible to devise a partitioning strategy followed by a message passing algorithm such that efficiency can be improved significantly.
In particular, our approach adds additional terms to the energy function (i.e., regions to the Hasse
diagram) such that the high-order factors can be pre-computed and remain constant during local
message passing within each machine. As a consequence, high-order factors are only accessed once
before sending messages across machines. This contrasts tightening approaches [27, 28, 2, 26],
where additional regions are added to better approximate the marginal polytope at the cost of additional computations, while we are mainly interested in computational efficiency. In contrast to
re-scheduling strategies [6, 30, 2], our rescheduling is fixed and does not require additional computation.
Our experimental evaluations show that state-of-the-art techniques [9, 22] have difficulties optimizing energy functions that correspond to densely connected graphs with high-order factors. In contrast our approach is able to achieve more than one order of magnitude speed-ups while retrieving
the same solution in the complex task of jointly estimating 3D room layout and image segmentation
from a single RGB-D image.

2

Background: Dual Decomposition for Message Passing

We start by reviewing dual-decomposition approaches for inference in graphical models with highQN
order factors. To this end, we consider distributions defined over a discrete domain S = i=1 Si ,
which is composed of a product of N smaller discrete spaces Si = {1, . . . , |Si |}. We model our distribution to depend log-linearly on a scoring function ?(s) defined over the aforementioned discrete
product space S, i.e., p(s) = Z1 exp ?(s), with Z the partition function. Given the scoring function
?(s) of a configuration s, it is unfortunately generally #P-complete to compute its probability since
the partition function Z is required. Its logarithm equals the following variational program [12]:
X
log Z = max
p(s)?(s) + H(p),
(1)
p??

s

where H denotes the entropy and ? indicates the probability simplex.
The variational program in Eq. (1) is challenging as it operates on the exponentially sized domain
S. However, we can make use of the fact that for many
Prelevant applications the scoring function
?(s) is additively composed of local terms, i.e., ?(s) = r?R ?r (sr ). These local scoring functions
?r depend on a subset of variables sr = (si )i?r , defined on a domain Q
Sr ? S, which is specified by
the restriction often referred to as region r ? {1, . . . , N }, i.e., Sr = i?r Si . We refer to R as the
set of all restriction required to compute the scoring function ?.
P
Locality of the scoring function allows to equivalently rewrite
P
P the expected score via s p(s)?(s) =
r,sr pr (sr )?r (sr ) by employing marginals pr (sr ) =
s\sr p(s). Unfortunately an exact decomposition of the entropy H(p) using marginals is not possible.
P Instead, the entropy is typically
approximated by a weighted sum of local entropies H(p) ?
r cr H(pr ), with cr the counting
numbers. The task remains intractable despite the entropy approximation since the marginals pr (sr )
are required to arise from a valid joint distribution p(s). However, if we require the marginals to be
consistent only locally, we obtain a tractable approximation [34]. We thus introduce local beliefs
br (sr ) to denote the approximation, not to be confused with
P the true marginals pr . The beliefs are
required to fulfill local marginalization constraints, i.e., sp \sr bp (sp ) = br (sr ) ?r, sr , p ? P (r),
where the set P (r) subsumes the set of all parents of region r for which we want marginalization to
hold.
Putting all this together, we obtain the following approximation:
X
X
max
br (sr )?r (sr ) +
cr H(br )
b

r,sr

r


s.t.

?r

br ? C =

br ? ?
br : P
sp \sr bp (sp ) = br (sr ) ?sr , p ? P (r).

(2)

The computation and memory requirements can be too demanding when dealing with large graphical models. To address this issue, [13, 22] showed that this task can be distributed onto multiple
2

Algorithm: Distributed Message Passing Inference
Let a = 1/|M (r)| and repeat until convergence
1. For every ? in parallel: iterate T times over r ? R(?):
?p ? P (r), sr
P
??p (sp ) ?
?p?p0 (sp ) +
?p?r (sr ) = ?
cp ln

X

p0 ?P (p)

exp

P

?r0 ?p (sr0 ) + ???p (sp )

r 0 ?C(p)??\r

(3)

?
cp

sp \sr

?

?
?r?p (sr ) ?

c?r +

c?p
P

c?p

???r (sr ) +

X

?c?r (sc ) + ???r (sr ) +

c?C(r)??

p?P (r)

X

?p?r (sr )?? ?p?r (sr )(4)

p?P (r)

2. Exchange information by iterating once over r ? G ?? ? M (r)
X
X
X
X
???r (sr ) = a
?c?r (sc ) ?
?c?r (sc ) +
?r?p (sr ) ? a
?r?p (sr ) (5)
c?C(r)

c?C(r)??

p?P (r)

??M (r),p?P (r)

Figure 1: A block-coordinate descent algorithm for the distributed inference task.
computers ? by employing dual decomposition techniques. More specifically, the task is partitioned
into multiple independent tasks with constraints at the boundary ensuring consistency of the parts
upon convergence. Hence, an additional constraint is added to make sure that all beliefs b?r that
are assigned to multiple computers, i.e., those at the boundary of the parts, are consistent upon
convergence and equal a single region belief br . The distributed program is then:
X
X
?
?r (sr ) +
max
b
(s
)
?
c?r H(b?r )
r
r
?
br ,br ??

?,r,sr

?,r

??, r ? R? , sr , p ? P (r)
s.t.

??, r ? R? , sr

b?p (sp ) = b?r (sr )
sp \sr
b?r (sr ) = br (sr ),
P

where R? refers to regions on comptuer ?. We uniformly distributed the scores ?r (sr ) and the
counting numbers cr of a region r to all overlapping machines. Thus ??r = ?r /|M (r)| and c?r =
cr /|M (r)| with M (r) the set of machines that are assigned to region r.
Note that this program operates on the regions defined by the energy decomposition. To derive an
efficient algorithm making use of the structure incorporated in the constraints we follow [22] and
change to the dual domain. For the marginalization constraints we introduce Lagrange multipliers
??r?p (sr ) for every computer ?, all regions r ? R? assigned to that computer, all its states sr
and all its parents p. For the consistency constraint we introduce Lagrange multipliers ???r (sr )
for all computers, regions and states. The arrows indicate that the Lagrange multipliers can be
interpreted as messages sent between different nodes in a Hasse diagram with nodes corresponding
to the regions.
The resulting distributed inference algorithm [22] is summarized in Fig. 1. It consists of two parts,
the first of which is a standard message passing on the Hasse-diagram defined locally on each computer ?. The second operation interrupts message passing occasionally to exchange information
between computers. This second task of exchanging messages is often visualized on a graph G with
nodes corresponding to computers and additional vertices denoting shared regions.
Fig. 2(a) depicts a region graph with four unary regions and two high-order ones, i.e., R =
{{1}, {2}, {3}, {4}, {1, 2, 3}, {1, 2, 3, 4}}. We partition this region graph onto two computers
?1 , ?2 as indicated via the dashed rectangles. The graph G containing as nodes both computers
and the shared region is provided as well. The connections between all regions are labeled with the
corresponding message, i.e., ?, ? and ?. We emphasize that the consistency messages ? are only
modified when sending information between computers ?. Investigating the provided example in
Fig. 2(a) more carefully we observe that the computation of ? as defined in Eq. (3) in Fig. 1 involves summing over the state-space of the third-order region {1, 2, 3} and the fourth-order region
{1, 2, 3, 4}. The presence of those high-order regions make dual decomposition approaches [22]
3

? = {1, 2, 3}

??

?? ??

? = {1, 2, 3, 4}

?? ? ?

2 ??

? = {1, 2, 3}

? = {1, 2, 3, 4}

? = {1, 2, 3}

??

1

1

? = {1, 2, 3}

?? ??

2 ??

? = {1, 2, 3, 4}

? = {1, 2, 3}

?? ?1

?2? ?

? ? ?2

?3??

?? ?3

?? ?2

?1? ?

?? ? ?

? ? ?4

?4? ?

?1

{1}

? ? ?3

?2??

?2

{3}

??

? = {1, 2, 3}

?1

{4}

? = {1, 2, 3, 4}

? ? ??

?? ?3
? = {3, 4}

?? ?4

?3??

?? ?2

{1}

2 ??

?? ? ?

? ? ??
?3??

?? ?1
{2}

? = {1, 2, 3, 4}

1

? = {1, 2}

?1??

? ? ?1

?? ? ?

2 ??

? = {1, 2, 3, 4}

?? ??
?? ??

?3? ?
?1??

??

1

?2??

?? ?3
{2}

(a)

?2

{3}

?4??
{4}

(b)

Figure 2: Standard distributed message passing operating on an inference task partitioned to two
computers (left) is compared to the proposed approach (right) where newly introduced regions (yellow) ensure constant messages ? from the high-order regions.
impractical. In the next section we show how message passing algorithms can become orders of
magnitude faster when adding additional regions.

3

Efficient Message Passing for High-order Models

The distributed message passing procedure described in the previous section involves summations
over large state-spaces when computing the messages ?. In this section we derive an approach
that can significantly reduce the computation by adding additional regions and performing messagepassing with a specific message scheduling. Our key observation is that computation can be greatly
reduced if the high-order regions are singly-connected since their outgoing message ? remains constant. Generally, singly-connected high-order regions do not occur in graphical models. However, in
many cases we can use dual decomposition to distribute the computation in a way that the high-order
regions become singly-connected if we introduce additional intermediate regions located between
the high-order regions and the low-order ones (e.g., unary regions).
At first sight, adding regions increases computational complexity since we have to iterate over additional terms. However, we add regions only if they result in constant messages from regions with
even larger state space. By pre-computing those constant messages rather than re-evaluating them at
every iteration, we hence decrease computation time despite augmenting the graph with additional
regions, i.e., additional marginal beliefs br .
Specifically, we observe that there are no marginalization constraints for the singly-connected high? ? : P (r) = ?, |C(r)| = 1}, since their set
order regions, subsumed in the set H? = {r ? R
of parents is empty. An important observation made precise in Claim 1 is that the corresponding
messages ? are constant for high-order regions unless ???r changes. Therefore we can improve the
message passing algorithm discussed in the previous section by introducing additional regions to
increase the size of the set |H? | as much as possible while not changing the cost function. The latter
is ensured by requiring the additional counting numbers and potentials to equal zero. However, we
note that the program will change since the constraint set is augmented.
? ? be the set of all regions, i.e., the regions R? of the original task on computer
More formally, let R
? ? \ R? . Let H? = {r ? R
? ? : P (r) = ?, |C(r)| = 1}
? in addition to the newly added regions r? ? R
be the set of high-order regions on computer ? that are singly connected and have no parent. Further,
? ? \ H? denote all remaining regions. The inference task is given by
let its complement H? = R
X
X
max
b?r (sr )??r (sr ) +
c?r H(b?r )
?
br ,br ??

?,r,sr

?,r

??, r ? H? , sr , p ? P (r)
s.t.

? ? , sr
??, r ? R

b?p (sp ) = b?r (sr )
sp \sr
b?r (sr ) = br (sr ).
P

(9)

? ? \R? ,
Even though we set ?r (sr ) ? 0 for all states sr , and c?r = 0 for all newly added regions r ? R
the inference task is not identical to the original problem since the constraint set is not the same. Note
that new regions introduce new marginalization constraints. Next we show that messages leaving
singly-connected high-order regions are constant.
4

Algorithm: Message Passing for Large Scale Graphical Models with High Order Potentials
Let a = 1/|M (r)| and repeat until convergence
1. For every ? in parallel: Update singly-connected regions p ? H? : let r = C(p) ?sr
P
P
??p (sp ) ?
?p?p0 (sp ) +
?r0 ?p (sr0 ) + ???p (sp )
X
p0 ?P (p)
r 0 ?C(p)??\r
?p?r (sr ) = ?
cp ln
exp
?
cp
sp \sr

? ?:
2. For every ? in parallel: iterate T times over r ? R
?p ? P (r) \ H? , sr
P
??p (sp ) ?
?p?p0 (sp ) +
?p?r (sr ) = ?
cp ln

X

p0 ?P (p)

exp

P

?r0 ?p (sr0 ) + ???p (sp )

r 0 ?C(p)??\r

(6)

?
cp

sp \sr

?p ? P (r), sr
?
?r?p (sr ) ?

c?r +

c?p
P

c?p

p?P (r)

?

???r (sr ) +

X

?c?r (sc ) + ???r (sr ) +

c?C(r)??

X

?p?r (sr )?? ?p?r (sr )(7)

p?P (r)

3. Exchange information by iterating once over r ? G ?? ? M (r)
X
X
X
X
???r (sr ) = a
?c?r (sc ) ?
?c?r (sc ) +
?r?p (sr ) ? a
?r?p (sr ) (8)
c?C(r)

c?C(r)??

p?P (r)

??M (r),p?P (r)

Figure 3: A block-coordinate descent algorithm for the distributed inference task.
Claim 1. During message passing updates defined in Fig. 1 the multiplier ?p?r (sr ) is constant for
singly-connected high-order regions p.
P
Proof: More carefully investigating Eq. (3) which defines ?, it follows that p0 ?P (p) ?p?p0 (sp ) =
0 because P (p) = ? since p is assumed singly-connected. For the same reason we obtain
P
0
0
0
r 0 ?C(p)??\r ?r ?p (sr ) = 0 because r ? C(p) ? ? \ r = ? and ???p (sp ) is constant upon
each exchange of information. Therefore, ?p?r (sr ) is constant irrespective of all other messages
and can be pre-computed upon exchange of information.

We can thus pre-compute the constant messages before performing message passing. Our approach
is summarized in Fig. 3. We now provide its convergence properties in the following claim.
Claim 2. The algorithm outlined in Fig. 3 is guaranteed to converge to the global optimum of the
program given in Eq. (9) for cr > 0 ?r and is guaranteed to converge in case cr ? 0 ?r.
Proof: The message passing algorithm is derived as a block-coordinate descent algorithm in the
dual domain. Hence it inherits the properties of block-coordinate descent algorithms [31] which are
guaranteed to converge to a single global optimum in case of strict concavity (cr > 0 ?r) and which
are guaranteed to converge in case of concavity only (cr ? 0 ?r), which proves the claim.

We note that Claim 1 nicely illustrates the benefits of working with region graphs rather than factor
graphs. A bi-partite factor graph contains variable nodes connected to possibly high-order factors.
Assume that we distributed the task at hand such that every high-order region of size larger than two
is connected to at most two local variables. By adding a pairwise region in between the original
high-order factor node and the variable nodes we are able to reduce computational complexity since
the high-order factors are now singly connected. Therefore, we can guarantee that the complexity of
the local message-passing steps run in each machine reduces from the state-space size of the largest
factor to the size of the largest newly introduced region in each computer. This is summarized in the
following claim.
Claim 3. Assume we are given a high-order factor-graph representation of a graphical model. By
distributing the model onto multiple computers and by introducing additional regions we reduce the
complexity of the message passing iterations on every computer generally dominated by the state5

r3
vp0 y1

r4
vp2

y2

r1
vp0 y1

r3

r4
vp2

y2

y1

r2
y3
vp1

r2

y4

(a) Layout parameterization.

Compatibility Network

r1

y3
vp1

y3

y2
y4

y4

l1

l5

l3

Layout Network

(b) Compatibility.

l2
l4

Segmentation Network

(c) Joint model.

Figure 4: Parameterization of the layout task is visualized in (a). Compatibility of a superpixel
labeling with a wall parameterization using third-order functions is outlined in (b) and the graphical
model for the joint layout-segmentation task is depicted in (c).
rel. duality gap
Ours [s]
cBP [s]
dcBP [s]

1
0.1
0.01
rel. duality gap
1
0.1
0.01
0.78
5.92
51.59
Ours [s]
15.58 448.26 1150.1
31.60 986.54 1736.6
cBP [s]
411.81 4357.9 4479.9
19.48 1042.8 1772.6
dcBP [s]
451.71 4506.6 4585.3
=0
=1
Table 1: Average time to achieve the specified relative duality gap for  = 0 (left) and  = 1 (right).
space size of the largest region smax = maxr?R? |Sr | from O(smax ) to O(s0max ) with s0max =
maxr?R? ? |Sr?H? |.
Proof: The complexity of standard message passing on a region graph is linear on the largest statespace region, i.e., O(smax ). Since some operations can be pre-computed as per Claim 1 we emphasize that the largest newly introduced region on computer ? is of state-space size s0max which
concludes the proof.

Claim 3 indicates that distributing computation in addition to message rescheduling is a powerful
tool to cope with high-order potentials. To gain some insight, we illustrate our idea with a specific
example. Suppose we distribute the inference computation on two computers ?1 , ?2 as shown in
? regions, i.e., we introduce additional regions r? ? R
? \ R. The
Fig. 2(a). We compare it to a task on R
messages required in the augmented task are visualized in Fig. 2(b). Each computer (box highlighted
with dashed lines) is assigned a task specified by the contained region graph. As before we also
visualize the messages ? occasionally sent between the computers in a graph containing as nodes
the shared factors and the computers (boxes drawn with dashed lines). The algorithm proceeds by
passing messages ?, ? on each computer independently for T rounds. Afterwards messages ? are
exchanged between computers. Importantly, we note that messages for singly-connected high-order
regions within dashed boxes are only required to be computed once upon exchanging message ?.
This is the case for all high-order regions in Fig. 2(b) and for no high-order region in Fig. 2(a),
highlighting the obtained computational benefits.

4

Experimental Evaluation

We demonstrate the effectiveness of our approach in the task of jointly estimating the layout and
semantic labels of indoor scenes from a single RGB-D image. We use the dataset of [38], which is
a subset of the NYU v2 dataset [24]. Following [38], we utilize 202 images for training and 101 for
testing. Given the vanishing points (points where parallel lines meet at infinity), the layout task can
be formulated with four random variables s1 , . . . , s4 , each of which corresponds to angles for rays
originating from two distinct vanishing points [15]. We discretize each ray into |Si | = 25 states. To
define the segmentation task, we partition each image into super pixels. We then define a random
variable with six states for each super pixel si ? Si = {left, front, right, ceiling, floor, clutter} with
i > 4. We refer the reader to Fig. 4(a) and Fig. 4(b) for an illustration of the parameterization of the
problem. The graphical model for the joint problem is depicted in Fig. 4(c).
The score of the joint model is given by a sum of scores
?(s) = ?lay (s1 , . . . , s4 ) + ?label (s5 , . . . , sM +4 ) + ?comp (s),
where ?lay is defined as the sum of scores over the layout faces, which can be decomposed into a
sum of pairwise functions using integral geometry [23]. The labeling score ?label contains unary
6

2.5

5

ours primal
cBP primal
dcBP c = 1 primal
dcBP c = 2 primal
ours dual
cBP dual
dcBP c = 1 dual
dcBP c = 2 dual

2

1.5

ours primal
cBP primal
dcBP c = 1 primal
dcBP c = 2 primal
ours dual
cBP dual
dcBP c = 1 dual
dcBP c = 2 dual

4

3

2

1

1

0

?1

0.5
?2

0

0

1

10

2

10

3

10

?3

1

10

2

10

(normalized primal/dual  = 0)

3

10

10

(normalized primal/dual  = 1)
1

1

0.9
0.95

0.8
0.9

0.7
0.6

0.85

0.5
0.8

0.4
0.3

0.75

ours agreement
cBP agreement
dcBP c = 1 agreement
dcBP c = 2 agreement

0.7

0.65

0

100

200

300

400

500

600

700

800

900

ours agreement
cBP agreement
dcBP c = 1 agreement
dcBP c = 2 agreement

0.2
0.1

1000

0

0

500

1000

1500

2000

2500

3000

(factor agreement  = 0)
(factor agreement  = 1)
Figure 5: Average normalized primal/dual and factor agreement for  = 1 and  = 0.
potentials and pairwise regularization between neighboring superpixels. The third function, ?comp ,
couples the two tasks and encourages the layout and the segmentation to agree in their labels, e.g., a
superpixel on the left wall of the layout is more likely to be assigned the left-wall or the object label.
The compatibility
score decomposes into a sum of fifth-order scores, one for each superpixel, i.e.,
P
?comp (s) = i>4 ?comp,i (s1 , . . . , s4 , si ). Using integral geometry [23], we can further decompose
each superpixel score ?comp,i into a sum of third-order energies. As illustrated in Fig. 4(c), every
superpixel variable si , i > 4 is therefore linked to 4-choose-2 third order functions of state-space
size 6 ? 252 . These functions measure the overlap of each superpixel with a region specified by two
layout ray angles si , sj with i, j ? {1, . . . , 4}, i 6= j. This is illustrated in Fig. 4(b) for the area
highlighted in purple and the blue region defined by s2 and s3 . Since a typical image has around
250 superpixels, there are approximately 1000 third-order factors.
Following Claim 3 we recognize that the third-order functions are connected to at most two variables if we distribute the inference such that the layout task is assigned to one computer while the
segmentation task is divided onto other machines. Importantly, this corresponds to a roughly equal
split of the problem when using our approach, since all tasks are pairwise and the state-space of
the layout task is higher than the one of the semantic-segmentation. Despite the third-order regions
involved in the original model, every local inference task contains at most pairwise factors.
We use convex BP [35, 18, 9] and distributed convex BP [22] as baselines. For our method, we assign
layout nodes to the first machine and segmentation nodes to the second one. Without introducing
additional regions and pre-computations the workload of this split is highly unbalanced. This makes
distributed convex BP even slower than convex BP since many messages are exchanged over the
network. To be more fair to distributed convex BP, we split the nodes into two parts, each with 2
layout variables and half of the segmentation variables. For all experiments, we set cr = 1 and
evaluate the settings  = 1 and  = 0. For a fair comparison we employ a single core for our
approach and convex BP and two cores for distributed convex BP. Note that our approach can be run
in parallel to achieve even faster convergence.
We compare our method to the baselines using two metrics: Normalized primal/dual is a rescaled
version of the original primal and dual normalized by the absolute value of the optimal score. This
allow us to compare different images that might have fairly different energies. In case none of
the algorithms converged we normalize all energies using the mean of the maximal primal and the
minimum dual. The second metric is the factor agreement, which is defined as the proportion of
factors that agree with the connected node marginals.
Fig. 5 depicts the normalized primal/dual as well as the factor agreement for  = 0 (i.e., MAP)
and  = 1 (i.e., marginals). We observe that our proposed approach converges significantly faster
7

layout err: 0.90% segmentation err: 4.74%

layout err: 1.15% segmentation err: 5.12%

layout err: 1.75% segmentation err: 3.98%

layout err: 2.36% segmentation err: 4.06%

layout err: 2.38% segmentation err: 3.77%

layout err: 2.88% segmentation err: 6.01%

layout err: 2.89% segmentation err: 3.99%

layout err: 4.20% segmentation err: 3.65%

layout err: 4.79% segmentation err: 4.17%

layout err: 13.97% segmentation err: 32.08% layout err: 25.89% segmentation err: 16.70% layout err: 18.04% segmentation err: 5.34%

Figure 6: Qualitative Result ( = 0) : First column illustrates the inferred layout (blue) and layout
ground truth (red). The second and third columns are estimated and ground truth segmentations respectively. Failure modes are shown in the last row. They are due to bad vanishing point estimation.
than the baselines. We additionally observe that for densely coupled tasks, the performance of
dcBP degrades when exchanging messages every other iteration (yellow curves). Importantly, in
our experiments we never observed any of the other approaches to converge when our approach
did not converge. Tab. 1 depicts the time in seconds required to achieve a certain relative duality
gap. We observe that our proposed approach outperforms all baselines by more than one order of
magnitude. Fig. 6 shows qualitative results for  = 0. Note that our approach manages to accurately
predict layouts and corresponding segmentations. Some failure cases are illustrated in the bottom
row. They are largely due to failures in the vanishing point detection which our approach can not
recover from.

5

Conclusions

We have proposed a partitioning strategy followed by a message passing algorithm which is able to
speed-up significantly dual decomposition methods for parallel inference in Markov random fields
with high-order terms and dense connections. We demonstrate the effectiveness of our approach on
the task of joint layout and semantic segmentation estimation from single images, and show that our
approach is orders of magnitude faster than existing methods. In the future, we plan to investigate
the applicability of our approach to other scene understanding tasks.

References
[1] A. Amini, T. Wymouth, and R. Jain. Using Dynamic Programming for Solving Variational Problems in
Vision. PAMI, 1990.
[2] D. Batra, S. Nowozin, and P. Kohli. Tighter Relaxations for MAP-MRF Inference: A Local Primal-Dual
Gap based Separation Algorithm. In Proc. AISTATS, 2011.
[3] Y. Boykov, O. Veksler, and R. Zabih. Fast Approximate Energy Minimization via Graph Cuts. PAMI,
2001.
[4] M. Collins. Head-Driven Statistical Models for Natural Language Parsing. Computational Linguistics,
2003.
[5] R. Dechter. Reasoning with Probabilistic and Deterministic Graphical Models: Exact Algorithms. Morgan & Claypool, 2013.
[6] G. Elidan, I. McGraw, and D. Koller. Residual belief propagation: Informed scheduling for asynchronous
message passing. In Proc. UAI, 2006.
[7] L. R. Ford and D. R. Fulkerson. Maximal flow through a network. Canadian Journal of Mathematics,
1956.
[8] T. Hazan and T. Jaakkola. On the Partition Function and Random Maximum A-Posteriori Perturbations.
In Proc. ICML, 2012.
[9] T. Hazan and A. Shashua. Norm-Product Belief Propagation: Primal-Dual Message-Passing for LPRelaxation and Approximate-Inference. Trans. Information Theory, 2010.

8

[10] P. Kohli and P. Kumar. Energy Minimization for Linear Envelope MRFs. In Proc. CVPR, 2010.
[11] P. Kohli, L. Ladick`y, and P. H. S. Torr. Robust higher order potentials for enforcing label consistency.
IJCV, 2009.
[12] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT Press,
2009.
[13] N. Komodakis, N. Paragios, and G. Tziritas. MRF Optimization via Dual Decomposition: MessagePassing Revisited. In Proc. ICCV, 2007.
[14] P. Kr?ahenb?uhl and V. Koltun. Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials.
In Proc. NIPS, 2011.
[15] D. C. Lee, A. Gupta, M. Hebert, and T. Kanade. Estimating Spatial Layout of Rooms using Volumetric
Reasoning about Objects and Surfaces. In Proc. NIPS, 2010.
[16] V. Lempitsky, C. Rother, S. Roth, and A. Blake. Fusion Moves for Markov Random Field Optimization.
PAMI, 2010.
[17] Y. Li, D. Tarlow, and R. Zemel. Exploring compositional high order pattern potentials for structured
output learning. In Proc. CVPR, 2013.
[18] T. Meltzer, A. Globerson, and Y. Weiss. Convergent Message Passing Algorithms: a unifying view. In
Proc. UAI, 2009.
[19] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, 1988.
[20] M. Salzmann. Continuous Inference in Graphical Models with Polynomial Energies. In Proc. CVPR,
2013.
[21] M. Salzmann and R Urtasun. Beyond feature points: structured prediction for monocular non-rigid 3d
reconstruction. In Proc. ECCV, 2012.
[22] A. G. Schwing, T. Hazan, M. Pollefeys, and R. Urtasun. Distributed Message Passing for Large Scale
Graphical Models. In Proc. CVPR, 2011.
[23] A. G. Schwing, T. Hazan, M. Pollefeys, and R. Urtasun. Efficient Structured Prediction for 3D Indoor
Scene Understanding. In Proc. CVPR, 2012.
[24] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor Segmentation and Support Inference from
RGBD Images. In Proc. ECCV, 2012.
[25] D. A. Smith and J. Eisner. Dependency parsing by belief propagation. In Proc. EMNLP, 2008.
[26] D. Sontag, D. K. Choe, and Y. Li. Efficiently Searching for Frustrated Cycles in MAP Inference. In Proc.
UAI, 2012.
[27] D. Sontag and T. Jaakkola. New Outer Bounds on the Marginal Polytope. In Proc. NIPS, 2007.
[28] D. Sontag, T. Meltzer, A. Globerson, and T. Jaakkola. Tightening LP Relaxations for MAP using Message
Passing. In Proc. NIPS, 2008.
[29] D. Sun, C. Liu, and H. Pfister. Local Layering for Joint Motion Estimation and Occlusion Detection. In
Proc. CVPR, 2014.
[30] C. Sutton and A. McCallum. Improved dynamic schedules for belief propagation. In Proc. UAI, 2007.
[31] P. Tseng and D. P. Bertsekas. Relaxation Methods for Problems with Strictly Convex Separable Costs and
Linear Constraints. Mathematical Programming, 1987.
[32] L. Valgaerts, A. Bruhn, H. Zimmer, J. Weickert, C. Stroll, and C. Theobalt. Joint Estimation of Motion,
Structure and Geometry from Stereo Sequences. In Proc. ECCV, 2010.
[33] V. Vineet and P. H. S. Torr J. Warrell. Filter-based Mean-Field Inference for Random Fields with Higher
Order Terms and Product Label-Spaces. In Proc. ECCV, 2012.
[34] M. J. Wainwright and M. I. Jordan. Graphical Models, Exponential Families and Variational Inference.
Foundations and Trends in Machine Learning, 2008.
[35] Y. Weiss, C. Yanover, and T. Meltzer. MAP Estimation, Linear Programming and Belief Propagation with
Convex Free Energies. In Proc. UAI, 2007.
[36] C. Yanover, O. Schueler-Furman, and Y. Weiss. Minimizing and Learning Energy Functions for SideChain Prediction. J. of Computational Biology, 2008.
[37] J. Yao, S. Fidler, and R. Urtasun. Describing the scene as a whole: Joint object detection, scene classification and semantic segmentation. In Proc. CVPR, 2012.
[38] J. Zhang, K. Chen, A. G. Schwing, and R. Urtasun. Estimating the 3D Layout of Indoor Scenes and its
Clutter from Depth Sensors. In Proc. ICCV, 2013.

9

"
3561,2011,Learning person-object interactions for action recognition in still images,"We investigate a discriminatively trained model of person-object interactions for recognizing common human actions in still images. We build on the locally order-less spatial pyramid bag-of-features model, which was shown to perform extremely well on a range of object, scene and human action recognition tasks. We introduce three principal contributions. First, we replace the standard quantized local HOG/SIFT features with stronger discriminatively trained body part and object detectors. Second, we introduce new person-object interaction features based on spatial co-occurrences of individual body parts and objects. Third, we address the combinatorial problem of a large number of possible interaction pairs and propose a discriminative selection procedure using a linear support vector machine (SVM) with a sparsity inducing regularizer. Learning of action-specific body part and object interactions bypasses the difficult problem of estimating the complete human body pose configuration. Benefits of the proposed model are shown on human action recognition in consumer photographs, outperforming the strong bag-of-features baseline.","Learning person-object interactions for
action recognition in still images

Vincent Delaitre?
?
Ecole
Normale Sup?erieure

Josef Sivic*
INRIA Paris - Rocquencourt

Ivan Laptev*
INRIA Paris - Rocquencourt

Abstract
We investigate a discriminatively trained model of person-object interactions for
recognizing common human actions in still images. We build on the locally
order-less spatial pyramid bag-of-features model, which was shown to perform
extremely well on a range of object, scene and human action recognition tasks.
We introduce three principal contributions. First, we replace the standard quantized local HOG/SIFT features with stronger discriminatively trained body part
and object detectors. Second, we introduce new person-object interaction features
based on spatial co-occurrences of individual body parts and objects. Third, we
address the combinatorial problem of a large number of possible interaction pairs
and propose a discriminative selection procedure using a linear support vector
machine (SVM) with a sparsity inducing regularizer. Learning of action-specific
body part and object interactions bypasses the difficult problem of estimating the
complete human body pose configuration. Benefits of the proposed model are
shown on human action recognition in consumer photographs, outperforming the
strong bag-of-features baseline.

1

Introduction

Human actions are ubiquitous and represent essential information for understanding the content
of many still images such as consumer photographs, news images, sparsely sampled surveillance
videos, and street-side imagery. Automatic recognition of human actions and interactions, however,
remains a very challenging problem. The key difficulty stems from the fact that the imaged appearance of a person performing a particular action can vary significantly due to many factors such as
camera viewpoint, person?s clothing, occlusions, variation of body pose, object appearance and the
layout of the scene. In addition, motion cues often used to disambiguate actions in video [6, 27, 31]
are not available in still images.
In this work, we seek to recognize common human actions, such as ?walking?, ?running? or ?reading a book? in challenging realistic images. As opposed to action recognition in video [6, 27, 31],
action recognition in still images has received relatively little attention. A number of previous
works [21, 24, 37] focus on exploiting body pose as a cue for action recognition. In particular,
several methods address joint modeling of human poses, objects and relations among them [21, 40].
Reliable estimation of body configurations for people in arbitrary poses, however, remains a very
challenging research problem. Less structured representations, e.g. [11, 39] have recently emerged
as a promising alternative demonstrating state-of-the-art results for action recognition in static images.
In this work, we investigate discriminatively trained models of interactions between objects and
human body parts. We build on the locally orderless statistical representations based on spatial
?
?
WILLOW project, Laboratoire d?Informatique de l?Ecole
Normale Sup?erieure, ENS/INRIA/CNRS UMR
8548, Paris, France

1

pyramids [28] and bag-of-features models [9, 16, 34], which have demonstrated excellent performance on a range of scene [28], object [22, 36, 41] and action [11] recognition tasks. Rather than
relying on accurate estimation of body part configurations or accurate object detection in the image,
we represent human actions as locally orderless distributions over body parts and objects together
with their interactions. By opportunistically learning class-specific object and body part interactions
(e.g. relative configuration of leg and horse detections for the riding horse action, see Figure 1), we
avoid the extremely challenging task of estimating the full body configuration. Towards this goal,
we consider the following challenges: (i) what should be the representation of object and body part
appearance; (ii) how to model object and human body part interactions; and (iii) how to choose
suitable interaction pairs in the huge space of all possible combinations and relative configurations
of objects and body parts.
To address these challenges, we introduce the following three contributions. First, we replace the
quantized HOG/SIFT features, typically used in bag-of-features models [11, 28, 36] with powerful,
discriminatively trained, local object and human body part detectors [7, 25]. This significantly
enhances generalization over appearance variation, due to e.g. clothing or viewpoint while providing
a reliable signal on part locations. Second, we develop a part interaction representation, capturing
pair-wise relative position and scale between object/body parts, and include this representation in a
scale-space spatial pyramid model. Third, rather than choosing interacting parts manually, we select
them in a discriminative fashion. Suitable pair-wise interactions are first chosen from a large pool of
hundreds of thousands of candidate interactions using a linear support vector machine (SVM) with
a sparsity inducing regularizer. The selected interaction features are then input into a final, more
computationally expensive, non-linear SVM classifier based on the locally orderless spatial pyramid
representation.

2

Related work

Modeling person-object interactions for action recognition has recently attracted significant attention. Gupta et al. [21], Wang et al. [37], and Yao and Fei Fei [40] develop joint models of body
pose configuration and object location within the image. While great progress has been made on
estimating body pose configurations [5, 19, 25, 33], inferring accurate human body pose in images
of common actions in consumer photographs remains an extremely challenging problem due to a
significant amount of occlusions, partial truncation by image boundaries or objects in the scene,
non-upright poses, and large variability in camera viewpoint.
While we build on the recent body pose estimation work by using strong pose-specific body part
models [7, 25], we explicitly avoid inferring the complete body configuration. In a similar spirit,
Desai et al. [13] avoid inferring body configuration by representing a small set of body postures using
single HOG templates and represent relative position of the entire person and an object using simple
relations (e.g. above, to the left). They do not explicitly model body parts and their interactions with
objects as we do in this work. Yang et al. [38] model the body pose as a latent variable for action
recognition. Differently to our method, however, they do not attempt to model interactions between
people (their body parts) and objects. In a recent work, Maji et al. [30] also represent people by
activation responses of body part detectors (rather than inferring the actual body pose), however,
they model only interactions between person and object bounding boxes, not considering individual
body parts, as we do in this work.
Learning spatial groupings of low-level (SIFT) features for recognizing person-object interactions
has been explored by Yao and Fei Fei [39]. While we also learn spatial interactions, we build on
powerful body part and object detectors pre-learnt on separate training data, providing a degree of
generalization over appearance (e.g. clothing), viewpoint and illumination variation. Differently
to [39], we deploy dicriminative selection of interactions using SVM with sparsity inducing regularizer.
Spatial-pyramid based bag-of-features models have demonstrated excellent performance on action
recognition in still images [1, 11] outperforming body pose based methods [21] or grouplet models [40] on their datasets [11]. We build on these locally orderless representations but replace the
low-level features (HOG) with strong pre-trained detectors. Similarly, the object-bank representation [29], where natural scenes are represented by response vectors of densely applied pre-trained
2

Person bounding box

pj

C
pi

Detection dj
(left thigh)

v

Detection di (horse)

Figure 1: Representing person-object interactions by pairs of body part (cyan) and object (blue)
detectors. To get a strong interaction response, the pair of detectors (here visualized at positions pi
and pj ) must fire in a particular relative 3D scale-space displacement (given by the vector v) with a
scale-space displacement uncertainty (deformation cost) given by diagonal 3?3 covariance matrix
C (the spatial part of C is visualized as a yellow dotted ellipse). Our image representation is defined
by the max-pooling of interaction responses over the whole image, solved efficiently by the distance
transform.

object detectors, has shown a great promise for scene recognition. The work in [29], however, does
not attempt to model people, body parts and their interactions with objects.
Related work also includes models of contextual spatial and co-occurrence relationships between
objects [12, 32] as well as objects and the scene [22, 23, 35]. Object part detectors trained from
labelled data also form a key ingredient of attribute-based object representations [15, 26]. While we
build on this body of work, these approaches do not model interactions of people and their body
parts with objects and focus on object/scene recognition rather than recognition of human actions.

3

Representing person-object interactions

This section describes our image representation in terms of body parts, objects and interactions
among them.
3.1

Representing body parts and objects

We assume to have a set of n available detectors d1 , . . . , dn which have been pre-trained for different
body parts and object classes. Each detector i produces a map of dense 3D responses di (I, p) over
locations and scales of a given image I. We express the positions of detections p in terms of scalespace coordinates p = (x, y, ?) where (x, y) corresponds to the spatial location and ? = log ?
? is an
additive scale parameter log-related to the image scale factor ?
? making the addition in the position
vector space meaningful.
In this paper we use two types of detectors. For objects we use LSVM detector [17] trained on
PASCAL VOC images for ten object classes1 . For body parts we implement the method of [25]
and train ten body part detectors2 for each of sixteen pose clusters giving 160 body part detectors
in total (see [25] for further details). Both of our detectors use Histograms of Oriented Gradients
(HOG) [10] as an underlying low-level image representation.
1

The ten object detectors correspond to object classes bicycle, car, chair, cow, dining table, horse, motorbike,
person, sofa, tv/monitor
2
The ten body part detectors correspond to head, torso, {left, right} ? {forearm, upper arm, lower leg,
thigh}

3

3.2

Representing pairwise interactions

We define interactions by the pairs of detectors (di , dj ) as well as by the spatial and scale relations
among them. Each pair of detectors constitutes a two-node tree where the position and the scale of
the leaf are related to the root by scale-space offset and a spatial deformation cost. More precisely,
an interaction pair is defined by a quadruplet q = (i, j, v, C) ? N ? N ? R3 ? M3,3 where i and j
are the indices of the detectors at the root and leaf, v is the offset of the leaf relatively to the root and
C is a 3 ? 3 diagonal matrix defining the displacement cost of the leaf with respect to its expected
position. Figure 1 illustrates an example of an interaction between a horse and the left thigh for the
horse riding action.
We measure the response of the interaction q located at the root position p1 by:

r(I, q, p1 ) = max di (I, p1 ) + dj (I, p2 ) ? uT Cu
p2

(1)

where u = p2 ? (p1 + v) is the displacement vector corresponding to the drift of the leaf node with
respect to its expected position (p1 + v). Maximizing over p2 in (1) provides localization of the
leaf node with the optimal trade-off between the detector score and the displacement cost. For any
interaction q we compute its responses for all pairs of node positions p1 , p2 . We do this efficiently
in linear time with respect to p using distance transform [18].
3.3

Representing images by response vectors of pair-wise interactions

Given a set of M interaction pairs q1 , ? ? ? , qM , we wish to aggregate their responses (1), over an
image region A. Here A can be (i) an (extended) person bounding box, as used for selecting discriminative interaction features (Section 4.2) or (ii) a cell of the scale-space pyramid representation,
as used in the final non-linear classifier (Section 4.3). We define score s(I, q, A) of an interaction
pair q within A of an image I by max-pooling, i.e. as the maximum response of the interaction pair
within A:
s(I, q, A) = max r(I, q, p).
(2)
p?A

An image region A is then represented by a M -vector of interaction pair scores
z = (s1 , ? ? ? , sM ) with si = s(I, qi , A).

4

(3)

Learning person-object interactions

Given object and body part interaction pairs q introduced in the previous section, we wish to use
them for action classification in still images. A brute-force approach of analyzing all possible interactions, however, is computationally prohibitive since the space of all possible interactions is combinatorial in the number of detectors and scale-space relations among them. To address this problem,
we aim in this paper to select a set of M action-specific interaction pairs q1 , . . . , qM , which are both
representative and discriminative for a given action class. Our learning procedure consists of the
three main steps as follows. First, for each action we generate a large pool of candidate interactions,
each comprising a pair of (body part / object) detectors and their relative scale-space displacement.
This step is data-driven and selects candidate detection pairs which frequently occur for a particular
action in a consistent relative scale-space configuration. Next, from this initial pool of candidate interactions we select a set of M discriminative interactions which best separate the particular action
class from other classes in our training set. This is achieved using a linear Support Vector Machine
(SVM) classifier with a sparsity inducing regularizer. Finally, the discriminative interactions are
combined across classes and used as interaction features in our final non-linear spatial-pyramid like
SVM classifier. The three steps are detailed below.
4.1

Generating a candidate pool of interaction pairs

To initialize our model, we first generate a large pool of candidate interactions in a data-driven
manner. Following the suggestion in [17] that the accurate selection of the deformation cost C may
not be that important, we set C to a reasonable fixed value for all pairs, and focus on finding clusters
of frequently co-occurring detectors (di , dj ) in specific relative configurations.
For each detector i and an image I, we first collect a set of positions of all positive detector responses
PIi = {p | di (I, p) > 0}, where di (I, p) is the response of detector i at position p in image I. We
4

then apply a standard non-maxima suppression (NMS) step to eliminate multiple responses of a
detector in local image neighbourhoods and then limit PIi to the L top-scoring detections. The
intuition behind this step is that a part/object interaction is not likely to occur many times in an
image.
For each pair of detectors (di , dj ) we then gather relative displacements between their detections
S
from all the training images Ik : Dij = k {pj ? pi | pi ? PIi k and pj ? PIjk }. To discover potentially interesting interaction pairs, we perform a mean-shift clustering over Dij using a window of
radius R ? R3 (2D-image space and scale) equal to the inverse of the square root of the deformation
1
cost: R = diag(C? 2 ). We also discard clusters which contribute to less than ? percent of the training images. The set of m resulting candidate pairs (i, j, v1 , C), ? ? ? , (i, j, vm , C) is built from the
centers v1 , ? ? ? , vm of the remaining clusters. By applying this procedure to all pairs of detectors,
we generate a large pool (hundreds of thousands) of potentially interesting candidate interactions.
4.2

Discriminative selection of interaction pairs

The initialization described above produces a large number of candidate interactions. Many of
them, however, may not be informative resulting in unnecessary computational load at the training
and classification times. For this reason we wish to select a smaller number of M discriminative
interactions.
Given a set of N training images, each represented by an interaction response vector zi , described
in eq. (3) where A is the extended person bounding box given for each image, and a binary label
yi (in a 1-vs-all setup for each class), the learning problem for each action class can be formulated
using the binary SVM cost function:
J(w, b) = ?

N
X

max{0, 1 ? yi (w> zi + b)} + kwk1 ,

(4)

i=1

where w, b are parameters of the classifier and ? is the weighting factor between the (hinge) loss on
the training examples and the L1 regularizer of the classifier.
By minimizing (4) in a one-versus-all setting for each action class we search (by binary search) for
the value of the regularization parameter ? resulting in the sparse weight vector w with M nonzero elements. Selection of M interaction pairs corresponding to non-zero elements of w gives M
most discriminative (according to (4)) interaction pairs per action class. Note that other discriminative feature selection strategies such as boosting [20] can be also used. However, the proposed
approach is able to jointly search the entire set of candidate feature pairs by minimizing a convex
cost given in (4), whereas boosting implements a greedy feature selection procedure, which may be
sub-optimal.
4.3

Using interaction pairs for classification

Given a set of M discriminative interactions for each action class obtained as described above, we
wish to train a final non-linear action classifier. We use spatial pyramid-like representation [28],
aggregating responses in each cell of the pyramid using max-pooling as described by eq. (2), where
A is one cell of the spatial pyramid. We extend the standard 2D pyramid representation to scalespace resulting in a 3D pyramid with D = 1 + 23 + 43 = 73 cells. Using the scale-space pyramid
with D cells, we represent each image by concatenating M features from each of the K classes
into a M KD-dimensional vector. We train a non-linear SVM with RBF kernel and L2 regularizer
for each action class using a 5-fold cross-validation for the regularization and kernel band-width
parameters. We found that using this final non-linear classifier consistently improves classification
performance over the linear SVM given by equation (4). Note that feature selection (section 4.2) is
necessary in this case as applying the non-linear spatial pyramid classifier on the entire pool of all
candidate interactions would be computationally infeasible.

5

Experiments

We test our model on the Willow-action dataset downloaded from [4] and the PASCAL VOC 2010
action classification dataset [14]. The Willow-action dataset contains more than 900 images with
more than 1100 labelled person detections from 7 human action classes: Interaction with Computer,
5

Photographing, Playing Music, Riding Bike, Riding Horse, Running and Walking. The training set
contains 70 examples of each action class and the rest (at least 39 examples per class) is left for
testing. The PASCAL VOC 2010 dataset contains the 7 above classes together with 2 other actions:
Phoning and Reading. It contains a similar number of images. Each training and testing image
in both datasets is annotated with the smallest bounding box containing each person and by the
performed action(s). We follow the same experimental setup for both datasets.
Implementation details: We use our implementation of body part detectors described in [25] with
16 pose clusters trained on the publicly available 2000 image database [3], and 10 pre-trained PASCAL 2007 Latent SVM object detectors [2]: bicycle, car, chair, cow, dining table, horse, motorbike,
person, sofa, tvmonitor. In the human action training/test data, we extend each given person bounding box by 50% and resize the image so that the bounding box has a maximum size of 300 pixels.
We run the detectors over the transformed bounding boxes and consider the image scales sk = 2k/10
for k ? {?10, ? ? ? , 10}. At each scale we extract the detector response every 4 pixels and 8 pixels
for the body part and object detectors, respectively. The outputs of each detector are then normalized by subtracting the mean of maximum responses within the training bounding boxes and then
normalizing the variance to 1. We generate the candidate interaction pairs by taking the mean-shift
radius R = (30, 30, log(2)/2), L = 3 and ? = 8%. The covariance of the pair deformation cost C
is fixed in all experiments to R?2 . We select M = 310 discriminative interaction pairs to compute
the final spatial pyramid representation of each image.
Results: Table 1 summarizes per-class action classification results (reported using average precision for each class) for the proposed method (d. Interactions), and three baselines. The first baseline
(a. BOF) is the bag-of-features classifier [11], aggregating quantized responses of densely sampled
HOG features in spatial pyramid representation, using a (non-linear) intersection kernel. Note that
this is a strong baseline, which was shown [11] to outperform the recent person-object interaction
models of [39] and [21] on their own datasets. The second baseline (b. LSVM) is the latent SVM
classifier [17] trained in a 1-vs-all fashion for each class. To obtain a single classification score for
each person bounding box, we take the maximum LSVM detection score from the detections overlapping the extended bounding box with the standard overlap score [14] higher than 0.5. The final
baseline (c. Detectors) is a SVM classifier with an RBF kernel trained on max-pooled responses of
the entire bank of body part and object detectors in a spatial pyramid representation but without interactions. This baseline is similar in spirit to the object bank representation [29], but here targeted
to action classification by including a bank of pose-specific body part detectors as well as object
detectors. On average, the proposed method (d.) outperforms all baselines, obtaining the best result
on 4 out of 7 classes. The largest improvements are obtained on Riding Bike and Horse actions,
for which reliable object detectors are available. The improvement of the proposed method d. with
respect to using the plain bank of object and body part detectors c. directly demonstrates the benefit
of modeling interactions. Example detections of interaction pairs are shown in figure 2.
Table 2 shows the performance of the proposed interaction model (d. Interactions) and its combination with the baselines (e. BOF+LSVM+Inter.) on the Pascal VOC 2010 data. Interestingly, the
proposed approach is complementary to both the BOF (51.25 mAP) and LSVM (44.08 mAP) methods and by combining all three approaches (following [11]) the overall performance improves to
60.66 mAP. We also report results of the ?Poselet? method [30], which, similar to our method, is
trained from external non-Pascal data. Our combined approach achieves better overall performance
and also outperforms the ?Poselet? approach on 6 out of 9 classes. Finally, our combined approach
also obtains competitive performance compared to the overall best reported result on the Pascal VOC
2010 data ? ?SURREY MK KDA? [1] ? and outperforms this method on the ?Riding Horse? and
?Walking? classes.

6

Conclusion

We have developed person-object interaction features based on non-rigid relative scale-space displacement of pairs of body part and object detectors. Further, we have shown that such features can
be learnt in a discriminative fashion and can improve action classification performance over a strong
bag-of-features baseline in challenging realistic images of common human actions. In addition, the
learnt interaction features in some cases correspond to visually meaningful configurations of body
parts, and body parts with objects.
6

Inter. w/ Comp.
Blue: Screen
Cyan: L. Leg

Photographing
Blue: Head
Cyan: L. Thigh

Playing Instr.
Blue: L. Forearm
Cyan: L. Forearm

Riding Bike
Blue: R. Forearm
Cyan: Motorbike

Riding Horse
Blue: Horse
Cyan: L. Thigh

Running
Blue: L. Arm
Cyan: R. Leg

Walking
Blue: L. Arm
Cyan: Head

Figure 2: Example detections of discriminative interaction pairs. These body part interaction
pairs are chosen as discriminative (high positive weight wi ) for action classes indicated on the left.
In each row, the first three images show detections on the correct action class. The last image
shows a high scoring detection on an incorrect action class. In the examples shown, the interaction
features capture either a body part and an object, or two body part interactions. Note that while these
interaction pairs are found to be discriminative, due to the detection noise, they do not necessary
localize the correct body parts in all images. However, they may still fire at consistent locations
across many images as illustrated in the second row, where the head detector consistently detects
the camera lens, and the thigh detector fires consistently at the edge of the head. Similarly, the leg
detector seems to consistently fire on keyboards (see the third image in the first row for an example),
thus improving the confidence of the computer detections for the ?Interacting with computer? action.

7

Action / Method
(1) Inter. w/ Comp.
(2) Photographing
(3) Playing Music
(4) Riding Bike
(5) Riding Horse
(6) Running
(7) Walking
Average (mAP)

a. BOF [11]
58.15
35.39
73.19
82.43
69.60
44.53
54.18

b. LSVM
30.21
28.12
56.34
68.70
60.12
51.99
55.97

c. Detectors
45.64
36.35
68.35
86.69
71.44
57.65
57.68

d. Interactions
56.60
37.47
72.00
90.39
75.03
59.73
57.64

59.64

50.21

60.54

64.12

Table 1: Per-class average-precision for different methods on the Willow-actions dataset.
Action / Method
(1) Phoning
(2) Playing Instr.
(3) Reading
(4) Riding Bike
(5) Riding Horse
(6) Running
(7) Taking Photo
(8) Using Computer
(9) Walking
Average (mAP)

d. Interactions
42.11
30.78
28.70
84.93
89.61
81.28
26.89
52.31
70.12

e. BOF+LSVM+Inter.
48.61
53.07
28.56
80.05
90.67
85.81
33.53
56.10
69.56

Poselets[30]
49.6
43.2
27.7
83.7
89.4
85.6
31.0
59.1
67.9

MK-KDA[1]
52.6
53.5
35.9
81.0
89.3
86.5
32.8
59.2
68.6

56.30

60.66

59.7

62.2

Table 2: Per-class average-precision on the Pascal VOC 2010 action classification dataset.
We use only a small set of object detectors available at [2], however, we are now in a position
to include many more additional object (camera, computer, laptop) or texture (grass, road, trees)
detectors, trained from additional datasets, such as ImageNet or LabelMe. Currently, we consider
detections of entire objects, but the proposed model can be easily extended to represent interactions
between body parts and parts of objects [8].
Acknowledgements. This work was partly supported by the Quaero, OSEO, MSR-INRIA, ANR DETECT
(ANR-09-JCJC-0027-01) and the EIT-ICT labs.

References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]

[12]

http://pascallin.ecs.soton.ac.uk/challenges/voc/voc2010/results/index.html.
http://people.cs.uchicago.edu/?pff/latent/.
http://www.comp.leeds.ac.uk/mat4saj/lsp.html.
http://www.di.ens.fr/willow/research/stillactions/.
M. Andriluka, S. Roth, and B. Schiele. Pictorial structures revisited: People detection and articulated
pose estimation. In CVPR, 2009.
A. Bobick and J. Davis. The recognition of human movement using temporal templates. IEEE PAMI,
23(3):257?276, 2001.
L. Bourdev and J. Malik. Poselets: Body part detectors trained using 3D human pose annotations. In
ICCV, 2009.
T. Brox, L. Bourdev, S. Maji, and J. Malik. Object segmentation by alignment of poselet activations to
image contours. In CVPR, 2011.
G. Csurka, C. Bray, C. Dance, and L. Fan. Visual categorization with bags of keypoints. In WS-SLCV,
ECCV, 2004.
N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, pages I:886?893,
2005.
V. Delaitre, I. Laptev, and J. Sivic. Recognizing human actions in still images: a study of bagof-features and part-based representations. In Proc. BMVC., 2010. updated version, available at
http://www.di.ens.fr/willow/research/stillactions/.
C. Desai, D. Ramanan, and C. Fowlkes. Discriminative models for multi-class object layout. In ICCV,
2009.

8

[13] C. Desai, D. Ramanan, and C. Fowlkes. Discriminative models for static human-object interactions. In
SMiCV, CVPR, 2010.
[14] M. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisserman. The pascal visual object classes
(voc) challenge. IJCV, 2010. In press.
[15] A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing objects by their attributes. In CVPR, 2009.
[16] L. Fei-Fei and P. Perona. A Bayesian hierarchical model for learning natural scene categories. In CVPR,
Jun 2005.
[17] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively
trained part based models. IEEE PAMI, 2009.
[18] P. Felzenszwalb and D. Huttenlocher. Distance transforms of sampled functions. Technical report, Cornell
University CIS, Tech. Rep. 2004-1963, 2004.
[19] V. Ferrari, M. Marin-Jimenez, and A. Zisserman. Pose search: retrieving people using their pose. In
CVPR, 2009.
[20] Y. Freund and R. Schapire. A decision theoretic generalisation of online learning. Computer and System
Sciences, 55(1):119?139, 1997.
[21] A. Gupta, A. Kembhavi, and L. Davis. Observing human-object interactions: Using spatial and functional
compatibility for recognition. IEEE PAMI, 31(10):1775?1789, 2009.
[22] H. Harzallah, F. Jurie, and C. Schmid. Combining efficient object localization and image classification.
In ICCV, 2009.
[23] D. Hoiem, A. Efros, and M. Hebert. Putting objects in perspective. In CVPR, 2006.
[24] N. Ikizler, R. G. Cinbis, S. Pehlivan, and P. Duygulu. Recognizing actions from still images. In Proc.
ICPR, 2008.
[25] S. Johnson and M. Everingham. Learning effective human pose estimation from inaccurate annotation.
In CVPR, 2011.
[26] C. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by between-class
attribute transfer. In CVPR, 2009.
[27] I. Laptev, M. Marsza?ek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. In
CVPR, 2008.
[28] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features: spatial pyramid matching for recognizing
natural scene categories. In CVPR, pages II: 2169?2178, 2006.
[29] L. Li, H. Su, E. Xing, and L. Fei-Fei. Object bank: A high-level image representation for scene classification and semantic feature sparsification. In NIPS, 2010.
[30] S. Maji, L. Bourdev, and J. Malik. Action recognition from a distributed representation of pose and
appearance. In CVPR, 2011.
[31] T. B. Moeslund, A. Hilton, and V. Kruger. A survey of advances in vision-based human motion capture
and analysis. CVIU, 103(2-3):90?126, 2006.
[32] A. Rabinovich, A. Vedaldi, C. Galleguillos, E. Wiewiora, and S. Belongie. Objects in context. In ICCV,
2007.
[33] B. Sapp, A. Toshev, and B. Taskar. Cascaded models for articulated pose estimation. In ECCV, 2010.
[34] J. Sivic and A. Zisserman. Video Google: A text retrieval approach to object matching in videos. In
ICCV, 2003.
[35] A. Torralba. Contextual priming for object detection. IJCV, 53(2):169?191, July 2003.
[36] A. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman. Multiple kernels for object detection. In ICCV,
2009.
[37] Y. Wang, H. Jiang, M. S. Drew, Z. N. Li, and G. Mori. Unsupervised discovery of action classes. In
CVPR, pages II: 1654?1661, 2006.
[38] W. Yang, Y. Wang, and G. Mori. Recognizing human actions from still images with latent poses. In
CVPR, 2010.
[39] B. Yao and L. Fei-Fei. Grouplet: A structured image representation for recognizing human and object
interactions. In CVPR, 2010.
[40] B. Yao and L. Fei-Fei. Modeling mutual context of object and human pose in human-object interaction
activities. In CVPR, 2010.
[41] J. Zhang, M. Marszalek, S. Lazebnik, and C. Schmid. Local features and kernels for classification of
texture and object categories: a comprehensive study. IJCV, 73(2):213?238, 2007.

9

"
4083,2012,Mixing Properties of Conditional Markov Chains with Unbounded Feature Functions,"Conditional Markov Chains (also known as Linear-Chain Conditional Random Fields  in the literature) are a versatile class of discriminative models for the distribution of a sequence of hidden states conditional on a sequence of observable variables. Large-sample properties of Conditional Markov Chains have been first studied by Sinn and Poupart [1]. The paper extends this work in two directions: first, mixing properties of models with unbounded feature functions are being established; second, necessary conditions for model identifiability and the uniqueness of maximum likelihood estimates are being given.","Mixing Properties of Conditional Markov Chains
with Unbounded Feature Functions
Mathieu Sinn
IBM Research - Ireland
Mulhuddart, Dublin 15
mathsinn@ie.ibm.com

Bei Chen
McMaster University
Hamilton, Ontario, Canada
bei.chen@math.mcmaster.ca

Abstract
Conditional Markov Chains (also known as Linear-Chain Conditional Random
Fields in the literature) are a versatile class of discriminative models for the distribution of a sequence of hidden states conditional on a sequence of observable
variables. Large-sample properties of Conditional Markov Chains have been first
studied in [1]. The paper extends this work in two directions: first, mixing properties of models with unbounded feature functions are being established; second,
necessary conditions for model identifiability and the uniqueness of maximum
likelihood estimates are being given.

1

Introduction

Conditional Random Fields (CRF) are a widely popular class of discriminative models for the distribution of a set of hidden states conditional on a set of observable variables. The fundamental
assumption is that the hidden states, conditional on the observations, form a Markov random field
[2,3]. Of special importance, particularly for the modeling of sequential data, is the case where the
underlying undirected graphical model forms a simple linear chain. In the literature, this subclass
of models is often referred to as Linear-Chain Conditional Random Fields. This paper adopts the
terminology of [4] and refers to such models as Conditional Markov Chains (CMC).
Large-sample properties of CRFs and CMCs have been first studied in [1] and [5]. [1] defines CMCs
of infinite length and studies ergodic properties of the joint sequences of observations and hidden
states. The analysis relies on fundamental results from the theory of weak ergodicity [6]. The
exposition is restricted to CMCs with bounded feature functions which precludes the application,
e.g., to models with linear features and Gaussian observations. [5] considers weak consistency
and central limit theorems for models with a more general structure. Ergodicity and mixing of the
models is assumed, but no explicit conditions on the feature functions or on the distribution of the
observations are given. An analysis of model identifiability in the case of finite sequences can be
found in [7].
The present paper studies mixing properties of Conditional Markov Chains with unbounded feature
functions. The results are fundamental for analyzing the consistency of Maximum Likelihood estimates and establishing Central Limit Theorems (which are very useful for constructing statistical
hypothesis tests, e.g., for model misspecificiations and the signficance of features). The paper is organized as follows: Sec. 2 reviews the definition of infinite CMCs and some of their basic properties.
In Sec. 3 the ergodicity results from [1] are extended to models with unbounded feature functions.
Sec. 4 establishes various mixing properties. A key result is that, in order to allow for unbounded
feature functions, the observations need to follow a distribution such that Hoeffding-type concentration inequalities can be established. Furthermore, the mixing rates depend on the tail behaviour of
the distribution. In Sec. 5 the mixture properties are used to analyze model identifiability and consistency of the Maximum Likelihood estimates. Sec. 6 concludes with an outlook on open problems
for future research.

2

Conditional Markov Chains

Preliminaries. We use N, Z and R to denote the sets of natural numbers, integers and real numbers,
respectively. Let X be a metric space with the Borel sigma-field A, and Y be a finite set. Furthermore, consider a probability space (?, F, P) and let X = (Xt )t?Z , Y = (Yt )t?Z be sequences of
measurable mappings from ? into X and Y, respectively. Here,
? X is an infinite sequence of observations ranging in the domain X ,
? Y is an aligned sequence of hidden states taking values in the finite set Y.
For now, the distribution of X is arbitrary. Next we define Conditional Markov Chains, which
parameterize the conditional distribution of Y given X.
Definition. Consider a vector f of real-valued functions f : X ? Y ? Y ? R, called the feature
functions. Throughout this paper, we assume that the following condition is satisfied:
(A1) All feature functions are finite: |f (x, i, j)| < ? for all x ? X and i, j ? Y.
Associated with the feature functions is a vector ? of real-valued model-weights. The key in the
definition of Conditional Markov Chains is the matrix M (x) with the (i, j)-th component
m(x, i, j)

=

exp(?T f (x, i, j)).

In terms of statistical physics, m(x, i, j) measures the potential of the transition between the hidden
states i and j from time t?1 to t, given the observation x at time t. Next, for a sequence x = (xt )t?Z
in X and time points s, t ? Z with s ? t, introduce the vectors
?ts (x)

= M (xt )T . . . M (xs )T (1, 1, . . . , 1)T ,

? ts (x)

= M (xs+1 ) . . . M (xt ) (1, 1, . . . , 1)T ,

and write ?st (x, i) and ?st (x, j) to denote the ith respectively jth components. Intuitively, ?st (x, i)
measures the potential of the hidden state i at time t given the observations xs , . . . , xt and assuming
that at time s ? 1 all hidden states have potential equal to 1. Similarly, ?st (x, j) is the potential of
j at time s assuming equal potential of all hidden states at time t. Now let t ? Z and k ? N, and
define the distribution of the labels Yt , . . . , Yt+k conditional on X,
P(Yt = yt , . . . , Yt+k = yt+k | X)

:=

k
Y

m(Xt+i , yt+i?1 , yt+i )

i=1

? lim

t+k+n
t
(X, yt ) ?t+k
(X, yt+k )
?t?n

n??

?tt?n (X)T ? t+k+n
(X)
t

.

Note that, under assumption (A1), the limit on the right hand side is well-defined (see Theorem 2 in
[1]). Furthermore, the family of all marginal distributions obtained this way satisfies the consistency
conditions of Kolmogorov?s Extension Theorem. Hence we obtain a unique distribution for Y
conditional on X parameterized by the feature functions f and the model weights ?. Intuitively, the
distribution is obtained by conditioning the marginal distributions of Y on the finite observational
context (Xt?n , . . . , Xt+k+n ), and then letting the size of the context going to infinity.
Basic properties. We introduce the following notation: For any matrix P = (pij ) with strictly
positive entries let ?(P ) denote the mixing coefficient
?(P )

=

min

i,j,k,l

pik pjl
.
pjk pil

Note that 0 ? ?(P ) ? 1. This coefficient will play a key role in the analysis of mixing properties.
The following proposition summarizes fundamental properties of the distribution of Y conditional
on X, which directly follow from the above definition (also see Corollary 1 in [1]).

Proposition 1. Suppose that condition (A1) holds true. Then Y conditional on X forms a timeinhomogeneous Markov chain. Moreover, if X is strictly stationary, then the joint distribution
of the aligned sequences (X, Y ) is strictly stationary. The conditional transition probabilities
Pt (x, i, j) := P(Yt = j | Yt?1 = i, X = x) of Y given X = x have the following form:
Pt (x, i, j)

?tn (x, j)
.
n?? ? n (x, i)
t?1

= m(xt , i, j) lim

In particular, a lower bound for Pt (x, i, j) is given by
Pt (x, i, j) ?

m(xt , i, j) (mink?Y m(xt+1 , i, k))
,
|Y| (maxk?Y m(xt , j, k)) (maxk,l?Y m(xt+1 , k, l))

and the matrix of transition probabilities P t (x), with the (i, j)-th component given by Pt (x, i, j),
satisfies ?(P t (x)) = ?(M (xt )).

3

Ergodicity

In this section we establish conditions under which the aligned sequences (X, Y ) are jointly ergodic. Let us first recall the definition of ergodicity of X (see [8]): By X we denote the space of
sequences x = (xt )t?Z in X , and by A the corresponding product ?-field. Consider the probability
measure PX on (X , A) defined by PX (A) := P(X ? A) for A ? A. Finally, let ? denote the
operator on X which shifts sequences one position to the left: ? x = (xt+1 )t?Z . Then ergodicity of
X is formally defined as follows:
(A2) X is ergodic, that is, PX (A) = PX (? ?1 A) for every A ? A, and PX (A) ? {0, 1} for
every set A ? A satisfying A = ? ?1 A.
As a particular consequence of the invariance PX (A) = PX (? ?1 A), we obtain that X is strictly
stationary. Now we are able to formulate the key result of this section, which will be of central
importance in the later analysis. For simplicity, we state it for functions depending on the values
of X and Y only at time t. The generalization of the statement is straight-forward. In our later
analysis, we will use the theorem to show that the time average of feature functions f (Xt , Yt?1 , Yt )
converges to the expected value E[f (Xt , Yt?1 , Yt )].
Theorem 1. Suppose that conditions (A1) and (A2) hold, and g : X ? Y ? R is a function which
satisfies E[|g(Xt , Yt )|] < ?. Then
n
1X
g(Xt , Yt ) = E[g(Xt , Yt )] P-almost surely.
lim
n?? n
t=1
Proof. Consider the sequence Z = (Zt )t?N given by Zt := (? t?1 X, Yt ), where we write ? t?1 to
denote the (t ? 1)th iterate of ? . Note that Zt represents the hidden state at time t together with the
entire aligned sequence of observations ? t?1 X. In the literature, such models are known as Markov
sequences in random environments (see [9]). The key step in the proof is to show that Z
Pisn ergodic.
Then, for any function h : X ? Y ? R with E[|h(Zt )|] < ?, the time average n1 t=1 h(Zt )
converges to the expected value E[h(Zt )] P-almost surely. Applying this result to the composition
of the function g and the projection of (? t?1 X, Yt ) onto (Xt , Yt ) completes the proof. The details
of the proof that Z is ergodic can be found in an extended version of this paper, which is included
in the supplementary material.

4

Mixing properties

In this section we are going to study mixing properties of the aligned sequences (X, Y ). To establish
the results, we will assume that the distribution of the observations X satisfies conditions under
which certain concentration inequalities hold true:
Pn
(A3) Let A ? A be a measurable set, with p := P(Xt ? A) and Sn (x) := n1 t=1 1(xt ? A)
for x ? X . Then there exists a constant ? such that, for all n ? N and  > 0,
P(|Sn (X) ? p| ? ) ?

exp(?? 2 n).

If X is a sequence of independent random variables, then (A3) follows by Hoeffding?s inequality. In
the dependent case, concentration inequalities of this type can be obtained by imposing Martingale
or mixing conditions on X (see [12,13]). Furthermore, we will make the following assumption,
which relates the feature functions to the tail behaviour of the distribution of X:
(A4) Let h : [0, ?) ? [0, ?) be a differentiable decreasing function with h(z) = O(z ?(1+?) )
for some ? > 0. Furthermore, let
X
F (x) :=
|?T f (x, j, k)|
j,k?Y
?1

for x ? X . Then E[h(F (Xt ))

] and E[h0 (F (Xt ))?1 ] both exist and are finite.

The following theorem establishes conditions under which the expected conditional covariances of
square-integrable functions are summable. The result is obtained by studying ergodic properties of
the transition probability matrices.
Theorem 2. Suppose that conditions (A1) - (A3) hold true, and g : X ? Y ? R is a function with
finite second moment, E[|g(Xt , Yt )|2 ] < ?. Let ?t,k (X) = Cov(g(Xt , Yt ), g(Xt+k , Yt+k ) | X)
denote the covariance of g(Xt , Yt ) and g(Xt+k , Yt+k ) conditional on X. Then, for every t ? Z:
n
X
E[|?t,k (X)|] < ?.
lim
n??

k=1

Proof. Without loss of generality we may assume that g can be written as g(x, y) = g(x)1(y = i).
Hence, using H?older?s inequality, we obtain
E[|?t,k (X)|] ? E[|g(Xt )|] E[|g(Xt+k )|] E[|Cov(1(Yt = i), 1(Yt+k = i) | X)|].
According to the assumptions, we have E[|g(Xt )|] = E[|g(Xt+k )|] < ?, so we only need to bound
the expectation of the conditional covariance. Note that
Cov(1(Yt = i), 1(Yt+k = i) | X) = P(Yt = i, Yt+k = i | X) ? P(Yt = i | X) P(Yt+k = i | X).
Recall the definition of ?(P ) before Corollary 1. Using probabilistic arguments, it is not difficult to
show that the ratio of P(Yt = i, Yt+k = i | X) to P(Yt = i | X) P(Yt+k = i | X) is greater than or
equal to ?(P t+1 (X) . . . P t+k (X)), where P t+1 (X), . . . , P t+k (X) denote the transition matrices
introduced in Proposition 1. Hence,
|Cov(1(Yt = i), 1(Yt+k = i) | X)| ? P(Yt = i, Yt+k = i | X)[1 ? ?(P t+1 (X) . . . P t+k (X))].
Now, using results from the theory of weak ergodicity (see Chapter 3 in [6]), we can establish
p
p
k
Y
1 ? ?(P t+j (x))
1 ? ?(P t+1 (x) . . . P t+k (x))
p
p
?
1 + ?(P t+j (x))
1 + ?(P t+1 (x) . . . P t+k (x))
j=1
for all x ? X . Using Bernoulli?s inequality and the fact ?(P t+j (x)) = M (xt+j ) established in
Qk
Proposition 1, we obtain ?(P t+1 (x) . . . P t+k (x)) ? 1?4 j=1 [1??(M (xt+j ))]. Consequently,
|Cov(1(Yt = i), 1(Yt+k = i) | X)|

? 4

k
Y

[1 ? ?(M (Xt+j ))].

j=1

With the notation introduced in assumption (A3), let ? > 0 and A ? X with p > 0 be such that
x ? A implies ?(M (x)) ? ?. Furthermore, let  be a constant with 0 <  < p. In order to
bound |Cov(1(Yt = i), 1(Yt+k = i) | X)| for a given k ? N, we distinguish two different cases: If
|Sk (X) ? p| < , then we obtain
4

k
Y

1 ? ?(M (Xt+j ))



? 4 (1 ? ?)k(p?) .

j=1

If |Sk (X) ? p| ? , then we use the trivial upper bound 1. According to assumption (A3), the
probability of the latter event is bounded by an exponential, and hence
E[|Cov(1(Yt = i), 1(Yt+k = i) | X)|] ?

4 (1 ? ?)k(p?) + exp(?? 2 k).

Obviously, the sum of all these expectations is finite, which completes the proof.

The next theorem bounds the difference between the distribution of Y conditional on X and finite
approximations of it. Introduce the following notation: For t, k ? 0 with t + k ? n let
P(0:n) (Yt = yt , . . . , Yt+k = yt+k | X = x)
:=

k
Y

n
?0t (x, yt ) ?t+k
(x, yt+k )
.
n??
?t0 (x)T ? nt (x)

m(xt+i , yt+i?1 , yt+i ) lim

i=1
(0:n)

Accordingly, write E
for expectations taken with respect to P(0:n) . As emphasized by the superscrips, these quantities can be regarded as marginal distributions of Y conditional on the observations at times t = 0, 1, . . . , n. To simplify notation, the following theorem is stated for
1-dimensional conditional marginal distributions, however, the extension to the general case is
straight-forward.
Theorem 3. Suppose that conditions (A1) - (A4) hold true. Then the limit
P(0:?) (Yt = i | X)

:=

lim P(0:n) (Yt = i | X)

n??

is well-defined P-almost surely. Moreover, there exists a measurable function C(x) of x ? X with
finite expectation, E[|C(X)|] < ?, and a function h(z) satisfying the conditions in (A4) , such that
 (0:?)

P
(Yt = i | X) ? P(0:n) (Yt = i | X) ? C(? t X) h(n ? t).
Proof. Define Gn (x) := M (xt+1 ) . . . M (xn ) and write gn (x, i, j) for the (i, j)-th component
of Gn (x). Note that ? nt (x) = Gn (x)(1, 1, . . . , 1)T . According to Lemma 3.4 in [6], there exist
numbers rij (x) such that
gn (x, i, k)
n?? gn (xj, k)
lim

= rij (x)

for all k ? Y. Consequently, the ratio of ?tn (x, i) to ?tn (x, j) converges to rij (x), and hence
?0t (x, i) ?tn (x, i)
n
n?? ?t (x)T ? t (x)
0
lim

=

1
q i (x)T r i (x)

where we use the notation q i (x) = ?t0 (x)/?0t (x, i) and r i (x) denotes the vector with the kth
component rki (x). This proves the first part of the theorem. In order to prove the second part, note
that |x ? y| ? |x?1 ? y ?1 | for any x, y ? (0, 1], and hence


 (0:?)
?t (X)T ? nt (X) 

P
(Yt = i | X) ? P(0:n) (Yt = i | X) ? q i (X)T r i (X) ? t 0
.
?0 (X, i) ?tn (X, i)
To bound the latter expression, introduce the vectors r ni (x) and r ni (x) with the kth components




gn (x, k, l)
gn (x, k, l)
rnki (x) = min
and rnki (x) = max
.
l?Y
l?Y
gn (x, i, l)
gn (x, i, l)
It is easy to see that q i (x)T r ni (x) ? q i (x)T r i (x) ? q i (x)T r ni (x), and
q i (x)T r ni (x) ?

?t0 (x)T ? nt (x)
? q i (x)T r ni (x).
?0t (x, i) ?tn (x, i)

Hence,

?t (X)T ? nt (X) 

q i (X)T r i (X) ? t 0
 ?
?0 (X, i) ?tn (X, i)



q i (X)T (r ni (X) ? r n (X)).
i

Due to space limitations, we only give a sketch of the proof how the latter quantity can be bounded.
For details, see the extended version of this paper in the supplementary material. The first step is
to show the existence of a function C1 (x) with E[|C1 (X)|] < ? such that |rnki (X) ? rnki (X)| ?
C1 (? t X) (1 ? ?)n?t for some ? > 0. With the function F (x) introduced in assumption (A4), we
define C2 (x) := exp(F (x)) for x ? X and arrive at
 (0:?)

P
(Yt = i | X) ? P(0:n) (Yt = i | X) ? |Y|2 C1 (? t X) C2 (Xt ) (1 ? ?)n?t .

The next step is to construct a function C3 (x) satisfying the following two conditions: (i) If
C2 (x)(1 ? ?)k ? 1, then C3 (x)h(k) ? 1. (ii) If C2 (x)(1 ? ?)k < 1, then C3 (x)h(k) ?
C2 (x) (1 ? ?)k . Since the difference between two probabilities cannot exceed 1, we obtain
 (0:?)

P
(Yt = i | X) ? P(0:n) (Yt = i | X) ? |Y|2 C1 (? t X) C3 (Xt ) h(n ? t).
The last step is to show that E[|C3 (Xt )|] < ?.
The following result will play a key role in the later analysis of empirical likelihood functions.
Theorem 4. Suppose that conditions (A1) - (A4) hold, and the function g : X ? Y ? R satisfies
E[|g(Xt , Yt )|] < ?. Then
n
1 X (0:n)
lim
E
[g(Xt , Yt ) | X] = E[g(Xt , Yt )] P-almost surely.
n?? n
t=1
Proof. Without loss of generality we may assume that g can be written as g(x, y) = g(x)1(y = i).
Using the result from Theorem 3, we obtain
n
n
n
X

X
X


E(0:n) [g(Xt , Yt ) | X] ?
E(0:?) [g(Xt , Yt ) | X] ?
|g(Xt )| |C(? t X)| h(n ? t),

t=1

t=1

t=1

where h(z) is a function satisfying the conditions in assumption (A4). See the extended version of
this paper in the supplementary material for more details. Using the facts that X is ergodic and the
expectations of |g(Xt )| and |C(? t X)| are finite, we obtain
n
n

X
1  X (0:n)

E(0:?) [g(Xt , Yt ) | X] = 0.
E
[g(Xt , Yt ) | X] ?
lim 
n?? n
t=1
t=1
By similar arguments to the proof of the first part of Theorem 3 one can show that the difference
|E(0:?) [g(Xt , Yt ) | X] ? E[g(Xt , Yt ) | X]| tends to 0 as t ? ?. Thus,
n
n

X
1  X (0:?)

lim 
E
[g(Xt , Yt ) | X] ?
E[g(Xt , Yt ) | X] = 0.
n?? n
t=1
t=1
Now, noting that E[g(Xt , Yt ) | X] = E[g(X0 , Y0 ) | ? t X] for every t, the theorem follows by the
ergodicity of X.

5

Maximum Likelihood learning and model identifiability

In this section we apply the previous results to analyze asymptotic properties of the empirical
likelihood function. The setting is the following: Suppose that we observe finite subsequences
X n = (X0 , . . . , Xn ) and Y n = (Y0 , . . . , Yn ) of X and Y , where the distribution of Y conditional on X follows a conditional Markov chain with fixed feature functions f and unknown model
weights ?? . We assume that ?? lies in some parameter space ?, the structure of which will become important later. To emphasize the role of the model weights, we will use subscripts, e.g.,
P? and E? , to denote the conditional distributions. Our goal is to identify the unknown model
weights from the
Pnfinite samples, X n and Y n . In order to do so, introduce the shorthand notation
f (xn , y n ) =
t=1 f (xt , yt?1 , yt ) for xn = (x0 , . . . , xn ) and y n = (y0 , . . . , yn ). Consider the
normalized conditional likelihood,
X

1 T
? f (X n , Y n ) ? log
exp ?T f (X n , y n ) .
Ln (?) =
n
n+1
y n ?Y

Note that, in the context of finite Conditional Markov Chains, Ln (?) is the exact likelihood of Y n
conditional on X n . The Maximum Likelihood estimate of ?? is given by
? n := arg max Ln (?).
?
???

If Ln (?) is strictly concave, then the arg max is unique and can be found using gradient-based
search (see [14]). It is easy to see that Ln (?) is strictly concave if and only if |Y| > 1, and there
exists a sequence y n such that at least one component of f (X n , y n ) is non-zero. In the following,
we study strong consistency of the Maximum Likelihood estimates, a property which is of central
importance in large sample theory (see [15]). As we will see, a key problem is the identifiability and
uniqueness of the model weights.

5.1

Asymptotic properties of the likelihood function

In addition to the conditions (A1)-(A4) stated earlier, we will make the following assumptions:
(A5) The feature functions have finite second moments: E?? [|f (Xt , Yt?1 , Yt )|2 ] < ?.
(A6) The parameter space ? is compact.
The next theorem establishes asymptotic properties of the likelihood function Ln (?).
Theorem 5. Suppose that conditions (A1)-(A6) are satisfied. Then the following holds true:
(i) There exists a function L(?) such that limn?? Ln (?) = L(?) P?? -almost surely for
every ? ? ?. Moreover, the convergence of Ln (?) to L(?) is uniform on ?, that is,
limn?? sup??? |Ln (?) ? L(?)| = 0 P?? -almost surely.
(ii) The gradients satisfy limn?? ?Ln (?) = E?? [f (Xt , Yt?1 , Yt )] ? E? [f (Xt , Yt?1 , Yt )]
P?? -almost surely for every ? ? ?.
(iii) If the limit of the Hessian ?2 Ln (?) is finite and non-singular, then the function L(?) is
strictly concave on ?. As a consequence, the Maximum Likelihood estimates are strongly
consistent:
?n
lim ?

n??

= ??

P?? -almost surely.

Proof. The statements are obtained analogously to Lemma 4-6 and Theorem 4 in [1], using the
auxiliary results for Conditional Markov Chains with unbounded feature functions established in
Theorem 1, Theorem 2, and Theorem 4.
Next, we study the asymptotic behaviour of the Hessian ?2 Ln (?). In order to compute the derviatives, introduce the P
vectors ?1 , . . . , ?n with ?t = ? for t = 1, . . . , n. This allows us to write
n
T
?T f (X n , Y n ) =
t=1 ?t f (Xt , Yt?1 , Yt ). Now, regard the argument ? of the likelihood function as a stacked vector (?1 , . . . , ?n ). Same as in [1], this gives us the expressions
?2
Ln (?)
??t ??Tt+k


1
(0:n) 
Cov?
f (Xt , Yt?1 , Yt ), f (Xt+k , Yt+k?1 , Yt+k )T | X
n

=

(0:n)

(0:n)

where Cov?
is the covariance with respect to the measure P?
Using these expressions, the Hessian of Ln (?) can be written as
?2 Ln (?)

=

?

n
X
t=1

introduced before Theorem 3.

n?1

X n?k
X
?2
?2
L
(?)
+
2
L
(?)
.
n
n
??t ??Tt
??t ??Tt+k
k=1 t=1

The following theorem establishes an expression for the limit of ?2 Ln (?). It differs from the
expression given in Lemma 7 of [1], which is incorrect.
Theorem 6. Suppose that conditions (A1) - (A5) hold. Then
lim ?2 Ln (?)

n??

?


X
= ? ?? (0) + 2
?? (k)

P?? -almost surely

k=1

where ?? (k) = E[Cov? (f (Xt , Yt?1 , Yt ), f (Xt+k , Yt+k?1 , Yt+k ) | X)] is the expectation of the
conditional covariance (with respect to P? ) between f (Xt , Yt?1 , Yt ) and f (Xt+k , Yt+k?1 , Yt+k )
given X. In particular, the limit of ?2 Ln (?) is finite.
Proof. The key step is to show the existence of a positive measurable function U? (k, x) such that
lim

n??

n?1
X n?k
X




k=1 t=1


?2

L
(?)

n
T
??t ??t+k

?

lim

n??

n?1
X
k=1

E[U? (k, X)]

with the limit on the right hand side being finite. Then the rest of the proof is straight-forward:
Theorem 4 shows that, for fixed k ? 0,
n?k
X
?2
lim
Ln (?) = ?? (k) P?? -almost surely.
n??
??t ??Tt+k
t=1
Hence, in order to establish the theorem, it suffices to show that
n?k
n?1

X
X 
?2

L
(?)
lim
?? (k) ?
 ? 
n
T
n??
??t ??t+k
t=1
k=1
P?
for all  > 0. Now let  > 0 be fixed. According to Theorem 2 we have k=1 |?? (k)| < ?. Hence
we can find a finite N such that
n?1
n?1
X
X
lim
|?? (k)| + lim
E[U? (k, X)] ? .
n??

k=N

n??

On the other hand, Theorem 4 shows that
n?k
N
?1 
X
X

lim
?? (k) ?
n??

k=1

t=1

k=N


?2

L
(?)

n
T
??t ??t+k

=

0.

For details on how to construct U? (k, x), see the extended version of this paper.
5.2

Model identifiability

Let us conclude the analysis by investigating conditions under which the limit of the Hessian
?2 Ln (?) is non-singular. Note that ?2 Ln (?) is negative definite for every n, so also the limit
is negative definite, but not necessarily strictly negative definite. Using the result in Theorem 6, we
can establish the following statement:
Corollary 1. Suppose that assumptions (A1)-(A5) hold true. Then the following conditions are
necessary for the limit of ?2 Ln (?) to be non-singular:
(i) For each feature function f (x, i, j), there exists a set A ? X with P(Xt ? A) > 0 such
that, for every x ? A, we can find i, j, k, l ? Y with f (x, i, j) 6= f (x, k, l).
(ii) There does not exist a non-zero vector ? and a subset A ? X with P(Xt ? A) = 1 such
that ?T f (x, i, j) is constant for all x ? X and i, j ? Y.
Condition (i) essentially says: features f (x, i, j) must not be constant in i and j. Condition (ii)
says that features must not be expressible as linear combinations of each other. Clearly, features
violating condition (i) can be assigned arbitrary model weights without any effect on the conditional
distributions. If condition (ii) is violated, then there are infinitely many ways for parameterizing the
same model. In practice, some authors have found positive effects of including redundant features
(see, e.g., [16]), however, usually in the context of a learning objective with an additional penalizer.

6

Conclusions

We have established ergodicity and various mixing properties of Conditional Markov Chains with
unbounded feature functions. The main insight is that similar results to the setting with bounded
feature functions can be obtained, however, under additional assumptions on the distribution of the
observations. In particular, the proof of Theorem 2 has shown that the sequence of observations
needs to satisfy conditions under which Hoeffding-type concentration inequalities can be obtained.
The proof of Theorem 3 has reveiled an interesting interplay between mixing rates, feature functions, and the tail behaviour of the distribution of observations. By applying the mixing properties to the empirical likelihood functions we have obtained necessary conditions for the Maximum
Likelihood estimates to be strongly consistent. We see a couple of interesting problems for future
research: establishing Central Limit Theorems for Conditional Markov Chains; deriving bounds for
the asymptotic variance of Maximum Likelihood estimates; constructing tests for the significance
of features; generalizing the estimation theory to an infinite number of features; finally, finding
sufficient conditions for the model identifiability.

References
[1] Sinn, M. & Poupart, P. (2011) Asymptotic theory for linear-chain conditional random fields. In Proc. of the
14th International Conference on Artificial Intelligence and Statistics (AISTATS).
[2] Lafferty, J., McCallum, A. & Pereira, F. (2001) Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of the 18th IEEE International Conference on Machine Learning
(ICML).
[3] Sutton, C. & McCallum, A. (2006) An introduction to conditional random fields for relational learning. In:
Getoor, L. & Taskar, B. (editors), Introduction to Statistical Relational Learning. Cambridge, MA: MIT Press.
[4] Hofmann, T., Sch?olkopf, B. & Smola, A.J. (2008) Kernel methods in machine learning. The Annals of
Statstics, Vol. 36, No. 3, 1171-1220.
[5] Xiang, R. & Neville, J. (2011) Relational learning with one network: an asymptotic analysis. In Proc. of
the 14th International Conference on Artificial Intelligence and Statistics (AISTATS).
[6] Seneta, E. (2006) Non-Negative Matrices and Markov Chains. Revised Edition. New York, NY: Springer.
[7] Wainwright, M.J. & Jordan, M.I. (2008) Graphical models: exponential families, and variational inference.
R
in Machine Learning, Vol. 1, Nos. 1-2, 1-305.
Foundations and Trends 
[8] Cornfeld, I.P., Fomin, S.V. & Sinai, Y.G. (1982) Ergodic Theory. Berlin, Germany: Springer.
[9] Orey, S. (1991) Markov chains with stochastically stationary transition probabilities. The Annals of Probability, Vol. 19, No. 3, 907-928.
[10] Hern?andez-Lerma, O. & Lasserre, J.B. (2003) Markov Chains and Invariant Probabilities. Basel, Switzerland: Birkh?auser.
[11] Foguel, S.R. (1969) The Ergodic Theory of Markov Processes. Princeton, NJ: Van Nostrand.
[12] Samson, P.-M. (2000) Concentration of measure inequalities for Markov chains and ?-mixing processes.
The Annals of Probability, Vol. 28, No. 1, 416-461.
[13] Kontorovich, L. & Ramanan, K. (2008) Concentration inequalities for dependent random variables via the
martingale method. The Annals of Probability, Vol. 36, No. 6, 2126-2158.
[14] Sha, F. & Pereira, F. (2003) Shallow parsing with conditional random fields. In Proc. of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics
(HLT-NAACL).
[15] Lehmann, E.L. (1999) Elements of Large-Sample Theory. New York, NY: Springer.
[16] Hoefel, G. & Elkan, C. (2008) Learning a two-stage SVM/CRF sequence classifier. In Proc. of the 17th
ACM International Conference on Information and Knowledge Management (CIKM).

"
6099,2016,The non-convex Burer-Monteiro approach works on smooth semidefinite programs,"Semidefinite programs (SDP's) can be solved in polynomial time by interior point methods, but scalability can be an issue. To address this shortcoming, over a decade ago, Burer and Monteiro proposed to solve SDP's with few equality constraints via rank-restricted, non-convex surrogates. Remarkably, for some applications, local optimization methods seem to converge to global optima of these non-convex surrogates reliably. Although some theory supports this empirical success, a complete explanation of it remains an open question. In this paper, we consider a class of SDP's which includes applications such as max-cut, community detection in the stochastic block model, robust PCA, phase retrieval and synchronization of rotations. We show that the low-rank Burer-Monteiro formulation of SDP's in that class almost never has any spurious local optima.","The non-convex Burer?Monteiro approach works
on smooth semide?nite programs
Vladislav Voroninski?
Department of Mathematics
Massachusetts Institute of Technology
vvlad@math.mit.edu

Nicolas Boumal?
Department of Mathematics
Princeton University
nboumal@math.princeton.edu

Afonso S. Bandeira
Department of Mathematics and Center for Data Science
Courant Institute of Mathematical Sciences, New York University
bandeira@cims.nyu.edu

Abstract
Semide?nite programs (SDP?s) can be solved in polynomial time by interior point
methods, but scalability can be an issue. To address this shortcoming, over a
decade ago, Burer and Monteiro proposed to solve SDP?s with few equality constraints via rank-restricted, non-convex surrogates. Remarkably, for some applications, local optimization methods seem to converge to global optima of these nonconvex surrogates reliably. Although some theory supports this empirical success,
a complete explanation of it remains an open question. In this paper, we consider a
class of SDP?s which includes applications such as max-cut, community detection
in the stochastic block model, robust PCA, phase retrieval and synchronization of
rotations. We show that the low-rank Burer?Monteiro formulation of SDP?s in
that class almost never has any spurious local optima.

1

Introduction

We consider semide?nite programs (SDP?s) of the form
f? =

min ?C, X?

X?Sn?n

subject to

A(X) = b, X ? 0,

(SDP)

where ?C, X? = Tr(C ?X), C ? Sn?n is the symmetric cost matrix, A : Sn?n ? Rm is a linear operator capturing m equality constraints with right hand side b ? Rm and the variable X is
symmetric, positive semide?nite. Interior point methods solve (SDP) in polynomial time [Nesterov,
2004]. In practice however, for n beyond a few thousands, such algorithms run out of memory (and
time), prompting research for alternative solvers.
If (SDP) has a compact search space, then it admits a global optimum of rank at most r, where
r(r+1)
? m [Pataki, 1998, Barvinok, 1995]. Thus, if one restricts the search space of (SDP) to
2
matrices of rank at most p with p(p+1)
? m, then the globally optimal value remains unchanged.
2
This restriction is easily enforced by factorizing X = Y Y ? where Y has size n ? p, yielding an
equivalent quadratically constrained quadratic program:
q ? = min ?CY , Y ?
Y ?Rn?p

subject to

A(Y Y ?) = b.

?The ?rst two authors contributed equally.
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

(P)

In general, (P) is non-convex, making it a priori unclear how to solve it globally. Still, the bene?ts
are that it is lower dimensional than (SDP) and has no conic constraint. This has motivated Burer
and Monteiro [2003, 2005] to try and solve (P) using local optimization methods, with surprisingly
good results. They developed theory in support of this observation (details below). About their
results, Burer and Monteiro [2005, ?3] write (mutatis mutandis):
?How large must we take p so that the local minima of (P) are guaranteed to map
>m
to global minima of (SDP)? Our theorem asserts that we need only1 p(p+1)
2
(with the important caveat that positive-dimensional faces of (SDP) which are
??at? with respect to the objective function can harbor non-global local minima).?

The caveat?the existence or non-existence of non-global local optima, or their potentially adverse
effect for local optimization algorithms?was not further discussed.
In this paper, assuming p(p+1)
> m, we show that if the search space of (SDP) is compact and if
2
the search space of (P) is a smooth manifold, then, for almost all cost matrices C, if Y satis?es ?rstand second-order necessary optimality conditions for (P), then Y is a global optimum of (P) and,
? m, X = Y Y ? is a global optimum of (SDP); in other words, ?rst- and secondsince p(p+1)
2
order necessary optimality conditions for (P) are also suf?cient for global optimality?an unusual
theoretical guarantee in non-convex optimization.
Notice that this is a statement about the optimization problem itself, not about speci?c algorithms.
Interestingly, known algorithms for optimization on manifolds converge to second-order critical
points,2 regardless of initialization [Boumal et al., 2016].
For the speci?ed class of SDP?s, our result improves on those of [Burer and Monteiro, 2005] in
two important ways. Firstly, for almost all C, we formally exclude the existence of spurious local
optima.3 Secondly, we only require the computation of second-order critical points of (P) rather
than local optima (which is hard in general [Vavasis, 1991]). Below, we make a statement about
computational complexity, and we illustrate the practical ef?ciency of the proposed methods through
numerical experiments.
SDP?s which satisfy the compactness and smoothness assumptions occur in a number of applications including Max-Cut, robust PCA, Z2 -synchronization, community detection, cut-norm approximation, phase synchronization, phase retrieval, synchronization of rotations and the trust-region
subproblem?see Section 4 for references.
A simple example: the Max-Cut problem
Given an undirected graph, Max-Cut is the NP-hard problem of clustering the n nodes of this graph
in two classes, +1 and ?1, such that as many edges as possible join nodes of different signs. If C is
the adjacency matrix of the graph, Max-Cut is expressed as
n
1 ?
maxn
Cij (1 ? xi xj ) s.t. x21 = ? ? ? = x2n = 1.
(Max-Cut)
x?R 4
i,j=1
Introducing the positive semide?nite matrix X = xx?, both the cost and the constraints may be expressed linearly in terms of X. Ignoring that X has rank 1 yields the well-known convex relaxation
in the form of a semide?nite program (up to an af?ne transformation of the cost):
min
?C, X? s.t. diag(X) = 1, X ? 0.
(Max-Cut SDP)
n?n
X?S

If a solution X of this SDP has rank 1, then X = xx? for some x which is then an optimal cut. In the
general case of higher rank X, Goemans and Williamson [1995] exhibited the celebrated rounding
scheme to produce approximately optimal cuts (within a ratio of .878) from X.
1

The condition on p and m is slightly, but inconsequentially, different in [Burer and Monteiro, 2005].
Second-order critical points satisfy ?rst- and second-order necessary optimality conditions.
3
Before Prop. 2.3 in [Burer and Monteiro, 2005], the authors write: ?The change of variables X = Y Y ?
does not introduce any extraneous local minima.? This is sometimes misunderstood to mean (P) does not have
spurious local optima, when it actually means that the local optima of (P) are in exact correspondence with the
local optima of ?(SDP) with the extra constraint rank(X) ? p,? which is also non-convex and thus also liable
to having local optima. Unfortunately, this misinterpretation has led to some confusion in the literature.
2

2

The corresponding Burer?Monteiro non-convex problem with rank bounded by p is:
min ?CY , Y ?

Y ?Rn?p

s.t.

diag(Y Y ?) = 1.

(Max-Cut BM)

The constraint diag(Y Y ?) = 1 requires each row of Y to have unit norm; that is: Y is a point on the
Cartesian product of n unit spheres in Rp , which is a smooth manifold. Furthermore, all X feasible
for the SDP have identical trace equal to n, so that the search space of the SDP is compact. Thus,
our results stated below apply:
?? ?
For p =
2n , for almost all C, even though (Max-Cut BM) is non-convex, any
local optimum Y is a global optimum (and so is X = Y Y ?), and all saddle points
have an escape (the Hessian has a negative eigenvalue).
We note that, for p > n/2, the same holds for all C [Boumal, 2015].
Notation
Sn?n is the set of real, symmetric matrices of size n. A symmetric matrix X is positive semide?nite
(X ? 0) if and only if u?Xu ? 0 for all u ? Rn . For matrices A, B, the standard
? Euclidean inner
?
product is ?A, B? = Tr(A B). The associated (Frobenius) norm is ?A? = ?A, A?. Id is the
identity operator and In is the identity matrix of size n.

2

Main results

Our main result establishes conditions under which ?rst- and second-order necessary optimality
conditions for (P) are suf?cient for global optimality. Under those conditions, it is a fortiori true that
global optima of (P) map to global optima of (SDP), so that local optimization methods on (P) can
be used to solve the higher-dimensional, cone-constrained (SDP).
We now specify the necessary optimality conditions of (P). Under the assumptions of our main
result below (Theorem 2), the search space
M = Mp = {Y ? Rn?p : A(Y Y ?) = b}

(1)

TY M = {Y? ? Rn?p : A(Y? Y ? + Y Y? ?) = 0}.

(2)

is a smooth and compact manifold. As such, it can be linearized at each point Y ? M by a tangent
space, simply by differentiating the constraints [Absil et al., 2008, eq. (3.19)]:

Endowing the tangent spaces of M with the (restricted) Euclidean metric ?A, B? = Tr(A?B) turns
M into a Riemannian submanifold of Rn?p . In general, second-order optimality conditions can
be intricate to handle [Ruszczy?nski, 2006]. Fortunately, here, the smoothness of both the search
space (1) and the cost function
f (Y ) = ?CY , Y ?

(3)

make for straightforward conditions. In spirit, they coincide with the well-known conditions for unconstrained optimization. As further detailed in Appendix A, the Riemannian gradient gradf (Y ) is
the orthogonal projection of the classical gradient of f to the tangent space TY M. The Riemannian
Hessian of f at Y is a similarly restricted version of the classical Hessian of f to the tangent space.
De?nition 1. A (?rst-order) critical point for (P) is a point Y ? M such that
(1st order nec. opt. cond.)

gradf (Y ) = 0,

where gradf (Y ) ? TY M is the Riemannian gradient at Y of f restricted to M. A second-order
critical point for (P) is a critical point Y such that
(2nd order nec. opt. cond.)

Hessf (Y ) ? 0,

where Hessf (Y ) : TY M ? TY M is the Riemannian Hessian at Y of f restricted to M (a symmetric linear operator).
3

Proposition 1. All local (and global) optima of (P) are second-order critical points.
Proof. See [Yang et al., 2014, Rem. 4.2 and Cor. 4.2].
We can now state our main result. In the theorem statement below, ?for almost all C? means potentially troublesome cost matrices form at most a (Lebesgue) zero-measure subset of Sn?n , in the
same way that almost all square matrices are invertible. In particular, given any matrix C ? Sn?n ,
perturbing C to C + ?W where W is a Wigner random matrix results in an acceptable cost matrix
with probability 1, for arbitrarily small ? > 0.
Theorem 2. Given constraints A : Sn?n ? Rm , b ? Rm and p satisfying

p(p+1)
2

> m, if

(i) the search space of (SDP) is compact; and
(ii) the search space of (P) is a smooth manifold,
then for almost all cost matrices C ? Sn?n , any second-order critical point of (P) is globally
optimal. Under these conditions, if Y is globally optimal for (P), then the matrix X = Y Y ? is
globally optimal for (SDP).
The assumptions are discussed in the next section. The proof?see Appendix A?follows directly
from the combination of two intermediate results:
1. If Y is rank de?cient and second-order critical for (P), then it is globally optimal and
X = Y Y ? is optimal for (SDP); and
2. If

p(p+1)
2

> m, then, for almost all C, every ?rst-order critical Y is rank-de?cient.

The ?rst step holds in a more general context, as previously established by Burer and Monteiro
[2003, 2005]. The second step is new and crucial, as it allows to formally exclude the existence of
spurious local optima, generically in C, thus resolving the caveat mentioned in the introduction.
The smooth structure of (P) naturally suggests using Riemannian optimization to solve it [Absil et al.,
2008], which is something that was already proposed by Journ?ee et al. [2010] in the same context.
Importantly, known algorithms converge to second-order critical points regardless of initialization.
We state here a recent computational result to that effect.
Proposition 3. Under the numbered assumptions of Theorem 2, the Riemannian trust-region method
(RTR) [Absil et al., 2007] initialized with any Y0 ? M returns in O(1/?2g ?H + 1/?3H ) iterations a
point Y ? M such that
f (Y ) ? f (Y0 ),

?gradf (Y )? ? ?g ,

and

Hessf (Y ) ? ??H Id .

Proof. Apply the main results of [Boumal et al., 2016] using that f has locally Lipschitz continuous
gradient and Hessian in Rn?p and M is a compact submanifold of Rn?p .
Essentially, each iteration of RTR requires evaluation of one cost and one gradient, a bounded number of Hessian-vector applications, and one projection from Rn?p to M. In many important cases,
this projection amounts to Gram?Schmidt orthogonalization of small blocks of Y ?see Section 4.
Proposition 3 bounds worst-case iteration counts for arbitrary initialization. In practice, a good
initialization point may be available, making the local convergence rate of RTR more informative.
For RTR, one may expect superlinear or even quadratic local convergence rates near isolated local
minimizers [Absil et al., 2007]. While minimizers are not isolated in our case [Journ?ee et al., 2010],
experiments show a characteristically superlinear local convergence rate in practice [Boumal, 2015].
This means high accuracy solutions can be achieved, as demonstrated in Appendix B.
Thus, under the conditions of Theorem 2, generically in C, RTR converges to global optima. In
practice, the algorithm returns after a ?nite number of steps, and only approximate second-order
criticality is guaranteed. Hence, it is interesting to bound the optimality gap in terms of the approximation quality. Unfortunately, we do not establish such a result for small p. Instead, we give an
a posteriori computable optimality gap bound which holds for all p and for all C. In the following
statement, the dependence of M on p is explicit, as Mp . The proof is in Appendix A.
4

Theorem 4. Let R < ? be the maximal trace of any X feasible for (SDP). For any p such that Mp
and Mp+1 are smooth manifolds (even if p(p+1)
? m) and for any Y ? Mp , form Y? = [Y |0n?1 ]
2
in Mp+1 . The optimality gap at Y is bounded as
?
(4)
0 ? 2(f (Y ) ? f ? ) ? R?gradf (Y )? ? R?min (Hessf (Y? )).

If all feasible X have the same trace R and there exists a positive de?nite feasible X, then the bound
simpli?es to
0 ? 2(f (Y ) ? f ? ) ? ?R?min (Hessf (Y? ))

(5)

so that ?gradf (Y )? needs not be controlled explicitly. If p > n, the bounds hold with Y? = Y .
In particular, for p = n + 1, the bound can be controlled a priori: approximate second-order critical
points are approximately optimal, for any C.4
Corollary 5. Under the assumptions of Theorem 4, if p = n + 1 and Y ? M satis?es both
?gradf (Y )? ? ?g and Hessf (Y ) ? ??H Id, then Y is approximately optimal in the sense that
?
0 ? 2(f (Y ) ? f ? ) ? R?g + R?H .

Under the same condition as in Theorem 4, the bound can be simpli?ed to R?H .

This works well with Proposition 3. For any p, equation (4) also implies the following:
?
2(f (Y ) ? f ? ) ? R?gradf (Y )?
?
.
?min (Hessf (Y )) ? ?
R
That is, for any p and any C, an approximate critical point Y in Mp which is far from optimal maps
to a comfortably-escapable approximate saddle point Y? in Mp+1 .

This suggests an algorithm as follows. For a starting value of p such that Mp is a manifold, use
RTR to compute an approximate second-order critical point Y . Then, form Y? in Mp+1 and test
the left-most eigenvalue of Hessf (Y? ).5 If it is close enough to zero, this provides a good bound
on the optimality gap. If not, use an (approximate) eigenvector associated to ?min (Hessf (Y? )) to
escape the approximate saddle point and apply RTR from that new point in Mp+1 ; iterate. In the
worst-case scenario, p grows to n + 1, at which point
?? all? approximate second-order critical points
2m should suf?ce for C bounded away from
are approximate optima. Theorem 2 suggests p =
a zero-measure set. Such an algorithm already features with less theory in [Journ?ee et al., 2010]
and [Boumal, 2015]; in the latter, it is called the Riemannian staircase, for it lifts (P) ?oor by ?oor.
Related work
Low-rank approaches to solve SDP?s have featured in a number of recent research papers. We
highlight just two which illustrate different classes of SDP?s of interest.
Shah et al. [2016] tackle SDP?s with linear cost and linear constraints (both equalities and inequalities) via low-rank factorizations, assuming the matrices appearing in the cost and constraints are
positive semide?nite. They propose a non-trivial initial guess to partially overcome non-convexity
with great empirical results, but do not provide optimality guarantees.
Bhojanapalli et al. [2016a] on the other hand consider the minimization of a convex cost function
over positive semide?nite matrices, without constraints. Such problems could be obtained from
generic SDP?s by penalizing the constraints in a Lagrangian way. Here too, non-convexity is partially
overcome via non-trivial initialization, with global optimality guarantees under some conditions.
Also of interest are recent results about the harmlessness of non-convexity in low-rank matrix completion [Ge et al., 2016, Bhojanapalli et al., 2016b]. Similarly to the present work, the authors there
show there is no need for special initialization despite non-convexity.
4
With p = n + 1, problem (P) is no longer lower dimensional than (SDP), but retains the advantage of not
involving a positive semide?niteness constraint.
5
It may be more practical to test ?min (S) (14) rather than ?min (Hessf ). Lemma 7 relates the two.
See [Journ?ee et al., 2010, ?3.3] to construct escape tangent vectors from S.

5

3

Discussion of the assumptions

Our main result, Theorem 2, comes with geometric assumptions on the search spaces of both (SDP)
and (P) which we now discuss. Examples of SDP?s which ?t the assumptions of Theorem 2 are
featured in the next section.
The assumption that the search space of (SDP),
C = {X ? Sn?n : A(X) = b, X ? 0},

(6)

> m as follows. For (P) to reveal the global
is compact works in pair with the assumption p(p+1)
2
optima of (SDP), it is necessary that (SDP) admits a solution of rank at most p. One way to ensure
this is via the Pataki?Barvinok theorems [Pataki, 1998, Barvinok, 1995], which state that all extreme
? m. Extreme points are faces of dimension zero (such
points of C have rank r bounded as r(r+1)
2
as vertices for a cube). When optimizing a linear cost function ?C, X? over a compact convex set C,
at least one extreme point is a global optimum [Rockafellar, 1970, Cor. 32.3.2]?this is not true in
general if C is not compact. Thus, under the assumptions of Theorem 2, there is a point Y ? M such
that X = Y Y ? is an optimal extreme point of (SDP); then, of course, Y itself is optimal for (P).
In general, the Pataki?Barvinok bound is tight, in that there exist extreme points of rank up to that
upper-bound (rounded down)?see for example [Laurent and Poljak, 1996] for the Max-Cut SDP
and [Boumal, 2015] for the Orthogonal-Cut SDP. Let C (the cost matrix) be the negative of such an
?m
extreme point. Then, the unique optimum of (SDP) is that extreme point, showing that p(p+1)
2
is necessary for (SDP) and (P) to be equivalent for all C. We further require a strict inequality
because our proof relies on properties of rank de?cient Y ?s in M.

The assumption that M (eq. (1)) is a smooth manifold works in pair with the ambition that the result should hold for (almost) all cost matrices C. The starting point is that, for a given non-convex
smooth optimization problem?even a quadratically constrained quadratic program?computing local optima is hard in general [Vavasis, 1991]. Thus, we wish to restrict our attention to ef?ciently
computable points, such as points which satisfy ?rst- and second-order KKT conditions for (P)?
see [Burer and Monteiro, 2003, ?2.2] and [Ruszczy?nski, 2006, ?3]. This only makes sense if global
optima satisfy the latter, that is, if KKT conditions are necessary for optimality. A global optimum
Y necessarily satis?es KKT conditions if constraint quali?cations (CQ?s) hold at Y [Ruszczy?nski,
2006]. The standard CQ?s for equality constrained programs are Robinson?s conditions
or metric
?
?
regularity (they are here equivalent). They read as follows, assuming A(Y Y ?)i = Ai , Y Y ? for
some matrices A1 , . . . , Am ? Sn?n :
CQ?s hold at Y if A1 Y, . . . , Am Y are linearly independent in Rn?p .

(7)

Considering almost all C, global optima could, a priori, be almost anywhere in M. To simplify,
we require CQ?s to hold at all Y ?s in M rather than only at the (unknown) global optima. This
turns out to be a suf?cient condition for M to be a smooth manifold of codimension m [Absil et al.,
2008, Prop. 3.3.3]. Indeed, tangent vectors Y? ? TY M (2) are exactly those vectors that satisfy
?Ai Y , Y? ? = 0: under CQ?s, the Ai Y ?s form a basis of the normal space to the manifold at Y .

Once it is decided that M must be a manifold, we can step away from the speci?c representation
of it via the matrices A1 , . . . , Am and reason about optimality conditions on the manifold directly.
Adding redundant constraints (for example, duplicating A1 ) would break the CQ?s, but not the
manifold structure. Hence, stating Theorem 2 in terms of manifolds better captures the role of M
than stating it in terms of CQ?s. See also [Andreani et al., 2010, Thm. 3.3] for a proof that requiring
M to be a manifold around Y is a type of CQ.

Finally, we note that Theorem 2 only applies for almost all C, rather than all C. To justify this
restriction, if indeed it is justi?ed, one should exhibit a matrix C that leads to suboptimal secondorder critical points while other assumptions are satis?ed. We do not have such an example. We do
observe that (Max-Cut SDP) on cycles of certain even lengths has a unique solution of rank 1, while
the corresponding (Max-Cut BM) with p = 2 has suboptimal local optima (strictly, if we quotient
out symmetries). This at least suggests it is not enough, for generic C, to set p just larger than
the rank of the solutions of the SDP. (For those same examples, at p = 3, we consistently observe
convergence to global optima.)
6

4

Examples of smooth SDP?s

The canonical examples of SDP?s which satisfy the assumptions in Theorem 2 are those where the
diagonal blocks of X or their traces are ?xed. We note that the algorithms and the theory continue
to hold for complex matrices, where the set of Hermitian matrices of size n is treated as a real
vector space of dimension n2 (instead of n(n+1)
in the real case) with inner product ?H1 , H2 ? =
2
p(p+1)
?
? {Tr(H1 H2 )}, so that occurrences of 2 are replaced by p2 .

Certain concrete examples of SDP?s include:

min ?C, X? s.t. Tr(X) = 1, X ? 0;
X

min ?C, X? s.t. diag(X) = 1, X ? 0;
X

min ?C, X? s.t. Xii = Id , X ? 0.
X

(?xed trace)
(?xed diagonal)
(?xed diagonal blocks)

Their rank-constrained counterparts read as follows (matrix norms are Frobenius norms):
min ?CY , Y ? s.t. ?Y ? = 1;

(sphere)

Y : n?p

min ?CY , Y ? s.t. Y ? = [y1

???

yn ] and ?yi ? = 1 for all i;

min ?CY , Y ? s.t. Y ? = [Y1

???

Yq ] and Yi?Yi = Id for all i.

Y : n?p

Y : qd?p

(product of spheres)
(product of Stiefel)

The ?rst example has only one constraint: the SDP always admits an optimal rank 1 solution, corresponding to an eigenvector associated to the left-most eigenvalue of C. This generalizes to the
trust-region subproblem as well.
For the second example, in the real case, p = 1 forces yi = ?1, allowing to capture combinatorial
problems such as Max-Cut [Goemans and Williamson, 1995], Z2 -synchronization [Javanmard et al.,
2015] and community detection in the stochastic block model [Abbe et al., 2016, Bandeira et al.,
2016b]. The same SDP is central in a formulation of robust PCA [McCoy and Tropp, 2011] and
is used to approximate
?? the? cut-norm of a matrix [Alon and Naor, 2006]. Theorem 2 states that for
2n is suf?cient. In the complex case, p = 1 forces |yi | = 1, allowing to
almost all C, p =
capture problems where phases must be recovered; in particular, phase synchronization [Bandeira
et al., 2016a, Singer, 2011] and phase?retrieval via Phase-Cut [Waldspurger et al., 2015]. For almost
all C, it is then suf?cient to set p = ? n + 1?.

In the third example, Y of size n ? p is divided in q slices of size d ? p, with p ? d. Each
slice has orthonormal rows. For p = d, the slices are orthogonal (or unitary) matrices, allowing
to capture Orthogonal-Cut [Bandeira et al., 2016c] and the related problems of synchronization of
rotations [Wang and Singer, 2013] and permutations. Synchronization of rotations is an important
step in simultaneous
localization
and mapping, for example. Here, it is suf?cient for almost all C to
??
?
let p =
d(d + 1)q .

SDP?s with constraints that are combinations of the above examples can also have the smoothness
property; the right-hand sides 1 and Id can be replaced by any positive de?nite right-hand sides by a
change of variables. Another simple rule to check is if the constraint matrices A1 , . . . , Am ? Sn?n
such that A(X)i = ?Ai , X? satisfy Ai Aj = 0 for all i ?= j (note that this is stronger than requiring
?Ai , Aj ? = 0), see [Journ?ee et al., 2010].

5

Conclusions

The Burer?Monteiro approach consists in replacing optimization of a linear function ?C, X? over
the convex set {X ? 0 : A(X) = b} with optimization of the quadratic function ?CY , Y ? over the
non-convex set {Y ? Rn?p : A(Y Y ?) = b}. It was previously known that, if the convex set is
compact and p satis?es p(p+1)
? m where m is the number of constraints, then these two problems
2
have the same global optimum. It was also known from [Burer and Monteiro, 2005] that spurious
local optima Y , if they exist, must map to special faces of the compact convex set, but without
statement as to the prevalence of such faces or the risk they pose for local optimization methods. In
7

this paper we showed that, if the set of X?s is compact and the set of Y ?s is a smooth manifold, and
> m, then for almost all C, the non-convexity of the problem in Y is benign, in that all
if p(p+1)
2
Y ?s which satisfy second-order necessary optimality conditions are in fact globally optimal.
We further reference the Riemannian trust-region method [Absil et al., 2007] to solve the problem in
Y , as it was recently guaranteed to converge from any starting point to a point which satis?es secondorder optimality conditions, with global convergence rates [Boumal et al., 2016]. In addition, for p =
n + 1, we guarantee that approximate satisfaction of second-order conditions implies approximate
global optimality. We note that the 1/?3 convergence rate in our results may be pessimistic. Indeed,
the numerical experiments clearly show that high accuracy solutions can be computed fast using
optimization on manifolds, at least for certain applications.
Addressing a broader class of SDP?s, such as those with inequality constraints or equality constraints
that may violate our smoothness assumptions, could perhaps be handled by penalizing those constraints in the objective in an augmented Lagrangian fashion. We also note that, algorithmically,
the Riemannian trust-region method we use applies just as well to nonlinear costs in the SDP. We
believe that extending the theory presented here to broader classes of problems is a good direction
for future work.

Acknowledgment
VV was partially supported by the Of?ce of Naval Research. ASB was supported by NSF Grant
DMS-1317308. Part of this work was done while ASB was with the Department of Mathematics at
the Massachusetts Institute of Technology. We thank Wotao Yin and Michel Goemans for helpful
discussions.

References
E. Abbe, A.S. Bandeira, and G. Hall. Exact recovery in the stochastic block model. Information Theory, IEEE
Transactions on, 62(1):471?487, 2016.
P.-A. Absil, C. G. Baker, and K. A. Gallivan. Trust-region methods on Riemannian manifolds. Foundations of
Computational Mathematics, 7(3):303?330, 2007. doi:10.1007/s10208-005-0179-9.
P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton University
Press, Princeton, NJ, 2008. ISBN 978-0-691-13298-3.
N. Alon and A. Naor. Approximating the cut-norm via Grothendieck?s inequality. SIAM Journal on Computing,
35(4):787?803, 2006. doi:10.1137/S0097539704441629.
R. Andreani, C. E. Echag?ue, and M. L. Schuverdt. Constant-rank condition and second-order constraint quali?cation. Journal of Optimization Theory and Applications, 146(2):255?266, 2010. doi:10.1007/s10957010-9671-8.
A.S. Bandeira, N. Boumal, and A. Singer. Tightness of the maximum likelihood semide?nite relaxation for
angular synchronization. Mathematical Programming, pages 1?23, 2016a. doi:10.1007/s10107-016-1059-6.
A.S. Bandeira, N. Boumal, and V. Voroninski. On the low-rank approach for semide?nite programs arising
in synchronization and community detection. In Proceedings of The 29th Conference on Learning Theory,
COLT 2016, New York, NY, June 23?26, 2016b.
A.S. Bandeira, C. Kennedy, and A. Singer. Approximating the little Grothendieck problem over the orthogonal
and unitary groups. Mathematical Programming, pages 1?43, 2016c. doi:10.1007/s10107-016-0993-7.
A.I. Barvinok. Problems of distance geometry and convex properties of quadratic maps. Discrete & Computational Geometry, 13(1):189?202, 1995. doi:10.1007/BF02574037.
S. Bhojanapalli, A. Kyrillidis, and S. Sanghavi. Dropping convexity for faster semi-de?nite optimization.
Conference on Learning Theory (COLT), 2016a.
S. Bhojanapalli, B. Neyshabur, and N. Srebro. Global optimality of local search for low rank matrix recovery.
arXiv preprint arXiv:1605.07221, 2016b.
N. Boumal. A Riemannian low-rank method for optimization over semide?nite matrices with block-diagonal
constraints. arXiv preprint arXiv:1506.00575, 2015.

8

N. Boumal, B. Mishra, P.-A. Absil, and R. Sepulchre. Manopt, a Matlab toolbox for optimization on manifolds.
Journal of Machine Learning Research, 15:1455?1459, 2014. URL http://www.manopt.org.
N. Boumal, P.-A. Absil, and C. Cartis. Global rates of convergence for nonconvex optimization on manifolds.
arXiv preprint arXiv:1605.08101, 2016.
S. Burer and R.D.C. Monteiro. A nonlinear programming algorithm for solving semide?nite programs via lowrank factorization. Mathematical Programming, 95(2):329?357, 2003. doi:10.1007/s10107-002-0352-8.
S. Burer and R.D.C. Monteiro. Local minima and convergence in low-rank semide?nite programming. Mathematical Programming, 103(3):427?444, 2005.
CVX. CVX: Matlab software for disciplined convex programming. http://cvxr.com/cvx, August 2012.
R. Ge, J.D. Lee, and T. Ma.
arXiv:1605.07272, 2016.

Matrix completion has no spurious local minimum.

arXiv preprint

M.X. Goemans and D.P. Williamson. Improved approximation algorithms for maximum cut and satis?ability problems using semide?nite programming. Journal of the ACM (JACM), 42(6):1115?1145, 1995.
doi:10.1145/227683.227684.
C. Helmberg, F. Rendl, R.J. Vanderbei, and H. Wolkowicz. An interior-point method for semide?nite programming. SIAM Journal on Optimization, 6(2):342?361, 1996. doi:10.1137/0806020.
A. Javanmard, A. Montanari, and F. Ricci-Tersenghi. Phase transitions in semide?nite relaxations. arXiv
preprint arXiv:1511.08769, 2015.
M. Journ?ee, F. Bach, P.-A. Absil, and R. Sepulchre. Low-rank optimization on the cone of positive semide?nite
matrices. SIAM Journal on Optimization, 20(5):2327?2351, 2010. doi:10.1137/080731359.
M. Laurent and S. Poljak. On the facial structure of the set of correlation matrices. SIAM Journal on Matrix
Analysis and Applications, 17(3):530?547, 1996. doi:10.1137/0617031.
M. McCoy and J.A. Tropp. Two proposals for robust PCA using semide?nite programming. Electronic Journal
of Statistics, 5:1123?1160, 2011. doi:10.1214/11-EJS636.
Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87 of Applied optimization.
Springer, 2004. ISBN 978-1-4020-7553-7.
G. Pataki. On the rank of extreme matrices in semide?nite programs and the multiplicity of optimal eigenvalues.
Mathematics of operations research, 23(2):339?358, 1998. doi:10.1287/moor.23.2.339.
R.T. Rockafellar. Convex analysis. Princeton University Press, Princeton, NJ, 1970.
A.P. Ruszczy?nski. Nonlinear optimization. Princeton University Press, Princeton, NJ, 2006.
S. Shah, A. Kumar, D. Jacobs, C. Studer, and T. Goldstein. Biconvex relaxation for semide?nite programming
in computer vision. arXiv preprint arXiv:1605.09527, 2016.
A. Singer. Angular synchronization by eigenvectors and semide?nite programming. Applied and Computational Harmonic Analysis, 30(1):20?36, 2011. doi:10.1016/j.acha.2010.02.001.
K.C. Toh, M.J. Todd, and R.H. T?ut?unc?u. SDPT3?a MATLAB software package for semide?nite programming.
Optimization Methods and Software, 11(1?4):545?581, 1999. doi:10.1080/10556789908805762.
S.A. Vavasis. Nonlinear optimization: complexity issues. Oxford University Press, Inc., 1991.
I. Waldspurger, A. d?Aspremont, and S. Mallat. Phase recovery, MaxCut and complex semide?nite programming. Mathematical Programming, 149(1?2):47?81, 2015. doi:10.1007/s10107-013-0738-9.
L. Wang and A. Singer. Exact and stable recovery of rotations for robust synchronization. Information and
Inference, 2(2):145?193, 2013. doi:10.1093/imaiai/iat005.
Z. Wen and W. Yin. A feasible method for optimization with orthogonality constraints. Mathematical Programming, 142(1?2):397?434, 2013. doi:10.1007/s10107-012-0584-1.
W.H. Yang, L.-H. Zhang, and R. Song. Optimality conditions for the nonlinear programming problems on
Riemannian manifolds. Paci?c Journal of Optimization, 10(2):415?434, 2014.

9

"
3473,2010,Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression,"Predicting the execution time of computer programs is an important but challenging problem in the community of computer systems. Existing methods require experts to perform detailed analysis of program code in order to construct predictors or select important features. We recently developed a new system to automatically extract a large number of features from program execution on sample inputs, on which prediction models can be constructed without expert knowledge. In this paper we study the construction of predictive models for this problem. We propose the SPORE (Sparse POlynomial REgression) methodology to build accurate prediction models of program performance using feature data collected from program execution on sample inputs. Our two SPORE algorithms are able to build relationships between responses (e.g., the execution time of a computer program) and features, and select a few from hundreds of the retrieved features to construct an explicitly sparse and non-linear model to predict the response variable. The compact and explicitly polynomial form of the estimated model could reveal important insights into the computer program (e.g., features and their non-linear combinations that dominate the execution time), enabling a better understanding of the program?s behavior. Our evaluation on three widely used computer programs shows that SPORE methods can give accurate prediction with relative error less than 7% by using a moderate number of training data samples. In addition, we compare SPORE algorithms to state-of-the-art sparse regression algorithms, and show that SPORE methods, motivated by real applications, outperform the other methods in terms of both interpretability and prediction accuracy.","Predicting Execution Time of Computer Programs
Using Sparse Polynomial Regression

Ling Huang
Intel Labs Berkeley
ling.huang@intel.com
Byung-Gon Chun
Intel Labs Berkeley
byung-gon.chun@intel.com

Jinzhu Jia
UC Berkeley
jzjia@stat.berkeley.edu
Petros Maniatis
Intel Labs Berkeley
petros.maniatis@intel.com

Bin Yu
UC Berkeley
binyu@stat.berkeley.edu
Mayur Naik
Intel Labs Berkeley
mayur.naik@intel.com

Abstract
Predicting the execution time of computer programs is an important but challenging problem in the community of computer systems. Existing methods require experts to perform detailed analysis of program code in order to construct predictors
or select important features. We recently developed a new system to automatically
extract a large number of features from program execution on sample inputs, on
which prediction models can be constructed without expert knowledge. In this
paper we study the construction of predictive models for this problem. We propose the SPORE (Sparse POlynomial REgression) methodology to build accurate
prediction models of program performance using feature data collected from program execution on sample inputs. Our two SPORE algorithms are able to build
relationships between responses (e.g., the execution time of a computer program)
and features, and select a few from hundreds of the retrieved features to construct an explicitly sparse and non-linear model to predict the response variable.
The compact and explicitly polynomial form of the estimated model could reveal
important insights into the computer program (e.g., features and their non-linear
combinations that dominate the execution time), enabling a better understanding
of the program?s behavior. Our evaluation on three widely used computer programs shows that SPORE methods can give accurate prediction with relative error
less than 7% by using a moderate number of training data samples. In addition, we
compare SPORE algorithms to state-of-the-art sparse regression algorithms, and
show that SPORE methods, motivated by real applications, outperform the other
methods in terms of both interpretability and prediction accuracy.

1 Introduction
Computing systems today are ubiquitous, and range from the very small (e.g., iPods, cellphones,
laptops) to the very large (servers, data centers, computational grids). At the heart of such systems
are management components that decide how to schedule the execution of different programs over
time (e.g., to ensure high system utilization or efficient energy use [11, 15]), how to allocate to each
program resources such as memory, storage and networking (e.g., to ensure a long battery life or fair
resource allocation), and how to weather anomalies (e.g., flash crowds or attacks [6, 17, 24]).
These management components typically must make guesses about how a program will perform
under given hypothetical inputs, so as to decide how best to plan for the future. For example,
consider a simple scenario in a data center with two computers, fast computer A and slow computer
B, and a program waiting to run on a large file f stored in computer B. A scheduler is often faced
1

with the decision of whether to run the program at B, potentially taking longer to execute, but
avoiding any transmission costs for the file; or moving the file from B to A but potentially executing
the program at A much faster. If the scheduler can predict accurately how long the program would
take to execute on input f at computer A or B, he/she can make an optimal decision, returning
results faster, possibly minimizing energy use, etc.
Despite all these opportunities and demands, uses of prediction have been at best unsophisticated
in modern computer systems. Existing approaches either create analytical models for the programs
based on simplistic assumptions [12], or treat the program as a black box and create a mapping function between certain properties of input data (e.g., file size) and output response [13]. The success
of such methods is highly dependent on human experts who are able to select important predictors
before a statistical modeling step can take place. Unfortunately, in practice experts may be hard to
come by, because programs can get complex quickly beyond the capabilities of a single expert, or
because they may be short-lived (e.g., applications from the iPhone app store) and unworthy of the
attention of a highly paid expert. Even when an expert is available, program performance is often
dependent not on externally visible features such as command-line parameters and input files, but
on the internal semantics of the program (e.g., what lines of code are executed).
To address this problem (lack of expert and inherent semantics), we recently developed a new system [7] to automatically extract a large number of features from the intermediate execution steps of
a program (e.g., internal variables, loops, and branches) on sample inputs; then prediction models
can be built from those features without the need for a human expert.
In this paper, we propose two Sparse POlynomial REgression (SPORE) algorithms that use the
automatically extracted features to predict a computer program?s performance. They are variants of
each other in the way they build the nonlinear terms into the model ? SPORE-LASSO first selects
a small number of features and then entertains a full nonlinear polynomial expansion of order less
than a given degree; while SPORE-FoBa chooses adaptively a subset of the full expanded terms
and hence allows possibly a higher order of polynomials. Our algorithms are in fact new general
methods motivated by the computer performance prediction problem. They can learn a relationship
between a response (e.g., the execution time of a computer program given an input) and the generated
features, and select a few from hundreds of features to construct an explicit polynomial form to
predict the response. The compact and explicit polynomial form reveals important insights in the
program semantics (e.g., the internal program loop that affects program execution time the most).
Our approach is general, flexible and automated, and can adapt the prediction models to specific
programs, computer platforms, and even inputs.
We evaluate our algorithms experimentally on three popular computer programs from web search
and image processing. We show that our SPORE algorithms can achieve accurate predictions with
relative error less than 7% by using a small amount of training data for our application, and that our
algorithms outperform existing state-of-the-art sparse regression algorithms in the literature in terms
of interpretability and accuracy.
Related Work. In prior attempts to predict program execution time, Gupta et al. [13] use a variant of
decision trees to predict execution time ranges for database queries. Ganapathi et al. [11] use KCCA
to predict time and resource consumption for database queries using statistics on query texts and
execution plans. To measure the empirical computational complexity of a program, Trendprof [12]
constructs linear or power-law models that predict program execution counts. The drawbacks of such
approaches include their need for expert knowledge about the program to identify good features, or
their requirement for simple input-size to execution time correlations.
Seshia and Rakhlin [22, 23] propose a game-theoretic estimator of quantitative program properties,
such as worst-case execution time, for embedded systems. These properties depend heavily on the
target hardware environment in which the program is executed. Modeling the environment manually
is tedious and error-prone. As a result, they formulate the problem as a game between their algorithm
(player) and the program?s environment (adversary), where the player seeks to accurately predict the
property of interest while the adversary sets environment states and parameters.
Since expert resource is limited and costly, it is desirable to automatically extract features from program codes. Then machine learning techniques can be used to select the most important features
to build a model. In statistical machine learning, feature selection methods under linear regression models such as LASSO have been widely studied in the past decade. Feature selection with
2

non-linear models has been studied much less, but has recently been attracting attention. The most
notable are the SpAM work with theoretical and simulation results [20] and additive and generalized forward regression [18]. Empirical studies with data of these non-linear sparse methods are
very few [21]. The drawback of applying the SpAM method in our execution time prediction problem is that SpAM outputs an additive model and cannot use the interaction information between
features. But it is well-known that features of computer programs interact to determine the execution time [12]. One non-parametric modification of SpAM to replace the additive model has been
proposed [18]. However, the resulting non-parametric models are not easy to interpret and hence are
not desirable for our execution time prediction problem. Instead, we propose the SPORE methodology and propose efficient algorithms to train a SPORE model. Our work provides a promising
example of interpretable non-linear sparse regression models in solving real data problems.

2 Overview of Our System
Our focus in this paper is on algorithms for feature selection and model building. However we first
review the problem within which we apply these techniques to provide context [7]. Our goal is to
predict how a given program will perform (e.g., its execution time) on a particular input (e.g., input
files and command-line parameters). The system consists of four steps.
First, the feature instrumentation step analyzes the source code and automatically instruments it
to extract values of program features such as loop counts (how many times a particular loop has
executed), branch counts (how many times each branch of a conditional has executed), and variable
values (the k first values assigned to a numerical variable, for some small k such as 5).
Second, the profiling step executes the instrumented program with sample input data to collect values
for all created program features and the program?s execution times. The time impact of the data
collection is minimal.
Third, the slicing step analyzes each automatically identified feature to determine the smallest subset
of the actual program that can compute the value of that feature, i.e., the feature slice. This is the
cost of obtaining the value of the feature; if the whole program must execute to compute the value,
then the feature is expensive and not useful, since we can just measure execution time and we have
no need for prediction, whereas if only a little of the program must execute, the feature is cheap and
therefore possibly valuable in a predictive model.
Finally, the modeling step uses the feature values collected during profiling along with the feature
costs computed during slicing to build a predictive model on a small subset of generated features.
To obtain a model consisting of low-cost features, we iterate over the modeling and slicing steps,
evaluating the cost of selected features and rejecting expensive ones, until only low-cost features are
selected to construct the prediction model. At runtime, given a new input, the selected features are
computed using the corresponding slices, and the model is used to predict execution time from the
feature values.
The above description is minimal by necessity due to space constraints, and omits details on the
rationale, such as why we chose the kinds of features we chose or how program slicing works.
Though important, those details have no bearing in the results shown in this paper.
At present our system targets a fixed, overprovisioned computation environment without CPU job
contention or network bandwidth fluctuations. We therefore assume that execution times observed
during training will be consistent with system behavior on-line. Our approach can adapt to modest
change in execution environment by retraining on different environments. In our future research, we
plan to incorporate candidate features of both hardware (e.g., configurations of CPU, memory, etc)
and software environment (e.g., OS, cache policy, etc) for predictive model construction.

3 Sparse Polynomial Regression Model
Our basic premise for predictive program analysis is that a small but relevant set of features may explain the execution time well. In other words, we seek a compact model?an explicit form function
of a small number of features?that accurately estimates the execution time of the program.
3

To make the problem tractable, we constrain our models to the multivariate polynomial family, for at
least three reasons. First, a ?good program? is usually expected to have polynomial execution time in
some (combination of) features. Second, a polynomial model up to certain degree can approximate
well many nonlinear models (due to Taylor expansion). Finally, a compact polynomial model can
provide an easy-to-understand explanation of what determines the execution time of a program,
providing program developers with intuitive feedback and a solid basis for analysis.
For each computer program, our feature instrumentation procedure outputs a data set with n samples
as tuples of {yi , xi }ni=1 , where yi ? R denotes the ith observation of execution time, and xi denotes
the ith observation of the vector of p features. We now review some obvious alternative methods to
modeling the relationship between Y = [yi ] and X = [xi ], point out their drawbacks, and then we
proceed to our SPORE methodology.
3.1 Sparse Regression and Alternatives
Least square regression is widely used for finding the best-fitting f (x, ?) to a given set of responses
yi by minimizing the sum of the squares of the residuals [14]. Regression with subset selection
finds for each k ? {1, 2, . . . , m} the feature subset of size k that gives the smallest residual sum of
squares. However, it is a combinatorial optimization and is known to be NP-hard [14]. In recent
years a number of efficient alternatives based on model regularization have been proposed. Among
them, LASSO [25] finds the selected features with coefficients ?? given a tuning parameter ? as
follows:
X
1
?? = arg min kY ? X?k22 + ?
|?j |.
(1)
? 2
j
LASSO effectively enforces many ?j ?s to be 0, and selects a small subset of features (indexed by
non-zero ?j ?s) to build the model, which is usually sparse and has better prediction accuracy than
models created by ordinary least square regression [14] when p is large. Parameter ? controls the
complexity of the model: as ? grows larger, fewer features are selected.
Being a convex optimization problem is an important advantage of the LASSO method since several
fast algorithms exist to solve the problem efficiently even with large-scale data sets [9, 10, 16, 19].
Furthermore, LASSO has convenient theoretical and empirical properties. Under suitable assumptions, it can recover the true underlying model [8, 25]. Unfortunately, when predictors are highly
correlated, LASSO usually cannot select the true underlying model. The adaptive-LASSO [29]
defined below in Equation (2) can overcome this problem
1
?? = arg min kY ? X?k22 + ?
? 2

X ?j
| |,
wj
j

(2)

where wj can be any consistent estimate of ?. Here we choose wj to be a ridge estimate of ?:
wj = (X T X + 0.001I)?1 X T Y,
where I is the identity matrix.
Technically LASSO can be easily extended to create nonlinear models (e.g., using polynomial basis
functions up to degree d of all p features). However, this approach gives us p+d
terms, which is
d
very large when p is large (on the order of thousands) even for small d, making regression computationally expensive. We give two alternatives to fit the sparse polynomial regression model next.
3.2 SPORE Methodology and Two Algorithms
Our methodology captures non-linear effects of features?as well as non-linear interactions among
features?by using polynomial basis functions over those features (we use terms to denote the polynomial basis functions subsequently). We expand the feature set x = {x1 x2 . . . xk }, k ? p to
all the terms in the expansion of the degree-d polynomial (1 + x1 + . . . + xk )d , and use the terms
to construct a multivariate polynomial function f (x, ?) for the regression. We define expan(X, d)
as the mapping from the original data matrix X to a new matrix with the polynomial expansion
terms up to degree d as the columns. For example, using a degree-2 polynomial with feature set
4

x = {x1 x2 }, we expand out (1 + x1 + x2 )2 to get terms 1, x1 , x2 , x21 , x1 x2 , x22 , and use them as
basis functions to construct the following function for regression:
expan ([x1 , x2 ], 2) = [1, [x1 ], [x2 ], [x21 ], [x1 x2 ], [x22 ]],
f (x, ?)

=

?0 + ?1 x1 + ?2 x2 + ?3 x21 + ?4 x1 x2 + ?5 x22 .

Complete expansion on all p features is not necessary, because many of them have little contribution to the execution time. Motivated by this execution time application, we propose a general
methodology called SPORE which is a sparse polynomial regression technique. Next, we develop
two algorithms to fit our SPORE methodology.
3.2.1 SPORE-LASSO: A Two-Step Method
For a sparse polynomial model with only a few features, if we can preselect a small number of
features, applying the LASSO on the polynomial expansion of those preselected features will still
be efficient, because we do not have too many polynomial terms. Here is the idea:
Step 1: Use the linear LASSO algorithm to select a small number of features and filter out (often
many) features that hardly have contributions to the execution time.
Step 2: Use the adaptive-LASSO method on the expanded polynomial terms of the selected features
(from Step 1) to construct the sparse polynomial model.
Adaptive-LASSO is used in Step 2 because of the collinearity of the expanded polynomial features.
Step 2 can be computed efficiently if we only choose a small number of features in Step 1. We
present the resulting SPORE-LASSO algorithm in Algorithm 1 below.
Algorithm 1 SPORE-LASSO
Input: response Y , feature data X, maximum degree d, ?1 , ?2
Output: Feature index S, term index St , weights ?? for d-degree polynomial basis.
1: ?
? = arg min? 21 kY ? X?k22 + ?1 k?k1
2: S = {j : ?
? j 6= 0}
3: Xnew = expan(X(S), d)
T
T
4: w = (Xnew
Xnew + 0.001I)?1 Xnew
Y
P ?
1
2
5: ?? = arg min? 2 kY ? Xnew ?k2 + ?2 j | wjj |
6: St = {j : ??j 6= 0}
X(S) in Step 3 of Algorithm 1 is a sub-matrix of X containing only columns from X indexed by
S. For a new observation with feature vector X = [x1 , x2 , . . . , xp ], we first get the selected feature
vector X(S), then obtain the polynomial terms Xnew = expan(X(S), d), and finally we compute
? Note that the prediction depends on the choice of ?1 , ?2 and
the prediction: Y? = Xnew ? ?.
maximum degree d. In this paper, we fix d = 3. ?1 and ?2 are chosen by minimizing the Akaike
Information Criterion (AIC) on the LASSO solution paths. The AIC is defined as n log(kY ? Y? k22 )+
2s, where Y? is the fitted Y and s is the number of polynomial terms selected in the model. To be
precise, for the linear LASSO step (Step 1 of Algorithm 1), a whole solution path with a number of
?1 can be obtained using the algorithm in [10]. On the solution path, for each fixed ?1 , we compute
a solution path with varied ?2 for Step 5 of Algorithm 1 to select the polynomial terms. For each
?2 , we calculate the AIC, and choose the (?1 , ?2 ) with the smallest AIC.
One may wonder whether Step 1 incorrectly discards features required for building a good model
in Step 2. We next show theoretically this is not the case. Let S be a subset of {1, 2, . . . , p} and
its complement S c = {1, 2, . . . , p} \ S. Write the feature matrix X as X = [X(S), X(S c)]. Let
response Y = f (X(S)) + ?, where f (?) is any function and ? is additive noise. Let n be the number
of observations and s the size of S. We assume that X is deterministic, p and s are fixed, and ??i s are
i.i.d. and follow the Gaussian distribution with mean 0 and variance ? 2 . Our results also hold for
zero mean sub-Gaussian noise with parameter ? 2 . More general results regarding general scaling of
n, p and s can also be obtained.
Under the following conditions, we show that Step 1 of SPORE-LASSO, the linear LASSO, selects
the relevant features even if the response Y depends on predictors X(S) nonlinearly:
5

1. The columns (Xj , j = 1, . . . , p) of X are standardized:

1
T
n Xj Xj

= 1, for all j;

2. ?min ( n1 X(S)T X(S)) ? c with a constant c > 0;
3. min |(X(S)T X(S))?1 X(S)T f (X(S))| > ? with a constant ? > 0;
4.

T
T
?1
T
XS
XS
]f (XS )
c [I?XS (XS XS )
n

<

??c
?
,
2 s+1

for some 0 < ? < 1;

5. kXSTc XS (XST XS )?1 k? ? 1 ? ?;
where ?min (?) denotes the minimum eigenvalue of a matrix, kAk? is defined as maxi
and the inequalities are defined element-wise.

hP

j

i
|Aij |

Theorem 3.1. Under the conditions above, with probability ? 1 as n ? ?, there exists
some ?, such that ?? = (??S , ??S c ) is the unique solution of the LASSO (Equation (1)), where
??j 6= 0, for all j ? S and ??S c = 0.
Remark. The first two conditions are trivial: Condition 1 can be obtained by rescaling while Condition 2 assumes that the design matrix composed of the true predictors in the model is not singular.
Condition 3 is a reasonable condition which means that the linear projection of the expected response to the space spanned by true predictors is not degenerated. Condition 4 is a little bit tricky;
it says that the irrelevant predictors (XS c ) are not very correlated with the ?residuals? of E(Y ) after
its projection onto XS . Condition 5 is always needed when considering LASSO?s model selection
consistency [26, 28]. The proof of the theorem is included in the supplementary material.
3.2.2 Adaptive Forward-Backward: SPORE-FoBa
Using all of the polynomial expansions of a feature subset is not flexible. In this section, we propose
the SPORE-FoBa algorithm, a more flexible algorithm using adaptive forward-backward searching
over the polynomially expanded data: during search step k with an active set T (k) , we examine one
new feature Xj , and consider a small candidate set which consists of the candidate feature Xj , its
higher order terms, and the (non-linear) interactions between previously selected features (indexed
by S) and candidate feature Xj with total degree up to d, i.e., terms with form
X
dl ? d.
(3)
Xjd1 ?l?S Xldl , with d1 > 0, dl ? 0, and d1 +
Algorithm 2 below is a short description of the SPORE-FoBa, which uses linear FoBa [27] at step
5and 6. The main idea of SPORE-FoBa is that a term from the candidate set is added into the model
if and only if adding this term makes the residual sum of squares (RSS) decrease a lot. We scan all
of the terms in the candidate set and choose the one which makes the RSS drop most. If the drop in
the RSS is greater than a pre-specified value ?, we add that term to the active set, which contains the
currently selected terms by the SPORE-FoBa algorithm. When considering deleting one term from
the active set, we choose the one that makes the sum of residuals increase the least. If this increment
is small enough, we delete that term from our current active set.
Algorithm 2 SPORE-FoBa
Input: response Y , feature columns X1 , . . . , Xp , the maximum degree d
Output: polynomial terms and the weights
1: Let T = ?
2: while true do
3:
for j = 1, . . . , p do
4:
Let C be the candidate set that contains non-linear and interaction terms from Equation (3)
5:
Use Linear FoBa to select terms from C to form the new active set T .
6:
Use Linear FoBa to delete terms from T to form a new active set T .
7:
if no terms can be added or deleted then
8:
break

6

0.2

0.2

SPORE?LASSO
SPORE?FoBa

0.1

0.05

0
0

0.1

0.05

0.1

0.2
0.3
0.4
0.5
Percentage of Training data

(a) Lucene

0.6

0
0

SPORE?LASSO
SPORE?FoBa

0.15
Prediction Error

0.15
Prediction Error

Prediction Error

0.15

0.2

SPORE?LASSO
SPORE?FoBa

0.1

0.05

0.1

0.2
0.3
0.4
0.5
Percentage of Training data

(b) Find Maxima

0.6

0
0

0.1

0.2
0.3
0.4
0.5
Percentage of Training data

0.6

(c) Segmentation

Figure 1: Prediction errors of our algorithms across the three data sets varying training-set fractions.

4 Evaluation Results
We now experimentally demonstrate that our algorithms are practical, give highly accurate predictors for real problems with small training-set sizes, compare favorably in accuracy to other state-ofthe-art sparse-regression algorithms, and produce interpretable, intuitive models.
To evaluate our algorithms, we use as case studies three programs: the Lucene Search Engine [4],
and two image processing algorithms, one for finding maxima and one for segmenting an image
(both of which are implemented within the ImageJ image processing framework [3]). We chose
all three programs according to two criteria. First and most importantly, we sought programs with
high variability in the predicted measure (execution time), especially in the face of otherwise similar
inputs (e.g., image files of roughly the same size for image processing). Second, we sought programs
that implement reasonably complex functionality, for which an inexperienced observer would not
be able to trivially identify the important features.
Our collected datasets are as follows. For Lucene, we used a variety of text input queries from
two corpora: the works of Shakespeare and the King James Bible. We collected a data set with
n = 3840 samples, each of which consists of an execution time and a total of p = 126 automatically
generated features. The time values are in range of (0.88, 1.13) with standard deviation 0.19. For
the Find Maxima program within the ImageJ framework, we collected n = 3045 samples (from an
equal number of distinct, diverse images obtained from three vision corpora [1, 2, 5]), and a total of
p = 182 features. The execution time values are in range of (0.09, 2.99) with standard deviation
0.24. Finally, from the Segmentation program within the same ImageJ framework on the same image
set, we collected again n = 3045 samples, and a total of p = 816 features for each. The time values
are in range of (0.21, 58.05) with standard deviation 3.05. In all the experiments, we fix degree
d = 3 for polynomial expansion, and normalized each column of feature data into range [0, 1].
Prediction Error. We first show that our algorithms predict accurately, even when training on a
small number of samples, in both absolute and relative terms. The accuracy measure we use is the
P y?i ?yi
relative prediction error defined as n1t
| yi |, where nt is the size of the test data set, and y?i ?s
and yi ?s are the predicted and actual responses of test data, respectively.
We randomly split every data set into a training set and a test set for a given training-set fraction,
train the algorithms and measure their prediction error on the test data. For each training fraction,
we repeat the ?splitting, training and testing? procedure 10 times and show the mean and standard
deviation of prediction error in Figure 1. We see that our algorithms have high prediction accuracy,
even when training on only 10% or less of the data (roughly 300 - 400 samples). Specifically,
both of our algorithms can achieve less than 7% prediction error on both Lucene and Find Maxima
datasets; on the segmentation dataset, SPORE-FoBa achieves less than 8% prediction error, and
SPORE-LASSO achieves around 10% prediction error on average.
Comparisons to State-of-the-Art. We compare our algorithms to several existing sparse regression
methods by examining their prediction errors at different sparsity levels (the number of features used
in the model), and show our algorithms can clearly outperform LASSO, FoBa and recently proposed
non-parametric greedy methods [18] (Figure 2). As a non-parametric greedy algorithm, we use Additive Forward Regression (AFR), because it is faster and often achieves better prediction accuracy
than Generalized Forward Regression (GFR) algorithms. We use the Glmnet Matlab implementa7

tion of LASSO and to obtain the LASSO solution path [10]. Since FoBa and SPORE-FoBa naturally
produce a path by adding or deleting features (or terms), we record the prediction error at each step.
When two steps have the same sparsity level, we report the smallest prediction error. To generate
the solution path for SPORE-LASSO, we first use Glmnet to generate a solution path for linear
LASSO; then at each sparsity level k, we perform full polynomial expansion with d = 3 on the
selected k features, obtain a solution path on the expanded data, and choose the model with the
smallest prediction error among all models computed from all active feature sets of size k. From the
figure, we see that our SPORE algorithms have comparable performance, and both of them clearly
achieve better prediction accuracy than LASSO, FoBa, and AFR. None of the existing methods can
build models within 10% of relative prediction error. We believe this is because execution time of a
computer program often depends on non-linear combinations of different features, which is usually
not well-handled by either linear methods or the additive non-parametric methods. Instead, both of
our algorithms can select 2-3 high-quality features and build models with non-linear combinations
of them to predict execution time with high accuracy.

Prediction Error

Prediction Error

0.4
0.3
0.2
0.1
0
1

LASSO
FoBa
AFR
SPORE?LASSO
SPORE?FoBa

0.5
0.4
0.3
0.2
0.1

2

3

4
Sparsity

5

(a) Lucene

6

7

0
1

LASSO
FoBa
AFR
SPORE?LASSO
SPORE?FoBa

0.5
Prediction Error

LASSO
FoBa
AFR
SPORE?LASSO
SPORE?FoBa

0.5

0.4
0.3
0.2
0.1

2

3

4
Sparsity

5

(b) Find Maxima

6

7

0
1

2

3

4
Sparsity

5

6

7

(c) Segmentation

Figure 2: Performance of the algorithms: relative prediction error versus sparsity level.
Model Interpretability. To gain better understanding, we investigate the details of the model constructed by SPORE-FoBa for Find Maxima. Our conclusions are similar for the other case studies,
but we omit them due to space. We see that with different training set fractions and with different
sparsity configurations, SPORE-FoBa can always select two high-quality features from hundreds of
automatically generated ones. By consulting with experts of the Find Maxima program, we find that
the two selected features correspond to the width (w) and height (h) of the region of interest in the
image, which may in practice differ from the actual image width and height. Those are indeed the
most important factors for determining the execution time of the particular algorithm used. For a
10% training set fraction and ? = 0.01, SPORE-FoBa obtained
f (w, h) = 0.1 + 0.22w + 0.23h + 1.93wh + 0.24wh2
which uses non-linear feature terms(e.g., wh, wh2 ) to predict the execution time accurately (around
5.5% prediction error). Especially when Find Maxima is used as a component of a more complex
image processing pipeline, this model would not be the most obvious choice even an expert would
pick. On the contrary, as observed in our experiments, neither the linear nor the additive sparse
methods handle well such nonlinear terms, and result in inferior prediction performance. A more
detailed comparison across different methods is the subject of our on-going work.

5 Conclusion
In this paper, we proposed the SPORE (Sparse POlynomial REgression) methodology to build the
relationship between execution time of computer programs and features of the programs. We introduced two algorithms to learn a SPORE model, and showed that both algorithms can predict
execution time with more than 93% accuracy for the applications we tested. For the three test cases,
these results present a significant improvement (a 40% or more reduction in prediction error) over
other sparse modeling techniques in the literature when applied to this problem. Hence our work
provides one convincing example of using sparse non-linear regression techniques to solve real
problems. Moreover, the SPORE methodology is a general methodology that can be used to model
computer program performance metrics other than execution time and solve problems from other
areas of science and engineering.
8

References
[1] Caltech 101 Object Categories. http://www.vision.caltech.edu/Image_Datasets/
Caltech101/Caltech101.html.
[2] Event Dataset. http://vision.stanford.edu/lijiali/event_dataset/.
[3] ImageJ. http://rsbweb.nih.gov/ij/.
[4] Mahout. lucene.apache.org/mahout.
[5] Visual Object Classes Challenge 2008. http://pascallin.ecs.soton.ac.uk/challenges/
VOC/voc2008/.
[6] S. Chen, K. Joshi, M. A. Hiltunen, W. H. Sanders, and R. D. Schlichting. Link gradients: Predicting the
impact of network latency on multitier applications. In INFOCOM, 2009.
[7] B.-G. Chun, L. Huang, S. Lee, P. Maniatis, and M. Naik. Mantis: Predicting system performance through
program analysis and modeling. Technical Report, 2010. arXiv:1010.0019v1 [cs.PF].
[8] D. Donoho. For most large underdetermined systems of equations, the minimal 1-norm solution is the
sparsest solution. Communications on Pure and Applied Mathematics, 59:797829, 2006.
[9] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Annals of Statistics,
32(2):407?499, 2002.
[10] J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1), 2010.
[11] A. Ganapathi, H. Kuno, U. Dayal, J. L. Wiener, A. Fox, M. Jordan, and D. Patterson. Predicting multiple
metrics for queries: Better decisions enabled by machine learning. In ICDE, 2009.
[12] S. Goldsmith, A. Aiken, and D. Wilkerson. Measuring empirical computational complexity. In FSE,
2007.
[13] C. Gupta, A. Mehta, and U. Dayal. PQR: Predicting query execution times for autonomous workload
management. In ICAC, 2008.
[14] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer, 2009.
[15] M. Isard, V. Prabhakaran, J. Currey, U. Wieder, K. Talwar, and A. Goldberg. Quincy: fair scheduling for
distributed computing clusters. In Proceedings of SOSP?09, 2009.
[16] S.-J. Kim, K. Koh, M. Lustig, S. Boyd, and D. Gorinevsky. An interior-point method for large-scale
l1-regularized least squares. IEEE Journal on Selected Topics in Signal Processing, 1(4):606?617, 2007.
[17] Z. Li, M. Zhang, Z. Zhu, Y. Chen, A. Greenberg, and Y.-M. Wang. WebProphet: Automating performance
prediction for web services. In NSDI, 2010.
[18] H. Liu and X. Chen. Nonparametric greedy algorithm for the sparse learning problems. In NIPS 22, 2009.
[19] M. Osborne, B. Presnell, and B. Turlach. On the lasso and its dual. Journal of Computational and
Graphical Statistics, 9(2):319?337, 2000.
[20] P. Ravikumar, J. Lafferty, H. Liu, and L. Wasserman. Sparse additive models. Journal of the Royal
Statistical Society: Series B(Statistical Methodology), 71(5):1009?1030, 2009.
[21] P. Ravikumar, V. Vu, B. Yu, T. Naselaris, K. Kay, J. Gallant, and C. Berkeley. Nonparametric sparse hierarchical models describe v1 fmri responses to natural images. Advances in Neural Information Processing
Systems (NIPS), 21, 2008.
[22] S. A. Seshia and A. Rakhlin. Game-theoretic timing analysis. In Proceedings of the IEEE/ACM International Conference on Computer-Aided Design (ICCAD), pages 575?582. IEEE Press, Nov. 2008.
[23] S. A. Seshia and A. Rakhlin. Quantitative analysis of systems using game-theoretic learning. ACM
Transactions on Embedded Computing Systems (TECS), 2010. To appear.
[24] M. Tariq, A. Zeitoun, V. Valancius, N. Feamster, and M. Ammar. Answering what-if deployment and
configuration questions with wise. In ACM SIGCOMM, 2008.
[25] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal. Statist. Soc B., 1996.
[26] M. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using l1-constrained
quadratic programming (Lasso). IEEE Trans. Information Theory, 55:2183?2202, 2009.
[27] T. Zhang. Adaptive forward-backward greedy algorithm for sparse learning with linear models. Advances
in Neural Information Processing Systems, 22, 2008.
[28] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning Research,
7:2563, 2006.
[29] H. Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical Association,
101(476):1418?1429, 2006.

9

"
1036,2001,An Efficient Clustering Algorithm Using Stochastic Association Model and Its Implementation Using Nanostructures,Abstract Missing,"An Efficient Clustering Algorithm Using
Stochastic Association Model and Its
Implementation Using Nanostructures

Takashi Morie, Tomohiro Matsuura, Makoto Nagata, and Atsushi Iwata
Graduate School of Advanced Sciences of Matter, Hiroshima University
Higashi-hiroshima, 739-8526 Japan.
http://www.dsl.hiroshima-u.ac.jp
morie@dsl.hiroshima-u.ac.jp

Abstract
This paper describes a clustering algorithm for vector quantizers using a
?stochastic association model?. It offers a new simple and powerful softmax adaptation rule. The adaptation process is the same as the on-line
K-means clustering method except for adding random fluctuation in the
distortion error evaluation process. Simulation results demonstrate that
the new algorithm can achieve efficient adaptation as high as the ?neural
gas? algorithm, which is reported as one of the most efficient clustering
methods. It is a key to add uncorrelated random fluctuation in the similarity evaluation process for each reference vector. For hardware implementation of this process, we propose a nanostructure, whose operation
is described by a single-electron circuit. It positively uses fluctuation in
quantum mechanical tunneling processes.

1 Introduction
Vector quantization (VQ) techniques are used in a wide range of applications, including
speech and image processing, data compression. VQ techniques encode a data manifold
using only a finite set of reference vectors
. A data vector
is represented by the best-matching or ?winning? reference vector , which minimizes
the average distortion error:



where

'

	

 




&%(' -,
  !
# ""$  
 )+* 


.) is the probability distribution of data vectors over manifold

(1)
.

Various clustering algorithms to obtain the best reference vectors have been reported. Here,
we treat on-line training, in which the data point distribution is not given a priori, but instead
a stochastic sequence of incoming sample data points drives the adaptation procedure.
The straightforward approach is the well-known on-line K-means clustering algorithm, in
which only the nearest reference vector to the sample vector is adjusted;

/ 0132450  6
7
.8+9""$0:&

(2)

2

50

where, is the step size and  is the Kronecker delta. However, this simple clustering
algorithm is often stuck in a local minimum. To avoid this difficulty, a common approach
is to introduce a ?soft-max? adaptation rule that not only adjusts the ?winning? reference
vector but affects other reference vectors depending on their proximity to .


The maximum-entropy (ME) algorithm [1] adjusts all reference vectors 0 depending on
the Euclidean distance to  ;
/  0 324   
(3)
1
    
 
.8+9""$ 0 &











	





 

where parameter  defines the proximity.

/ 01324 
    
. 
8+9""$0 

The Kohonen?s self-organization map (SOM) algorithm [2] is another well-known model;


 



(4)

In this model, every reference vector is assigned to a site of a lattice. Each time a sample
vector is presented, not only the ?winning? reference vector is adjusted but also the reference vectors assigned to the lattice sites adjacent to the winner are updated according to
function    , which is typically chosen to be a Gaussian:


  




   



 



 

 	 


!  
"" 

%

 




(5)

where # is a parameter that defines the proximity.
The neural-gas (NG) clustering algorithm [3] is a powerful soft-max adaptation rule, in
which all reference vectors are adjusted depending on the ?neighborhood ranking?;

/ 0)2  
 0
. ++96
.7
8+ "" 0  
where 0
. +# is the ranking, which depends on  and the whole set 

 0  is typically as follows:

 7

$%

'&

&



%

(6)
. The function

(&



+* "" %

(7)
where parameter , defines the proximity. This algorithm exhibits faster convergence to
smaller distortion errors, however consumes higher computational power especially for
sorting. An efficient version of the NG clustering that adjusts only several reference vectors
having upper ranking was also proposed [4].
)%

(&

In the next section, we propose a new efficient soft-max adaptation algorithm. It employs the stochastic association model that we have proposed related to single-electron
circuits [5], [6]. In Sec. 3, it is demonstrated from simulation results that this new clustering algorithm is as powerful as the other algorithms. In Sec. 4, we propose a nanostructure
based on a single-electron circuit for implementing the stochastic association model.

2 Stochastic association algorithm
A usual associative memory is defined as a system that deterministically extracts the vector
most similar to the input vector from the stored reference vectors. This just corresponds to
the process choosing the winning reference vector for a certain data vector in all conventional clustering algorithms.
In our stochastic association (SA) model, the association probability depends on the similarity between the input and the reference vectors. The SA algorithm extracts not only the
reference vector most similar to the input but also other similar reference vectors with the
probability depending on the similarity.

0



0

In the SA algorithm, stochastic fluctuation is added in the evaluation process of distortion
error - between data vector and reference vector . We propose this algorithm inspired

?(rn ?Dn )

Pr

Ri






Rn

i

A (rn)

Di

rn

wi

Distance

Dn

wn

Figure 1: Probability distribution in evaluation of the distortion error between the data
vector and each reference vector.



by the quantum mechanical property of single-electron circuits as described in Sec. 4, and
we expect that such fluctuation helps to avoid getting stuck in local minima of .

#"" 0 0

The distortion error - can be the squared Euclidean distance
distance
. The evaluation result is represented by



 0  0 


6

-

0

 ""# 0

%

or the Manhattan

0

(8)

where is a random variable with probability distribution function
. Therefore, the
is also considered as a random variable. The probability that
has
evaluation result
value is represented by

(9)

	0



  0)	04
	0 "" 0:
The winning reference vector # is determined by
- 0  
  0 
(10)

The probability that reference vector  becomes the winner when  has value 	  for a
certain data vector is given by the product of  
	""   and the probability that 0!
	  #"" %'$ & as shown in Fig. 1. Therefore, the probability that   becomes the winner is
given by integrating it with 	 ;


 &(  *) *+	,- 
.	, ""  0/ 2 0 
.	, 
(11)
)
#
0
0
1

2 0 
.	, 43  )  
.	 "" 0:(*8	
(12)
576




-

)



-



-





If the winning reference vector is updated as expressed by eq. (2), the SA model can provide
a new soft-max adaptation rule. Figure 2 shows an architecture for clustering processing
using the SA model. The distortion error between the input vector and each stored reference vector is evaluated in parallel with stochastic fluctuation. The winner-take-all circuit
deterministically extracts the winner, and the winning reference vector is only updated with
a constant value. As in the K-means algorithm, only one reference vector is adjusted for
each adaptation step and the update value for the selected reference vector is independent of
similarity or proximity. However, unlike the K-means algorithm, the adjusted vector is not
always the most similar reference vector, and sometimes other similar vectors are adjusted.
The total adjusting tendency in the SA algorithm seems similar to the NG or ME algorithm
because the probability of reference vector selection is determined by the neighborhood
ranking and the distances between each reference vector and a given data vector.

update only one vector













 
 
 
 
 
 
 
 
 
 
 










 


Reference vectors

wi

Input vector v




distortion error

evaluation with

stochastic fluctuation ?












Winner-Take-All

wc
Figure 2: Architecture for clustering processing using the SA model.

(a) SA

(b) ME

tmax = 5000

t=0

tmax = 50000

Figure 3: Test problem and clustering results by SA and ME algorithms. Data samples uniformly distribute in square regions, and points represent reference vectors. Both algorithms
use the same initial state.

3 Simulation results
In order to test the performance of the SA algorithm in minimizing the distortion error and
to compare it with the other soft-max approaches, we performed the same simulation of
model clustering described by Ref. [3]. The data clusters are of square shape within a twodimensional input space as shown in Fig. 3. In the simulation, the number of clusters was
15, and that of reference vectors was 60. We averaged the results of 50 simulation runs for
each of which not only the initialization of the reference vectors were chosen randomly but
also the 15 clusters were placed randomly.

0

The SA algorithm in this simulation used the squared Euclidean distance as a distortion
error - and the normal distribution as the probability distribution of the stochastic fluctuation;

%



.67 

	    3     
+""   % 
%



(13)


   			 



 

 
	




   		 









 
		







 
   			 



  		 



 
		





 
		





Performance ?

2

SOM

1

Maximum-entropy(ME)

algorithm

parameter




ME
SOM
NG
SA
All







initial

final

1
2
10
0.2
0.5

10000
0.01
0.01
0.0001
0.005

Stochastic association (SA)

Neural-gas (NG)

0

50000
Total number of adaptation steps tmax

Figure 4: Clustering performance of SA algorithm comparing with other clustering methods. The optimized parameters used in the simulation are also shown.

Figure 3 shows an example of clustering by the SA algorithm compared with that by the
ME algorithm. The result of the SA algorithm demonstrates nearly perfect clustering for
. In contrast, the clustering result by the ME algorithm is not so good
although the parameters used were optimized.

8 	 	 	 	

Here, all the clustering algorithms including the SA algorithm use an annealing procedure
to escape local minima. The parameters were gradually reduced during adaptation:

 
.8+7  0 
  "" ! 0 # #%$'&)(
""

8



*%+   )9(2


#

,

(14)

where
is the total number of adaptation steps. The values optimized by numerous
preliminary simulations are shown in Fig. 4, which were used in the simulation described
here.

, .-  
8 7""  /10 ! /
8 

2/

In order to compare the performance of the algorithms, we used a performance measure
, where
is the minimal distortion error in this problem. The
relationships between
and for the four algorithms are shown in Fig. 4. The clustering performance of the SA algorithm is nearly equal to that of the NG algorithm, which is
the most efficient clustering method in this test problem. The number of adaptation steps
to reach the steady state and the distortion error at the steady state in the SA algorithm are
nearly the same as those in the NG algorithm.

,

3

We also performed other simulations, one of which was vector quantization of a real image
(?Lena?, 256 256 pixels, 8-bit grayscale). In this case, the SOM demonstrated the best
performance, and the SA algorithm also had the nearly equal performance.
Consequently, comparing with the other soft-max algorithms, the SA algorithm has nearly
the best clustering performance. Moreover, it does not require a sorting process unlike the
NG algorithm nor a searching process of adjacent lattice sites unlike the SOM; only one
reference vector is adjusted per adaptation step. Thus, the computational power required
by the SA algorithm is much less than that required by the other soft-max algorithm. If the
number of reference vectors is , the total updating steps of reference vectors in the SA
algorithm are
times as many as those in the other algorithms. Thus, the SA algorithm
is the most efficient clustering method.


!



D1

Vd2

Dc

Energy

Ah



Vr3
Vr2

Av
Co

MOSFET

(b)


C2

Ah

C1

Vd2
Vd1

C3

: Electron e M C2
Cj : 0.1aF
C1 : 0.06aF
C2 : 0.02aF
C3 (parasitic) : 0.002aF
Co : 100aF

D1

Vr1

Dv1

Av

Ne ~ ? |Vdi ? Vri|

Co

D1
Position of eM

10-4

10-6

10-8

H-H state
L-H state

i

200

Vbg

Dc

0

t0

(d)

D5
Dv3

D1
Position of eM

-400

C1

Cj

Dc

Dc
400

Position of eM

Vr2

D5

D1

-400

Data unmatched (L-H state)

Energy

Vr1



0

Position of eM

Energy (meV)

Vd1

D1

400
Energy (meV)




 




Data matched (H-H state)

(c)

Vd3

eM moving time tM (sec)

(a)

300
Temperature (K)

2
	



Figure 5: Nanostructure evaluating Hamming distance. (a) Schematic of nanostructure,
where dot arrays are extremely enlarged compared with a MOSFET to emphasize the dot
structures. (b) Single-electron circuit. (c) Potential profile in dot array
. (d) 
moving
time for bit comparator operation.

4 Nanostructure implementing SA model
The key for implementing the SA model is adding random fluctuation as expressed by
eq. (8). We have already proposed single-electron circuits and nanostructures evaluating
Hamming distance for the SA model [5]-[9].
Figure 5(a) and (b) show a nanostructure and the corresponding single-electron circuit, respectively, which are the most sophisticated version of our circuits and structures [9]. The
nanostructure consists of plural ( ) dot structures arranged on a MOS transistor gate electrode. Each dot structure consists of 1-D dot arrays
() and
(), where means the number of dots at a side of
. (From Monte
Carlo single-electron circuit simulation, should be more than 3). The dot diameter assumed is around 1 nm. The capacitance
corresponds to the gate capacitance of an
ultrasmall MOS transistor. An electron 
is introduced in array , which is for example
performed by using Fowler-Nordheim tunneling from the grounded plate over - . Electron

, which is initially located at - , can move along array
through tunneling junctions
, but it cannot move to
through the normal capacitor . Digital (High/Low) voltages
and
(
) are applied at both edges of
, which correspond to
elements of data and reference vectors, respectively. Each dot structure simply works as an
exclusive-NOR logic gate (bit comparator) with random fluctuation as explained below.

2


 %





&



2


5 0    

  0



 



&

 

2	 
 &    2  	   
2 	

2	%
 
2 	

 



If the two digital data bits ( or ) are matched, electron 
stabilizes at center dot - ,
otherwise 
moves to an off-center position. After stabilizing  , by changing voltages

 0 5 0 


2



2

 





,
and back-gate voltage
, vertical dot array
detects whether 
stays at - or

not; only if
stays at - ,
is polarized and an electron is induced at the gate electrode
of . The total number of induced electrons (  ) is proportional to the number of dot
structures with matched bits; thus the Hamming distance can be measured by counting the
induced electrons using the ultrasmall MOS transistor. (If one of the input digital data is
applied through an inverter, the number of unmatched bits can be calculated).







	

2



The detail of operation stabilizing 
is as follows: Because of the charging energy of 
itself, the total energy as a function of the position of 
in array
has two peaks at the
midpoints of each side of the array, and has minimal values at - and both of - as shown
in Fig. 5(c). The energy barrier height for 
at is assumed larger than the thermal
energy at room temperature.

















In L-L state, the energy at rises up, thus 
is most strongly stabilized at - . On
the other hand, in H-L(L-H) or H-H state, the energy barrier is lower than that of L-L
can more easily overcome the barrier by using thermal noise. Figure 5(d)
state, thus 
shows the relation between operation temperature and time ( ) required until 
moves
to - , which was obtained by Monte Carlo single-electron circuit simulation. The moving
process assisted by thermal noise is purely stochastic, thus
scatters in a wide range.
However, because the energy barrier height in H-L(L-H) states is lower than that in H-H
state as shown in Fig. 5(c), there exists a certain time span within which 
in H-L(L-H)
in H-H state stays at - . At room temperature (300K), is
states moves to - while 
several microseconds in this case although depends on the tunneling resistance. If the
detection process starts after , nearly perfect exclusive-NOR (bit comparison) operation
is achieved. On the other hand, if the start timing is shifted from , arbitrary amount of
fluctuation can be included in the bit comparison result. Thus, we utilize quantum mechanical tunneling processes assisted by thermal noise in this structure, which is similar to a
phenomenon known as stochastic resonance.







8/

8/

/
8


8
8





8/

8/

Although digital data are treated in the above explanation, analog data can be treated in the
same circuit by using pulse-width modulation (PWM) signals, which have a digital amplitude and an analog pulse width [10]. Therefore, instead of the Hamming distance, the
Manhattan distance can be evaluated by using this nanostructure. Because random fluctuation is naturally added in our nanostructure, it can implement the calculation expressed
by eq. (8). The annealing procedure described by eqs. (13) and (14) can be performed by
changing the time scale in the stabilization operation; that means the scaling of pulse-width
modulation.
The proposed nanostructure has not yet been fabricated using the present VLSI technology,
but the basic technology related to nanocrystalline floating-dot MOSFET devices, which
are closely related to our structure, is now being developed [11]-[13]. Furthermore, wellcontrolled self-assembly processes using molecular manipulation technology, especially
using DNA [14], would be utilized to fabricate our nanostructure. Thus, it could be constructed in the near future.

5 Conclusions
The stochastic association algorithm offers a simple and powerful soft-max adaptation rule
for vector quantizers. Although it is the same as the simple on-line K-means clustering
method except for adding random fluctuation in the distortion error evaluation process, our
new method has an efficient adaptation performance as high as the neural-gas (NG) or the
SOM algorithms. Moreover, our method needs no additional process such as sorting and
only one reference vector is adjusted at each adaptation step; thus the computational effort
is much smaller compared with the conventional soft-max clustering algorithms.

By employing the nanostructure proposed in this paper, very high performance clustering
hardware could be constructed.

Acknowledgments
The authors wish to thank Prof. Masataka Hirose for his support and encouragement. This
work has been supported in part by Grants-in-aid for the Core Research for Evolutional
Science and Technology (CREST) from Japan Science and Technology Corporation(JST).

References
[1] K. Rose, E. Gurewitz, and G. C. Fox, ?Statistical Mechanics and Phase Transitions in Clustering,? Physical Review Letters, vol. 65, no. 8, pp. 945?948, 1990.
[2] T. Kohonen, Self-Organization and Associative Memory, Springer-Verlag, Berlin, 1984.
[3] T. M. Martinetz, S. G. Berkovich, and K. J. Schulten, ??Neural-Gas? Network for Vector Quantization and its Apllication to Time-Series Prediction,? IEEE Trans. Neural Networks, vol. 4,
pp. 558?569, 1993.
[4] S. Rovetta and R. Zunino, ?Efficient Training of Neural Gas Vector Quantizers with Analog
Circuit Implementation,? IEEE Trans. Circuits & Syst., vol. 46, pp. 688?698, 1999.
[5] M. Saen, T. Morie, M. Nagata, and A. Iwata, ?A Stochastic Associative Memory Using SingleElectron Tunneling Devices,? IEICE Trans. Electron., vol. E81-C, no. 1, pp. 30?35, 1998.
[6] T. Yamanaka, T. Morie, M. Nagata, and A. Iwata, ?A Single-Electron Stochastic Associative Processing Circuit Robust to Random Background-Charge Effects and Its Structure Using
Nanocrystal Floating-Gate Transistors,? Nanotechnology, vol. 11, no. 3, pp. 154?160, 2000.
[7] T. Morie, T. Matsuura, S. Miyata, T. Yamanaka, M. Nagata, and A. Iwata, ?Quantum Dot Structures Measuring Hamming Distance for Associative Memories,? Superlattices & Microstructures, vol. 27, no. 5/6, pp. 613?616, 2000.
[8] T. Matsuura, T. Morie, M. Nagata, and A. Iwata, ?A Multi-Quantum-Dot Associative Circuit
Using Thermal-Noise Assisted Tunneling,? in Ext. Abs. of Int. Conf. on Solid State Devices and
Materials, pp. 306?307, Sendai, Japan, Aug. 2000.
[9] T. Morie, T. Matsuura, M. Nagata, and A. Iwata, ?Quantum Dot Structures Measuring Hamming
Distance for Associative Memories,? in Extended Abstracts, 4th International Workshop on
Quantum Functional Devices (QFD2000), pp. 210?213, Kanazawa, Japan, Nov. 2000.
[10] A. Iwata and M. Nagata, ?A Concept of Analog-Digital Merged Circuit Architecture for Future
VLSI?s,? IEICE Trans. Fundamentals., vol. E79-A, no. 2, pp. 145?157, 1996.
[11] S. Tiwari, F. Rana, H. Hanafi,A. Hartstein, E. F. Crabb?e, and K. Chan, ?A Silicon Nanocrystals
Based Memory,? Appl. Phys. Lett., vol. 68, no. 10, pp. 1377?1379, 1996.
[12] A. Kohno, H. Murakami, M. Ikeda, H. Nishiyama, S. Miyazaki, and M. Hirose, ?Transient
Characteristics of Electron Charging in Si-Quantum-Dot Floating Gate MOS Memories,? in
Ext. Abs. of Int. Conf. on Solid State Devices and Materials, pp. 124?125, Sendai, Japan, Aug.
2000.
[13] R. Ohba, N. Sugiyama, J. Koga, K. Uchida, and A. Toriumu, ?Novel Si Quantum Memory
Structure with Self-Alighed Stacked Nanocrystalline Dots,? in Ext. Abs. of Int. Conf. on Solid
State Devices and Materials, pp. 122?123, Sendai, Japan, Aug. 2000.
[14] R. A. Kiehl, ?Nanoelectronic Array Architecture,? in Extended Abstracts, 4th International
Workshop on Quantum Functional Devices (QFD2000), pp. 49?51, Kanazawa, Japan, Nov.
2000.

"
6656,2017,Adversarial Symmetric Variational Autoencoder,"A new form of variational autoencoder (VAE) is developed, in which the joint distribution of data and codes is considered in two (symmetric) forms: (i) from observed data fed through the encoder to yield codes, and (ii) from latent codes drawn from a simple prior and propagated through the decoder to manifest data. Lower bounds are learned for marginal log-likelihood fits observed data and latent codes. When learning with the variational bound, one seeks to minimize the symmetric Kullback-Leibler divergence of joint density functions from (i) and (ii), while simultaneously seeking to maximize the two marginal log-likelihoods. To facilitate learning, a new form of adversarial training is developed. An extensive set of experiments is performed, in which we demonstrate state-of-the-art data reconstruction and generation on several image benchmarks datasets.","Adversarial Symmetric Variational Autoencoder

Yunchen Pu, Weiyao Wang, Ricardo Henao, Liqun Chen, Zhe Gan, Chunyuan Li
and Lawrence Carin
Department of Electrical and Computer Engineering, Duke University
{yp42, ww109, r.henao, lc267, zg27,cl319, lcarin}@duke.edu

Abstract
A new form of variational autoencoder (VAE) is developed, in which the joint
distribution of data and codes is considered in two (symmetric) forms: (i) from
observed data fed through the encoder to yield codes, and (ii) from latent codes
drawn from a simple prior and propagated through the decoder to manifest data.
Lower bounds are learned for marginal log-likelihood fits observed data and latent
codes. When learning with the variational bound, one seeks to minimize the
symmetric Kullback-Leibler divergence of joint density functions from (i) and (ii),
while simultaneously seeking to maximize the two marginal log-likelihoods. To
facilitate learning, a new form of adversarial training is developed. An extensive
set of experiments is performed, in which we demonstrate state-of-the-art data
reconstruction and generation on several image benchmark datasets.

1

Introduction

Recently there has been increasing interest in developing generative models of data, offering the
promise of learning based on the often vast quantity of unlabeled data. With such learning, one
typically seeks to build rich, hierarchical probabilistic models that are able to fit to the distribution of
complex real data, and are also capable of realistic data synthesis.
Generative models are often characterized by latent variables (codes), and the variability in the codes
encompasses the variation in the data [1, 2]. The generative adversarial network (GAN) [3] employs
a generative model in which the code is drawn from a simple distribution (e.g., isotropic Gaussian),
and then the code is fed through a sophisticated deep neural network (decoder) to manifest the data.
In the context of data synthesis, GANs have shown tremendous capabilities in generating realistic,
sharp images from models that learn to mimic the structure of real data [3, 4, 5, 6, 7, 8]. The quality
of GAN-generated images has been evaluated by somewhat ad hoc metrics like inception score [9].
However, the original GAN formulation does not allow inference of the underlying code, given
observed data. This makes it difficult to quantify the quality of the generative model, as it is not
possible to compute the quality of model fit to data. To provide a principled quantitative analysis of
model fit, not only should the generative model synthesize realistic-looking data, one also desires the
ability to infer the latent code given data (using an encoder). Recent GAN extensions [10, 11] have
sought to address this limitation by learning an inverse mapping (encoder) to project data into the
latent space, achieving encouraging results on semi-supervised learning. However, these methods still
fail to obtain faithful reproductions of the input data, partly due to model underfitting when learning
from a fully adversarial objective [10, 11].
Variational autoencoders (VAEs) are designed to learn both an encoder and decoder, leading to
excellent data reconstruction and the ability to quantify a bound on the log-likelihood fit of the
model to data [12, 13, 14, 15, 16, 17, 18, 19]. In addition, the inferred latent codes can be utilized
in downstream applications, including classification [20] and image captioning [21]. However, new
images synthesized by VAEs tend to be unspecific and/or blurry, with relatively low resolution. These
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

limitations of VAEs are becoming increasingly understood. Specifically, the traditional VAE seeks to
maximize a lower bound on the log-likelihood of the generative model, and therefore VAEs inherit
the limitations of maximum-likelihood (ML) learning [22]. Specifically, in ML-based learning one
optimizes the (one-way) Kullback-Leibler (KL) divergence between the distribution of the underlying
data and the distribution of the model; such learning does not penalize a model that is capable of
generating data that are different from that used for training.
Based on the above observations, it is desirable to build a generative-model learning framework with
which one can compute and assess the log-likelihood fit to real (observed) data, while also being
capable of generating synthetic samples of high realism. Since GANs and VAEs have complementary
strengths, their integration appears desirable, with this a principal contribution of this paper. While
integration seems natural, we make important changes to both the VAE and GAN setups, to leverage
the best of both. Specifically, we develop a new form of the variational lower bound, manifested
jointly for the expected log-likelihood of the observed data and for the latent codes. Optimizing
this variational bound involves maximizing the expected log-likelihood of the data and codes, while
simultaneously minimizing a symmetric KL divergence involving the joint distribution of data and
codes. To compute parts of this variational lower bound, a new form of adversarial learning is invoked.
The proposed framework is termed Adversarial Symmetric VAE (AS-VAE), since within the model
(i) the data and codes are treated in a symmetric manner, (ii) a symmetric form of KL divergence is
minimized when learning, and (iii) adversarial training is utilized. To illustrate the utility of AS-VAE,
we perform an extensive set of experiments, demonstrating state-of-the-art data reconstruction and
generation on several benchmarks datasets.

2

Background and Foundations

Consider an observed data sample x, modeled as being drawn from p? (x|z), with model parameters
? and latent code z. The prior distribution on the code is denoted p(z), typically a distribution that is
easy to draw from, such as isotropic Gaussian. The posterior distribution on the code given data x
is p? (z|x), and since this is typically intractable, it is approximated as q? (z|x), parameterized by
learned parameters ?. Conditional distributions q? (z|x) and p? (x|z) are typically designed such
that they are easily sampled and, for flexibility, modeled in terms of neural networks [12]. Since z
is a latent code for x, q? (z|x) is also termed a stochastic encoder, with p? (x|z) a corresponding
stochastic decoder. The observed data are assumed drawn from q(x), for which we do not have a
explicit form, but from which we have samples, i.e., the ensemble {xi }i=1,N used for learning.
R
Our goal is to learn the model p? (x) = p? (x|z)p(z)dz such that it synthesizes samples that are
well matched to those drawn from q(x). We simultaneously seek to learn a corresponding encoder
q? (z|x) that is both accurate and efficient to implement. Samples x are synthesized via x ? p? (x|z)
with z ? p(z); z ? q? (z|x) provides an efficient coding of observed x, that may be used for other
purposes (e.g., classification or caption generation when x is an image [21]).
2.1

Traditional Variational Autoencoders and Their Limitations

Maximum likelihood (ML) learning of ? based on direct evaluation of p? (x) is typically intractable.
The VAE [12, 13] seeks to bound p? (x) by maximizing variational expression LVAE (?, ?), with
respect to parameters {?, ?}, where


p? (x, z)
LVAE (?, ?) = Eq? (x,z) log
= Eq(x) [log p? (x) ? KL(q? (z|x)kp? (z|x))] (1)
q? (z|x)
= ?KL(q? (x, z)kp? (x, z)) + const ,
(2)
with expectations Eq? (x,z) and Eq(x) performed approximately via sampling. Specifically, to evaluate
Eq? (x,z) we draw a finite set of samples z i ? q? (z i |xi ), with xi ? q(x) denoting the observed
data, and for Eq(x) , we directly use observed data xi ? q(x). When learning {?, ?}, the expectation
using samples from z i ? q? (z i |xi ) is implemented via the ?reparametrization trick? [12].
PN
Maximizing LVAE (?, ?) wrt {?, ?} provides a lower bound on N1 i=1 log p? (xi ), hence the VAE
PN
setup is an approximation to ML learning of ?. Learning ? based on N1 i=1 log p? (xi ) is equivalent
to learning ? based on minimizing KL(q(x)kp? (x)), again implemented in terms of the N observed
samples of q(x). As discussed in [22], such learning does not penalize ? severely for yielding x
2

of relatively high probability in p? (x) while being simultaneously of low probability in q(x). This
means that ? seeks to match p? (x) to the properties of the observed data samples, but p? (x) may
also have high probability of generating samples that do not look like data drawn from q(x). This is
a fundamental limitation of ML-based learning [22], inherited by the traditional VAE in (1).
One
R reason for the failing
R of ML-based learning of ? is that the cumulative posterior on latent codes
p? (z|x)q(x)dx ? q? (z|x)q(x)dx = q? (z) is typically different from p(z), which implies that
x ? p? (x|z), with z ? p(z) may yield samples x that are different from those generated from q(x).
Hence, when learning {?, ?} one may seek to match p? (x) to samples of q(x), as done in (1), while
simultaneously matching q? (z) to samples of p(z). The expression in (1) provides a variational
bound for matching p? (x) to samples of q(x), thus one may naively think to simultaneously set a
similar variational expression for q? (z), with these two variational expressions optimized jointly.
However, to compute this additional variational expression we require an analytic expression for
q? (x, z) = q? (z|x)q(x), which also means we need an analytic expression for q(x), which we do
not have.
Examining (2), we also note that LVAE (?, ?) approximates ?KL(q? (x, z)kp? (x, z)), which has
limitations aligned with those discussed above for ML-based learning of ?. Analogous to the above
discussion, we would also like to consider ?KL(p? (x, z)kq? (x, z)). So motivated, in Section 3 we
PN
develop a new form of variational lower bound, applicable to maximizing N1 i=1 log p? (xi ) and
P
M
1
j=1 log q? (z j ), where z j ? p(z) is the j-th of M samples from p(z). We demonstrate that this
M
new framework leverages both KL(p? (x, z)kq? (x, z)) and KL(q? (x, z)kp? (x, z)), by extending
ideas from adversarial networks.
2.2

Adversarial Learning

The original idea of GAN [3] was to build an effective generative model p? (x|z), with z ? p(z), as
discussed above. There was no desire to simultaneously design an inference network q? (z|x). More
recently, authors [10, 11, 23] have devised adversarial networks that seek both p? (x|z) and q? (z|x).
As an important example, Adversarial Learned Inference (ALI) [10] considers the following objective
function:
min max LALI (?, ?, ?) = Eq? (x,z) [log ?(f? (x, z))] + Ep? (x,z) [log(1 ? ?(f? (x, z)))] ,
?,?

?

(3)

where the expectations are approximated with samples, as in (1). The function f? (x, z), termed a
discriminator, is typically implemented using a neural network with parameters ? [10, 11]. Note that
in (3) we need only sample from p? (x, z) = p? (x|z)p(z) and q? (x, z) = q? (z|x)q(x), avoiding
the need for an explicit form for q(x).
The framework in (3) can, in theory, match p? (x, z) and q? (x, z), by finding a Nash equilibrium
of their respective non-convex objectives [3, 9]. However, training of such adversarial networks
is typically based on stochastic gradient descent, which is designed to find a local mode of a cost
function, rather than locating an equilibrium [9]. This objective mismatch may lead to the well-known
instability issues associated with GAN training [9, 22].
To alleviate this problem, some researchers add a regularization term, such as reconstruction loss
[24, 25, 26] or mutual information [4], to the GAN objective, to restrict the space of suitable mapping
functions, thus avoiding some of the failure modes of GANs, i.e., mode collapsing. Below we
will formally match the joint distributions as in (3), and reconstruction-based regularization will be
manifested by generalizing the VAE setup via adversarial learning. Toward this goal we consider the
following lemma, which is analogous to Proposition 1 in [3, 23].
Lemma 1 Consider Random Variables (RVs) x and z with joint distributions, p(x, z) and q(x, z).
The optimal discriminator D? (x, z) = ?(f ? (x, z)) for the following objective
max Ep(x,z) log[?(f (x, z))] + Eq(x,z) [log(1 ? ?(f (x, z)))] ,
f

(4)

is f ? (x, z) = log p(x, z) ? log q(x, z).
Under Lemma 1, we are able to estimate the log q? (x, z) ? log p? (x)p(z) and log p? (x, z) ?
log q(x)q? (z) using the following corollary.
3

Corollary 1.1 For RVs x and z with encoder joint distribution q? (x, z) = q(x)q? (z|x) and
decoder joint distribution p? (x, z) = p(z)p? (x|z), consider the following objectives:
max LA1 (? 1 ) = Ex?q(x),z?q? (z|x) log[?(f?1 (x, z))]
?1

(5)

+ Ex?p? (x|z0 ),z0 ?p(z),z?p(z) [log(1 ? ?(f?1 (x, z)))] ,
max LA2 (? 2 ) = Ez?p(z),x?p? (x|z) log[?(f?2 (x, z))]
?2

(6)
+ Ez?q? (z|x0 ),x0 ?q(x),x?q(x) [log(1 ? ?(f?2 (x, z)))] ,

If the parameters ? and ? are fixed, with f??1 the optimal discriminator for (5) and f??2 the optimal
discriminator for (6), then
f??1 (x, z) = log q? (x, z) ? log p? (x)p(z),

f??2 (x, z) = log p? (x, z) ? log q? (z)q(x) . (7)

The proof is provided in the Appendix A. We also assume in Corollary 1.1 that f?1 (x, z) and
f?2 (x, z) are sufficiently flexible such that there are parameters ? ?1 and ? ?2 capable of achieving
the equalities in (7). Toward that end, f?1 and f?2 are implemented as ? 1 - and ? 2 -parameterized
neural networks (details below), to encourage universal approximation [27].

3

Adversarial Symmetric Variational Auto-Encoder (AS-VAE)

Consider variational expressions
LVAEx (?, ?) = Eq(x) log p? (x) ? KL(q? (x, z)kp? (x, z))

(8)

LVAEz (?, ?) = Ep(z) log q? (z) ? KL(p? (x, z)kq? (x, z)) ,

(9)

where all expectations are again performed approximately using samples from q(x) and p(z). Recall
that Eq(x) log p? (x) = ?KL(q(x)kp? (x)) + const, and Ep(z) log p? (z) = ?KL(p(z)kq? (z)) +
const, thus (8) is maximized when q(x) = p? (x) and q? (x, z) = p? (x, z). Similarly, (9) is
maximized when p(z) = q? (z) and q? (x, z) = p? (x, z). Hence, (8) and (9) impose desired
constraints on both the marginal and joint distributions. Note that the log-likelihood terms in (8)
and (9) are analogous to the data-fit regularizers discussed above in the context of ALI, but here
implemented in a generalized form of the VAE. Direct evaluation of (8) and (9) is not possible, as it
requires an explicit form for q(x) to evaluate q? (x, z) = q? (z|x)q(x).
One may readily demonstrate that
LVAEx (?, ?) = Eq? (x,z) [log p? (x)p(z) ? log q? (x, z) + log p? (x|z)]
= Eq? (x,z) [log p? (x|z) ? f??1 (x, z)] .
A similar expression holds for LVAEz (?, ?), in terms of f??2 (x, z). This naturally suggests the
cumulative variational expression
LVAExz (?, ?, ? 1 , ? 2 ) = LVAEx (?, ?) + LVAEz (?, ?)
(10)
= Eq? (x,z) [log p? (x|z) ? f?1 (x, z)] + Ep? (x,z) [log q? (x|z) ? f?2 (x, z)] ,
where ? 1 and ? 2 are updated using the adversarial objectives in (5) and (6), respectively.
Note that to evaluate (10) we must be able to sample from q? (x, z) = q(x)q? (z|x) and
p? (x, z) = p(z)p? (x|z), both of which are readily available, as discussed above. Further, we
require explicit expressions for q? (z|x) and p? (x|z), which we have. For (5) and (6) we similarly
must be able to sample from the distributions involved, and we must be able to evaluate f?1 (x, z)
and f?2 (x, z), each of which is implemented via a neural network. Note as well that the bound in
(1) for Eq(x) log p? (x) is in terms of the KL distance between conditional distributions q? (z|x) and
p? (z|x), while (8) utilizes the KL distance between joint distributions q? (x, z) and p? (x, z) (use
of joint distributions is related to ALI). By combining (8) and (9), the complete variational bound
LVAExz employs the symmetric KL between these two joint distributions. By contrast, from (2),
the original variational lower bound only addresses a one-way KL distance between q? (x, z) and
p? (x, z). While [23] had a similar idea of employing adversarial methods in the context variational
learning, it was only done within the context of the original form in (1), the limitations of which were
discussed in Section 2.1.
4

In the original VAE, in which (1) was optimized, the reparametrization trick [12] was invoked
wrt q? (z|x), with samples z ? (x, ) and  ? N (0, I), as the expectation was performed wrt this
distribution; this reparametrization is convenient for computing gradients wrt ?. In the AS-VAE
in (10), expectations are also needed wrt p? (x|z). Hence, to implement gradients wrt ?, we
also constitute a reparametrization of p? (x|z). Specifically, we consider samples x? (z, ?) with
? ? N (0, I). LVAExz (?, ?, ? 1 , ? 2 ) in (10) is re-expressed as


LVAExz (?, ?, ? 1 , ? 2 ) = Ex?q(x),?N (0,I) f?1 (x, z ? (x, )) ? log p? (x|z ? (x, ))


+ Ez?p(z),??N (0,I) f?2 (x? (z, ?), z) ? log q? (z|x? (z, ?)) .
(11)
The expectations in (11) are approximated via samples drawn from q(x) and p(z), as well as samples
of  and ?. x? (z, ?) and z ? (x, ) can be implemented with a Gaussian assumption [12] or via
density transformation [14, 16], detailed when presenting experiments in Section 5.
The complete objective of the proposed Adversarial Symmetric VAE (AS-VAE) requires the cumulative variational in (11), which we maximize wrt ? 1 and ? 1 as in (5) and (6), using the results in (7).
Hence, we write
min max ?LVAExz (?, ?, ? 1 , ? 2 ) .
(12)
?,? ? 1 ,? 2

The following proposition characterizes the solutions of (12) in terms of the joint distributions of x
and z.
Proposition 1 The equilibrium for the min-max objective in (12) is achieved by specification
{? ? , ?? , ? ?1 , ? ?2 } if and only if (7) holds, and p?? (x, z) = q?? (x, z).
The proof is provided in the Appendix A. This theoretical result implies that (i) ? ? is an estimator that
yields good reconstruction, and (ii) ?? matches the aggregated posterior q? (z) to prior distribution
p(z).

4

Related Work

VAEs [12, 13] represent one of the most successful deep generative models developed recently.
Aided by the reparameterization trick, VAEs can be trained with stochastic gradient descent. The
original VAEs implement a Gaussian assumption for the encoder. More recently, there has been a
desire to remove this Gaussian assumption. Normalizing flow [14] employs a sequence of invertible
transformation to make the distribution of the latent codes arbitrarily flexible. This work was followed
by inverse auto-regressive flow [16], which uses recurrent neural networks to make the latent codes
more expressive. More recently, SteinVAE [28] applies Stein variational gradient descent [29] to
infer the distribution of latent codes, discarding the assumption of a parametric form of posterior
distribution for the latent code. However, these methods are not able to address the fundamental
limitation of ML-based models, as they are all based on the variational formulation in (1).
GANs [3] constitute another recent framework for learning a generative model. Recent extensions of
GAN have focused on boosting the performance of image generation by improving the generator [5],
discriminator [30] or the training algorithm [9, 22, 31]. More recently, some researchers [10, 11, 33]
have employed a bidirectional network structure within the adversarial learning framework, which in
theory guarantees the matching of joint distributions over two domains. However, non-identifiability
issues are raised in [32]. For example, they have difficulties in providing good reconstruction in latent
variable models, or discovering the correct pairing relationship in domain transformation tasks. It was
shown that these problems are alleviated in DiscoGAN [24], CycleGAN [26] and ALICE [32] via
additional `1 , `2 or adversarial losses. However, these methods lack of explicit probabilistic modeling
of observations, thus could not directly evaluate the likelihood of given data samples.
A key component of the proposed framework concerns integrating a new VAE formulation with
adversarial learning. There are several recent approaches that have tried to combining VAE and
GAN [34, 35], Adversarial Variational Bayes (AVB) [23] is the one most closely related to our work.
AVB employs adversarial learning to estimate the posterior of the latent codes, which makes the
encoder arbitrarily flexible. However, AVB seeks to optimize the original VAE formulation in (1),
and hence it inherits the limitations of ML-based learning of ?. Unlike AVB, the proposed use of
adversarial learning is based on a new VAE setup, that seeks to minimize the symmetric KL distance
between p? (x, z) and q? (x, z), while simultaneously seeking to maximize the marginal expected
likelihoods Eq(x) [log p? (x)] and Ep(z) [log p? (z)].
5

5

Experiments

We evaluate our model on three datasets: MNIST, CIFAR-10 and ImageNet. To balance performance
and computational cost, p? (x|z) and q? (z|x) are approximated with a normalizing flow [14] of
length 80 for the MNIST dataset, and a Gaussian approximation for CIFAR-10 and ImageNet data.
All network architectures are provided in the Appendix B. All parameters were initialized with Xavier
[36], and optimized via Adam [37] with learning rate 0.0001. We do not perform any dataset-specific
tuning or regularization other than dropout [38]. Early stopping is employed based on average
reconstruction loss of x and z on validation sets.
We show three types of results, using part of or all of our model to illustrate each component. i)
AS-VAE-r: This model trained with the first half of the objective in (11) to minimize LVAEx (?, ?)
in (8); it is an ML-based method which focuses on reconstruction. ii) AS-VAE-g: This model trained
with the second half of the objective in (11) to minimize LVAEz (?, ?) in (9); it can be considered as
maximizing the likelihood of q? (z), and designed for generation. iii) AS-VAE This is our proposed
model, developed in Section 3.
5.1

Evaluation

We evaluate our model on both reconstruction and generation. The performance of the former is
evaluated using negative log-likelihood (NLL) estimated via the variational lower bound defined
in (1). Images are modeled as continuous. To do this, we add [0, 1]-uniform noise to natural images
(one color channel at the time), then divide by 256 to map 8-bit images (256 levels) to the unit
interval. This technique is widely used in applications involving natural images [12, 14, 16, 39, 40],
since it can be proved that in terms of log-likelihood, modeling in the discrete space is equivalent
to modeling in the continuous space (with added noise) [39, 41]. During testing, the likelihood is
computed as p(x = i|z) = p? (x ? [i/256, (i + 1)/256]|z) where i = 0, . . . , 255. This is done to
guarantee a fair comparison with prior work (that assumed quantization). For the MNIST dataset, we
treat the [0, 1]-mapped continuous input as the probability of a binary pixel value (on or off) [12]. The
inception score (IS), defined as exp(Eq (x)KL(p(y|x)kp(y))), is employed to quantitatively evaluate
the quality of generated natural images, where p(y) is the empirical distribution of labels (we do not
leverage any label information during training) and p(y|x) is the output of the Inception model [42]
on each generated image.
To the authors? knowledge, we are the first to report both inception score (IS) and NLL for natural
images from a single model. For comparison, we implemented DCGAN [5] and PixelCNN++ [40] as
baselines. The implementation of DCGAN is based on a similar network architectures as our model.
Note that for NLL a lower value is better, whereas for IS a higher value is better.
5.2

MNIST

We first evaluate our model on the MNIST dataset. The log-likelihood results are summarized in
Table 1. Our AS-VAE achieves a negative log-likelihood of 82.51 nats, outperforming normalizing
flow (85.1 nats) with a similar architecture. The perfomance of AS-VAE-r (81.14 nats) is competitive
to the state-of-the-art (79.2 nats). The generated samples are showed in Figure 1. AS-VAE-g and
AS-VAE both generate good samples while the results of AS-VAE-r are slightly more blurry, partly
due to the fact that AS-VAE-r is an ML-based model.
5.3

CIFAR

Next we evaluate our models on the CIFAR-10 dataset. The quantitative results are listed in Table 2.
AS-VAE-r and AS-VAE-g achieve encouraging results on reconstruction and generation, respectively,
while our AS-VAE model (leveraging the full objective) achieves a good balance between these
two tasks, which demonstrates the benefit of optimizing a symmetric objective. Compared with

Table 1: NLL on MNIST.
Method

NF (k=80) [14]

IAF [16]

AVB [23]

PixelRNN [39]

AS-VAE-r

AS-VAE-g

AS-VAE

NLL (nats)

85.1

80.9

79.5

79.2

81.14

146.32

82.51

6

state-of-the-art ML-based models [39, 40], we achieve competitive results on reconstruction but
provide a much better performance on generation, also outperforming other adversarially-trained
models. Note that our negative ELBO (evidence lower bound) is an upper bound of NLL as reported
in [39, 40]. We also achieve a smaller root-mean-square-error (RMSE). Generated samples are shown
in Figure 2. Additional results are provided in the Appendix C.
ALI [10], which also seeks to match Table 2: Quantitative Results on CIFAR-10; ? 2.96 is based on our
the joint encoder and decoder distribu- implementation and 2.92 is reported in [40].
tion, is also implemented as a baseline.
Since the decoder in ALI is a deterMethod
NLL(bits)
RMSE
IS
ministic network, the NLL of ALI is
WGAN [43]
3.82
impractical to compute. Alternatively,
MIX+WassersteinGAN [43]
4.05
we report the RMSE of reconstruction
DCGAN [5]
4.89
as a reference. Figure 3 qualitatively
ALI
14.53
4.79
compares the reconstruction perforPixelRNN [39]
3.06
mance of our model, ALI and VAE.
PixelCNN++ [40]
2.96 (2.92)?
3.289
5.51
As can be seen, the reconstruction of
AS-VAE-r
3.09
3.17
2.91
ALI is related to but not faithful reproAS-VAE-g
93.12
13.12
6.89
duction of the input data, which eviAS-VAE
3.32
3.36
6.34
dences the limitation in reconstruction
ability of adversarial learning. This is
also consistent in terms of RMSE.
5.4

ImageNet

ImageNet 2012 is used to evaluate the scalability of our model to large datasets. The images are
resized to 64?64. The quantitative results are shown in Table 3. Our model significantly improves the
performance on generation compared with DCGAN and PixelCNN++, while achieving competitive
results on reconstruction compared with PixelRNN and PixelCNN++.
Note that the PixelCNN++ takes more than two weeks
(44 hours per epoch) for training and 52.0 seconds/image Table 3: Quantitative Results on ImageNet.
for generating samples while our model only requires less
than 2 days (4 hours per epoch) for training and 0.01 secMethod
NLL
IS
onds/image for generating on a single TITAN X GPU. As a
DCGAN [5]
5.965
reference, the true validation set of ImageNet 2012 achieves
3.63
PixelRNN [39]
53.24% accuracy. This is because ImageNet has much
7.65
PixelCNN++ [40] 3.27
greater variety of images than CIFAR-10. Figure 4 shows
AS-VAE
3.71 11.14
generated samples based on trained with ImageNet, compared with DCGAN and PixelCNN++. Our model is able
to produce sharp images without label information while capturing more local spatial dependencies
than PixelCNN++, and without suffering from mode collapse as DCGAN. Additional results are
provided in the Appendix C.

6

Conclusions

We presented Adversarial Symmetrical Variational Autoencoders, a novel deep generative model for
unsupervised learning. The learning objective is to minimizing a symmetric KL divergence between
the joint distribution of data and latent codes from encoder and decoder, while simultaneously maximizing the expected marginal likelihood of data and codes. An extensive set of results demonstrated
excellent performance on both reconstruction and generation, while scaling to large datasets. A
possible direction for future work is to apply AS-VAE to semi-supervised learning tasks.

Acknowledgements
This research was supported in part by ARO, DARPA, DOE, NGA, ONR and NSF.

7

Figure 1: Generated samples trained on MNIST. (Left) AS-VAE-r; (Middle) AS-VAE-g (Right) AS-VAE.

Figure 2: Samples generated by AS-VAE Figure 3: Comparison of reconstruction with ALI [10].
In each block: column one for ground-truth, column two
when trained on CIFAR-10.
for ALI and column three for AS-VAE.

Figure 4: Generated samples trained on ImageNet. (Top) AS-VAE; (Middle) DCGAN [5];(Bottom) PixelCNN++ [40].

8

References
[1] Y. Pu, X. Yuan, A. Stevens, C. Li, and L. Carin. A deep generative deconvolutional image
model. Artificial Intelligence and Statistics (AISTATS), 2016.
[2] Y. Pu, X. Yuan, and L. Carin. Generative deep deconvolutional learning. In ICLR workshop,
2015.
[3] I.. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.l Ozair, A. Courville,
and Y. Bengio. Generative adversarial nets. In NIPS, 2014.
[4] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. Infogan: Interpretable
representation learning by information maximizing generative adversarial nets. In NIPS, 2016.
[5] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016.
[6] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee. Generative adversarial text
to image synthesis. In ICML, 2016.
[7] Y. Zhang, Z. Gan, K. Fan, Z. Chen, R. Henao, D. Shen, and L. Carin. Adversarial feature
matching for text generation. In ICML, 2017.
[8] Y. Zhang, Z. Gan, and L. Carin. Generating text with adversarial training. In NIPS workshop,
2016.
[9] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved
techniques for training gans. In NIPS, 2016.
[10] V. Dumoulin, I. Belghazi, B. Poole, O. Mastropietro, A. Lamb, M. Arjovsky, and A. Courville.
Adversarially learned inference. In ICLR, 2017.
[11] J. Donahue, . Kr?henb?hl, and T. Darrell. Adversarial feature learning. In ICLR, 2017.
[12] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR, 2014.
[13] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate
inference in deep generative models. In ICML, 2014.
[14] D.J. Rezende and S. Mohamed. Variational inference with normalizing flows. In ICML, 2015.
[15] Y. Burda, R. Grosse, and R. Salakhutdinov. Importance weighted autoencoders. In ICLR, 2016.
[16] D. P. Kingma, T. Salimans, R. Jozefowicz, X.i Chen, I. Sutskever, and M. Welling. Improving
variational inference with inverse autoregressive flow. In NIPS, 2016.
[17] Y. Zhang, D. Shen, G. Wang, Z. Gan, R. Henao, and L. Carin. Deconvolutional paragraph
representation learning. In NIPS, 2017.
[18] L. Chen, S. Dai, Y. Pu, C. Li, and Q. Su Lawrence Carin. Symmetric variational autoencoder
and connections to adversarial learning. In arXiv, 2017.
[19] D. Shen, Y. Zhang, R. Henao, Q. Su, and L. Carin. Deconvolutional latent-variable model for
text sequence matching. In arXiv, 2017.
[20] D.P. Kingma, D.J. Rezende, S. Mohamed, and M. Welling. Semi-supervised learning with deep
generative models. In NIPS, 2014.
[21] Y. Pu, Z. Gan, R. Henao, X. Yuan, C. Li, A. Stevens, and L. Carin. Variational autoencoder for
deep learning of images, labels and captions. In NIPS, 2016.
[22] M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial
networks. In ICLR, 2017.
[23] L. Mescheder, S. Nowozin, and A. Geiger. Adversarial variational bayes: Unifying variational
autoencoders and generative adversarial networks. In arXiv, 2016.
9

[24] T. Kim, M. Cha, H. Kim, J. Lee, and J. Kim. Learning to discover cross-domain relations with
generative adversarial networks. In arXiv, 2017.
[25] C. Li, K. Xu, J. Zhu, and B. Zhang. Triple generative adversarial nets. In arXiv, 2017.
[26] JY Zhu, T. Park, P. Isola, and A. Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In arXiv, 2017.
[27] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal
approximators. Neural networks, 1989.
[28] Y. Pu, Z. Gan, R. Henao, C. Li, S. Han, and L. Carin. Vae learning via stein variational gradient
descent. In NIPS, 2017.
[29] Q. Liu and D. Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. In NIPS, 2016.
[30] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative adversarial network. In ICLR,
2017.
[31] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan. In arXiv, 2017.
[32] C. Li, H. Liu, C. Chen, Y. Pu, L. Chen, R. Henao, and L. Carin. Alice: Towards understanding
adversarial learning for joint distribution matching. In NIPS, 2017.
[33] Z. Gan, L. Chen, W. Wang, Y. Pu, Y. Zhang, H. Liu, C. Li, and Lawrence Carin. Triangle
generative adversarial networks. In NIPS, 2017.
[34] A. Makhzani, J. Shlens, N. Jaitly, I. Goodfellow, and B. Frey. Adversarial autoencoders. In
arXiv, 2015.
[35] A. B. L. Larsen, S. K. S?nderby, H. Larochelle, and O. Winther. Autoencoding beyond pixels
using a learned similarity metric. In ICML, 2016.
[36] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural
networks. In AISTATS, 2010.
[37] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
[38] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple
way to prevent neural networks from overfitting. JMLR, 2014.
[39] A. Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel recurrent neural network. In ICML,
2016.
[40] T. Salimans, A. Karpathy, X. Chen, and D. P. Kingma. Pixelcnn++: Improving the pixelcnn
with discretized logistic mixture likelihood and other modifications. In ICLR, 2017.
[41] L. Thei, A. Oord, and M. Bethge. A note on the evaluation of generative models. In ICLR,
2016.
[42] C. Szegedy, W. Liui, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and
A. Rabinovich. Going deeper with convolutions. In CVPR, 2015.
[43] S. Arora, R. Ge, Y. Liang, T. Ma, and Y. Zhang. Generalization and equilibrium in generative
adversarial nets. In arXiv, 2017.

10

"
4715,1991,Improved Hidden Markov Model Speech Recognition Using Radial Basis Function Networks,Abstract Missing,"Improved Hidden Markov Model
Speech Recognition Using
Radial Basis Function Networks

Elliot Singer and Richard P. Lippmann
Lincoln Laboratory, MIT
Lexington, MA 02173-9108, USA

Abstract
A high performance speaker-independent isolated-word hybrid speech recognizer was developed which combines Hidden Markov Models (HMMs)
and Radial Basis Function (RBF) neural networks. In recognition experiments using a speaker-independent E-set database, the hybrid recognizer had an error rate of 11.5% compared to 15.7% for the robust
unimodal Gaussian HMM recognizer upon which the hybrid system was
based. These results and additional experiments demonstrate that RBF
networks can be successfully incorporated in hybrid recognizers and suggest that they may be capable of good performance with fewer parameters
than required by Gaussian mixture classifiers. A global parameter optimization method designed to minimize the overall word error rather than
the frame recognition error failed to reduce the error rate.

1

HMM/RBF HYBRID RECOGNIZER

A hybrid isolated-word speech recognizer was developed which combines neural
network and Hidden Markov Model (HMM) approaches. The hybrid approach is
an attempt to capitalize on the superior static pattern classification performance of
neural network classifiers [6] while preserving the temporal alignment properties of
HMM Viterbi decoding. Our approach is unique when compared to other studies
[2, 5] in that we use Radial Basis Function (RBF) rather than multilayer sigmoidal
networks. RBF networks were chosen because their static pattern classification
performance is comparable to that of other networks and they can be trained rapidly
using a one-pass matrix inversion technique [8] .
The hybrid HMM/RBF isolated-word recognizer is shown in Figure 1. For each
159

160

Singer and Lippmann
BEST WORD MATCH

WORD
MODELS

UNKNOWN
WORD

BACKGROUND
NOISE MODEL

_

Figure 1: Block diagram of the hybrid recognizer for a two word vocabulary.

pattern presented at the input layer, the RBF network produces nodal outputs
which are estimates of Bayesian probabilities [9]. The RBF network consists of an
input layer, a hidden layer composed of Gaussian basis functions, and an output
layer. Connections from the input layer to the hidden layer are fixed at unity
while those from the hidden layer to the output layer are trained by minimizing
the overall mean-square error between actual and desired output values. Each
RBF output node has a corresponding state in a set of HMM word models which
represent the words in the vocabulary. HMM word models are left-to-right with
no skip states and have a one-state background noise model at either end. The
background noise models are identical for all words. In the simplified diagram of
Figure 1, the vocabulary consists of 2 E-set words and the HMMs contain 3 states
per word model. The number of RBF output nodes (classes) is thus equal to the
total number of HMM non-background states plus one to account for background
noise. In recognition, Viterbi decoders use the nodal outputs of the RBF network
as observation probabilities to produce word likelihood scores. Since the outputs of
the RBF network can take on any value, they were initially hard limited to 0.0 and
1.0. The transition probabilities estimated as part of HMM training are retained.
The final response of the recognizer corresponds to that word model which produces
the highest Viterbi likelihood. Note that the structure of the HMM/RBF hybrid
recognizer is identical to that of a tied-mixture HMM recognizer. For a discussion
and comparison of the two recognizers, see [10].
Training of the hybrid recognizer begins with the preliminary step of training an
HMM isolated-word recognizer. The robust HMM recognizer used provides good
recognition performance on many standard difficult isolated-word speech databases
[7]. It uses continuous density, unimodal diagonal-covariance Gaussian classifiers
for each word state. Variances of all states are equal to the grand variance averaged
over all words and states. The trained HMM recognizer is used to force an alignment
of every training token and assign a label to each frame. Labels correspond to both
states of HMM word models and output nodes of the RBF network.
The Gaussian centers in the RBF hidden layer are obtained by performing k-means

Improved Hidden Markov Model Speech Recognition Using Radial Basis Function Networks

clustering on speech frames and separate clustering on noise frames, where speech
and noise frames are distinguished on the basis of the initial Viterbi alignment. The
RBF weights from the hidden layer to the output layer are computed by presenting
input frames to the RBF network and setting the desired network outputs to 1.0
for the output node corresponding to the frame label and 0.0 for all other nodes.
The RBF hidden node outputs and their correlations are accumulated across all
training tokens and are used to estimate weights to the RBF output nodes using a
fast one-pass algorithm [8]. Unlike the performance of the system reported in [5],
additional training iterations using the hybrid recognizer to label frames did not
improve performance.

2

DATABASE

All experiments were performed using a large, speaker-independent E-set (9 word)
database derived from the ISOLET Spoken Letter Database [4]. The training set
consisted of 1,080 tokens (120 tokens per word) spoken by 60 female and 60 male
speakers for a total of 61,466 frames. The test set consisted of 540 tokens (60
tokens per word) spoken by a different set of 30 female and 30 male speakers for
a total of 30,406 frames . Speech was sampled at 16 kHz and had an average SNR
of 31.5 dB. Input vectors were based on a mel-cepstrum analysis of the speech
waveform as described in [7]. The input analysis window was 20ms wide and was
advanced at 10ms intervals. Input vectors were created by adjoining the first 12
non-energy cepstral coefficients, the first 13 first-difference cepstral coefficients, and
the first 13 second-difference cepstral coefficients. Since the hybrid was based on
an 8 state-per-word robust HMM recognizer, the RBF network contained a total of
73 output nodes (72 speech nodes and 1 background node). The error rate of the 8
state-per-word robust HMM recognizer on the speaker-independent E-set task was
15.7%.

3

MODIFICATIONS TO THE HYBRID RECOGNIZER

The performance of the baseline HMM/RBF hybrid recognizer described in Section 1 is quite poor. We found it necessary to select the recognizer structure carefully
and utilize intermediate outputs properly to achieve a higher level of performance.
A full description of these modifications is presented in [10]. Briefly, they include
normalizing the hidden node outputs to sum to 1.0, normalizing the RBF outputs
by the corresponding a priori class probabilities as estimated from the initial Viterbi
alignment, expanding the RBF network into three individually trained subnetworks
corresponding to the ceptrum, first difference cepstrum, and second difference cepstrum data streams, setting a lower limit of 10- 5 on the values produced at the RBF
output nodes, adjusting a global scaling factor applied to the variances of the RBF
centers, and setting the number of centers to 33,33, and 65 for the first, second, and
third subnets, respectively. The structure of the final hybrid recognizer is shown in
Figure 2. This recognizer has an error rate of 11.5% (binomial standard deviation
= ?1.4) on the E-set test data compared to 15.7% (?1.6) for the 8 state-per-word
unimodal Gaussian HMM recognizer, and 9.6% (?1.3) for a considerably more complex tied-mixture HMM recognizer [10]. The final hybrid system contained a total
of 131 Gaussians and 9,563 weights. On a SUN SPARCstation 2, training time for

161

162

Singer and Lippmann

the final hybrid recognizer was about 1 hour and testing time was about 10 minutes.
BEST WORD MATCH

Figure 2: Block diagram of multiple sub net hybrid recognizer.

4

GLOBAL OPTIMIZATION

In the hybrid recognizer described above, discriminative training is performed at
the frame level. A preliminary segmentation by the HMM recognizer assigns each
speech frame to a specific RBF output node or, equivalently, an HMM word state.
The RBF network weights are then computed to minimize the squared error between the network output and the desired output over all input frames. The goal of
the recognizer, however, is to classify words. To meet this goal, discriminant training should be performed on word-level rather than frame-level outputs. Recently,
several investigators have described techniques that optimize parameters based on
word-level discriminant criteria [1, 3]. These techniques seek to maximize a mutual
information type of criterion:
Lc
C logy,

=

where Lc. is the likelihood score of the word model corresponding to the correct
Lw Lw is the sum of the word likelihood scores for all models. By
result and L
computing oC/oO, the gradient of C with respect to parameter 0, we can optimize
any parameter in the hybrid recognizer using the update equation

=

where 0 is the new value of parameter 0, () is the previous value, and TJ is a gain
term proportional to the learning rate. Following [1], we refer to the word-level
optimization technique as ""global optimization.""

Improved Hidden Markov Model Speech Recognition Using Radial Basis Function Networks

To apply global optimization to the HMM/RBF hybrid recognizer, we derived the
formulas for the gradient of C with respect to
the weight connecting RBF center
i to RBF output node j in subnet k; Pj, the RBF output normalization factor for
RBF output node j in subnet k; and mfl' the Ith element of the mean of center i of
subnet k. For each token of length T frames, these are given by

wt '

8C
J:lwk
U

ij

= (be; L- Pw )
w

T

<I>~

""'""' frjt{3jt
kIt'
t=1

L..J

St

and

likelihood score for word model w,
Lw / Lw Lw is the normalized word likelihood,
{ I if RBF output node j is a member of the correct word model
o otherwise,
forward partial probability of HMM state j at time t,
backward partial probability of HMM state j at time t,
unnormalized output of RBF node j of subnet k at time t,
normalized output of ith Gaussian center of sub net k at time t,
~
~,. <I>~t
I

=1

,

Ith element of the input vector for subnet k at time t,
global scaling factor for the variances of sub net k,
[th component of the standard deviation of the ith Gaussian center
of subnet k,
number of RBF output nodes in sub net k.

In implementing global optimization, the frame-level training procedure described
earlier serves to initialize system parameters and hill climbing methods are used to
reestimate parameters iteratively. Thus, weights are initialized to the values derived
using the one-pass matrix inversion procedure, RBF output normalization factors
are initialized to the class priors, and Gaussian means are initialized to the k-means
clustering values. Note that while the priors sum to one, no such constraint was
placed on the RBF output normalization factors during global optimization.
It is worth noting that since the RBF network outputs in the hybrid recognizer

are a posteriori probabilities normalized by a priori class probabilities, their values
may exceed 1. The accumulation of these quantities in the Viterbi decoders often
leads to values of (Xjt{3jt and Lw in the range of 10 80 or greater. Numerical problems
with the implementation of the global optimization equations were avoided by using
log arithmetic for intermediate operations and working with the quantity {3jt! Lw
throughout. Values of 7J which produced reasonable results were generally in the
range of 10- 10 to 10- 6

163

164

Singer and Lippmann

The results of using the global optimization technique to estimate the RBF weights
are shown in Figure 3. Figure 3( a) shows the recognition performance on the training and test sets versus the number of training iterations and Figure 3(b) tracks
the value of the criterion C = Lei L on the training and test set under the same
conditions. It is apparent that the method succeeds in iteratively increasing the
value of the criterion and in significantly lowering the error rate on the training
data. Unfortunately, this behavior does not extend to improved performance on
the test data. This suggests that global optimization is overfitting the hybrid word
models to the training data. Results using global optimization to estimate RBF
output normalization factors and the Gaussian means produced similar results.
20

%ERROR

,.---~----r-----,r-----""

TEST

C = log (Lc I L)

0
-0.2

TRAIN

-0.4
10

-0.6

TEST

-0.8

o o~--~---~--~~--~
5
10
15
20
NUMBER OF ITERATIONS

-1

0

5

10

15

20

NUMBER OF ITERATIONS

Figure 3: (a) Error rates for training and test data. (b) Criterion C for training
and test data.

5

ACCURACY OF BAYES PROBABILITY
ESTIMATION

Three methods were used to determine how well RBF outputs estimate Bayes probabilities. First, since network outputs must sum to one if they are probabilities, we
computed the RMS error between the sum of the RBF outputs and unity for all
frames of the test data. The average RMS error was low (10- 4 or less for each
subnet). Second, the average output of each RBF node was computed because this
should equal the a priori probability of the class associated with the node [9]. This
condition was true for each subnet with an average RMS error on the order of 10- 5 .
For the final method, we partitioned the outputs into 100 equal size bins between
0.0 and 1.0. For each input pattern, we used the output values to select the appropriate bins and incremented the corresponding bin counts by one. In addition, we
incremented the correct-class bin count for the one bin which corresponded to the
class of the input pattern. For example, data indicated that for the 61,466 frames
of training tokens, nodal outputs of the cepstra subnet in the range 0.095-0.105 occurred 29,698 times and were correct classifications (regardless of class) 3,067 times.
If the outputs of the network were true Bayesian probabilities, we would expect the

Improved Hidden Markov Model Speech Recognition Using Radial Basis Function Networks

relative frequency of correct labeling to be close to 0.1. Similarly, relative frequencies measured in other intervals would also be expected to be close to the value of
the corresponding center of the interval. Thus, a plot of the relative frequencies for
each bin versus the bin centers should show the measured values lying close to the
diagonal.
The measured relative frequency data for the cepstra subnet and ?2u bounds for
the binomial standard deviations of the relative frequencies are shown in Figure 4.
Outputs below 0.0 and above 1.0 are fixed at 0.0 and 1.0, respectively. Although
the relative frequencies tend to be clustered around the diagonal, many values lie
test
outside the bounds. Furthermore, goodness-of-fit measurements using the
indicate that fits fail at significance levels well below .01. We conclude that although
the system provides good recognition accuracy, better performance may be obtained
with improved estimation of Bayesian probabilities.

x:

1r-------------------------------------------------------------~?
.J

~0.9

:3 0.8
t;

wO.7
IX
IX

0 0 .6

o

~0.5

gO.4
IX

~0.3
1= 0.2

:5

~O.1

0.1

0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
RBF NETWORK OUTPUT (All Nodes)

1

Figure 4: Relative frequency of correct class labeling and ?2u bounds for the binomial standard deviation, cepstra subnet.

6

SUMMARY AND CONCLUSIONS

This paper describes a hybrid isolated-word speech recognizer which successfully
integrates Radial Basis Function neural networks and Hidden Markov Models. The
hybrid's performance is better than that of a tied-mixture recognizer of comparable
complexity and near that of a tied-mi..xture recognizer of considerably greater complexity. The structure of the RBF networks and the processing of network outputs
had to be carefully selected to provide this level of performance. A global optimization technique designed to maximize a word discrimination criterion did not
succeed in improving performance further. Statistical tests indicated that the accuracy of the Bayesian probability estimation performed by the RBF networks could

165

166

Singer and Lippmann

be improved. We conclude that RBF networks can be used to provide good performance and short training times in hybrid recognizers and that these systems may
require fewer parameters than Gaussian-mixture-based recognizers at comparable
performance levels.

Acknowledgements
This work was sponsored by the Defense Advanced Research Projects Agency. The
views expressed are those of the authors and do not reflect the official policy or
position of the U.S. Government.

References
[1] Yoshua Bengio, Renato De Mori, Giovanni Flammia, and Ralf Kompe. Global
optimization of a neural network - Hidden Markov model hybrid. Technical
Report TR-SOCS-90.22, MgGill University School of Computer Science, Montreal, Qc., Canada, December 1990.
[2] Herve Bourlard and Nelson Morgan. A continuous speech recognition system
embedding MLP into HMM. In D. Touretzky, editor, Advances in Neural
Information Processing 2, pages 186-193. Morgan Kaufmann, San Mateo, CA,
1990.
[3] John S. Bridle. Alpha-nets: A recurrent neural network architecture with a
hidden Markov model interpretation. Speech Communication, 9:83-92, 1990.
[4] Ron Cole, Yeshwant Muthusamy, and Mark Fanty. The Isolet spoken letter
database. Technical Report CSE 90-004, Oregon Graduate Institute of Science
and Technology, Beverton, OR, March 1990.
[5] Michael Franzini, Kai-Fu Lee, and Alex Waibel. Connectionist viterbi training: A new hybrid method for continuous speech recognition. In Proceedings
of IEEE International Conference on Acoustics Speech and Signal Processing.

IEEE, April 1990.
[6] Richard P. Lippmann. Pattern classification using neural networks. IEEE
Communications Magazine, 27(11):47-54, November 1989.
[7] Richard P. Lippmann and Ed A. Martin. Mqlti-style training for robust
isolated-word speech recognition. In Proceedings International Conference on
Acoustics Speech and Signal Processing, pages 705-708. IEEE, April 1987.
[8] Kenney Ng and Richard P. Lippmann. A comparative study of the practical characteristics of neural network and conventional pattern classifiers. In
R. P. Lippmann, J. Moody, and D. S. Touretzky, editors, Advances in Neural
Information Processing 3. Morgan Kaufmann, San Mateo, CA, 1991.
[9] Mike D. Richard and Richard P. Lippmann. Neural network classifiers estimate
Bayesian a posteriori probabilities. Neural Computation, In Press.
[10] Elliot Singer and Richard P. Lippmann. A speech recognizer using radial basis function neural networks in an HMM framework. In Proceedings of the
International Conference on Acoustics, Speech, and Signal Processing. IEEE,
1992.

"
6191,1992,Learning Sequential Tasks by Incrementally Adding Higher Orders,Abstract Missing,"Learning Sequential Tasks by
Incrementally Adding Higher Orders

Mark Ring
Department of Computer Sciences, Taylor 2.124
University of Texas at Austin
Austin, Texas 78712
(ring@cs. utexas.edu)

Abstract
An incremental, higher-order, non-recurrent network combines two
properties found to be useful for learning sequential tasks: higherorder connections and incremental introduction of new units. The
network adds higher orders when needed by adding new units that
dynamically modify connection weights. Since the new units modify the weights at the next time-step with information from the
previous step, temporal tasks can be learned without the use of
feedback, thereby greatly simplifying training. Furthermore, a theoretically unlimited number of units can be added to reach into
the arbitrarily distant past. Experiments with the Reber grammar have demonstrated speedups of two orders of magnitude over
recurrent networks.

1

INTRODUCTION

Second-order recurrent networks have proven to be very powerful [8], especially
when trained using complete back propagation through time [1, 6, 14]. It has also
been demonstrated by Fahlman that a recurrent network that incrementally adds
nodes during training-his Recurrent Cascade-Correlation algorithm [5]-can be
superior to non-incremental, recurrent networks [2,4, 11, 12, 15].
The incremental, higher-order network presented here combines advantages of both
of these approaches in a non-recurrent network. This network (a simplified, con115

116

Ring

tinuous version of that introduced in [9]), adds higher orders when they are needed
by the system to solve its task. This is done by adding new units that dynamically
modify connection weights. The new units modify the weights at the next time-step
with information from the last, which allows temporal tasks to be learned without
the use of feedback.

2

GENERAL FORMULATION

Each unit (U) in the network is either an input (I), output (0), or high-level (L)
unit.
Ui(t)
value of ith unit at time t.

Ii(t)

Ui(t) where i is an input unit.

Oi(t)

Ui(t) where i is an output unit.

ret)

Target value for Oi (t) at time t.

L~y(t)

Ui(t) where i is the higher-order unit that
modifies weight w xy at time t. 1

The output and high-level units are collectively referred to as non-input (N) units:

{ Oi (t)

L~y(t)

if Ui
if Ui

= Oi.

=L~y.

In a given time-step, the output and high-level units receive a summed input from
the input units.
Ni(t) ==
Ij (t)g(i, j, t).
(1)

L
j

is a gating function representing the weight of a particular connection at a particular time-step. If there is a higher-order unit assigned to that connection, then
the input value of that unit is added to the connection's weight at that time-step.2

g

g

( .. t) _ { Wij(t)
Z,),
Wij (t)

+ Lij(t -

1)

If Lij exists
Otherwise

(2)

At each time-step, the values of the output units are calculated from the input units
and the weights (possibly modified by the activations of the high-level units from the
previous time-step). The values of the high-level units are calculated at the same
time in the same way. The output units generate the output of the network. The
high-level units simply alter the weights at the next time-step. All unit activations
can be computed simultaneously since the activations of the L units are not required
connection may be modified by at most one L unit. Therefore Li , Lzy , and L~y are
identical but used as appropriate for notational convenience.
21t can be seen that this is a higher-order connection in the usual sense if one substitutes
the right-hand side of equation 1 for L'0 in equation 2 and then replaces g in equation 1 with
the result. In fact, as the network increases in height, ever higher orders are introduced,
while lower orders are preserved.
1A

Learning Sequential Tasks by Incrementally Adding Higher Orders

until the following time-step. The network is arranged hierarchically in that every
higher-order units is always higher in the hierarchy than the units on either side
of the weight it affects. Since higher-order units have no outgoing connections, the
network is not recurrent. It is therefore impossible for a high-level unit to affect,
directly or indirectly, its own input.
There are no hidden units in the traditional sense, and all units have a linear activation function. (This does not imply that non-linear functions cannot be represented,
since non-linearities do result from the multiplication of higher-level and input units
in equations 1 and 2.)
Learning is done through gradient descent to reduce the sum-squared error.

! L:(Ti(t) -

E(t)

2

,.

Oi(t?2

(3)
where 1] is the learning rate. Since it may take several time-steps for the value of
a weight to affect the network's output and therefore the error, equation 3 can be
rewritten as:
~Wij(t)

BE(t)
= BWij (t - r'"") ,

where

=

{

0

if U i
if U i

(4)

=
=L~y
Oi

1 + rX
The value ri is constant for any given unit i and specifies how ""high"" in the hierarchy
unit i is. It therefore also specifies how many time-steps it takes for a change in
unit i's activation to affect the network's output.
ri

Due to space limitations, the derivation of the gradient is not shown, but is given
elsewhere [10]. The resulting weight change rule, however, is:

=

~ ..(t) - Ii (t _ i) { Ti(t) - Oi(t) If u~ O~
w')
r
~W xy (t)
If U' =
LI
xy

(5)

The weights are changed after error values for the output units have been collected.
Since each high-level unit is higher in the hierarchy than the units on either side
of the weight it affects, weight changes are made bottom up, and the ~Wxy(t) in
equation 5 will already have been calculated at the time ~Wij(t) is computed.
The intuition behind the learning rule is that each high-level unit learns to utilize
the context from the previous time-step for adjusting the connection it influences
at the next time-step so that it can minimize the connection's error in that context.
Therefore, if the information necessary to decide the correct value of a connection
at one time-step is available at the previous time-step, then that information is used
by the higher-order unit assigned to that connection. If the needed information is
not available at the previous time-step, then new units may be built to look for
the information at still earlier steps. This method concentrating on unexpected
events is similar to the ""hierarchy of decisions"" of Dawkins [3], and the ""history
compression"" of Schmidhuber [13].

117

118

Ring

3

WHEN TO ADD NEW UNITS

A unit is added whenever a weight is being pulled strongly in opposite directions
(i.e. when learning is forcing the weight to increase and to decrease at the same
time) . The unit is created to determine the contexts in which the weight is pulled
in each direction. This is done in the following way: Two long-term averages are
kept for each connection. The first of these records the average change made to the
weight,
~Wij(t) = O'~Wij(t) + (1 - O')~Wij(t - 1); 0 S; 0' S; l.
The second is the long-term mean absolute deviation, given by:

The parameter, 0', specifies the duration of the long-term average. A lower value
of 0' means that the average is kept for a longer period of time. When ~Wij(t) is
small, but I~Wij(t)1 is large, then the weight is being pulled strongly in conflicting
directions, and a new unit is built.
if

I~Wij(t)1

c + I~Wij(t)1

>8

then build unit L~ +1,

where c is a small constant that keeps the denominator from being zero, 8 is a
threshold value, and N is the number of units in the network. A related method for
adding new units in feed-forward networks was introduced by Wynne-Jones [16].
When a new unit is added, its incoming weights are initially zero. It has no output
weights but simply learns to anticipate and reduce the error at each time-step of
the weight it modifies. In order to keep the number of new units low, whenever a
unit, Lij is created, the statistics for all connections into the destination unit (U i )
are reset: I~Wij(t)1 ~ 0.0 and ~Wij(t) ~ 1.0.

4

RESULTS

The Reber grammar is a small finite-state grammar of the following form:

y. .. .
sO

B

...

~?

TO

X

~ ...
.../v
E

Transitions from one node to the next are made by way of the labeled arcs. The
task of the network is: given as input the label of the arc just traversed, predict

Learning Sequential Tasks by Incrementally Adding Higher Orders

Elman
Network
Sequences Seen:
""Hidden"" Units

Mean
Best

20,000
15

RTRL

19,000
2

Recurrent
Cascade
Correlation
25,000
2-3

Incremental
Higher-Order
Network
206
176
40

Table 1: The incremental higher-order network is compared against recurrent networks on the Reber grammar. The results for the recurrent networks are quoted
from other sources [2, 5]. The mean and/or best performance is shown when available. RTRL is the real-time recurrent learning algorithm [15].

the arc that will be traversed next. A training sequence, or string, is generated
by starting with a B transition and then randomly choosing an arc leading away
from the current state until the final state is reached. Both inputs and outputs are
encoded locally, so that there are seven output units (one each for B, T, S, X, V, P,
and E) and eight input units (the same seven plus one bias unit). The network is
considered correct if its highest activated outputs correspond to the arcs that can be
traversed from the current state. Note that the current state cannot be determined
from the current input alone.
An Elman-type recurrent network was able to learn this task after 20,000 string
presentations using 15 hidden units [2]. (The correctness criteria for the Elman
net was slightly more stringent than that described in the previous paragraph.)
Recurrent Cascade-Correlation (RCC) was able to learn this task using only two or
three hidden units in an average of 25,000 string presentations [5].
The incremental, higher-order network was trained on a continuous stream of input:
the network was not reset before beginning a new string. Training was considered
to be complete only after the network had correctly classified 100 strings in a row.
Using this criterion, the network completed training after an average of 206.3 string
presentations with a standard deviation of 16.7. It achieved perfect generalization
on test sets of 128 randomly generated strings in all ten runs. Because the Reber
grammar is stochastic, a ceiling of 40 higher-order units was imposed on the network
to prevent it from continually creating new units in an attempt to outguess the
random number generator.
Complete results for the network on the Reber grammar task are given in table 1.
The parameter settings were: TJ = 0.04, (J"" = 0.08, e = 1.0, f = 0.1 and Bias = 0.0.
(The network seemed to perform better with no bias unit.)
The network has also been tested on the ""variable gap"" tasks introduced by Mozer
[7], as shown in figure 1. These tasks were intended to test performance of networks
over long time-delays. Two sequences are alternately presented to the network.
Each sequence begins with an X or a Y and is followed by a fixed string of characters
with an X or a Y inserted some number of time-steps from the beginning. In
figure 1 the number of time-steps, or ""gap"", is 2. The only difference between the
two sequences is that the first begins with an X and repeats the X after the gap,
while the second begins with a Y and repeats the Y after the gap. The network
must learn to predict the next item in the sequence given the current item as input

119

120

Ring
Time-step:
Sequence 1:
Sequence 2:

0
X
Y

1
a
a

2
b

3

4

5

6

X

b

Y

c
c

d
d

e
e

10

789
f g h
f g h

11
J
J

12
k

k

Figure 1: An example of a ""variable gap"" training sequence [7]. One item is presented to the network at each time-step. The target is the next item in the sequence.
Here the ""gap"" is two, because there are two items in the sequence between the first
X or Y and the second X or Y . In order to correctly predict the second X or Y, the
network must remember how the sequence began.

(where all inputs are locally encoded). In order for the network to predict the
second occurrence of the X or Y, it must remember how the sequence began. The
length of the gap can be increased in order to create tasks of greater difficulty.
Results of the ""gap"" tasks are given in table 2. The values for the standard recurrent
network and for Mozer's own variation are quoted from Mozer's paper [7]. The
incremental higher-order net had no difficulty with gaps up to 24, which was the
largest gap I tested. The same string was used for all tasks (except for the position
of the second X or V), and had no repeated characters (again with the exception
of the X and Y). The network continued to scale linearly with every gap size both
in terms of units and epochs required for training. Because these tasks are not
stochastic, the network always stopped building units as soon as it had created
those necessary to solve each task.

=

=

=

=

=

The parameter settings were: TJ
1.5, (j
0.2, e
1.0, f
0.1 and Bias
0.0.
The network was considered to have correctly predicted an element in the sequence
if the most strongly activated output unit was the unit representing the correct
prediction. The sequence was considered correctly predicted if all elements (other
than the initial X or Y) were correctly predicted.

Gap
2
4
6
8
10
24

Mean number of Training sets required by:
Standard
Mozer
Incremental
Recurrent Net Network Higher-Order Net
468
4
328
7406
584
6
992
9830
8
1312
10
> 10000
12
> 10000
1630
26

Umts
Created
10
15
19
23
27
49

Table 2: A comparison on the ""gap"" tasks of a standard recurrent-network and
a network devised specifically for long time-delays (quoted from Mozer [7], who
reported results for gaps up to ten) against an incremental higher-order network.
The last column is the number of units created by the incremental higher-order net.

Learning Sequential Tasks by Incrementally Adding Higher Orders

5

CONCLUSIONS

The incremental higher-order network performed much better than the networks
that it was compared against on these tiny tests. A few caveats are in order,
however. First, the parameters given for the tasks above were customized for those
tasks. Second, the network may add a large number of new units if it contains
many context-dependent events or if it is inherently stochastic. Third, though the
network in principle can build an ever larger hierarchy that searches further and
further back in time for a context that will predict what a connection's weight
should be, many units may be needed to bridge a long time-gap. Finally, once a
bridge across a time-delay is created, it does not generalize to other time-delays.
On the other hand, the network learns very fast due to its simple structure that
adds high-level units only when needed. Since there is no feedback (i.e. no unit ever
produces a signal that will ever feed back to itself), learning can be Qone without
back propagation through time. Also, since the outputs and high-level units have a
fan-in equal to the number of inputs only, the number of connections in the system
is much smaller than the number of connections in a traditional network with the
same number of hidden units.
Finally, the network can be thought of as a system of continuous-valued conditionaction rules that are inserted or removed depending on another set of such rules
that are in turn inserted or removed depending on another set, etc. When new rules
(new units) are added, they are initially invisible to the system, (i.e., they have no
effect), but only gradually learn to have an effect as the opportunity to decrease
error presents itself.

Acknowledgements
This work was supported by NASA Johnson Space Center Graduate Student Researchers Program training grant, NGT 50594. I would like to thank Eric Hartman,
Kadir Liano, and my advisor Robert Simmons for useful discussions and helpful
comments on drafts of this paper. I would also like to thank Pavilion Technologies,
Inc. for their generous contribution of computer time and office space required to
complete much of this work.

References
[1] Jonathan Richard Bachrach. Connectionist Modeling and Control of Finite
State Environments. PhD thesis, Department of Computer and Information
Sciences, University of Massachusetts, February 1992.
[2] Axel Cleeremans, David Servan-Schreiber, and James L. McClelland. Finite
state automata and simple recurrent networks. Neural Computation, 1(3):372381, 1989.
[3] Richard Dawkins. Hierarchical organisation: a candidate principle for ethology.
In P. P. G. Bateson and R. A. Hinde, editors, Growing Points in Ethology, pages
7-54, Cambridge, 1976. Cambridge University Press.
[4] Jeffrey L. Elman. Finding structure in time. CRL Technical Report 8801,
University of California, San Diego, Center for Research in Language, April
1988.

121

122

Ring

[5] Scott E. Fahlman. The recurrent cascade-correlation architecture. In R. P.
Lippmann, J. E. Moody, and D. S. Touretzky, editors, Advances in Neural
Information Processing Systems 3, pages 190-196, San Mateo, California, 1991.
Morgan Kaufmann Publishers.
[6] C. L. Giles, C. B. Miller, D. Chen, G. Z. Sun, H. H. Chen, and Y. C. Lee.
Extracting and learning an unknown grammar with recurrent neural networks.
In J. E. Moody, S. J. Hanson, and R. P. Lippman, editors, Advances in Neural
Information Processing Systems 4, pages 317-324, San Mateo, California, 1992.
Morgan Kaufmann Publishers.
[7] Michael C. Mozer. Induction of multiscale temporal structure. In John E.
Moody, Steven J. Hanson, and Richard P. Lippmann, editors, Advances in
Neural Information Processing Systems 4, pages 275-282, San Mateo, California, 1992. Morgan Kaufmann Publishers.
[8] Jordan B. Pollack. The induction of dynamical recognizers. Machine Learning,
7:227-252, 1991.
[9] Mark B. Ring. Incremental development of complex behaviors through automatic construction of sensory-motor hierarchies. In Lawrence A. Birnbaum
and Gregg C. Collins, editors, Machine Learning: Proceedings of the Eighth International Workshop (ML91), pages 343-347. Morgan Kaufmann Publishers,
June 1991.
[10] Mark B. Ring. Sequence learning with incremental higher-order neural networks. Technical Report AI 93-193, Artificial Intelligence Laboratory, University of Texas at Austin, January 1993.
[11] A. J. Robinson and F. Fallside. The utility driven dynamic error propagation
network. Technical Report CUED/F-INFENG/TR.l, Cambridge University
Engineering Department, 1987.

[12] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by error propagation. In D. E. Rumelhart and J. L. McClelland,
editors, Parallel Distributed Processing: Explorations in the Microstructure of
Cognition. V1: Foundations. MIT Press, 1986.
[13] Jiirgen Schmidhuber. Learning unambiguous reduced sequence descriptions.
In J. E. Moody, S. J. Hanson, and R. P. Lippman, editors, Advances in Neural
Information Processing Systems 4, pages 291-298, San Mateo, California, 1992.
Morgan Kaufmann Publishers.
[14] Raymond L. Watrous and Gary M. Kuhn. Induction of finite-state languages
using second-order recurrent networks. In J. E. Moody, S. J. Hanson, and
R. P. Lippman, editors, Advances in Neural Information Processing Systems
4, pages 309-316, San Mateo, California, 1992. Morgan Kaufmann Publishers.
[15] Ronald J. Williams and David Zipser. A learning algorithm for continually
running fully recurrent neural networks. Neural Computation, 1(2):270-280,
1989.
[16] Mike Wynn-Jones. Node splitting: A constructive algorithm for feed-forward
neural networks. Neural Computing and Applications, 1(1):17-22, 1993.

"
6754,2017,Triangle Generative Adversarial Networks,"A Triangle Generative Adversarial Network ($\Delta$-GAN) is developed for semi-supervised cross-domain joint distribution matching, where the training data consists of samples from each domain, and supervision of domain correspondence is provided by only a few paired samples. $\Delta$-GAN consists of four neural networks, two generators and two discriminators. The generators are designed to learn the two-way conditional distributions between the two domains, while the discriminators implicitly define a ternary discriminative function, which is trained to distinguish real data pairs and two kinds of fake data pairs. The generators and discriminators are trained together using adversarial learning. Under mild assumptions, in theory the joint distributions characterized by the two generators concentrate to the data distribution.  In experiments, three different kinds of domain pairs are considered, image-label, image-image and image-attribute pairs. Experiments on semi-supervised image classification, image-to-image translation and attribute-based image generation demonstrate the superiority of the proposed approach.","Triangle Generative Adversarial Networks

Zhe Gan? , Liqun Chen? , Weiyao Wang, Yunchen Pu, Yizhe Zhang,
Hao Liu, Chunyuan Li, Lawrence Carin
Duke University
zhe.gan@duke.edu

Abstract
A Triangle Generative Adversarial Network (?-GAN) is developed for semisupervised cross-domain joint distribution matching, where the training data consists of samples from each domain, and supervision of domain correspondence
is provided by only a few paired samples. ?-GAN consists of four neural networks, two generators and two discriminators. The generators are designed to
learn the two-way conditional distributions between the two domains, while the
discriminators implicitly define a ternary discriminative function, which is trained
to distinguish real data pairs and two kinds of fake data pairs. The generators
and discriminators are trained together using adversarial learning. Under mild
assumptions, in theory the joint distributions characterized by the two generators
concentrate to the data distribution. In experiments, three different kinds of domain pairs are considered, image-label, image-image and image-attribute pairs.
Experiments on semi-supervised image classification, image-to-image translation
and attribute-based image generation demonstrate the superiority of the proposed
approach.

1

Introduction

Generative adversarial networks (GANs) [1] have emerged as a powerful framework for learning
generative models of arbitrarily complex data distributions. When trained on datasets of natural
images, significant progress has been made on generating realistic and sharp-looking images [2, 3].
The original GAN formulation was designed to learn the data distribution in one domain. In practice,
one may also be interested in matching two joint distributions. This is an important task, since
mapping data samples from one domain to another has a wide range of applications. For instance,
matching the joint distribution of image-text pairs allows simultaneous image captioning and textconditional image generation [4], while image-to-image translation [5] is another challenging problem
that requires matching the joint distribution of image-image pairs.
In this work, we are interested in designing a GAN framework to match joint distributions. If paired
data are available, a simple approach to achieve this is to train a conditional GAN model [4, 6],
from which a joint distribution is readily manifested and can be matched to the empirical joint
distribution provided by the paired data. However, fully supervised data are often difficult to acquire.
Several methods have been proposed to achieve unsupervised joint distribution matching without
any paired data, including DiscoGAN [7], CycleGAN [8] and DualGAN [9]. Adversarially Learned
Inference (ALI) [10] and Bidirectional GAN (BiGAN) [11] can be readily adapted to this case as
well. Though empirically achieving great success, in principle, there exist infinitely many possible
mapping functions that satisfy the requirement to map a sample from one domain to another. In
order to alleviate this nonidentifiability issue, paired data are needed to provide proper supervision to
inform the model the kind of joint distributions that are desired.
This motivates the proposed Triangle Generative Adversarial Network (?-GAN), a GAN framework that allows semi-supervised joint distribution matching, where the supervision of domain
?

Equal contribution.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

Figure 1: Illustration of the Triangle Generative Adversarial Network (?-GAN).
correspondence is provided by a few paired samples. ?-GAN consists of two generators and two
discriminators. The generators are designed to learn the bidirectional mappings between domains,
while the discriminators are trained to distinguish real data pairs and two kinds of fake data pairs.
Both the generators and discriminators are trained together via adversarial learning.
?-GAN bears close resemblance to Triple GAN [12], a recently proposed method that can also be
utilized for semi-supervised joint distribution mapping. However, there exist several key differences
that make our work unique. First, ?-GAN uses two discriminators in total, which implicitly defines
a ternary discriminative function, instead of a binary discriminator as used in Triple GAN. Second,
?-GAN can be considered as a combination of conditional GAN and ALI, while Triple GAN
consists of two conditional GANs. Third, the distributions characterized by the two generators in
both ?-GAN and Triple GAN concentrate to the data distribution in theory. However, when the
discriminator is optimal, the objective of ?-GAN becomes the Jensen-Shannon divergence (JSD)
among three distributions, which is symmetric; the objective of Triple GAN consists of a JSD term
plus a Kullback-Leibler (KL) divergence term. The asymmetry of the KL term makes Triple GAN
more prone to generating fake-looking samples [13]. Lastly, the calculation of the additional KL
term in Triple GAN is equivalent to calculating a supervised loss, which requires the explicit density
form of the conditional distributions, which may not be desirable. On the other hand, ?-GAN is
a fully adversarial approach that does not require that the conditional densities can be computed;
?-GAN only require that the conditional densities can be sampled from in a way that allows gradient
backpropagation.
?-GAN is a general framework, and can be used to match any joint distributions. In experiments,
in order to demonstrate the versatility of the proposed model, we consider three domain pairs:
image-label, image-image and image-attribute pairs, and use them for semi-supervised classification,
image-to-image translation and attribute-based image editing, respectively. In order to demonstrate
the scalability of the model to large and complex datasets, we also present attribute-conditional image
generation on the COCO dataset [14].

2
2.1

Model
Generative Adversarial Networks (GANs)

Generative Adversarial Networks (GANs) [1] consist of a generator G and a discriminator D that
compete in a two-player minimax game, where the generator is learned to map samples from an
arbitray latent distribution to data, while the discriminator tries to distinguish between real and
generated samples. The goal of the generator is to ?fool? the discriminator by producing samples that
are as close to real data as possible. Specifically, D and G are learned as
min max V (D, G) = Ex?p(x) [log D(x)] + Ez?pz (z) [log(1 ? D(G(z)))] ,
G

D

(1)

where p(x) is the true data distribution, and pz (z) is usually defined to be a simple distribution, such
as the standard normal distribution. The generator G implicitly defines a probability distribution
pg (x) as the distribution of the samples G(z) obtained when z ? pz (z). For any fixed generator
2

p(x)
G, the optimal discriminator is D(x) = pg (x)+p(x)
. When the discriminator is optimal, solving this
adversarial game is equivalent to minimizing the Jenson-Shannon Divergence (JSD) between p(x)
and pg (x) [1]. The global equilibrium is achieved if and only if p(x) = pg (x).

2.2

Triangle Generative Adversarial Networks (?-GANs)

We now extend GAN to ?-GAN for joint distribution matching. We first consider ?-GAN in the
supervised setting, and then discuss semi-supervised learning in Section 2.4. Consider two related
domains, with x and y being the data samples for each domain. We have fully-paired data samples
that are characterized by the joint distribution p(x, y), which also implies that samples from both the
marginal p(x) and p(y) can be easily obtained.
?-GAN consists of two generators: (i) a generator Gx (y) that defines the conditional distribution
px (x|y), and (ii) a generator Gy (x) that characterizes the conditional distribution in the other
direction py (y|x). Gx (y) and Gy (x) may also implicitly contain a random latent variable z as input,
i.e., Gx (y, z) and Gy (x, z). In the ?-GAN game, after a sample x is drawn from p(x), the generator
? following the conditional distribution py (y|x). Hence, the fake data
Gy produces a pseudo sample y
? ) is a sample from the joint distribution py (x, y) = py (y|x)p(x). Similarly, a fake data
pair (x, y
pair (?
x, y) can be sampled from the generator Gx by first drawing y from p(y) and then drawing
? from px (x|y); hence (?
x
x, y) is sampled from the joint distribution px (x, y) = px (x|y)p(y). As
such, the generative process between px (x, y) and py (x, y) is reversed.
The objective of ?-GAN is to match the three joint distributions: p(x, y), px (x, y) and py (x, y). If
this is achieved, we are ensured that we have learned a bidirectional mapping px (x|y) and py (y|x)
? ) are indistinguishable from the true
that guarantees the generated fake data pairs (?
x, y) and (x, y
data pairs (x, y). In order to match the joint distributions, an adversarial game is played. Joint pairs
are drawn from three distributions: p(x, y), px (x, y) or py (x, y), and two discriminator networks
are learned to discriminate among the three, while the two conditional generator networks are trained
to fool the discriminators.
The value function describing the game is given by
min max V (Gx , Gy , D1 , D2 ) = E(x,y)?p(x,y) [log D1 (x, y)]
h

i
+ Ey?p(y),?x?px (x|y) log (1 ? D1 (?
x, y)) ? D2 (?
x, y)
h

i
? )) ? (1 ? D2 (x, y
? )) .
+ Ex?p(x),?y?py (y|x) log (1 ? D1 (x, y

Gx ,Gy D1 ,D2

(2)

The discriminator D1 is used to distinguish whether a sample pair is from p(x, y) or not, if this
sample pair is not from p(x, y), another discriminator D2 is used to distinguish whether this sample
pair is from px (x, y) or py (x, y). D1 and D2 work cooperatively, and the use of both implicitly
defines a ternary discriminative function D that distinguish sample pairs in three ways. See Figure 1
for an illustration of the adversarial game and Appendix B for an algorithmic description of the
training procedure.
2.3

Theoretical analysis

?-GAN shares many of the theoretical properties of GANs [1]. We first consider the optimal
discriminators D1 and D2 for any given generator Gx and Gy . These optimal discriminators then
allow reformulation of objective (2), which reduces to the Jensen-Shannon divergence among the
joint distribution p(x, y), px (x, y) and py (x, y).
Proposition 1. For any fixed generator Gx and Gy , the optimal discriminator D1 and D2 of the
game defined by V (Gx , Gy , D1 , D2 ) is
D1? (x, y) =

p(x, y)
px (x, y)
, D2? (x, y) =
.
p(x, y) + px (x, y) + py (x, y)
px (x, y) + py (x, y)

Proof. The proof is a straightforward extension of the proof in [1]. See Appendix A for details.
Proposition 2. The equilibrium of V (Gx , Gy , D1 , D2 ) is achieved if and only if p(x, y) =
px (x, y) = py (x, y) with D1? (x, y) = 31 and D2? (x, y) = 21 , and the optimum value is ?3 log 3.
3

Proof. Given the optimal D1? (x, y) and D2? (x, y), the minimax game can be reformulated as:
C(Gx , Gy ) = max V (Gx , Gy , D1 , D2 )
D1 ,D2


= ?3 log 3 + 3 ? JSD p(x, y), px (x, y), py (x, y) ? ?3 log 3 ,

(3)
(4)

where JSD denotes the Jensen-Shannon divergence (JSD) among three distributions. See Appendix
A for details.
Since p(x, y) = px (x, y) = py (x, y) can be achieved in theory, it can be readily seen that the
learned conditional generators can reveal the true conditional distributions underlying the data, i.e.,
px (x|y) = p(x|y) and py (y|x) = p(y|x).
2.4

Semi-supervised learning

In order to further understand ?-GAN, we write (2) as
? ))]
V = Ep(x,y) [log D1 (x, y)] + Epx (?x,y) [log(1 ? D1 (?
x, y))] + Epy (x,?y) [log(1 ? D1 (x, y
{z
}
|

(5)

conditional GAN

? ))] .
+ Epx (?x,y) [log D2 (?
x, y)] + Epy (x,?y) [log(1 ? D2 (x, y
|
{z
}

(6)

BiGAN/ALI

The objective of ?-GAN is a combination of the objectives of conditional GAN and BiGAN. The
BiGAN part matches two joint distributions: px (x, y) and py (x, y), while the conditional GAN part
provides the supervision signal to notify the BiGAN part what joint distribution to match. Therefore,
?-GAN provides a natural way to perform semi-supervised learning, since the conditional GAN part
and the BiGAN part can be used to account for paired and unpaired data, respectively.
However, when doing semi-supervised learning, there is also one potential problem that we need
to be cautious about. The theoretical analysis in Section 2.3 is based on the assumption that the
dataset is fully supervised, i.e., we have the ground-truth joint distribution p(x, y) and marginal
distributions p(x) and p(y). In the semi-supervised setting, p(x) and p(y) are still available but
p(x, y) is not. We can only obtain the joint distribution pl (x, y) characterized by the few paired data
samples. Hence, in the semi-supervised setting, px (x, y) and py (x, y) will try to concentrate to the
empirical distribution pl (x, y). We make the assumption that pl (x, y) ? p(x, y), i.e., the paired
data can roughly characterize the whole dataset. For example, in the semi-supervised classification
problem, one usually strives to make sure that labels are equally distributed among the labeled dataset.
2.5

Relation to Triple GAN

?-GAN is closely related to Triple GAN [12]. Below we review Triple GAN and then discuss the
main differences. The value function of Triple GAN is defined as follows:
? ))]
V =Ep(x,y) [log D(x, y)] + (1 ? ?)Epx (?x,y) [log(1 ? D(?
x, y))] + ?Epy (x,?y) [log(1 ? D(x, y
+Ep(x,y) [? log py (y|x)] ,

(7)

where ? ? (0, 1) is a contant that controls the relative importance of the two generators. Let Triple
GAN-s denote a simplified Triple GAN model with only the first three terms. As can be seen, Triple
GAN-s can be considered as a combination of two conditional GANs, with the importance of each
condtional GAN weighted by ?. It can be proven that Triple GAN-s achieves equilibrium if and
only if p(x, y) = (1 ? ?)px (x, y) + ?py (x, y), which is not desirable. To address this problem, in
Triple GAN a standard supervised loss RL = Ep(x,y) [? log py (y|x)] is added. As a result, when the
discriminator is optimal, the cost function in Triple GAN becomes:


2JSD p(x, y)||((1 ? ?)px (x, y) + ?py (x, y)) + KL(p(x, y)||py (x, y)) + const. (8)
This cost function has the good property that it has a unique minimum at p(x, y) = px (x, y) =
py (x, y). However, the objective becomes asymmetrical. The second KL term pays low cost
for generating fake-looking samples [13]. By contrast ?-GAN directly optimizes the symmetric Jensen-Shannon divergence among three distributions. More importantly, the calculation of
4

Ep(x,y) [? log py (y|x)] in Triple GAN also implies that the explicit density form of py (y|x) should
be provided, which may not be desirable. On the other hand,
R ?-GAN only requires that py (y|x) can
be sampled from. For example, if we assume py (y|x) = ?(y ? Gy (x, z))p(z)dz, and ?(?) is the
Dirac delta function, we can sample y through sampling z, however, the density function of py (y|x)
is not explicitly available.
2.6

Applications

?-GAN is a general framework that can be used for any joint distribution matching. Besides
the semi-supervised image classification task considered in [12], we also conduct experiments on
image-to-image translation and attribute-conditional image generation. When modeling image pairs,
both px (x|y) and py (y|x) are implemented without introducing additional latent variables, i.e.,
px (x|y) = ?(x ? Gx (y)), py (y|x) = ?(y ? Gy (x)).
A different strategy is adopted when modeling the image-label/attribute pairs. Specifically, let x
denote samples in the image domain, y denote samples in the label/attribute domain. y is a one-hot
vector or a binary vector when representing labels and attributes, respectively. When modeling
px (x|y), we assume that Rx is transformed by the latent style variables z given the label or attribute
vector y, i.e., px (x|y) = ?(x ? Gx (y, z))p(z)dz, where p(z) is chosen to be a simple distribution
(e.g., uniform or standard normal). When learning py (y|x), py (y|x) is assumed to be a standard
multi-class or multi-label classfier without latent variables z. In order to allow the training signal
backpropagated from D1 and D2 to Gy , we adopt the REINFORCE algorithm as in [12], and use the
label with the maximum probability to approximate the expectation over y, or use the output of the
sigmoid function as the predicted attribute vector.

3

Related work

The proposed framework focuses on designing GAN for joint-distribution matching. Conditional
GAN can be used for this task if supervised data is available. Various conditional GANs have been
proposed to condition the image generation on class labels [6], attributes [15], texts [4, 16] and
images [5, 17]. Unsupervised learning methods have also been developed for this task. BiGAN [11]
and ALI [10] proposed a method to jointly learn a generation network and an inference network
via adversarial learning. Though originally designed for learning the two-way transition between
the stochastic latent variables and real data samples, BiGAN and ALI can be directly adapted to
learn the joint distribution of two real domains. Another method is called DiscoGAN [7], in which
two generators are used to model the bidirectional mapping between domains, and another two
discriminators are used to decide whether a generated sample is fake or not in each individual
domain. Further, additional reconstructon losses are introduced to make the two generators strongly
coupled and also alleviate the problem of mode collapsing. Similiar work includes CycleGAN [8],
DualGAN [9] and DTN [18]. Additional weight-sharing constraints are introduced in CoGAN [19]
and UNIT [20].
Our work differs from the above work in that we aim at semi-supervised joint distribution matching.
The only work that we are aware of that also achieves this goal is Triple GAN. However, our model is
distinct from Triple GAN in important ways (see Section 2.5). Further, Triple GAN only focuses on
image classification, while ?-GAN has been shown to be applicable to a wide range of applications.
Various methods and model architectures have been proposed to improve and stabilize the training
of GAN, such as feature matching [21, 22, 23], Wasserstein GAN [24], energy-based GAN [25],
and unrolled GAN [26] among many other related works. Our work is orthogonal to these methods,
which could also be used to improve the training of ?-GAN. Instead of using adversarial loss, there
also exists work that uses supervised learning [27] for joint-distribution matching, and variational
autoencoders for semi-supervised learning [28, 29]. Lastly, our work is also closely related to the
recent work of [30, 31, 32], which treats one of the domains as latent variables.

4

Experiments

We present results on three tasks: (i) semi-supervised classification on CIFAR10 [33]; (ii) imageto-image translation on MNIST [34] and the edges2shoes dataset [5]; and (iii) attribute-to-image
generation on CelebA [35] and COCO [14]. We also conduct a toy data experiment to further
demonstrate the differences between ?-GAN and Triple GAN. We implement ?-GAN without
introducing additional regularization unless explicitly stated. All the network architectures are
provided in the Appendix.
5

(a) real data

(b) Triangle GAN

(c) Triple GAN

Figure 2: Toy data experiment on ?-GAN and Triple GAN. (a) the joint distribution p(x, y) of real data. For
(b) and (c), the left and right figure is the learned joint distribution px (x, y) and py (x, y), respectively.
Table 1: Error rates (%) on the partially labeled CIFAR10 dataset.
Algorithm

n = 4000

CatGAN [36]
Improved GAN [21]
ALI [10]
Triple GAN [12]

19.58 ? 0.58
18.63 ? 2.32
17.99 ? 1.62
16.99 ? 0.36

?-GAN (ours)

16.80 ? 0.42

Table 2: Classification accuracy (%) on the MNIST-toMNIST-transpose dataset.
n = 100

n = 1000

All

DiscoGAN
Triple GAN

?
63.79 ? 0.85

?
84.93 ? 1.63

15.00? 0.20
86.70 ? 1.52

?-GAN

83.20? 1.88

88.98? 1.50

93.34? 1.46

Algorithm

4.1 Toy data experiment
We first compare our method with Triple GAN on a toy dataset. We synthesize data by drawing
(x, y) ? 14 N (?1 , ?1 ) + 14 N (?2 , ?2 ) + 14 N (?3 , ?3 ) + 14 N (?4 , ?4 ), where ?1 = [0, 1.5]> , ?2 =
0 ) and ? = ? = ( 0.025 0 ). We
[?1.5, 0]> , ?3 = [1.5, 0]> , ?4 = [0, ?1.5]> , ?1 = ?4 = ( 30 0.025
2
3
0 3
generate 5000 (x, y) pairs for each mixture component.
In
order
to implement ?-GAN andR Triple
R
GAN-s, we model px (x|y) and py (y|x) as px (x|y) = ?(x ? Gx (y, z))p(z)dz, py (y|x) = ?(y ?
Gy (x, z))p(z)dz where both Gx and Gy are modeled as a 4-hidden-layer multilayer perceptron
(MLP) with 500 hidden units in each layer. p(z) is a bivariate standard Gaussian distribution. Triple
GAN can be implemented by specifying both px (x|y) and py (y|x) to be distributions with explicit
density form, e.g., Gaussian distributions. However, the performance can be bad since it fails to
capture the multi-modality of px (x|y) and py (y|x). Hence, only Triple GAN-s is implemented.
Results are shown in Figure 2. The joint distributions px (x, y) and py (x, y) learned by ?-GAN
successfully match the true joint distribution p(x, y). Triple GAN-s cannot achieve this, and can only
guarantee 12 (px (x, y) + py (x, y)) matches p(x, y). Although this experiment is limited due to its
simplicity, the results clearly support the advantage of our proposed model over Triple GAN.
4.2 Semi-supervised classification
We evaluate semi-supervised classification on the CIFAR10 dataset with 4000 labels. The labeled
data is distributed equally across classes and the results are averaged over 10 runs with different
random splits of the training data. For fair comparison, we follow the publically available code of
Triple GAN and use the same regularization terms and hyperparameter settings as theirs. Results
are summarized in Table 1. Our ?-GAN achieves the best performance among all the competing
methods. We also show the ability of ?-GAN to disentangle classes and styles in Figure 3. ?-GAN
can generate realistic data in a specific class and the injected noise vector encodes meaningful style
patterns like background and color.
4.3 Image-to-image translation
We first evaluate image-to-image translation on the edges2shoes dataset. Results are shown in
Figure 4(bottom). Though DiscoGAN is an unsupervised learning method, it achieves impressive
results. However, with supervision provided by 10% paired data, ?-GAN generally generates more
accurate edge details of the shoes. In order to provide quantitative evaluation of translating shoes to
edges, we use mean squared error (MSE) as our metric. The MSE of using DiscoGAN is 140.1; with
10%, 20%, 100% paired data, the MSE of using ?-GAN is 125.3, 113.0 and 66.4, respectively.
To further demonstrate the importance of providing supervision of domain correspondence, we
created a new dataset based on MNIST [34], where the two image domains are the MNIST images
and their corresponding tranposed ones. As can be seen in Figure 4(top), ?-GAN matches images
6

DiscoGAN

-GAN

Input:
Output:
Input:
Output:
Input:
GT Output:
DiscoGAN:
-GAN:

Figure 3: Generated CIFAR10 samples, where

Figure 4: Image-to-image translation experiments

each row shares the same label and each column
uses the same noise.

on the MNIST-to-MNIST-transpose and edges2shoes
datasets.

Input
images

Predicted
attributes

Big Nose,
Black Hair,
Bushy
Eyebrows,
Male, Young,
Sideburns

Attractive,
Smiling, High
Cheekbones,
Mouth Slightly
Open, Wearing
Lipstick

Attractive,
Black Hair,
Male, High
Cheekbones,
Smiling,
Straight Hair

Big Nose,
Chubby,
Goatee, Male,
Oval Face,
Sideburns,
Wearing Hat

Attractive,
Blond Hair, No
Beard, Pointy
Nose, Straight
Hair, Arched
Eyebrows

High
Cheekbones,
Mouth Slightly
Open, No
Beard, Oval
Face, Smiling

Attractive,
Brown Hair,
Heavy
Makeup, No
Beard, Wavy
Hair, Young

Attractive,
Eyeglasses, No
Beard, Straight
Hair, Wearing
Lipstick,
Young

Generated
images

Figure 5: Results on the face-to-attribute-to-face experiment. The 1st row is the input images; the 2nd row is
the predicted attributes given the input images; the 3rd row is the generated images given the predicted attributes.

Table 3: Results of P@10 and nDCG@10 for attribute predicting on CelebA and COCO.
Dataset
Method
Triple GAN
?-GAN

CelebA

COCO

1%

10%

100%

10%

50%

100%

40.97/50.74
53.21/58.39

62.13/73.56
63.68/75.22

70.12/79.37
70.37/81.47

32.64/35.91
34.38/37.91

34.00/37.76
36.72/40.39

35.35/39.60
39.05/42.86

betwen domains well, while DiscoGAN fails in this task. For supporting quantitative evaluation,
we have trained a classifier on the MNIST dataset, and the classification accuracy of this classifier
on the test set approaches 99.4%, and is, therefore, trustworthy as an evaluation metric. Given an
input MNIST image x, we first generate a transposed image y using the learned generator, and then
manually transpose it back to normal digits y T , and finally send this new image y T to the classifier.
Results are summarized in Table 2, which are averages over 5 runs with different random splits of the
training data. ?-GAN achieves significantly better performance than Triple GAN and DiscoGAN.
4.4

Attribute-conditional image generation

We apply our method to face images from the CelebA dataset. This dataset consists of 202,599
images annotated with 40 binary attributes. We scale and crop the images to 64 ? 64 pixels. In
order to qualitatively evaluate the learned attribute-conditional image generator and the multi-label
classifier, given an input face image, we first use the classifier to predict attributes, and then use
the image generator to produce images based on the predicted attributes. Figure 5 shows example
results. Both the learned attribute predictor and the image generator provides good results. We further
show another set of image editing experiment in Figure 6. For each subfigure, we use a same set of
attributes with different noise vectors to generate images. For example, for the top-right subfigure,
7

1st row + pale skin = 2nd row

1st row + eyeglasses = 2nd row

1st row + mouth slightly open = 2nd row

1st row + wearing hat = 2nd row

Figure 6: Results on the image editing experiment.
Input

Predicted attributes

Generated images

Input

Predicted attributes

baseball, standing, next,
player, man, group,
person, field, sport, ball,
outdoor, game, grass,
crowd

tennis, player, court,
man, playing, field,
racket, sport, swinging,
ball, outdoor, holding,
game, grass

surfing, people, woman,
water, standing, wave,
man, top, riding, sport,
ocean, outdoor, board

skiing, man, group,
covered, day, hill,
person, snow, riding,
outdoor

!

!

red, sign, street, next,
pole, outdoor, stop, grass

pizza, rack, blue, grill,
plate, stove, table, pan,
holding, pepperoni,
cooked

!

sink, shower, indoor,
tub, restroom, bathroom,
small, standing, room,
tile, white, stall, tiled,
black, bath

computer, laptop, room,
front, living, indoor,
table, desk

!

!

Generated images

Figure 7: Results on the image-to-attribute-to-image experiment.

all the images in the 1st row were generated based on the following attributes: black hair, female,
attractive, and we then added the attribute of ?sunglasses? when generating the images in the 2nd row.
It is interesting to see that ?-GAN has great flexibility to adjust the generated images by changing
certain input attribtutes. For instance, by switching on the wearing hat attribute, one can edit the face
image to have a hat on the head.
In order to demonstrate the scalablility of our model to large and complex datasets, we also present
results on the COCO dataset. Following [37], we first select a set of 1000 attributes from the caption
text in the training set, which includes the most frequent nouns, verbs, or adjectives. The images in
COCO are scaled and cropped to have 64 ? 64 pixels. Unlike the case of CelebA face images, the
networks need to learn how to handle multiple objects and diverse backgrounds. Results are provided
in Figure 7. We can generate reasonably good images based on the predicted attributes. The input
and generated images also clearly share a same set of attributes. We also observe diversity in the
samples by simply drawing multple noise vectors and using the same predicted attributes.
Precision (P) and normalized Discounted Cumulative Gain (nDCG) are two popular evaluation
metrics for multi-label classification problems. Table 3 provides the quantatitive results of P@10 and
nDCG@10 on CelebA and COCO, where @k means at rank k (see the Appendix for definitions). For
fair comparison, we use the same network architecures for both Triple GAN and ?-GAN. ?-GAN
consistently provides better results than Triple GAN. On the COCO dataset, our semi-supervised
learning approach with 50% labeled data achieves better performance than the results of Triple GAN
using the full dataset, demonstrating the effectiveness of our approach for semi-supervised joint
distribution matching. More results for the above experiments are provided in the Appendix.

5

Conclusion

We have presented the Triangle Generative Adversarial Network (?-GAN), a new GAN framework
that can be used for semi-supervised joint distribution matching. Our approach learns the bidirectional
mappings between two domains with a few paired samples. We have demonstrated that ?-GAN may
be employed for a wide range of applications. One possible future direction is to combine ?-GAN
with sequence GAN [38] or textGAN [23] to model the joint distribution of image-caption pairs.
Acknowledgements This research was supported in part by ARO, DARPA, DOE, NGA and ONR.
8

References
[1] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
[2] Emily Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. Deep generative image models using a
laplacian pyramid of adversarial networks. In NIPS, 2015.
[3] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In ICLR, 2016.
[4] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.
Generative adversarial text to image synthesis. In ICML, 2016.
[5] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional
adversarial networks. In CVPR, 2017.
[6] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv:1411.1784, 2014.
[7] Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jungkwon Lee, and Jiwon Kim. Learning to discover crossdomain relations with generative adversarial networks. In ICML, 2017.
[8] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using
cycle-consistent adversarial networks. In ICCV, 2017.
[9] Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dualgan: Unsupervised dual learning for image-to-image
translation. In ICCV, 2017.
[10] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro, and
Aaron Courville. Adversarially learned inference. In ICLR, 2017.
[11] Jeff Donahue, Philipp Kr?henb?hl, and Trevor Darrell. Adversarial feature learning. In ICLR, 2017.
[12] Chongxuan Li, Kun Xu, Jun Zhu, and Bo Zhang. Triple generative adversarial nets. In NIPS, 2017.
[13] Martin Arjovsky and L?on Bottou. Towards principled methods for training generative adversarial networks.
In ICLR, 2017.
[14] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll?r,
and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.
[15] Guim Perarnau, Joost van de Weijer, Bogdan Raducanu, and Jose M ?lvarez. Invertible conditional gans
for image editing. arXiv:1611.06355, 2016.
[16] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei Huang, Xiaogang Wang, and Dimitris
Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks.
In ICCV, 2017.
[17] Christian Ledig, Lucas Theis, Ferenc Husz?r, Jose Caballero, Andrew Cunningham, Alejandro Acosta,
Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image superresolution using a generative adversarial network. In CVPR, 2017.
[18] Yaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised cross-domain image generation. In ICLR,
2017.
[19] Ming-Yu Liu and Oncel Tuzel. Coupled generative adversarial networks. In NIPS, 2016.
[20] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In
NIPS, 2017.
[21] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved
techniques for training gans. In NIPS, 2016.
[22] Yizhe Zhang, Zhe Gan, and Lawrence Carin. Generating text via adversarial training. In NIPS workshop
on Adversarial Training, 2016.
[23] Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, and Lawrence Carin. Adversarial
feature matching for text generation. In ICML, 2017.
[24] Martin Arjovsky, Soumith Chintala, and L?on Bottou. Wasserstein gan. arXiv:1701.07875, 2017.

9

[25] Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network. In ICLR,
2017.
[26] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks.
In ICLR, 2017.
[27] Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, and Tie-Yan Liu. Dual supervised learning. In
ICML, 2017.
[28] Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens, and Lawrence Carin.
Variational autoencoder for deep learning of images, labels and captions. In NIPS, 2016.
[29] Yunchen Pu, Zhe Gan, Ricardo Henao, Chunyuan Li, Shaobo Han, and Lawrence Carin. Vae learning via
stein variational gradient descent. In NIPS, 2017.
[30] Chunyuan Li, Hao Liu, Changyou Chen, Yunchen Pu, Liqun Chen, Ricardo Henao, and Lawrence Carin.
Alice: Towards understanding adversarial learning for joint distribution matching. In NIPS, 2017.
[31] Yunchen Pu, Weiyao Wang, Ricardo Henao, Liqun Chen, Zhe Gan, Chunyuan Li, and Lawrence Carin.
Adversarial symmetric variational autoencoder. In NIPS, 2017.
[32] Yunchen Pu, Liqun Chen, Shuyang Dai, Weiyao Wang, Chunyuan Li, and Lawrence Carin. Symmetric
variational autoencoder and connections to adversarial learning. In NIPS, 2017.
[33] Alex Krizhevsky. Learning multiple layers of features from tiny images. Citeseer, 2009.
[34] Yann LeCun, L?on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 1998.
[35] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
ICCV, 2015.
[36] Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks. arXiv:1511.06390, 2015.
[37] Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu, Kenneth Tran, Jianfeng Gao, Lawrence Carin, and
Li Deng. Semantic compositional networks for visual captioning. In CVPR, 2017.
[38] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: sequence generative adversarial nets with
policy gradient. In AAAI, 2017.

10

"
6311,2017,One-Shot Imitation Learning,"Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning.  Specifically, we consider the setting where there is a very large (maybe infinite) set of tasks, and each task has many instantiations.  For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states.  At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks.  A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. Our experiments show that the use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks.","One-Shot Imitation Learning

Yan Duan?? , Marcin Andrychowicz? , Bradly Stadie?? , Jonathan Ho?? ,
Jonas Schneider? , Ilya Sutskever? , Pieter Abbeel?? , Wojciech Zaremba?
?
Berkeley AI Research Lab, ? OpenAI
?
Work done while at OpenAI
{rockyduan, jonathanho, pabbeel}@eecs.berkeley.edu
{marcin, bstadie, jonas, ilyasu, woj}@openai.com

Abstract
Imitation learning has been commonly applied to solve different tasks in isolation.
This usually requires either careful feature engineering, or a significant number of
samples. This is far from what we desire: ideally, robots should be able to learn
from very few demonstrations of any given task, and instantly generalize to new
situations of the same task, without requiring task-specific engineering. In this
paper, we propose a meta-learning framework for achieving such capability, which
we call one-shot imitation learning.
Specifically, we consider the setting where there is a very large (maybe infinite)
set of tasks, and each task has many instantiations. For example, a task could be
to stack all blocks on a table into a single tower, another task could be to place
all blocks on a table into two-block towers, etc. In each case, different instances
of the task would consist of different sets of blocks with different initial states.
At training time, our algorithm is presented with pairs of demonstrations for a
subset of all tasks. A neural net is trained such that when it takes as input the first
demonstration demonstration and a state sampled from the second demonstration,
it should predict the action corresponding to the sampled state. At test time, a full
demonstration of a single instance of a new task is presented, and the neural net
is expected to perform well on new instances of this new task. Our experiments
show that the use of soft attention allows the model to generalize to conditions and
tasks unseen in the training data. We anticipate that by training this model on a
much greater variety of tasks and settings, we will obtain a general system that can
turn any demonstrations into robust policies that can accomplish an overwhelming
variety of tasks.

1

Introduction

We are interested in robotic systems that are able to perform a variety of complex useful tasks, e.g.
tidying up a home or preparing a meal. The robot should be able to learn new tasks without long
system interaction time. To accomplish this, we must solve two broad problems. The first problem is
that of dexterity: robots should learn how to approach, grasp and pick up complex objects, and how
to place or arrange them into a desired configuration. The second problem is that of communication:
how to communicate the intent of the task at hand, so that the robot can replicate it in a broader set of
initial conditions.
Demonstrations are an extremely convenient form of information we can use to teach robots to overcome these two challenges. Using demonstrations, we can unambiguously communicate essentially
any manipulation task, and simultaneously provide clues about the specific motor skills required to
perform the task. We can compare this with an alternative form of communication, namely natural
language. Although language is highly versatile, effective, and efficient, natural language processing
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

Policy for task F

Many
demonstrations

Imitation
Learning
Algorithm

Policy for
task A

Task A
e.g. stack blocks into towers of height 3

One-Shot Imitator
(Neural Network)

Meta Learning
Algorithm

obs

action

Single demonstration
for task F

Many demonstrations
for task A

Many demonstrations
for task B

obs

action

?
more demonstrations for
more tasks

Environment

Environment

(b) One-Shot Imitation Learning

Demo1

Many
demonstrations

Imitation
Learning
Algorithm

sample

Policy for
task B

action

Task B
e.g. stack blocks into towers of height 2

Many demonstrations
for task A
One-Shot Imitator
(Neural Network)

obs
Many demonstrations
for task B

predicted
action

observation from
Demo2

Environment

corresponding
action in Demo2
Supervised loss

(a) Traditional Imitation Learning

(c) Training the One-Shot Imitator

Figure 1: (a) Traditionally, policies are task-specific. For example, a policy might have been
trained through an imitation learning algorithm to stack blocks into towers of height 3, and then
another policy would be trained to stack blocks into towers of height 2, etc. (b) In this paper, we
are interested in training networks that are not specific to one task, but rather can be told (through a
single demonstration) what the current new task is, and be successful at this new task. For example,
when it is conditioned on a single demonstration for task F, it should behave like a good policy for
task F. (c) We can phrase this as a supervised learning problem, where we train this network on a set
of training tasks, and with enough examples it should generalize to unseen, but related tasks. To train
this network, in each iteration we sample a demonstration from one of the training tasks, and feed it
to the network. Then, we sample another pair of observation and action from a second demonstration
of the same task. When conditioned on both the first demonstration and this observation, the network
is trained to output the corresponding action.

systems are not yet at a level where we could easily use language to precisely describe a complex
task to a robot. Compared to language, using demonstrations has two fundamental advantages: first,
it does not require the knowledge of language, as it is possible to communicate complex tasks to
humans that don?t speak one?s language. And second, there are many tasks that are extremely difficult
to explain in words, even if we assume perfect linguistic abilities: for example, explaining how to
swim without demonstration and experience seems to be, at the very least, an extremely challenging
task.
Indeed, learning from demonstrations have had many successful applications . However, so far
these applications have either required careful feature engineering, or a significant amount of system
interaction time. This is far from what what we desire: ideally, we hope to demonstrate a certain task
only once or a few times to the robot, and have it instantly generalize to new situations of the same
task, without long system interaction time or domain knowledge about individual tasks.
In this paper we explore the one-shot imitation learning setting illustrated in Fig. 1, where the
objective is to maximize the expected performance of the learned policy when faced with a new,
previously unseen, task, and having received as input only one demonstration of that task. For the
tasks we consider, the policy is expected to achieve good performance without any additional system
interaction, once it has received the demonstration.
We train a policy on a broad distribution over tasks, where the number of tasks is potentially infinite.
For each training task we assume the availability of a set of successful demonstrations. Our learned
policy takes as input: (i) the current observation, and (ii) one demonstration that successfully solves
a different instance of the same task (this demonstration is fixed for the duration of the episode).
The policy outputs the current controls. We note that any pair of demonstrations for the same task
provides a supervised training example for the neural net policy, where one demonstration is treated
as the input, while the other as the output.
2

To make this model work, we made essential use of soft attention [6] for processing both the (potentially long) sequence of states and action that correspond to the demonstration, and for processing the
components of the vector specifying the locations of the various blocks in our environment. The use
of soft attention over both types of inputs made strong generalization possible. In particular, on a
family of block stacking tasks, our neural network policy was able to perform well on novel block
configurations which were not present in any training data. Videos of our experiments are available
at http://bit.ly/nips2017-oneshot.

2

Related Work

Imitation learning considers the problem of acquiring skills from observing demonstrations. Survey
articles include [48, 11, 3].
Two main lines of work within imitation learning are behavioral cloning, which performs supervised
learning from observations to actions (e.g., [41, 44]); and inverse reinforcement learning [37], where
a reward function [1, 66, 29, 18, 22] is estimated that explains the demonstrations as (near) optimal
behavior. While this past work has led to a wide range of impressive robotics results, it considers
each skill separately, and having learned to imitate one skill does not accelerate learning to imitate
the next skill.
One-shot and few-shot learning has been studied for image recognition [61, 26, 47, 42], generative
modeling [17, 43], and learning ?fast? reinforcement learning agents with recurrent policies [16, 62].
Fast adaptation has also been achieved through fast-weights [5]. Like our algorithm, many of the
aforementioned approaches are a form of meta-learning [58, 49, 36], where the algorithm itself is
being learned. Meta-learning has also been studied to discover neural network weight optimization
algorithms [8, 9, 23, 50, 2, 31]. This prior work on one-shot learning and meta-learning, however,
is tailored to respective domains (image recognition, generative models, reinforcement learning,
optimization) and not directly applicable in the imitation learning setting. Recently, [19] propose a
generic framework for meta learning across several aforementioned domains. However they do not
consider the imitation learning setting.
Reinforcement learning [56, 10] provides an alternative route to skill acquisition, by learning through
trial and error. Reinforcement learning has had many successes, including Backgammon [57],
helicopter control [39], Atari [35], Go [52], continuous control in simulation [51, 21, 32] and on
real robots [40, 30]. However, reinforcement learning tends to require a large number of trials and
requires specifying a reward function to define the task at hand. The former can be time-consuming
and the latter can often be significantly more difficult than providing a demonstration [37].
Multi-task and transfer learning considers the problem of learning policies with applicability and
re-use beyond a single task. Success stories include domain adaptation in computer vision [64, 34,
28, 4, 15, 24, 33, 59, 14] and control [60, 45, 46, 20, 54]. However, while acquiring a multitude of
skills faster than what it would take to acquire each of the skills independently, these approaches do
not provide the ability to readily pick up a new skill from a single demonstration.
Our approach heavily relies on an attention model over the demonstration and an attention model
over the current observation. We use the soft attention model proposed in [6] for machine translations,
and which has also been successful in image captioning [63]. The interaction networks proposed
in [7, 12] also leverage locality of physical interaction in learning. Our model is also related to
the sequence to sequence model [55, 13], as in both cases we consume a very long demonstration
sequence and, effectively, emit a long sequence of actions.

3
3.1

One Shot Imitation Learning
Problem Formalization

We denote a distribution of tasks by T, an individual task by t ? T, and a distribution of demonstrations for the task t by D(t). A policy is symbolized by ?? (a|o, d), where a is an action, o is
an observation, d is a demonstration, and ? are the parameters of the policy. A demonstration
d ? D(t) is a sequence of observations and actions : d = [(o1 , a1 ), (o2 , a2 ), . . . , (oT , aT )]. We
assume that the distribution of tasks T is given, and that we can obtain successful demonstrations for
each task. We assume that there is some scalar-valued evaluation function Rt (d) (e.g. a binary value
3

indicating success) for each task, although this is not required during training. The objective is to
maximize the expected performance of the policy, where the expectation is taken over tasks t ? T,
and demonstrations d ? D(t).
3.2

Block Stacking Tasks

To clarify the problem setting, we describe a concrete example of a distribution of block stacking
tasks, which we will also later study in the experiments. The compositional structure shared among
these tasks allows us to investigate nontrivial generalization to unseen tasks. For each task, the goal is
to control a 7-DOF Fetch robotic arm to stack various numbers of cube-shaped blocks into a specific
configuration specified by the user. Each configuration consists of a list of blocks arranged into
towers of different heights, and can be identified by a string. For example, ab cd ef gh means
that we want to stack 4 towers, each with two blocks, and we want block A to be on top of block B,
block C on top of block D, block E on top of block F, and block G on top of block H. Each of these
configurations correspond to a different task. Furthermore, in each episode the starting positions
of the blocks may vary, which requires the learned policy to generalize even within the training
tasks. In a typical task, an observation is a list of (x, y, z) object positions relative to the gripper,
and information if gripper is opened or closed. The number of objects may vary across different
task instances. We define a stage as a single operation of stacking one block on top of another. For
example, the task ab cd ef gh has 4 stages.
3.3

Algorithm

In order to train the neural network policy, we make use of imitation learning algorithms such
as behavioral cloning and DAGGER [44], which only require demonstrations rather than reward
functions to be specified. This has the potential to be more scalable, since it is often easier to
demonstrate a task than specifying a well-shaped reward function [38].
We start by collecting a set of demonstrations for each task, where we add noise to the actions in order
to have wider coverage in the trajectory space. In each training iteration, we sample a list of tasks
(with replacement). For each sampled task, we sample a demonstration as well as a small batch of
observation-action pairs. The policy is trained to regress against the desired actions when conditioned
on the current observation and the demonstration, by minimizing an `2 or cross-entropy loss based on
whether actions are continuous or discrete. A high-level illustration of the training procedure is given
in Fig. 1(c). Across all experiments, we use Adamax [25] to perform the optimization with a learning
rate of 0.001.

4

Architecture

While, in principle, a generic neural network could learn the mapping from demonstration and current
observation to appropriate action, we found it important to use an appropriate architecture. Our
architecture for learning block stacking is one of the main contributions of this paper, and we believe
it is representative of what architectures for one-shot imitation learning could look like in the future
when considering more complex tasks.
Our proposed architecture consists of three modules: the demonstration network, the context network,
and the manipulation network. An illustration of the architecture is shown in Fig. 2. We will describe
the main operations performed in each module below, and a full specification is available in the
Appendix.
4.1

Demonstration Network

The demonstration network receives a demonstration trajectory as input, and produces an embedding
of the demonstration to be used by the policy. The size of this embedding grows linearly as a function
of the length of the demonstration as well as the number of blocks in the environment.
Temporal Dropout: For block stacking, the demonstrations can span hundreds to thousands of time
steps, and training with such long sequences can be demanding in both time and memory usage.
Hence, we randomly discard a subset of time steps during training, an operation we call temporal
dropout, analogous to [53, 27]. We denote p as the proportion of time steps that are thrown away.
4

Attention
over
Current
State

Context Network
Block# A B C D E

Attention over
Demonstration
Neighborhood Attention
+
Temporal Convolution

F G H

I

J

Context Embedding

Hidden layers

Hidden layers

Temporal Dropout

Action

Demonstration Network

Manipulation Network
Demonstration

Current State

Figure 2: Illustration of the network architecture.

In our experiments, we use p = 0.95, which reduces the length of demonstrations by a factor of 20.
During test time, we can sample multiple downsampled trajectories, use each of them to compute
downstream results, and average these results to produce an ensemble estimate. In our experience,
this consistently improves the performance of the policy.
Neighborhood Attention: After downsampling the demonstration, we apply a sequence of operations, composed of dilated temporal convolution [65] and neighborhood attention. We now describe
this second operation in more detail.
Since our neural network needs to handle demonstrations with variable numbers of blocks, it must
have modules that can process variable-dimensional inputs. Soft attention is a natural operation which
maps variable-dimensional inputs to fixed-dimensional outputs. However, by doing so, it may lose
information compared to its input. This is undesirable, since the amount of information contained
in a demonstration grows as the number of blocks increases. Therefore, we need an operation that
can map variable-dimensional inputs to outputs with comparable dimensions. Intuitively, rather than
having a single output as a result of attending to all inputs, we have as many outputs as inputs, and
have each output attending to all other inputs in relation to its own corresponding input.
We start by describing the soft attention module as specified in [6]. The input to the attention includes
a query q, a list of context vectors {cj }, and a list of memory vectors {mj }. The ith attention weight
is given by wi ? v T tanh(q + ci ), where v is a learned weight vector. The output of attention is a
weighted combination of the memory content, where the weights are given by a softmax operation
P
i)
over the attention weights. Formally, we have output ? i mi Pexp(w
. Note that the output has
j exp(wj )
the same dimension as a memory vector. The attention operation can be generalized to multiple query
heads, in which case there will be as many output vectors as there are queries.
Now we turn to neighborhood attention. We assume there are B blocks in the environment. We
denote the robot?s state as srobot , and the coordinates of each block as (x1 , y1 , z1 ), . . . , (xB , yB , zB ).
in
The input to neighborhood attention is a list of embeddings hin
1 , . . . , hB of the same dimension,
which can be the result of a projection operation over a list of block positions, or the output of a
previous neighborhood attention operation. Given this list of embeddings, we use two separate linear
layers to compute a query vector and a context embedding for each block: qi ? Linear(hin
i ), and
ci ? Linear(hin
i ). The memory content to be extracted consists of the coordinates of each block,
concatenated with the input embedding. The ith query result is given by the following soft attention
in
B
operation: resulti ? SoftAttn(query: qi , context: {cj }B
j=1 , memory: {((xj , yj , zj ), hj ))}j=1 ).
Intuitively, this operation allows each block to query other blocks in relation to itself (e.g. find the
closest block), and extract the queried information. The gathered results are then combined with
each block?s own information, to produce the output embedding per block. Concretely, we have
5

outputi ? Linear(concat(hin
i , resulti , (xi , yi , zi ), srobot )). In practice, we use multiple query
heads per block, so that the size of each resulti will be proportional to the number of query heads.
4.2

Context network

The context network is the crux of our model. It processes both the current state and the embedding
produced by the demonstration network, and outputs a context embedding, whose dimension does
not depend on the length of the demonstration, or the number of blocks in the environment. Hence, it
is forced to capture only the relevant information, which will be used by the manipulation network.
Attention over demonstration: The context network starts by computing a query vector as a function
of the current state, which is then used to attend over the different time steps in the demonstration
embedding. The attention weights over different blocks within the same time step are summed
together, to produce a single weight per time step. The result of this temporal attention is a vector
whose size is proportional to the number of blocks in the environment. We then apply neighborhood
attention to propagate the information across the embeddings of each block. This process is repeated
multiple times, where the state is advanced using an LSTM cell with untied weights.
Attention over current state: The previous operations produce an embedding whose size is independent of the length of the demonstration, but still dependent on the number of blocks. We then
apply standard soft attention over the current state to produce fixed-dimensional vectors, where the
memory content only consists of positions of each block, which, together with the robot?s state, forms
the context embedding, which is then passed to the manipulation network.
Intuitively, although the number of objects in the environment may vary, at each stage of the
manipulation operation, the number of relevant objects is small and usually fixed. For the block
stacking environment specifically, the robot should only need to pay attention to the position of the
block it is trying to pick up (the source block), as well as the position of the block it is trying to place
on top of (the target block). Therefore, a properly trained network can learn to match the current
state with the corresponding stage in the demonstration, and infer the identities of the source and
target blocks expressed as soft attention weights over different blocks, which are then used to extract
the corresponding positions to be passed to the manipulation network. Although we do not enforce
this interpretation in training, our experiment analysis supports this interpretation of how the learned
policy works internally.
4.3

Manipulation network

The manipulation network is the simplest component. After extracting the information of the source
and target blocks, it computes the action needed to complete the current stage of stacking one block
on top of another one, using a simple MLP network.1 This division of labor opens up the possibility
of modular training: the manipulation network may be trained to complete this simple procedure,
without knowing about demonstrations or more than two blocks present in the environment. We leave
this possibility for future work.

5

Experiments

We conduct experiments with the block stacking tasks described in Section 3.2.2 These experiments
are designed to answer the following questions:
? How does training with behavioral cloning compare with DAGGER?
? How does conditioning on the entire demonstration compare to conditioning on the final
state, even when it already has enough information to fully specify the task?
? How does conditioning on the entire demonstration compare to conditioning on a ?snapshot?
of the trajectory, which is a small subset of frames that are most informative?
1
In principle, one can replace this module with an RNN module. But we did not find this necessary for the
tasks we consider.
2
Additional experiment results are available in the Appendix, including a simple illustrative example of
particle reaching tasks and further analysis of block stacking

6

? Can our framework generalize to tasks that it has never seen during training?
To answer these questions, we compare the performance of the following architectures:
? BC: We use the same architecture as previous, but and the policy using behavioral cloning.
? DAGGER: We use the architecture described in the previous section, and train the policy
using DAGGER.
? Final state: This architecture conditions on the final state rather than on the entire demonstration trajectory. For the block stacking task family, the final state uniquely identifies the
task, and there is no need for additional information. However, a full trajectory, one which
contains information about intermediate stages of the task?s solution, can make it easier to
train the optimal policy, because it could learn to rely on the demonstration directly, without
needing to memorize the intermediate steps into its parameters. This is related to the way in
which reward shaping can significantly affect performance in reinforcement learning [38].
A comparison between the two conditioning strategies will tell us whether this hypothesis is
valid. We train this policy using DAGGER.
? Snapshot: This architecture conditions on a ?snapshot? of the trajectory, which includes the
last frame of each stage along the demonstration trajectory. This assumes that a segmentation
of the demonstration into multiple stages is available at test time, which gives it an unfair
advantage compared to the other conditioning strategies. Hence, it may perform better than
conditioning on the full trajectory, and serves as a reference, to inform us whether the policy
conditioned on the entire trajectory can perform as well as if the demonstration is clearly
segmented. Again, we train this policy using DAGGER.
We evaluate the policy on tasks seen during training, as well as tasks unseen during training. Note
that generalization is evaluated at multiple levels: the learned policy not only needs to generalize to
new configurations and new demonstrations of tasks seen already, but also needs to generalize to new
tasks.
Concretely, we collect 140 training tasks, and 43 test tasks, each with a different desired layout of the
blocks. The number of blocks in each task can vary between 2 and 10. We collect 1000 trajectories
per task for training, and maintain a separate set of trajectories and initial configurations to be used
for evaluation. The trajectories are collected using a hard-coded policy.
5.1

Performance Evaluation
100%

Policy Type

Demo
BC
DAGGER
Snapshot
Final state

Demo
BC
DAGGER
Snapshot
Final state

80%

Average Success Rate

80%

Average Success Rate

100%

Policy Type

60%

40%

60%

40%

20%

20%

0%

0%
1

2

3

4

5

6

7

2

Number of Stages

4

5

6

7

8

Number of Stages

(a) Performance on training tasks.

(b) Performance on test tasks.

Figure 3: Comparison of different conditioning strategies. The darkest bar shows the performance of the
hard-coded policy, which unsurprisingly performs the best most of the time. For architectures that use temporal
dropout, we use an ensemble of 10 different downsampled demonstrations and average the action distributions.
Then for all architectures we use the greedy action for evaluation.

Fig. 3 shows the performance of various architectures. Results for training and test tasks are presented
separately, where we group tasks by the number of stages required to complete them. This is because
tasks that require more stages to complete are typically more challenging. In fact, even our scripted
policy frequently fails on the hardest tasks. We measure success rate per task by executing the greedy
policy (taking the most confident action at every time step) in 100 different configurations, each
conditioned on a different demonstration unseen during training. We report the average success rate
over all tasks within the same group.
7

From the figure, we can observe that for the easier tasks with fewer stages, all of the different
conditioning strategies perform equally well and almost perfectly. As the difficulty (number of stages)
increases, however, conditioning on the entire demonstration starts to outperform conditioning on the
final state. One possible explanation is that when conditioned only on the final state, the policy may
struggle about which block it should stack first, a piece of information that is readily accessible from
demonstration, which not only communicates the task, but also provides valuable information to help
accomplish it.
More surprisingly, conditioning on the entire demonstration also seems to outperform conditioning
on the snapshot, which we originally expected to perform the best. We suspect that this is due
to the regularization effect introduced by temporal dropout, which effectively augments the set of
demonstrations seen by the policy during training.
Another interesting finding was that training with behavioral cloning has the same level of performance
as training with DAGGER, which suggests that the entire training procedure could work without
requiring interactive supervision. In our preliminary experiments, we found that injecting noise into
the trajectory collection process was important for behavioral cloning to work well, hence in all
experiments reported here we use noise injection. In practice, such noise can come from natural
human-induced noise through tele-operation, or by artificially injecting additional noise before
applying it on the physical robot.
5.2

Visualization

We visualize the attention mechanisms underlying the main policy architecture to have a better
understanding about how it operates. There are two kinds of attention we are mainly interested in,
one where the policy attends to different time steps in the demonstration, and the other where the
policy attends to different blocks in the current state. Fig. 4 shows some of the attention heatmaps.

(a) Attention over blocks in the current state.

(b) Attention over downsampled demonstration.

Figure 4: Visualizing attentions performed by the policy during an entire execution. The task
being performed is ab cde fg hij. Note that the policy has multiple query heads for each type
of attention, and only one query head per type is visualized. (a) We can observe that the policy
almost always focuses on a small subset of the block positions in the current state, which allows the
manipulation network to generalize to operations over different blocks. (b) We can observe a sparse
pattern of time steps that have high attention weights. This suggests that the policy has essentially
learned to segment the demonstrations, and only attend to important key frames. Note that there are
roughly 6 regions of high attention weights, which nicely corresponds to the 6 stages required to
complete the task.

6

Conclusions

In this work, we presented a simple model that maps a single successful demonstration of a task to
an effective policy that solves said task in a new situation. We demonstrated effectiveness of this
approach on a family of block stacking tasks. There are a lot of exciting directions for future work.
We plan to extend the framework to demonstrations in the form of image data, which will allow
more end-to-end learning without requiring a separate perception module. We are also interested in
enabling the policy to condition on multiple demonstrations, in case where one demonstration does
not fully resolve ambiguity in the objective. Furthermore and most importantly, we hope to scale up
8

our method on a much larger and broader distribution of tasks, and explore its potential towards a
general robotics imitation learning system that would be able to achieve an overwhelming variety of
tasks.

7

Acknowledgement

We would like to thank our colleagues at UC Berkeley and OpenAI for insightful discussions. This
research was funded in part by ONR through a PECASE award. Yan Duan was also supported by a
Huawei Fellowship. Jonathan Ho was also supported by an NSF Fellowship.

References
[1] Pieter Abbeel and Andrew Ng. Apprenticeship learning via inverse reinforcement learning. In
International Conference on Machine Learning (ICML), 2004.
[2] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom
Schaul, and Nando de Freitas. Learning to learn by gradient descent by gradient descent. In
Neural Information Processing Systems (NIPS), 2016.
[3] Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot
learning from demonstration. Robotics and autonomous systems, 57(5):469?483, 2009.
[4] Yusuf Aytar and Andrew Zisserman. Tabula rasa: Model transfer for object category detection.
In 2011 International Conference on Computer Vision, pages 2252?2259. IEEE, 2011.
[5] Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast
weights to attend to the recent past. In Neural Information Processing Systems (NIPS), 2016.
[6] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
[7] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction
networks for learning about objects, relations and physics. In Advances in Neural Information
Processing Systems, pages 4502?4510, 2016.
[8] Samy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei. On the optimization of a
synaptic learning rule. In Optimality in Artificial and Biological Neural Networks, pages 6?8,
1992.
[9] Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule.
Universit? de Montr?al, D?partement d?informatique et de recherche op?rationnelle, 1990.
[10] Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming: an overview. In
Decision and Control, 1995., Proceedings of the 34th IEEE Conference on, volume 1, pages
560?564. IEEE, 1995.
[11] Sylvain Calinon. Robot programming by demonstration. EPFL Press, 2009.
[12] Michael B Chang, Tomer Ullman, Antonio Torralba, and Joshua B Tenenbaum. A compositional
object-based approach to learning physical dynamics. In Int. Conf. on Learning Representations
(ICLR), 2017.
[13] Kyunghyun Cho, Bart Van Merri?nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoderdecoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
[14] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor
Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML,
pages 647?655, 2014.
[15] Lixin Duan, Dong Xu, and Ivor Tsang. Learning with augmented features for heterogeneous
domain adaptation. arXiv preprint arXiv:1206.4660, 2012.
9

[16] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2 :
Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779,
2016.
[17] Harrison Edwards and Amos Storkey. Towards a neural statistician. International Conference
on Learning Representations (ICLR), 2017.
[18] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal
control via policy optimization. In Proceedings of the 33rd International Conference on Machine
Learning, volume 48, 2016.
[19] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. arXiv preprint arXiv:1703.03400, 2017.
[20] Abhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Learning
invariant feature spaces to transfer skills with reinforcement learning. In Int. Conf. on Learning
Representations (ICLR), 2017.
[21] Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval Tassa.
Learning continuous control policies by stochastic value gradients. In Advances in Neural
Information Processing Systems, pages 2944?2952, 2015.
[22] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in
Neural Information Processing Systems, pages 4565?4573, 2016.
[23] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient
descent. In International Conference on Artificial Neural Networks. Springer, 2001.
[24] Judy Hoffman, Erik Rodner, Jeff Donahue, Trevor Darrell, and Kate Saenko. Efficient learning
of domain-invariant image representations. arXiv preprint arXiv:1301.3224, 2013.
[25] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings
of the 3rd International Conference on Learning Representations (ICLR), 2014.
[26] Gregory Koch. Siamese neural networks for one-shot image recognition. ICML Deep Learning
Workshop, 2015.
[27] David Krueger, Tegan Maharaj, J?nos Kram?r, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron Courville, et al. Zoneout:
Regularizing rnns by randomly preserving hidden activations. arXiv preprint arXiv:1606.01305,
2016.
[28] Brian Kulis, Kate Saenko, and Trevor Darrell. What you saw is not what you get: Domain
adaptation using asymmetric kernel transforms. In Computer Vision and Pattern Recognition
(CVPR), 2011 IEEE Conference on, pages 1785?1792. IEEE, 2011.
[29] S. Levine, Z. Popovic, and V. Koltun. Nonlinear inverse reinforcement learning with gaussian
processes. In Advances in Neural Information Processing Systems (NIPS), 2011.
[30] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep
visuomotor policies. Journal of Machine Learning Research, 17(39):1?40, 2016.
[31] Ke Li and Jitendra Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.
[32] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
[33] Mingsheng Long and Jianmin Wang. Learning transferable features with deep adaptation
networks. CoRR, abs/1502.02791, 1:2, 2015.
[34] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning
bounds and algorithms. arXiv preprint arXiv:0902.3430, 2009.
10

[35] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G
Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.
Human-level control through deep reinforcement learning. Nature, 518(7540):529?533, 2015.
[36] Devang K Naik and RJ Mammone. Meta-neural networks that learn by learning. In International
Joint Conference on Neural Netowrks (IJCNN), 1992.
[37] Andrew Ng and Stuart Russell. Algorithms for inverse reinforcement learning. In International
Conference on Machine Learning (ICML), 2000.
[38] Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, volume 99, pages 278?287,
1999.
[39] Andrew Y Ng, H Jin Kim, Michael I Jordan, Shankar Sastry, and Shiv Ballianda. Autonomous
helicopter flight via reinforcement learning. In NIPS, volume 16, 2003.
[40] Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients.
Neural networks, 21(4):682?697, 2008.
[41] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Advances in
Neural Information Processing Systems, pages 305?313, 1989.
[42] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Under
Review, ICLR, 2017.
[43] Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor, and Daan Wierstra.
One-shot generalization in deep generative models. International Conference on Machine
Learning (ICML), 2016.
[44] St?phane Ross, Geoffrey J Gordon, and Drew Bagnell. A reduction of imitation learning and
structured prediction to no-regret online learning. In AISTATS, volume 1, page 6, 2011.
[45] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,
Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv
preprint arXiv:1606.04671, 2016.
[46] Fereshteh Sadeghi and Sergey Levine. (cad)2 rl: Real single-image flight without a single real
image. 2016.
[47] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap.
Meta-learning with memory-augmented neural networks. In International Conference on
Machine Learning (ICML), 2016.
[48] Stefan Schaal. Is imitation learning the route to humanoid robots? Trends in cognitive sciences,
3(6):233?242, 1999.
[49] Jurgen Schmidhuber. Evolutionary principles in self-referential learning. On learning how to
learn: The meta-meta-... hook.) Diploma thesis, Institut f. Informatik, Tech. Univ. Munich, 1987.
[50] J?rgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic
recurrent networks. Neural Computation, 1992.
[51] John Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, and Philipp Moritz. Trust
region policy optimization. In ICML, pages 1889?1897, 2015.
[52] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484?489,
2016.
[53] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine
Learning Research, 15(1):1929?1958, 2014.
11

[54] Bradlie Stadie, Pieter Abbeel, and Ilya Sutskever. Third person imitation learning. In Int. Conf.
on Learning Representations (ICLR), 2017.
[55] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural
networks. In Advances in neural information processing systems, pages 3104?3112, 2014.
[56] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1.
MIT press Cambridge, 1998.
[57] Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM,
38(3):58?68, 1995.
[58] Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media,
1998.
[59] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain
confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.
[60] Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Xingchao Peng, Pieter Abbeel, Sergey
Levine, Kate Saenko, and Trevor Darrell. Towards adapting deep visuomotor representations
from simulated to real environments. arXiv preprint arXiv:1511.07111, 2015.
[61] Oriol Vinyals, Charles Blundell, Tim Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. In Neural Information Processing Systems (NIPS), 2016.
[62] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,
Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn.
arXiv preprint arXiv:1611.05763, 2016.
[63] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C Courville, Ruslan Salakhutdinov,
Richard S Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation
with visual attention. In ICML, volume 14, pages 77?81, 2015.
[64] Jun Yang, Rong Yan, and Alexander G Hauptmann. Cross-domain video concept detection
using adaptive svms. In Proceedings of the 15th ACM international conference on Multimedia,
pages 188?197. ACM, 2007.
[65] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In
International Conference on Learning Representations (ICLR), 2016.
[66] B. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement
learning. In AAAI Conference on Artificial Intelligence, 2008.

12

"
343,1996,Interpolating Earth-science Data using RBF Networks and Mixtures of Experts,Abstract Missing,"Interpolating Earth-science Data using RBF
Networks and Mixtures of Experts
E.VVan

D.Bone

Division of Infonnation Technology
Canberra Laboratory, CSIRO
GPO Box 664, Canberra, ACT, 2601, Australia
{ernest, don} @cbr.dit.csiro.au

Abstract
We present a mixture of experts (ME) approach to interpolate sparse,
spatially correlated earth-science data. Kriging is an interpolation
method which uses a global covariation model estimated from the data
to take account of the spatial dependence in the data. Based on the
close relationship between kriging and the radial basis function (RBF)
network (Wan & Bone, 1996), we use a mixture of generalized RBF
networks to partition the input space into statistically correlated
regions and learn the local covariation model of the data in each
region. Applying the ME approach to simulated and real-world data,
we show that it is able to achieve good partitioning of the input space,
learn the local covariation models and improve generalization.

1. INTRODUCTION
Kriging is an interpolation method widely used in the earth sciences, which models the
surface to be interpolated as a stationary random field (RF) and employs a linear model.
The value at an unsampled location is evaluated as a weighted sum of the sparse,
spatially correlated data points. The weights take account of the spatial correlation
between the available data points and between the unknown points and the available data
points. The spatial dependence is specified in the form of a global covariation model.
Assuming global stationarity, the kriging predictor is the best unbiased linear predictor
of the un sampled value when the true covariation model is used, in the sense that it
minimizes the squared error variance under the unbiasedness constraint. However, in
practice, the covariation of the data is unknown and has to be estimated from the data by
an initial spatial data analysis. The analysis fits a covariation model to a covariation
measure of the data such as the sample variogram or the sample covariogram, either
graphically or by means of various least squares (LS) and maximum likelihood (ML)
approaches. Valid covariation models are all radial basis functions.
Optimal prediction is achieved when the true covariation model of the data is used. In
general, prediction (or generalization) improves as the covariation model used more

Interpolating Earth-science Data using RBFN and Mixtures of Experts

989

closely matches the true covariation of the data. Nevertheless, estimating the covariation
model from earth-science data has proved to be difficult in practice due to the sparseness
of data samples. Furthermore for many data sets the global stationarity assumption is
not valid. To address this, data sets are commonly manually partitioned into smaller
regions within which the stationarity assumption is valid or approximately so.
In a previous paper, we showed that there is a close, formal relationship between kriging
and RBF networks (Wan & Bone, 1996). In the equivalent RBF network formulation of
kriging, the input vector is a coordinate and the output is a scalar physical quantity of
interest. We pointed out that, under the stationarity assumption, the radial basis function
used in an RBF network can be viewed as a covariation model of the data. We showed
that an RBF network whose RBF units share an adaptive norm weighting matrix, can be
used to estimate the parameters of the postulated covariation model, outperforming more
conventional methods. In the rest of this paper we will refer to such a generalization of
the RBF network as a generalized RBF (GRBF) network.
In this paper, we discuss how a mixture of GRBF networks can be used to partition the
input space into statistically correlated regions and learn the local covariation model of
each region. We demonstrate the effectiveness of the ME approach with a simulated
data set and an aero-magnetic data set. Comparisons are also made of prediction
accuracy of a single GRBF network and other more traditional RBF networks.

2 MIXTURE OF GRBF EXPERTS
Mixture of experts (Jacobs et al , 1991) is a modular neural network architecture in
which a number of expert networks augmented by a gating network compete to learn the
data. The gating network learns to assign probability to the experts according to their
performance over various parts of the input space, and combines the outputs of the
experts accordingly. During training, each expert is made to focus on modelling the
local mapping it performs best, improving its performance further. Competition among
the experts achieves a soft partitioning of the input space into regions with each expert
network learning a separate local mapping. An hierarchical generalization of ME, the
hierarchical mixture of experts (HME), in which each expert is allowed to expand into a
gating network and a set of sub-experts, has also been proposed (Jordan & Jacobs, 1994).
Under the global stationarity assumption, training a GRBF network by minimizing the
mean squared prediction error involves adjusting its norm weighting matrix. This can
be interpreted as an attempt to match the RBF to the covariation of the data. It then
seems natural to use a mixture of GRBF networks when only local stationarity can be
assumed. After training, the gating network soft partitions the input space into
statistically correlated regions and each GRBF network provides a model of the
covariation of the data for a local region. Instead of an ME architecture, an HME
architecture can be used. However, to simplify the discussion we restrict ourselves to the
ME architecture.
Each expert in the mixture is a GRBF network. The output of expert i is given by:

...

Yi(X;Oi) =

L Wijq,(x;cij~Mi)+ WiD

(2.1)

j =\

where ni is the number of RBF units, 0i = {{wi);~o,{cij}i=\,Md are the parameters
of the expert and q,(x;c,M)=qX:II x-c II M). Assuming zero-mean Gaussian error and
common variance a/, the conditional probability of y given x and ~ is given by:
(2.3)

990

E. Wan and D. Bone

Since the radial basis functions we used bave compact support and eacb expert only
learns a local covariation model, small GRBF networks spanning overlapping regions
can be used to reduce computation at the expense of some resolution in locating the
boundaries of the regions. Also, only the subset of data within and around the region
spanned by a GRBF network is needed to train it, further reducing computational effort.
With m experts, the i lb output of the gating network gives the probability of selecting the
expert i and is given by the normalized function:

g, (x~'U) = P(ilx, '0) = Il, exp(q(x~'UJ)/ ~lllj exp{q(x;'U

J)

(2.4)

wbere'U = { raj::\, {'UJ::1}. Using q(x~ '0,) = 'U;[x T If and setting all a, 's to 1, the
gating network implements the softmax function and partitions the input space into a
smoothed planar tessellation.
Alternatively, with q(x~1>i)=-IITi(X-u;)112 (wbere
1>i={u;,Td consists of a location vector and an affine transformation matrix) and
restricting the a/s to be non-negative, the gating network divides the input space into
packed anisotropic ellipsoids. These two partitionings are quite convenient and adequate
for most earth-science applications wbere x is a 2D or 3D coordinate.
The output of the experts are combined to give the overall output of the mixture:

Y{x~a) =

III

III

i=1

i=1

L P(ilx, '\?)9i (x;a i ) = L g, (x; '0 )Yi (x;a,)

(2.5)

wbere a = {'U, {ai }::1} and the conditional probability of observing y given x and a is:
III

p(ylx,a) =

3

L
P(ilx, '0 )p(ylx,a,) .
,=1

(2.6)

THE TRAINING ALGORITHM

The Expectation-Maximization (EM) algorithm of Jordan and Jacobs is used to train the
mixture of GRBF networks. Instead of computing the ML estimates, we extend the
algorithm by including priors on the parameters of the experts and compute the
maximum a posteriori (MAP) estimates. Since an expert may be focusing on a small
subset of the data, the priors belp to prevent over-fitting and improve generalization.
Jordan & Jacobs introduced a set of indicator random variables Z = {Z<t)}~1 as missing
data to label the experts that generate the observable data D = ((x(t), y<t?} ~1. The log
joint probability of the complete data Dc = {D, Z} and parameters a can be written as:

wbere A. is a set of byperparameters. Assuming separable priors on the parameters of the
model i.e. p(alA.) = p('UIAo)D p(ail~) with A. =
N

In p(Dc,alA.) =

{~}:o' (3.1) can be rewritten as:

III

L L Zi(t) In P(ilx(t) , '0)+ In P('UIAo)
/=1 ,=1

(3.2)

Interpolating Earth-science Data using RBFN and Mixtures of Experts

991

Since the posterior probability of the model parameters is proportional to the joint
probability, maximizing (3.2) is equivalent to maximizing the log posterior. In the Estep, the observed data and the current network parameters are used to compute the
expected value of the complete-data log joint probability:
Q(OIO(k) ) =

LN L'"" h;(k)(t)In P(ilx(I), '\))+ In p( '\)IAo)
1=1 1=1

(3.3)

(3.4)

where

e

In the M-step, Q(OIO(k) is maximized with respect to
to obtain 0(1+1). As a result of
the use of the indicator variables, the problem is decoupled into a separate set of interim
MAP estimations:
'\)(k+1)

= arg max L L hi(k)(t) In P(ilx(I), '\)) + In p( '\)IAo)
N

1)

O~HI) = arg lI}.ax
I

'""

(3.5)

1=1 i=1
N

L ~(1)(t)In P(l')lx(I),OJ+ In p(OP't)

(3.6)

1=1

We assume a flat prior for the gating network parameters and the prior
P(Oi I~) = exp(-t ~

II;

II;

LL

WiT

Wi.rq,(C iT -Ci.r? I ZR(~) where ZR(A-.) is a normalization

constant, for the experts. This smoothness prior is used on the GRBF networks because
it can be derived from regularization theory (Girosi & Poggio, 1990) and at the same
time is consistent with the interpretation of the radial basis function as a covariation
model. Hence, maximizing i with (3.6) is equivalent to minimizing the cost function:

e

where A-.' = ~(ji2. The value of the effective regularization parameter, ~', can be set by
generalized cross validation (GCV) (Orr, 1995) or by the 'evidence' method of (Mackay,
1991) using re-estimation formulas. However, in the simulations, for simplicity, we
preset the value of the regularization parameter to a fixed value.

4 SIMULATION RESULTS
Using the Cholesky decomposition method (Cressie, 1993), we generate four 2D data
sets using the four different covariation models shown in Figure 1. The four data set are
then joined together to form a single 64x64 data set. Figure 3a shows the original data
set and the hard boundaries of the 4 statistically distinct regions. We randomly sample
the data to obtain a 400 sample training set and use the rest of the data for validation.
Two GRBF networks, with 64 and 144 adaptive anistropic spherical! units respectively,
are used to learn the postulated global covariation model and the mapping. A 2-level
I

The spherical model is widely used in geostatistics and when used as a covariance function is defined as

lI'(h;a) = 1- {7(~) -

t<l!)3} for ~llhll~ and rp{b;a) = 0 for Ilhll>a. Spherical does NOT mean isotropic.

992

E. Wan and D. Bone

HME with 4 GRBF network experts each with 36 spherical units are used to learn the
local covariation models and the mapping. Softmax gating networks are used and each
expert is somewhat 'localized' in each quadrant of the input space. The units of the
experts are located at the same locations as the units of the 64-unit GRBF network with
24 overlapping units between any two of the experts. The design ensures that the HME
does not have an advantage over the 64-unit GRBF network if the data is indeed globally
stationary. Figure 2 shows the local covariation models learned by the HME with the
smoothness priors and Figure 3b shows the interpolant generated and the partitioning.
(a) NW (exponential) (b) NE (spherical)

~
-10

(a) NW (spherical) (b) NE (spherical)

lij"" ~ ~""01
.

~ [11"""" ~ ~.01

-10

-10

-10

-20
-20
-20 -10 0 10 20 -20 -10 0 10 20
(c) SW (spherical) (d) SE (spherical)

-20-20
-20 -10 0 10 20 -20 -10 0 10 20
(c) SW (spherical) (d) SE (spherical)

~~""~1
~~""1""
~

-10

-20
-20
-20-10 0 1020 -20-10 0 1020

-20-20
-20-10 0 1020 -20-10 0 1020

-10

-10

~~~[I}.

?

Figure 1: The profile of the true local
covariation models of the simulated data set.
Exponential and spherical models are used.
(a)

-10

Figure 2: The profile of the local
covariation models learned by the HME.
(b)

(c)

60

60

60

40

40

40

20

20

20

20

40

60

20

40

60

20

40

60

Figure 3: (a) Simulated data set and true partitions. (b) Interpolant generated by the 144
spherical unit GRBFN. (c) The HME interpolant and the soft partitioning learned (0.5,
0.9 probability contours of the 4 experts shown in solid and dotted lines respectively)
Table 1: Nonnalized mean squared prediction error for the simulated data set.

Network
RBFN (isotropic RBF units with width set to the
distance to the nearest neighbor)
RBFN (identical isotropic RBF units with adaptive
width)
GRBFN (identical RBF units with adaptive norm
weiRhtinR matrix)
HME (2 levels, 4 GRBFN eXlJerts) without lJriors
HME (2 levels 4 GRBFN eXlJerts) with lJriors
kriging predictor (usinR true local models)

RBF unit
64, Gaussian
144, Gaussian
400, Gaussian
64, Gaussian
144, Gaussian
64, spherical
144, spherical
4x36, spherical
4x36, spherical

NMSE
0.761
0.616
0.543
0.477
0.475
0.506
0.431
0.938
0.433
0.372

For comparison, a number of ordinary RBF networks are also used to learn the mapping.
In all cases, the RBF units of networks of the same size share the same locations which

993

Interpolating Earth-science Data using RBFN and Mixtures of Experts

are preset by a Kohonen map. Table 1 summarizes the normalized mean squared
prediction error (NMSE)- the squared prediction error divided by the variance of the
validation set - for each network. With the exception of HME, all results listed are
obtained with a smoothness prior and a regularization parameter of 0.1. Ordinary
weight decay is used for RBF networks with units of varying widths and the smoothness
prior discussed in section 3 are used for the remaining networks. The NMSE of the
kriging predictor that uses the true local models is also listed as a reference.
Similar experiments are also conducted on a real aero-magnetic data set. The flight
paths along which the data is collected are divided into a 740 data points training set and
a 1690 points validation set. The NMSE for each network is summarized in Table 2, the
local covariation models learned by the HME is shown in Figure 4, and the interpolant
generated by the HME and the partitioning is shown in Figure 5b.
(a) NW (spherical)

-::.::~
-100

0

100 -100

(c) SW (spherical)

0

'~[I]'~~
-100
-100

0

l00~

100 -100

0

120

120

80

80

40

40
40

100

(d) SE (spherical)

(b)

(a)

(b) NE (spherical)

80 120

40

80 120

Figure 5: (a) Thin-plate interpolant of the
entire aero-magnetic data set. (b) The HME
interpolant and the soft partitioning (0.5, 0.9
probability contours of the 4 experts shown
in solid and dotted lines respectively).

100

Figure 4: The profile of the local covariation models
of the aero-magnetic data set learned by the HME.
Table 2: Normalized mean squared prediction error for the aero-magnetic data set.
Network
RBFN (isotropic RBF units with width set to the
distance to the nearest neighbor)
RBFN (isotropic RBF units with width set to the
mean distance to the 8 nearest neighbors)
RBFN (identical isotropic RBF units with adaptive
width)
GRBFN (identical RBF units with adaptive norm
weighting matrix)
HME (2 levels, 4 GRBFN experts) without priors
HME (2 levels, 4 GRBFN expertsl with priors

RBF units
49, Gaussian
100, Gaussian
49, Gaussian
100, Gaussian
49, Gaussian
100, Gaussian
49, spherical
100 spherical
4x25, spherical
4x25, spherical

NMSE
1.158
1.256
0.723
0.699
0.692
0.614
0.684
0.612
0.389
0.315

5 DISCUSSION
The ordinary RBF networks perform worst with both the simulated data and the aeromagnetic data. As neither data set is globally stationary, the GRBF networks do not
improve prediction accuracy over the corresponding RBF networks that use isotropic
Gaussian units. In both cases, the hierarchical mixture of GRBF networks improves the
prediction accuracy when the smoothness priors are used. Without the priors, the ML
estimates of the HME parameters lead to improbably high and low predictions.

994

E. Wan and D. Bone

The improvement in prediction accuracy is more significant for the aero-magnetic data
set than for the simulated data set due to some apparent global covariation of the
simulated data which only becomes evident when the directional variograms of the data
are plotted. However, despite the similar NMSE, Figure 3 shows that the interpolant
generated by the 144-unit GRBF network does not contain the structural information
that is captured by the HME interpolant and is most evident in the north-east region.
In the case of the simulated data set, the HME learns the local covariation models
accurately despite the fact that the bottom level gating networks fail to partition the input
space precisely along the north-south direction. The availability of more data and the
straight east-west discontinuity allows the upper gating network to partition the input
space precisely along the east-west direction. In the north-west region, although the class
of function the expert used is different from that of the true model, the model learned
still resembles the true model especially in the inner region where it matters most.
In the case of the aero-magnetic data set, the RBF and GRBF networks perform poorly
due to the considerable extrapolation that is required in the prediction and the absence of
global stationarity. However, the HME whose units capture the local covariation of the
data interpolates and extrapolates significantly better. The partitioning as well as the
local covariation model learned by the HME seems to be reasonably accurate and leads
to the construction of prominent ridge-like structures in the north-west and south-east
which are only apparent in the thin-plate interpolant of the entire data set of Figure Sa.

6

CONCLUSIONS

We show that a mixture of GRBF networks can be used to learn the local covariation of
spatial data and improve prediction (or generalization) when the data is approximately
locally stationary - a viable assumption in many earth-science applications. We believe
that the improvement will be even more significant for data sets with larger spatial
extent especially if the local regions are more statistically distinct. The estimation of the
local covariation models of the data and the use of these models in producing the
interpolant helps to capture the structural information in the data which, apart from
accuracy of the prediction, is of critical importance to many earth-science applications.
The ME approach allows the objective and automatic partitioning of the input space into
statistically correlated regions. It also allows the use of a number of small local GRBF
networks each trained on a subset of the data making it scaleable to large data sets.
The mixture of GRBF networks approach is motivated by the statistical interpolation
method of kriging. The approach therefore has a very sound physical interpretation and
all the parameters of the network have clear statistical and/or physical meanings.

References
Cressie, N. A (1993). Statistics for Spatial Data. Wiley, New York.
Jacobs, R. A, Jordan, M. I., Nowlan, S. J. & Hinton, G. E. (1991). Adaptive Mixtures of Local
Experts. Neural Computation 3, pp. 79-87.
Jordan, M. I. & Jacobs, R. A (1994). Hierarchical Mixtures of Experts and the EM Algorithm.
Neural Computation 6, pp. 181-214.
MacKay, D. J. (1992). Bayesian Interpolation. Neural Computation 4, pp. 415-447.
Orr, M. J. (1995). Regularization in the Selection of Radial Basis Function Centers. Neural
Computation 7, pp. 606-623.
Poggio, T. & Girosi, F. (1990). Networks for Approximation and Learning. In Proceedings of the
IEEE 78, pp. 1481-1497.
Wan, E. & Bone, D. (1996). A Neural Network Approach to Covariation Model Fitting and the
Interpolation of Sparse Earth-science Data. In Proceedings of the Seventh Australian
Conference on Neural Networks, pp. 121-126.

"
5543,2015,No-Regret Learning in Bayesian Games,"Recent price-of-anarchy analyses of games of complete information suggest that coarse correlated equilibria, which characterize outcomes resulting from no-regret learning dynamics, have near-optimal welfare. This work provides two main technical results that lift this conclusion to games of incomplete information, a.k.a., Bayesian games. First, near-optimal welfare in Bayesian games follows directly from the smoothness-based proof of near-optimal welfare in the same game when the private information is public.  Second, no-regret learning dynamics converge to Bayesian coarse correlated equilibrium in these incomplete information games. These results are enabled by interpretation of a Bayesian game as a stochastic game of complete information.","No-Regret Learning in Bayesian Games

Vasilis Syrgkanis
Microsoft Research
New York, NY
vasy@microsoft.com

Jason Hartline
Northwestern University
Evanston, IL
hartline@northwestern.edu

? Tardos
Eva
Cornell University
Ithaca, NY
eva@cs.cornell.edu

Abstract
Recent price-of-anarchy analyses of games of complete information suggest that
coarse correlated equilibria, which characterize outcomes resulting from no-regret
learning dynamics, have near-optimal welfare. This work provides two main technical results that lift this conclusion to games of incomplete information, a.k.a.,
Bayesian games. First, near-optimal welfare in Bayesian games follows directly
from the smoothness-based proof of near-optimal welfare in the same game when
the private information is public. Second, no-regret learning dynamics converge
to Bayesian coarse correlated equilibrium in these incomplete information games.
These results are enabled by interpretation of a Bayesian game as a stochastic
game of complete information.

1

Introduction

A recent confluence of results from game theory and learning theory gives a simple explanation for
why good outcomes in large families of strategically-complex games can be expected. The advance
comes from (a) a relaxation the classical notion of equilibrium in games to one that corresponds to
the outcome attained when players? behavior ensures asymptotic no-regret, e.g., via standard online
learning algorithms such as weighted majority, and (b) an extension theorem that shows that the
standard approach for bounding the quality of classical equilibria automatically implies the same
bounds on the quality of no-regret equilibria. This paper generalizes these results from static games
to Bayesian games, for example, auctions.
Our motivation for considering learning outcomes in Bayesian games is the following. Many important games model repeated interactions between an uncertain set of participants. Sponsored search,
and more generally, online ad-auction market places, are important examples of such games. Platforms are running millions of auctions, with each individual auction slightly different and of only
very small value, but such market places have high enough volume to be the financial basis of large
industries. This online auction environment is best modeled by a repeated Bayesian game: the auction game is repeated over time, with the set of participants slightly different each time, depending
on many factors from budgets of the players to subtle differences in the opportunities.
A canonical example to which our methods apply is a single-item first-price auction with players?
values for the item drawn from a product distribution. In such an auction, players simultaneously
submit sealed bids and the player with the highest bid wins and pays her bid. The utility of the
winner is her value minus her bid; the utilities of the losers are zero. When the values are drawn from
non-identical continuous distributions the Bayes-Nash equilibrium is given by a differential equation
1

that is not generally analytically tractable, cf. [8] (and generalizations of this model, computationally
hard, see [3]). Again, though their Bayes-Nash equilibria are complex, we show that good outcomes
can be expected in these kinds of auctions.
Our approach to proving that good equilibria can be expected in repeated Bayesian games is to
extend an analogous result for static games,1 i.e., the setting where the same game with the same
payoffs and the same players is repeated. Nash equilibrium is the classical model of equilibrium for
each stage of the static game. In such an equilibrium the strategies of players may be randomized;
however, the randomizations of the players are independent. To measure the quality of outcomes in
games Koutsoupias and Papadimitriou [9] introduced the price of anarchy, the ratio of the quality
of the worst Nash equilibrium over a socially optimal solution. Price of anarchy results have been
shown for large families of games, with a focus on those relevant for computer networks. Roughgarden [11] identified the canonical approach for bounding the price of anarchy of a game as showing
that it satisfies a natural smoothness condition.
There are two fundamental flaws with Nash equilibrium as a description of strategic behavior. First,
computing a Nash equilibrium can be PPAD hard and, thus, neither should efficient algorithms for
computing a Nash equilibrium be expected nor should any dynamics (of players with bounded computational capabilities) converge to a Nash equilibrium. Second, natural behavior tends to introduce
correlations in strategies and therefore does not converge to Nash equilibrium even in the limit.
Both of these issues can be resolved for large families of games. First, there are relaxations of Nash
equilibrium which allow for correlation in the players? strategies. Of these, this paper will focus
on coarse correlated equilibrium which requires the expected payoff of a player for the correlated
strategy be no worse than the expected payoff of any action at the player?s disposal. Second, it was
proven by Blum et al. [2] that the (asymptotic) no-regret property of many online learning algorithms
implies convergence to the set of coarse correlated equilibria.2
Blum et al. [2] extended the definition of the price of anarchy to outcomes obtained when each
player follows a no-regret learning algorithm.3 As coarse correlated equilibrium generalize Nash
equilibrium it could be that the worst case equilibrium under the former is worse than the latter.
Roughgarden [11], however, observed that there is often no degradation; specifically, the very same
smoothness property that he identified as implying good welfare in Nash equilibrium also proves
good welfare of coarse correlated equilibrium (equivalently: for outcomes from no-regret learners).
Thus, for a large family of static games, we can expect strategic behavior to lead to good outcomes.
This paper extends this theory to Bayesian games. Our contribution is two-fold: (i) We show an
analog of the convergence of no-regret learning to coarse correlated equilibria in Bayesian games,
which is of interest independently of our price of anarchy analysis; and (ii) we show that the coarse
correlated equilibria of the Bayesian version of any smooth static game have good welfare. Combining these results, we conclude that no-regret learning in smooth Bayesian games achieves good
welfare.
These results are obtained as follows. It is possible to view a Bayesian game as a stochastic game,
i.e., where the payoff structure is fixed but there is a random action on the part of Nature. This
viewpoint applied to the above auction example considers a population of bidders associated for
each player and, in each stage, Nature uniformly at random selects one bidder from each population
to participate in the auction. We re-interpret and strengthen a result of Syrgkanis and Tardos [12]
by showing that the smoothness property of the static game (for any fixed profile of bidder values)
implies smoothness of this stochastic game. From the perspective of coarse correlated equilibrium,
there is no difference between a stochastic game and the non-stochastic game with each random
variable replaced with its expected value. Thus, the smoothness framework of Roughgarden [11]
extends this result to imply that the coarse correlated equilibria of the stochastic game are good.
To show that we can expect good outcomes in Bayesian games, it suffices to show that no-regret
learning converges to the coarse correlated equilibrium of this stochastic game. Importantly, when
we consider learning algorithms there is a distinction between the stochastic game where players?
payoffs are random variables and the non-stochastic game where players? payoffs are the expectation
1

In the standard terms of the game theory literature, we extend results for learning in games of complete
information to games of incomplete information.
2
This result is a generalization of one of Foster and Vohra [7].
3
They referred to this price of anarchy for no-regret learners as the price of total anarchy.

2

of these variables. Our analysis addressed this distinction and, in particular, shows that, in the
stochastic game on populations, no-regret learning converges almost surely to the set of coarse
correlated equilibrium. This result implies that the average welfare of no-regret dynamics will be
good, almost surely, and not only in expectation over the random draws of Nature.

2

Preliminaries

This section describes a general game theoretic environment which includes auctions and resource
allocation mechanisms. For this general environment we review the results from the literature for
analyzing the social welfare that arises from no-regret learning dynamics in repeated game play.
The subsequent sections of the paper will generalize this model and these results to Bayesian games,
a.k.a., games of incomplete information.
General Game Form. A general game M is specified by a mapping from a profile a ? A ?
A1 ? ? ? ? ? An of allowable actions of players to an outcome. Behavior in a game may result in
(possibly correlated) randomized actions a ? ?(A).4 Player i?s utility in this game is determined
by a profile of individual values v ? V ? V1 ? ? ? ? ? Vn and the (implicit) outcome of the game; it
is denoted Ui (a; vi ) = Ea?a [Ui (a; vi )]. In games with a social planner or principal who does not
take an action in the game, the utility of the principal is R(a) = Ea?a [R(a)]. In many games of
interest, such as auctions or allocation mechanisms, the utility of the principal is the revenue from
payments from the players. We will use the term mechanism and game interchangeably.
In a static game the payoffs of the players (given by v) are fixed. Subsequent sections will consider
Bayesian games in the independent private value model, i.e., where player i?s value vi is drawn
independently from the other players? values and is known only privately to player i. Classical
game theory assumes complete information for static games, i.e., that v is known, and incomplete
information in Bayesian games, i.e., that the distribution over V is known. For our study of learning
in games no assumptions of knowledge are made; however, to connect to the classical literature
we will use its terminology of complete and incomplete information to refer to static and Bayesian
games, respectively.
Social Welfare. We will be interested in analyzing the quality of the outcome of the game as
defined by the social welfare,
Pwhich is the sum of the utilities of the players and the principal. We
will denote by SW (a; v) = i?[n] Ui (a; vi ) + R(a) the expected social welfare of mechanism M
under a randomized action profile a. For any valuation profile v ? V we will denote the optimal
social welfare, i.e, the maximum over outcomes of the game of the sum of utilities, by O PT(v).
No-regret Learning and Coarse Correlated Equilibria. For complete information games, i.e.,
fixed valuation profile v, Blum et al. [2] analyzed repeated play of players using no-regret learning
algorithms, and showed that this play converges to a relaxation of Nash equilibrium, namely, coarse
correlated equilibrium.
Definition 1 (no regret). A player achieves no regret in a sequence of play a1 , . . . , aT if his regret
against any fixed strategy a0i vanishes to zero:
PT
limT ?? T1 t=1 (Ui (a0i , at?i ; vi ) ? Ui (at ; vi )) = 0.
(1)
Definition 2 (coarse correlated equilibrium, CCE). A randomized action profile a ? ?(A) is a
coarse correlated equilibrium of a complete information game with valuation profile v if for every
player i and a0i ? Ai :
Ea [Ui (a; vi )] ? Ea [Ui (a0i , a?i ; vi )]
(2)
Theorem 3 (Blum et al. [2]). The empirical distribution of actions of any no-regret sequence in a
repeated game converges to the set of CCE of the static game.
Price of Anarchy of CCE. Roughgarden [11] gave a unifying framework for comparing the social
welfare, under various equilibrium notions including coarse correlated equilibrium, to the optimal
social welfare by defining the notion of a smooth game. This framework was extended to games like
auctions and allocation mechanisms by Syrgkanis and Tardos [12].
4

Bold-face symbols denote random variables.

3

Game/Mechanism
Simultaneous First Price Auction with Submodular Bidders
First Price Multi-Unit Auction
First Price Position Auction
All-Pay Auction
Greedy Combinatorial Auction with d-complements
Proportional Bandwitdth Allocation Mechanism
Submodular Welfare Games
Congestion Games with Linear Delays

(?, ?)
(1 ? 1/e, 1)
(1 ? 1/e, 1)
(1/2, 1)
(1/2, 1)
(1 ? 1/e, d)
(1/4, 1)
(1, 1)
(5/3, 1/3)

P OA
e
e?1
e
e?1

2
2
de
e?1

4
2
5/2

Reference
[12]
[5]
[12]
[12]
[10]
[12]
[13, 11]
[11]

Figure 1: Examples of smooth games and mechanisms

Definition 4 (smooth mechanism). A mechanism M is (?, ?)-smooth for some ?, ? ? 0 there exists
an independent randomized action profile a? (v) ? ?(A1 ) ? ? ? ? ? ?(An ) for each valuation profile
v, such that for any action profile a ? A and valuation profile v ? V:
P
?
(3)
i?[n] Ui (ai (v), a?i ; vi ) ? ? ? O PT (v) ? ? ? R(a).
Many important games and mechanisms satisfy this smoothness definition for various parameters
of ? and ? (see Figure 1); the following theorem shows that the welfare of any coarse correlated
equilibrium in any of these games is nearly optimal.
Theorem 5 (efficiency of CCE; [12]). If a mechanism is (?, ?)-smooth then the social welfare of
?
any course correlated equilibrium at least max{1,?}
of the optimal welfare, i.e., the price of anarchy
satisfies P OA ?

max{1,?}
.
?

Price of Anarchy of No-regret Learning. Following Blum et al. [2], Theorem 3 and Theorem 5
imply that no-regret learning dynamics have near-optimal social welfare.
Corollary 6 (efficiency of no-regret dyhamics; [12]). If a mechanism is (?, ?)-smooth then the
average welfare of any no-regret dynamics of the repeated game with a fixed player set and valuation
?
profile, achieves average social welfare at least max{1,?}
of the optimal welfare, i.e., the price of
anarchy satisfies P OA ?

max{1,?}
.
?

Importantly, Corollary 6 holds the valuation profile v ? V fixed throughout the repeated game play.
The main contribution of this paper is in extending this theory to games of incomplete information,
e.g., where the values of the players are drawn at random in each round of game play.

3

Population Interpretation of Bayesian Games

In the standard independent private value model of a Bayesian game there are n players. Player i
has type vi drawn uniformly from the set of type Vi (and this distribution is denoted Fi ).5 We will
restrict attention to the case when the type space Vi is finite. A player?s strategy in this Bayesian
game is a mapping si : Vi ? Ai from a valuation vi ? Vi to an action ai ? Ai . We will denote
i
with ?i = AV
i the strategy space of each player and with ? = ?1 ? . . . ? ?n . In the game, each
player i realizes his type vi from the distribution and then makes action si (vi ) in the game.
In the population interpretation of the Bayesian game, also called the agent normal form representation [6], there are n finite populations of players. Each player in population i has a type vi which we
assume to be distinct for each player in each population and across populations.6 The set of players
in the population is denoted Vi . and the player in population i with type vi is called player vi . In the
population game, each player vi chooses an action si (vi ). Nature uniformly draws one player from
5

The restriction to the uniform distribution is without loss of generality for any finite type space and for any
distribution over the type space that involves only rational probabilities.
6
The restriction to distinct types is without of loss of generality as we can always augment a type space with
an index that does not affect player utilities.

4

each population, and the game is played with those players? actions. In other words, the utility of
player vi from population i is:
AG
Ui,v
(s) = Ev [Ui (s(v); vi ) ? 1{vi = vi }]
i

(4)

Notice that the population interpretation of the Bayesian game is in fact a stochastic game of complete information.
There are multiple generalizations of coarse correlated equilibria from games of complete information to games of incomplete information (c.f. [6], [1], [4]). One of the canonical definitions is simply
the coarse correlated equilibrium of the stochastic game of complete information that is defined by
the population interpretation above.7
Definition 7 (Bayesian coarse correlated equilibrium - BAYES -CCE). A randomized strategy profile
s ? ?(?) is a Bayesian coarse correlated equilibrium if for every a0i ? Ai and for every vi ? Vi :
Es Ev [Ui (s(v); vi ) | vi = vi ] ? Es Ev [Ui (a0i , s?i (v?i ); vi ) | vi = vi ]

(5)

In a game of incomplete information the welfare in equilibrium will be compared to the expected
ex-post optimal social welfare Ev [O PT(v)]. We will refer to the worst-case ratio of the expected
optimal social welfare over the expected social welfare of any BAYES -CCE as BAYES -CCE-P OA.

4

Learning in Repeated Bayesian Game

Consider a repeated version of the population interpretation of a Bayesian game. At each iteration
one player vi from each population is sampled uniformly and independently from other populations.
The set of chosen players then participate in an instance of a mechanism M. We assume that each
player vi ? Vi , uses some no-regret learning rule to play in this repeated game.8 In Definition 8, we
describe the structure of the game and our notation more elaborately.
Definition 8. The repeated Bayesian game of M proceeds as follows. In stage t:
1. Each player vi ? Vi in each population i picks an action sti (vi ) ? Ai . We denote with
|V |
sti ? Ai i the function that maps a player vi ? Vi to his action.
2. From each population i one player vit ? Vi is selected uniformly at random. Let v t =
(v1t , . . . , vnt ) be the chosen profile of players and st (v t ) = (st1 (v1t ), . . . , stn (vnt )) be the
profile of chosen actions.
3. Each player vit participates in an instance of game M, in the role of player i ? [n], with
action sti (vit ) and experiences a utility of Ui (st (v t ); vit ). All players not selected in Step 2
experience zero utility.
Remark. We point out that for each player in a population to achieve no-regret he does not need
to know the distribution of values in other populations. There exist algorithms that can achieve the
no-regret property and simply require an oracle that returns the utility of a player at each iteration.
Thus all we need to assume is that each player receives as feedback his utility at each iteration.
Remark. We also note that our results would extend to the case where at each period multiple
matchings are sampled independently and players potentially participate in more than one instance
of the mechanism M and potentially with different players from the remaining population. The only
thing that the players need to observe in such a setting is their average utility that resulted from their
action sti (vi ) ? Ai from all the instances that they participated at the given period. Such a scenario
seems an appealing model in online ad auction marketplaces where players receive only average
utility feedback from their bids.
7

This notion is the coarse analog of the agent normal form Bayes correlated equilibrium defined in Section
4.2 of Forges [6].
8
An equivalent and standard way to view a Bayesian game is that each player draws his value independently
from his distribution each time the game is played. In this interpretation the player plays by choosing a strategy
that maps his value to an action (or distribution over actions). In this interpretation our no-regret condition
requires that the player not regret his actions for each possible value.

5

Bayesian Price of Anarchy for No-regret Learners. In this repeated game setting we want to
compare the average social welfare of any sequence of play where each player uses a vanishing
regret algorithm versus the average optimal welfare. Moreover, we want to quantify the worst-case
such average welfare over all possible valuation distributions within each population:
sup
F1 ,...,Fn

lim sup
T ??

PT

t
t=1 O PT (v )
M (st (v t );v t )
SW
t=1

PT

(6)

We will refer to this quantity as the Bayesian price of anarchy for no-regret learners. The numerator
of this term is simply the average optimal welfare when players from each population are drawn
independently in each stage; it converges almost surely to the expected ex-post optimal welfare
Ev [O PT(v)] of the stage game. Our main theorem is that if the mechanism is smooth and players
follow no-regret strategies then the expected welfare is guaranteed to be close to the optimal welfare.
Theorem 9 (Main Theorem). If a mechanism is (?, ?)-smooth then the average (over time) welfare
of any no-regret dynamics of the repeated Bayesian game achieves average social welfare at least
max{1,?}
?
, almost surely.
max{1,?} of the average optimal welfare, i.e. P OA ?
?
Roadmap of the proof. In Section 5, we show that any vanishing regret sequence of play of the
repeated Bayesian game, will converge almost surely to the Bayesian version of a coarse correlated
equilibrium of the incomplete information stage game. Therefore the Bayesian price of total anarchy
will be upper bounded by the efficiency of guarantee of any Bayesian coarse correlated equilibrium.
Finally, in Section 6 we show that the price of anarchy bound of smooth mechanisms directly extends
to Bayesian coarse correlated equilibria, thereby providing an upper bound on the Bayesian price of
total anarchy of the repeated game.
Remark. We point out that our definition of BAYES -CCE is inherently different and more restricted
than the one defined in Caragiannis et al. [4]. There, a BAYES -CCE is defined as a joint distribution
D over V ? A, such that if (v, a) ? D then for any vi ? Vi and a0i (vi ) ? Ai :
E(v,a) [Ui (a; vi )] ? E(v,a) [Ui (a0i (vi ), a?i ; vi )]
(7)
The main difference is that the product distribution defined by a distribution in ?(?) and the distribution of values, cannot produce any possible joint distribution over (V, A), but the type of joint
distributions are restricted to satisfy a conditional independence property described by [6]. Namely
that player i?s action is conditionally independent of some other player j?s value, given player i?s
type. Such a conditional independence property is essential for the guarantees that we will present
in this work to extend to a BAYES -CCE and hence do not seem to extend to the notion given in [4].
However, as we will show in Section 5, the no-regret dynamics that we analyze, which are mathematically equivalent to the dynamics in [4], do converge to this smaller set of BAYES -CCE that
we define and for which our efficiency guarantees will extend. This extra convergence property is
not needed when the mechanism satisfies the stronger semi-smoothness property defined in [4] and
thereby was not needed to show efficiency bounds in their setting.

5

Convergence of Bayesian No-Regret to BAYES -CCE

In this section we show that no-regret learning in the repeated Bayesian game converges almost
surely to the set of Bayesian coarse correlated equilibria. Any given sequence of play of the repeated
Bayesian game, which we defined in Definition 8, gives rise to a sequence of strategy-value pairs
i
(st , v t ) where st = (st1 , . . . , stn ) and sti ? AV
i , captures the actions that each player vi in population
i would have chosen, had they been picked. Then observe that all that matters to compute the average
social welfare of the game for any given time step T , is the empirical distribution of pairs (s, v), up
till time step T , denoted as DT , i.e. if (sT , vT ) is a random sample from DT :


PT
1
t t
t
T
T
T
(8)
t=1 SW (s (v ); v ) = E(sT ,vT ) SW (s (v ); v )
T
Lemma 10 (Almost sure convergence to BAYES -CCE). Consider a sequence of play of the random
matching game, where each player uses a vanishing regret algorithm and let DT be the empirical
distribution of (strategy, valuation) profile pairs up till time step T . Consider any subsequence of
{DT }T that converges in distribution to some distribution D. Then, almost surely, D is a product
distribution, i.e. D = Ds ? Dv , with Ds ? ?(?) and Dv ? ?(V) such that Dv = F and
Ds ? BAYES -CCE of the static incomplete information game with distributional beliefs F.
6

Proof. We will denote with
ri (a?i , a; vi ) = Ui (a?i , a?i ; vi ) ? Ui (a; vi ),
the regret of player vi from population i, for action a?i at action profile a. For a vi ? Vi let xti (vi ) =
1{vit = vi }. Since the sequence has vanishing regret for each player vi in population Pi , it must be
that for any s?i ? ?i :
PT
t
?
t t
(9)
t=1 xi (vi ) ? ri (si (vi ), s (v ); vi ) ? o(T )
For any fixed T , let DsT ? ?(?) denote the empirical distribution of st and let s be a random sample
from DsT . For each s ? ?, let Ts ? [T ] denote the time steps such that st = s for each t ? Ts . Then
we can re-write Equation (9) as:
h
i
P
)
(10)
Es |T1s | t?Ts xti (vi ) ? ri (s?i (vi ), st (v t ); vi ) ? o(T
T
For any s ? ? and w ? V, let Ts,w = {t ? Ts : v t = w}. Then we can re-write Equation (10) as:
i
hP
|Ts,w |
o(T )
?
Es
(11)
w?V |Ts | 1{wi = vi } ? ri (si (vi ), s(w); vi ) ? T
|T

|

Now we observe that |Ts,w
is the empirical frequency of the valuation vector w ? V, when filtered
s|
at time steps where the strategy vector was s. Since at each time step t the valuation vector v t is
picked independently from the distribution of valuation profiles F, this is the empirical frequency
of Ts independent samples from F.
By standard arguments from empirical processes theory, if Ts ? ? then this empirical distribution
converges almost surely to the distribution F. On the other hand if Ts doesn?t go to ?, then the
empirical frequency of strategy s vanishes to 0 as T ? ? and therefore has measure zero in the
above expectation as T ? ?. Thus for any convergent subsequence of {DT }, if D is the limit
distribution, then if s is in the support of D, then almost surely the distribution of w conditional on
strategy s is F. Thus we can write D as a product distribution Ds ? F.
Moreover, if we denote with w the random variable that follows distribution F, then the limit of
Equation (11) for any convergent sub-sequence, will give that:
a.s.: Es?Ds Ew?F [1{wi = vi } ? ri (s?i (vi ), s(w); vi )] ? 0
Equivalently, we get that Ds will satisfy that for all vi ? Vi and for all s?i :
a.s.: Es?Ds Ew?F [ri (s?i (wi ), s(w); wi ) | wi = vi ] ? 0
The latter is exactly the BAYES -CCE condition from Definition 7. Thus Ds is in the set of
BAYES -CCE of the static incomplete incomplete information game among n players, where the
type profile is drawn from F.
Given the latter convergence theorem we can easily conclude the following the following theorem,
whose proof is given in the supplementary material.
Theorem 11. The price of anarchy for Bayesian no-regret dynamics is upper bounded by the price
of anarchy of Bayesian coarse correlated equilibria, almost surely.

6

Efficiency of Smooth Mechanisms at Bayes Coarse Correlated Equilibria

In this section we show that smoothness of a mechanism M implies that any BAYES -CCE of the
?
incomplete information setting achieves at least max{1,?}
of the expected optimal welfare. To show
this we will adopt the interpretation of BAYES -CCE that we used in the previous section, as coarse
correlated equilibria of a more complex normal form game; the stochastic agent normal form representation of the Bayesian game. We can interpret this complexP
normal form game as the game
that arises from a complete information mechanism MAG among i |Vi | players, which randomly
samples one player from each of the n population and where the utility of a player in the complete
information mechanism MAG is given by Equation (4). The set of possible outcomes in this agent
7

game corresponds to the set of mappings from a profile of chosen players to an outcome in the underlying mechanism M. The optimal welfare of this game, is then the expected ex-post optimal
welfare O PTAG = Ev [O PT(v)].
The main theorem that we will show is that whenever mechanism M is (?, ?)-smooth, then also
mechanism MAG is (?, ?)-smooth. Then we will invoke a theorem of [12, 11], which shows that
?
any coarse correlated equilibrium of a complete information mechanism achieves at least max{1,?}
of the optimal welfare. By the equivalence between BAYES -CCE and CCE of this complete infor?
mation game, we get that every BAYES -CCE of the Bayesian game achieves at least max{1,?}
of the
expected optimal welfare.
Theorem 12 (From complete information to Bayesian smoothness). If a mechanism M is (?, ?)smooth, then for any vector of independent valuation distributions F = (F1 , . . . , Fn ), the complete
information mechanism MAG is also (?, ?)-smooth.
Proof. Consider the following randomized deviation for each player vi ? Vi in population i: He
random samples a valuation profile w ? F. Then he plays according to the randomized action
s?i (vi , w?i ), i.e., the player deviates using the randomized action guaranteed by the smoothness
property of mechanism M for his type vi and the random sample of the types of the others w?i .
Consider an arbitrary action profile s = (s1 , . . . , sn ) for all players in all populations. In this
P
|V |
context it is better to think of each si as a |Vi | dimensional vector in Ai i and to view s as a i |Vi |
dimensional vector. Then with s?vi we will denote all the components of this large vector except
the ones corresponding to player vi ? Vi . Moreover, we will be denoting with v a sample from F
drawn by mechanism MAG . We now argue about the expected utility of player vi from this deviation,
which is:
 AG ?

Ew Ui,v
(si (vi , w?i ), s?vi ) = Ew Ev [Ui (s?i (vi , w?i ), s?i (v?i ); vi ) ? 1{vi = vi }]
i
Summing the latter over all players vi ? Vi in population i:
X
 AG ?

P

?
Ew Ui,v
(si (vi , w?i ), s?vi ) = Ew,v
vi ?Vi Ui (si (vi , w?i ), s?i (v?i ); vi ) ? 1{vi = vi }
i
vi ?Vi

= Ev,w [Ui (s?i (vi , w?i ), s?i (v?i ); vi )]
= Ev,w [Ui (s?i (wi , w?i ), s?i (v?i ); wi )]
= Ev,w [Ui (s?i (w), s?i (v?i ); wi )] ,
where the second to last equation is an exchange of variable names and regrouping using independence. Summing over populations and using smoothness of M, we get smoothness of MAG :
hP
i
X X
 AG ?

?
Ew Ui,v
(s
(v
,
w
),
s
)
=
E
U
(s
(w),
s
(v
);
w
)
i
?i
?vi
v,w
?i ?i
i
i
i?[n] i i
i
i?[n] vi ?Vi

? Ev,w [?O PT(w) ? ?R(s(v))] = ?Ew [O PT(w)] ? ?RAG (s)
Corollary 13. Every BAYES -CCE of the incomplete information setting of a smooth mechanism
?
M, achieves expected welfare at least max{1,?}
of the expected optimal welfare.

7

Finite Time Analysis and Convergence Rates

In the previous section we argued about the limit average efficiency of the game as time goes to
infinity. In this section we analyze the convergence rate to BAYES -CCE and we show approximate
efficiency results even for finite time, when players are allowed to have some -regret.
Theorem 14. Consider the repeated matching game with a (?, ?)-smooth mechanism. Suppose that
for any T ? T 0 , each player in each of the n populations has regret at most n . Then for every ?
and ?, there exists a T ? (?, ?), such that for any T ? min{T 0 , T ? }, with probability 1 ? ?:
PT
1
?
t t
t
(12)
t=1 SW (s (v ); v ) ? max{1,?} Ev [O PT (v)] ? ? ? ? ? 
T
 
3
2
3
?H
Moreover, T ? (?, ?) ? 54?n ?|?|?|V|
log ?2 .
?3
8

References
[1] Dirk Bergemann and Stephen Morris. Correlated Equilibrium in Games with Incomplete Information. Cowles Foundation Discussion Papers 1822, Cowles Foundation for Research in
Economics, Yale University, October 2011.
[2] Avrim Blum, MohammadTaghi Hajiaghayi, Katrina Ligett, and Aaron Roth. Regret minimization and the price of total anarchy. In Proceedings of the Fortieth Annual ACM Symposium on
Theory of Computing, STOC ?08, pages 373?382, New York, NY, USA, 2008. ACM.
[3] Yang Cai and Christos Papadimitriou. Simultaneous bayesian auctions and computational
complexity. In Proceedings of the fifteenth ACM conference on Economics and Computation,
EC ?14, pages 895?910, New York, NY, USA, 2014. ACM.
[4] Ioannis Caragiannis, Christos Kaklamanis, Panagiotis Kanellopoulos, Maria Kyropoulou,
? Tardos. Bounding the inefficiency of outcomes
Brendan Lucier, Renato Paes Leme, and Eva
in generalized second price auctions. Journal of Economic Theory, (0):?, 2014.
[5] Bart de Keijzer, Evangelos Markakis, Guido Schfer, and Orestis Telelis. Inefficiency of standard multi-unit auctions. In HansL. Bodlaender and GiuseppeF. Italiano, editors, Algorithms
ESA 2013, volume 8125 of Lecture Notes in Computer Science, pages 385?396. Springer
Berlin Heidelberg, 2013.
[6] Franoise Forges. Five legitimate definitions of correlated equilibrium in games with incomplete
information. Theory and Decision, 35(3):277?310, 1993.
[7] Dean P Foster and Rakesh V Vohra. Asymptotic calibration. Biometrika, 85(2):379?390, 1998.
[8] ToddR. Kaplan and Shmuel Zamir. Asymmetric first-price auctions with uniform distributions:
analytic solutions to the general case. Economic Theory, 50(2):269?302, 2012.
[9] Elias Koutsoupias and Christos Papadimitriou. Worst-case equilibria. In Proceedings of the
16th annual conference on Theoretical aspects of computer science, STACS?99, pages 404?
413, Berlin, Heidelberg, 1999. Springer-Verlag.
[10] B. Lucier and A. Borodin. Price of anarchy for greedy auctions. In Proceedings of the TwentyFirst Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ?10, pages 537?553,
Philadelphia, PA, USA, 2010. Society for Industrial and Applied Mathematics.
[11] T. Roughgarden. Intrinsic robustness of the price of anarchy. In Proceedings of the 41st annual
ACM symposium on Theory of computing, STOC ?09, pages 513?522, New York, NY, USA,
2009. ACM.
? Tardos. Composable and efficient mechanisms. In Proceedings of
[12] Vasilis Syrgkanis and Eva
the Forty-fifth Annual ACM Symposium on Theory of Computing, STOC ?13, pages 211?220,
New York, NY, USA, 2013. ACM.
[13] A. Vetta. Nash equilibria in competitive societies, with applications to facility location, traffic routing and auctions. In Foundations of Computer Science, 2002. Proceedings. The 43rd
Annual IEEE Symposium on, pages 416?425, 2002.

9

"
6374,2017,Optimal Sample Complexity of M-wise Data for Top-K Ranking,"We explore the top-K rank aggregation problem in which one aims to recover a consistent ordering that focuses on top-K ranked items based on partially revealed preference information. We examine an M-wise comparison model that builds on the Plackett-Luce (PL) model where for each sample, M items are ranked according to their perceived utilities modeled as noisy observations of their underlying true utilities. As our result, we characterize the minimax optimality on the sample size for top-K ranking. The optimal sample size turns out to be inversely proportional to M. We devise an algorithm that effectively converts M-wise samples into pairwise ones and employs a spectral method using the refined data. In demonstrating its optimality, we develop a novel technique for deriving tight $\ell_\infty$ estimation error bounds, which is key to accurately analyzing the performance of top-K ranking algorithms, but has been challenging. Recent work relied on an additional maximum-likelihood estimation (MLE) stage merged with a spectral method to attain good estimates in $\ell_\infty$ error to achieve the limit for the pairwise model. In contrast, although it is valid in slightly restricted regimes, our result demonstrates a spectral method alone to be sufficient for the general M-wise model. We run numerical experiments using synthetic data and confirm that the optimal sample size decreases at the rate of 1/M. Moreover, running our algorithm on real-world data, we find that its applicability extends to settings that may not fit the PL model.","Optimal Sample Complexity of M -wise Data for
Top-K Ranking

Minje Jang?
School of Electrical Engineering
KAIST
jmj427@kaist.ac.kr

Sunghyun Kim?
Electronics and Telecommunications Research Institute
Daejeon, Korea
koishkim@etri.re.kr

Changho Suh
School of Electrical Engineering
KAIST
chsuh@kaist.ac.kr
Sewoong Oh
Industrial and Enterprise Systems Engineering Department
UIUC
swoh@illinois.edu

Abstract
We explore the top-K rank aggregation problem in which one aims to recover a
consistent ordering that focuses on top-K ranked items based on partially revealed
preference information. We examine an M -wise comparison model that builds on
the Plackett-Luce (PL) model where for each sample, M items are ranked according
to their perceived utilities modeled as noisy observations of their underlying true
utilities. As our result, we characterize the minimax optimality on the sample size
for top-K ranking. The optimal sample size turns out to be inversely proportional to
M . We devise an algorithm that effectively converts M -wise samples into pairwise
ones and employs a spectral method using the refined data. In demonstrating
its optimality, we develop a novel technique for deriving tight `? estimation
error bounds, which is key to accurately analyzing the performance of top-K
ranking algorithms, but has been challenging. Recent work relied on an additional
maximum-likelihood estimation (MLE) stage merged with a spectral method to
attain good estimates in `? error to achieve the limit for the pairwise model. In
contrast, although it is valid in slightly restricted regimes, our result demonstrates
a spectral method alone to be sufficient for the general M -wise model. We run
numerical experiments using synthetic data and confirm that the optimal sample
size decreases at the rate of 1/M . Moreover, running our algorithm on real-world
data, we find that its applicability extends to settings that may not fit the PL model.

1

Introduction

Rank aggregation has been explored in a variety of contexts such as social choice [15, 6], web search
and information retrieval [20], recommendation systems [7], and crowd sourcing [16], to name a few.
It aims to bring a consistent ordering to a collection of items, given partial preference information.
?

Equal contribution.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

Preference information can take various forms depending on the context. One such form, which we
examine in this paper, is ordinal; preferences for alternatives are represented as an ordering. Consider
crowd-sourced data collected by annotators asked to rank a few given alternatives based on their
preference. The aggregated data can be used to identify the most preferred. One example can be
a review process for conference papers (e.g., NIPS) where reviewers are asked to not only review
papers, but also order them based on how much they enjoy them. The collected data could be used to
highlight papers that may interest a large audience. Alternatively, consider sports (races or the like)
and online games where a number of players compete. One may wish to rank them according to skill.
Its broad range of applications has led to a volume of work done. Of numerous schemes developed,
arguably most dominant paradigms are spectral algorithms [14, 20, 37, 41, 47, 45] and maximum
likelihood estimation (MLE) [22, 28]. Postulating the existence of underlying real-valued preferences
of items, they aim to produce preference estimates consistent in a global sense, e.g., measured by low
squared loss. But such estimates do not necessarily guarantee optimal ranking accuracy. Accurate
ranking has more to do with how well the ordering of estimates matches that of the true preferences,
and less to do with how close the estimates are to the true preferences minimizing overall errors.
Moreover, in practice, what we expect from accurate ranking is an ordering that precisely separates
only a few items ranked highest from the rest, not an ordering that respects the entire items.
Main contributions. In light of it, we explore top-K ranking which aims to recover the correct set
of top-ranked items only. We examine the Plackett-Luce (PL) model which has been extensively
explored [24, 18, 5, 25, 38, 43, 33, 4]. It is a special case of random utility models [46] where true
utilities of items are presumed and a user?s revealed preference is a partial ordering according to
noisy manifestations of the utilities. It satisfies the ?independence of irrelevant alternatives? property
in social choice theory [34, 35] and is the most popular model in studying human choice behavior
given multiple alternatives (see Section 2). It is well-known that it subsumes as a special case the
Bradley-Terry-Luce (BTL) model [12, 32] which concerns two items. We consider an M -wise
comparison model where comparisons are given as a preference ordering of M items. In this setting,
we characterize the minimax limit on the sample size (i.e., sample complexity) needed to reliably
identify the set of top-K ranked items, which turns out to be inversely proportional to M . To the best
of our knowledge, it is the first result that characterizes the limit under an M -wise comparison model.
In achieving the limit, we propose an algorithm that consists of sample breaking and Rank Centrality
[37], one spectral method we choose among other variants [10, 9, 37, 33]. First, it converts M -wise
samples into many more pairwise ones, and in doing so, it carefully chooses only M out of all M
2
pairwise samples obtainable from each M -wise sample. This sample breaking (see Section 3.1)
extracts only the essential information needed to achieve the limit from given M -wise data. Next,
using the refined pairwise data, the algorithm runs a spectral method to identify top-ranked items.
A novel technique we develop to attain tight `? estimation error bounds has been instrumental to
our progress. Analyzing `? error bounds is a critical step to characterizing the minimax sample
complexity for top-K ranking as presented in [17], but has been technically challenging. Even after
decades of research since the introduction of spectral methods and MLE, two dominant approaches in
the field, we lack notable results for tight `? error bounds. This is largely because techniques proven
useful to obtain good `2 error bounds do not translate into attaining good `? error bounds. In this
regard, our result contributes to progress on `? error analysis (see Section 3.2 and the supplementary).
We can compare our result to that of [17] by considering M = 2. Although the two optimal sample
complexities match, the conditions under which they do differ; our result turns out to be valid under a
slightly restricted condition (see Section 3.3). In terms of achievability, the algorithm in [17] merges
an additional MLE stage with a spectral method, whereas we employ only a spectral method. From
numerical experiments, we speculate that the condition under which the result of [17] holds may not
be sufficient for spectral methods alone to achieve optimality (see Section 4.1).
We conduct numerical experiments to support our result. Using synthetic data, we show that the
minimax optimal sample size indeed decreases at the rate of 1/M . We run our algorithm on real-world
data collected from a popular online game (League of Legends) and find its applicability to extend to
settings that may not necessarily match the PL model. From the collected data, we extract M -wise
comparisons and rank top users in terms of skill. We examine its robustness aspect against partial
data and also evaluate its rank result with respect to the official rank League of Legends provides. In
both cases, we compare it with a counting-based algorithm [42, 11] and demonstrate its advantages.

2

Related work. To the best of our knowledge, [17] investigated top-K identification under the random
comparison model of interest for the first time. A key distinction here is that we examine the random
listwise comparison model based on the PL model. Rank Centrality was developed in [37] based on
which we devise our ranking scheme tailored for listwise comparison data.
In the PL model, some viewed ranking as parameter estimation. Maystre and Grossglauser [33]
developed an algorithm that shares a spirit of spectral ranking and showed its performance is the
same as MLE for estimating underlying preference scores. Hajek et al. [25] derived minimax lower
bounds of parameter estimation error, and examined gaps with upper bounds of MLE as well as MLE
with a rank-breaking scheme that decomposes partial rankings into pairwise comparisons.
Some works examined several sample breaking methods that convert listwise data into pairwise data
in the PL model. Azari Soufiani et al. [5] considered various methods to see if they sustain some
statistical
 property in parameter estimation. It examined full breaking that converts an M -wise sample
into M
2 pairwise ones, and adjacent breaking that converts an ordinal M -wise sample into M ? 1
pairwise ones whose associated items are adjacent in the sample. Ashish and Oh [4] considered a
method that converts an M -wise sample into multiple pairwise ones and assigns different importance
weights to each, and examined the method on several types of comparison graphs.
There are a number of works that explored ranking problems in different models and with different
interests. Some works [43, 2] have adopted PAC (probably approximately correct) [44] or regret
[21, 8, 23] as their metric to allow some margin of error, in contrast to our work where 0/1 loss
(the most stringent criterion) is considered to investigate the worst-case scenario (see Section 2).
Rajkumar and Agarwal [40] put forth statistical assumptions that ensure the convergence of rank
aggregation methods including Rank Centrality and MLE to an optimal ranking. Active ranking
where samples are obtained adaptively has received attention as well. Jamieson and Nowak [29]
considered perfect total ranking and characterized the query complexity gain of adaptive sampling in
the noise-free case, and the works of [29, 1] explored the query complexity in the presence of noise
aiming at approximate total rankings. Recently, Braverman et al. [13] considered three noisy models,
examining if their algorithm can achieve reliable top-K ranking. Heckel et al. [27] considered a
model where noisy pairwise observations are given, with a goal to partition the items into sets of
pre-specified sizes based on their scores, which includes top-K ranking as a special case. Mohajer
et al. [36] considered a fairly general noisy model which subsumes as special cases various models.
They derived upper bounds on the sample size required for reliable top-K sorting as well as top-K
partitioning, and showed that active ranking can provide significant gains over passive ranking.

2

Problem Formulation

Notation. We denote by [n] to represent {1, 2, . . . , n}, and by G = ([n], E (M ) ) to represent an
M -wise comparison graph in which total n vertices reside and each hyper-edge is connected if there
is a comparison among M vertices, and di to represent the out-degree of vertex i.
Comparison model and assumptions. Suppose we perform a few evaluations on n items. We
assume the comparison outcomes are generated based on the PL model [39]. We consider M -wise
models where the comparison outcomes are obtained in the form of a preference ordering of M items.
Preference scores. The PL model assumes the existence of underlying preferences w :=
{w1 , w2 , . . . , wn }, where wi represents the preference score of item i. The outcome of each comparison depends solely on the latent scores of the items being compared. Without loss of generality, we
assume that w1 ? w2 ? ? ? ? ? wn > 0. We assume the range of scores to be fixed irrespective of n.
For some positive constants wmin and wmax , wi ? [wmin , wmax ], 1 ? i ? n. We note that the case
where the range wmax /wmin grows with n can be translated into the above fixed-range regime by
separating out those items with vanishing scores (e.g. via a voting method like Borda count [11, 3]).
Comparison model. We denote by G = ([n], E (M ) ) a comparison graph where a set of M items
I = {i1 , i2 , . . . , iM } are compared if and only if I belongs to the hyper-edge set E (M ) . We examine
random graphs, constructed in a similar manner according to the Erd?os-R?nyi random graph model;
each set of M vertices is connected by a hyper-edge independently with probability p. Notice that
when M = 2, such random graphs we consider follow precisely the Erd?os-R?nyi random model.
3

M -wise comparisons. We observe L samples for each I = {i1 , i2 , . . . , iM } ? E (M ) . Each
sample is an ordering of M items in order of preference. The outcome of the `th sample, de(`)
(`)
noted by
sI , is generated
 according to the PL model: sI = (i1 , i2 , . . . , iM ) with probability
QM 
PM
m=1 wim /
r=m wir , where item ia is preferred over item ib in I if ia appears to the left
(`)

of ib , which we also denote by ia  ib . We assume that conditional on G, sI ?s are jointly independent over I and `. We denote the collection of all samples by s := {sI : I ? E (M ) }, where
(1) (2)
(L)
sI = {sI , sI , . . . , sI }.
Performance metric and goal. Given comparison data, one wishes to know whether or not the
top-K ranked items are identifiable. We consider the probability of error Pe in identifying the correct
set of the top-K ranked items: Pe (?) := P {?(s) 6= [K]}, where ? is any ranking scheme that
returns a set of K indices and [K] is the set of the first K indices. Our goal in this work is to
characterize the admissible region Rw of (p, L) in which top-K ranking is feasible for a given PL
parameter w, in other words, Pe can be vanishingly small as n grows. The admissible region Rw is
defined as Rw := {(p, L) : limn?? Pe (?(s)) = 0}. In particular, we are 
interested
in the minimax

	
n
sample complexity of an estimator defined as S? :=
inf + sup M
pL : (p, L) ? Rv ,
p?[0,1],L?Z v???

where ?? = {v ? Rn : (vK ? vK+1 )/vmax ? ?}. Note that this definition shows that we
conservatively examine minimax scenarios where nature behaves adversely with the worst-case w.

3

Main Results

Separating the two items near the decision boundary (i.e., the K th and (K + 1)th ranked items) is
key in top-K ranking. Unless the gap is large enough, noise in the observations leads to erroneous
estimates which no ranking scheme can overcome. We pinpoint a separation measure as ?K :=
(wK ? wK+1 )/wmax , which turns out to be crucial in establishing the fundamental limit.
Noted in [22], if a comparison graph G is not connected, it is impossible to determine the relative preferences between two disconnected entities. Thus, we assume all comparison graphs to be connected.

n?1 2
To guarantee it, for a hyper-random graph with edge size M , we assume p > log n/ M
?1 .
Now, let us formally state our main results. First, for comparison graphs under M -wise observations,
we establish a necessary condition for top-K ranking.
(M )
Theorem 1. Fix  ? (0, 21 ). Given
), if
 anM -wise comparison graph G = ([n], E
n
n log n 1
,
pL ? c0 (1 ? )
M
?2K M

(1)

for some numerical constant c0 , then for any ranking scheme ?, there exists a preference score vector
w with separation measure ?K such that Pe (?) ? .
The proof is a generalization of Theorem 2 in [17], and we provide it in the supplementary. Next, for
comparison graphs under M -wise observations, we establish a sufficient condition for top-K ranking.
r
log n
(M )
Theorem 2. Given an M -wise comparison
graph
G
=
([n],
E
)
and
p
?
c
(M
?
1)
1
n?1 , if
 
(M
?1)
n
n log n 1
pL ? c2
,
(2)
M
?2K M
for some numerical constants c1 and c2 , then Rank Centrality correctly identifies the top-K ranked
1
items with probability at least 1 ? 2n? 15 .
We provide the proof of Theorem 2 in the supplementary. From below, we describe the algorithm we
use, sample breaking and Rank Centrality [37], and soon give an outline of the proof.
Note that Theorem 1 gives a necessary condition of the sample complexity S?K & n log n/M ?2K
and Theorem 2 gives a sufficient condition of it S?K . n log n/M ?2K , and they match. That is, we
establish the minimax optimality of Rank Centrality: n log n/M ?2K .

p > log n/ Mn?1 is derived in [19] as a sharp threshold for connectivity of hyper-graphs. We assume a
slightly more strict condition for ease of analysis. This does not make a big difference in our result, as the two
conditions are almost identical order-wise given M < n/2, a reasonable condition for regimes where n is large.
2

4

3.1

Algorithm description

Algorithm 1 Rank Centrality [37]

	
Input the collection of statistics s = sI : I ? E (M ) .
Convert the M -wise sample for each hyper-edge I into M pairwise samples:
1. Choose a circular permutation of the items in I uniformly at random,
2. Break it into the M pairs of adjacent items, and denote the set of pairs by ?(I),
3. Use the (pairwise) data of the pairs in ?(I).
? 1
if i 6= j;
? 2dmax yij
P
?
?
?
?
Compute the transition matrix P = [Pij ]1?i,j?n : Pij =
1 ? k:k6=j Pkj if i = j;
?
0
otherwise.,
where dmax is the maximum out-degree of vertices in E (M ) .
?.
Output the stationary distribution of matrix P
Rank Centrality aims to estimate rankings from pairwise comparison data. Thus, to make use of
M -wise comparison data for Rank Centrality, we apply a sample breaking method that converts
M -wise data into pairwise data. To be more specific, if there is a hyper-edge I = {1, 2, . . . , M }, we
choose a circular permutation of the items in I uniformly at random. Suppose we pick a circular
permutation (1, 2, . . . , M ? 1, M, 1). Then, we break it into M pairs of items in the order specified
by the permutation: {1, 2}, {2, 3}, . . . , {M ? 1, M }, {M, 1} (see Section 3.3 for a remark on why
we do not lose optimality by our sample breaking method). Let us denote by ?(I) this set of pairs.
We use the converted pairwise comparison data associated with the pairs in ?(I)3 :

L
X
1 X (`)
1 if {i, j} ? ?(I) and i  j;
(`)
yij,I =
, yij :=
yij,I .
(3)
0 otherwise
L
I:{i,j}??(I)

`=1

In an ideal scenario where we obtain an infinite number of samples per M -wise comparison, i.e.,
PL (`)
L ? ?, sufficient statistics L1 l=1 yij,I converge to wi /(wi + wj ). Then, the constructed matrix
? defined in Algorithm 1 becomes a matrix P whose entries [Pij ]1?i,j?n are defined as
P
? 1 P
i
for I ? E (M ) ;
? 2dmaxP I:{i,j}??(I) wiw+w
j
Pij =
(4)
1 ? k:k6=j Pkj
if i = j;
?
0
otherwise.
The entries for observed item pairs represent the relative likelihood of item i being preferred over
item j. Intuitively, random walks of P in the long run visit some states more often, if they have been
preferred over other frequently-visited states and/or preferred over many other states. The random
walks are reversible as wi Pji = wj Pij holds, and irreducible under the connectivity assumption. Once
we obtain the unique stationary distribution, it is equal to w = {w1 , . . . , wn } up to some constant
? , a noisy version of P , will give us an approximation of w.
scaling. It is clear that random walks of P
3.2

Proof outline

We outline the proof of Theorem 2 by introducing Theorem 3, which we show leads to Theorem 2.
Theorem 3. When Rank Centrality is employed, with high probability, the `? norm estimation error
is upper-bounded by
s
r
? ? wk?
kw
n log n
1

.
,
(5)
n
kwk?
M
pL
M
r
n
where p ? c1 (M ? 1) log
n?1 , and c1 is some numerical constant.
(M
?1)
3
In comparison, the adjacent breaking method [5] directly follows the ordering evaluated in each sample; if it
is 1 ? 2 ? ? ? ? ? M ? 1 ? M , it is broken into pairs of adjacent items: 1 ? 2 up to M ? 1 ? M . Our method
Pr[y =1]
wi
turns out to be consistent, i.e., Pr[yij
= w
(see (4)), whereas the adjacent breaking method is not [5].
ji =0]
j

5

Let
q kwk?  = pwmax = 1 for ease of presentation. Suppose ?K = wK ? wK+1 &
n
log n/ M
pL 1/M . Then, w
?i ? w
?j ? wi ? wj ? |w
?i ? wi | ? |w
?j ? wj | ? wK ? wK+1 ?
? ? wk? > 0, for all 1 ? i ?qK and j ? K + 1. That is, the top-K items are identified as
2kw
 p

n
n
desired. Hence, as long as ?K & log n/ M
pL 1/M , i.e., M
pL & n log n/M ?2K , reliable
top-K ranking is achieved with the sample size of n log n/M ?2K .
Now, let us prove Theorem 3. To find an `? error bound, we first derive an upper bound on the
point-wise error between the score estimate of item i and its true score, which consists of three terms:
X

 
X

?
?
?

|w
? i ? wi | ? | w
?i ? wi | Pii +
|w
?j ? wj | Pij + 
(wi + wj ) Pji ? Pji .
(6)
j:j6=i

j:j6=i

?w
? =P
? and w = P w. We then obtain upper bounds on the three terms:
We can obtain (6) from w
s
s
r
r
X

 

n log n
1 X
n log n
1
?
?
?




Pii < 1, 
(wi + wj ) Pji ? Pji  .
,
|w
?j ? wj | Pij .
,
n
n
M
M
M pL
M pL
j:j6=i

j:j6=i

(7)
with high probability (Lemmas 1, 2 and 3 in the supplementary). (7) ends the proof. We obtain the
firstqtwo from Hoeffding?s inequality. The last is key; this is where we sharply link an `2 error bound
 p
n
of n log n/ M
pL 1/M (Theorem 4 in the supplementary) to the desired `? error bound (5).
On the left hand side of the third inequality, the point-wise error of item j which affects that of
item i as expressed in (6), may not be captured for some j, since there may be no hyper-edge that
includes items i and j. This makes it hard to draw a link from the obtained `2 error bound to the
inequality, since `2 errors can be seen as the sum of all point-wise errors. To include them all, we
recursively apply (6) to |w
?j ? wj | in the third inequality and then apply the rest two properly (for
detailed derivation, see the beginning of the proof of Lemma 3 in the supplementary). Then, we get
s
r
X
X X
n log n
1
?
?
?

|w
?j ? wj | Pij .
|w
?k ? wk | Pjk Pij +
.
(8)
n
M
pL
M
j:j6=i

j:j6=i k:k6=j

Manipulating the first term of the right hand side (for derivation, see the proof of Lemma 3), we get
v
 X
2
uX
n
X
X
u n
?
?
?
?
t
? ? wk2
|w
?k ? wk |
Pjk Pij ? kw
Pjk Pij .
(9)
j:j ?{i,k}
/

k=1

k=1

j:j ?{i,k}
/

P
We show that j:j ?{i,k}
P?jk P?ij concentrates on the order of 1/n for all k?s in the proof of Lemma
?/
?
?
? ? wk2 / n ? kw
? ? wk2 /kwk2 . We derive this `2
3. Since kwk2 ? qnkwk? = n, we get kw
 p
n
error bound to be n log n/ M pL 1/M (Theorem 4 in the supplementary), matching (5).
P
To describe the concentration of j:j ?{i,k}
P?jk P?ij , we need to consider dependencies in it. To see
/
them, we upper-bound it as follows (for details, see the proof of Lemma 3 in the supplementary).
X
j:j ?{i,k}
/

P?ij P?jk ?

1

X

X

4d2max
I1 :i,j?I1 ,I2 :j,k?I2
j:j ?{i,k}
/

XI1 I2 ,

(10)

where XI1 I2 := I [{i, j} ? ?(I1 )] I [{j, k} ? ?(I2 )] . For M > 2, there can exist ja and jb such that
{i, ja , jb } ? I1 , ja ? I2 and jb ?
/ I2 . Then, summing over j, XI1 I2 and XI1 I3 , where I3 is another
hyper-edge that includes jb and k, are dependent concerning the same hyper-edge I1 . To handle this,
we use Janson?s inquality [30], one of concentration inequalities that consider dependencies.
To derive a necessary condition matching our sufficient condition, we use a generalized version of
Fano?s inequality [26] as in the proof of Theorem 3 in [17] and complete combinatorial calculations.
6

3.3

Discussion

Optimality versus M ? intuition behind our sample breaking method: For each M -wise sample, we
form a circular permutation uniformly at random, and extract M pairwise samples each of which
concerns two adjacent items in it. Suppose we have an M -wise sample 1 ? 2 ? ? ? ? ? M , and
for simplicity we happen to form a circular permutation as (1, 2, . . . , M ? 1, M, 1); we extract M
pairwise samples as 1 ? 2, 2 ? 3, . . . , (M ? 1) ? M , 1 ? M . Let us provide the intuition behind
why this leads us to the optimal sample complexity. For the case of M = 2, Rank Centrality achieves
the optimal order-wise sample complexity of n log n/?2K as characterized in [17]. In addition, one
M -wise sample in the PL model can be broken into M ? 1 independent pairwise ones, since pairwise
data of two arbitrary items among the M items depend on the true scores of the two items only. In our
example, one can convert the M -wise sample into M ? 1 independent pairwise ones as 1 ? 2, 2 ? 3,
. . . , (M ? 1) ? M . From these, it is intuitive to see that we can achieve reliable top-K ranking with
an order-wise sample complexity of n log n/(M ? 1)?2K by converting each M -wise sample into
M ? 1 independent pairwise ones. Notice a close gap to the optimal sample complexity in Section 3.
Tight `? error bounds: As shown in 3.2, deriving a tight `? error bound is critical to analyzing the
performance of an algorithm for top-K rank aggregation. Recent work [17] has relied on combining
an additional stage of local refinement in series with Rank Centrality to derive it, and characterized
the optimal sample complexity for the pairwise model. In contrast, although it is valid in a slightly
restricted regime (see the next remark), we employ only Rank Centrality and still succeed in achieving
optimality for the M -wise model that includes the pairwise model. Deriving tight `? error bounds
being crucial, it is hard for one to attain this result without a fine analytical technique. It is our main
theoretical contribution to develop one. For details, see the proof of Lemma 3 in the supplementary
that sharply links an `? error bound (Theorem 3 therein) and an `2 error bound (Theorem 4 therein).
Rank Centrality has been shown to achieve the performance nearly as good as MLE in terms of `2
error, but little has been known in terms of `? error, until now. Our result has made clear progress.
Analytical technique: Our analysis is not limited to Rank Centrality. Whenever one wishes to compute
the difference between the leading eigenvector of any matrix and that of its noisy version, one can
obtain (6), (8) and (9). Thus, it can be adopted to link `2 and `? error bounds for any spectral method.
Dense regimes:
q Our main result concerns a slightly denser regime, indicated by the condition
n?1
p & (M ? 1) log n/ M
?1 , where many distinct item pairs are likely to be compared. One can see
that this dense regime condition is not necessary for top-K ranking; for the pairwise case M = 2, it is
p & log n/n as shown in [17]. However, it is not clear yet whether or not the dense regime condition
is required under our approach that employs only a spectral method. Our speculation
q from numerical


n?1
n?1
experiments is that the sparse regime condition, log n/ M
.
p
.
(M
?
1)
log n/ M
?1
?1 , may
not be sufficient for spectral methods to achieve reliable top-K ranking (see Section 4).

Experimental Results
Synthetic data simulation
1

?? norm of estimation errors

0.5

empirical success rate

Spectral MLE: p = 0.25

0.3

0.2

0.1

0
1

5

10

15

20

L: number of repeated comparisons

0.5

0.9

Rank Centrality: p = 0.25

0.4

25

0.8

0.4

0.7
0.6

Spectral MLE: p = 0.025

0.3

0.5
0.4

0.2

0.3

Rank Centrality: p = 0.25

0.2

Spectral MLE: p = 0.25

0.1

Borda Count: p = 0.25

0
1

1
0.9

Rank Centrality: p = 0.025

empirical success rate

4.1

?? norm of estimation errors

4

5

10

15

20

0.1

L: number of repeated comparisons

0.7
0.6
0.5
0.4

Rank Centrality: p = 0.025

0.3

Spectral MLE: p = 0.025

0.2

Borda Count: p = 0.025

0.1
0
10

25

0.8

50

100

150

200

L: number of repeated comparisons

250

0
10

50

100

150

200

250

L: number of repeated comparisons

Figure 1: Dense regime (pdense = 0.25, first two figures): empirical `? estimation error v.s. L (left);
empirical success rate v.s. L (right). Sparse regime (psparse = 0.025, last two figures): empirical `?
estimation error v.s. L (left); empirical success rate v.s. L (right).
First, we conduct a synthetic data experiment for M = 2, the pairwise comparison model,
p to compare
our result in Theorem 2 p
to that in recent work [17]. We consider both dense (p & log n/n) and
sparse (log n/n . p . log n/n) regimes. We set constant c1 = 2, and set pdense = 0.25 and
psparse = 0.025, to make each be in its proper range. We use n = 500, K = 10, and ?K = 0.1.
Each result in all numerical simulations is obtained by averaging over 10000 Monte Carlo trials.
7

In Figure 1, the first two figures show the experiments in the dense regime. We see that as L increases,
meaning as we obtain pairwise samples beyond the minimal sample complexity, (1) the `? error
of Rank Centrality decreases and meets that of Spectral MLE (left); (2) the success rate of Rank
Centrality increases and soon hits
p 100% along with Spectral MLE (right). The curves support our
result; in the dense regime p & log n/n, Rank Centrality alone can achieve reliable top-K ranking.
The last two figures show the experiments in the sparse regime. We see that as L increases, (1) the
`? error of Rank Centrality decreases but does not meet that of Spectral MLE (left); (2) the success
rate of Rank Centrality increases but does not reach that of Spectral MLE which hits nearly
p 100%
(right). The curves lead us to speculate that the sparse regime condition log n/n . p . log n/n
may not be sufficient for spectral methods to achieve reliable top-K ranking.
Empirical

10000

Curve ?tting: 1/M

8000
6000
4000
2000
0
3

4

5

6

7

8

9

10

11

12

13

14

?105

Empirical

2.5

Curve ?tting: 1/?2K

2
1.5
1
0.5
0
0.1

15

2

Minimal sample complexity

3

Minimal sample complexity

Minimal sample complexity

12000

0.15

0.2

0.25

?K

M : Size of hyper-edges

?105

Empirical
1.5

Curve ?tting: n log n

1

0.5

0

0.3

500

1000

1500

n

Figure 2: Empirical minimal sample complexity v.s. M (first), ?K (second), and n log n (third).
Next, we corroborate our optimal sample complexity result in Theorem 2. We examine whether the
empirical minimal sample complexity decreases at the rate of 1/M and 1/?2K , and increases at the
rate of n log n. To verify its reduction at the rate of 1/M , we run experiments for M ranging from
3 to 15. We increase the number of samples by increasing p until the success rate reaches 95% for
each M . The number of samples we use to achieve it is considered as the empirical minimal sample
complexity for each M . We set the other parameters as n = 100, L = 20, K = 5 and ?K = 0.3.
The result for each M in all simulations is obtained by averaging over 1000 Monte Carlo trials. To
verify the other two relations, we follow similar procedures. As for 1/?2K , we set n = 200, M = 2,
L = 20 and K = 5. As for n log n, we set M = 2, L = 4, K = 5 and ?K = 0.4.
The first figure in Figure 2 shows the reduction in empirical minimal sample complexity with a blue
solid curve. The red dashed curve is obtained by curve-fitting. We can see that the empirical minimal
sample complexity drops inversely proportional to M . From the second and third figures, we can see
that in terms of ?K and n log n, it also behaves as our result in Theorem 2 predicts.
1

1

0.7
0.6
0.5
0.4

Spectral MLE
Proposed
Least Square
Counting

0.3
0.2
0.1
15

20

25

30

L

35

40

45

Spectral MLE

0.9

80

Least Square
0.8

Counting

Percentile

Normalized overlap (K = 5)

Success rate

0.8

0
10

100
Proposed

0.9

0.7
0.6

60

40
Proposed

0.5

Spectral MLE

20

Least Square

0.4

Counting
50

1

0.9

0.8

0.7

f: Fraction of samples used

0.6

0.5

0

1

2

3

4

5

Top-5 users based on average league point per match

Figure 3: (First) Empirical success rates of four algorithms: our algorithm (blue circle), heuristic
Spectral MLE (red cross), least square (green plus), and counting (purple triangle); (Second) Top-5
ranked users: normalized overlap v.s. fraction of samples used; (Third) Top-5 users? (sorted by
average League of Legends points earned per match) percentile in the ranks by our algorithm, heuristic
Spectral MLE, least square, and counting. For instance, the user who earns largest points per match
(first entry) is at around the 80-th percentile according to our algorithm and heuristic Spectral MLE,
the 60-th percentile according to least square, and the 10-th percentile according to counting.
Last, we evaluate the success rates of various algorithms on M -wise comparison data. We consider
our proposed algorithm, Spectral MLE, least square (HodgeRank [31]), and counting. Since Spectral
MLE has been developed for pairwise data, we heuristically extend it. We apply our sample breaking
method to obtain pairwise data needed. For any parameters required to run Spectral MLE, we
heuristically find the best ones which give rise to the highest success rate. In the other two algorithms,
we first apply our sample breaking method as well. Then, for least square, we find a score vector
P
2
? such that the squared error (i,j)?E (log(w
w
? i /w
?j ) ? log(yij /yji )) , where E is the edge set for
the converted pairwise data, is minimized. For counting, we count each item?s number of wins in all
8

q

n?1
involved pairwise data. We use n = 100, M = 4, p = 0.0025 ? (M ? 1) log n/ M
?1 , K = 5 and
?K = 0.3. Each result in all simulations is obtained by averaging over 5000 Monte Carlo trials.
The first figure in Figure 3 shows that our algorithm and heuristic Spectral MLE perform best (the
latter being marginally better), achieving near-100% success rates for large L. It also shows that they
outperform the other two algorithms which do not achieve near-100% success rates even for large L.
4.2

Real-world data simulation

One natural setting where we can obtain M -wise comparison data is an online game. Users randomly
get together and play, and the results depend on their skills. We find League of Legends to be a proper
fit4 . In extracting M -wise data, we adopt a measure widely accepted as a factor that rates users? skill
in the user community5 . We incorporate this measure into our model as follows. For each match
(M -wise sample), we have 10 users, each associated with its measure. In breaking M -wise samples,
for each user pair (i, j), we compare their measures and declare user i wins if its measure is larger
(`)
than user j?s. This corresponds to yij in our model. We assign 1 if user i wins and 0 otherwise. They
PLij (`)
may play together in multiple, say Lij , matches. We can compute yij := ( `=1
yij )/Lij to use for
Rank Centrality. As M -wise data is extracted from team competitions, League of Legends does not
perfectly fit our model. Yet one main reason to run this experiment is to see whether our algorithm
works well in other settings that do not necessarily fit the PL model, being broadly applicable.
We first investigate the robustness aspect by evaluating the performance against partial information.
To this end, we use all collected data and obtain a ranking result for each algorithm which we consider
as its baseline. Then, for each algorithm, we reduce sample sizes by discarding some of the data, and
compare the results to the baseline to see how robust each algorithm is against partial information.
We conduct this experiment for four algorithms: our proposed algorithm, the heuristic extension of
Spectral MLE, least square and counting.
We choose our metric as a normalized overlap: |Scomp ? Spart |/K, where K = 5, Scomp is the set of
top-K users identified using the complete dataset and Spart is that identified using partial datasets.
In choosing partial data, we set f ? (0.5, 1), and discard each match result with probability f
independently. We compute the metric for each f by averaging over 1000 Monte Carlo trials.
The second figure of Figure 3 shows that over the range of f where overlaps above 60% are retained,
our algorithm, along with some others, demonstrates good robustness against partial information.
In addition, we compare the ranks estimated by the four algorithms to the rank provided by League of
Legends. By computing the average points earned per match for each user, we infer the rank of the
users determined by official standards. In the third figure of Figure 3, the x-axis indicates the top-5
users identified by computing average League of Legends points earned per match and sorting them
in descending order. The y-axis indicates the percentile of these top-5 users according to the ranks by
the algorithms of interest. Notice that the top-5 ranked users by League of Legends standards are also
placed at high ranks when ranked by our algorithm and heuristic Spectral MLE; they are all placed at
the 80-th percentile or above. On the other hand, most of them (4 users out of the top-5 users) are
placed at noticeably lower ranks when ranked by least square and counting.

5

Conclusion

We characterized the minimax (order-wise) optimal sample complexity for top-K rank aggregation
in the M -wise comparison model that builds on the PL model. We corroborated our result using
synthetic data experiments and verified the applicability of our algorithm on real-world data.
4
Two teams of 5 users compete. Each user kills an opponent, assists a mate to kill one, and dies from an
attack. At the end, one team wins, and different points are given to the users. We use users? kill/assist/death data
(non-negative integers), which can be considered as noisy measurements of their skill, and rank them by skill.
5
We define a measure as {(# of kills + # of assists)/(1 + # of deaths)}?weight. We adopt this measure
since it is similar to the one officially provided (called KDA statistics). We assign winning users a weight of 1.1
and losing users a weight of 1.0, to give extra credit (10%) to users who lead their team?s winning.

9

Acknowledgments
This work was supported by Institute for Information & communications Technology Promotion(IITP)
grant funded by the Korea government(MSIT) (2017-0-00694, Coding for High-Speed Distributed
Networks).

References
[1] Ailon, N. (2012). Active learning ranking from pairwise preferences with almost optimal query complexity.
Journal of Machine Learning, 13, 137?164.
[2] Ailon, N. and Mohri, M. (2007). An efficient reduction of ranking to classification. arXiv preprint
arXiv:0710.2889.
[3] Ammar, A. and Shah, D. (2011). Ranking: Compare, don?t score. In Allerton Conference, pages 776?783.
IEEE.
[4] Ashish, K. and Oh, S. (2016). Data-driven rank breaking for efficient rank aggregation. Journal of Machine
Learning Research, 17, 1?54.
[5] Azari Soufiani, H., Chen, W., Parkes, D. C., and Xia, L. (2013). Generalized method-of-moments for rank
aggregation. In Neural Information Processing Systems, pages 2706?2714.
[6] Azari Soufiani, H., Parkes, D. C., and Xia, L. (2014). A statistical decision-theoretic framework for social
choice. In Neural Information Processing Systems, pages 3185?3193.
[7] Baltrunas, L., Makcinskas, T., and Ricci, F. (2010). Group recommendations with rank aggregation and
collaborative filtering. In ACM Conference on Recommender Systems, pages 119?126. ACM.
[8] Bell, D. (1982). Econometric models for probabilisitic choice among products. Operations Research, 30(5),
961?981.
[9] Bergstrom, C. T., W. J. D. and Wiseman, M. A. (2008). The eigenfactorTM metrics. Journal of Neuroscience,
28(45), 11433?11434.
[10] Bonacich, P. and Lloyd, P. (2001). Eigenvector-like measures of centrality for asymmetric relations. Social
networks, 23(3), 191?201.
[11] Borda, J. C. (1781). M?moire sur les ?lections au scrutin.
[12] Bradley, R. A. and Terry, M. E. (1952). Rank analysis of incomplete block designs: I. the method of paired
comparisons. Biometrika, 39(3-4), 324?345.
[13] Braverman, M., Mao, J., and Weinberg, S. M. (2016). Parallel algorithms for select and partition with
noisy comparisons. In ACM symposium on Theory of Computing, pages 851?862.
[14] Brin, S. and Page, L. (1998). The anatomy of a large-scale hypertextual web search engine. Computer
Networks and ISDN systems, 30(1), 107?117.
[15] Caplin, A. and Nalebuff, B. (1991). Aggregation and social choice: a mean voter theorem. Econometrica,
pages 1?23.
[16] Chen, X., Bennett, P. N., Collins-Thompson, K., and Horvitz, E. (2013). Pairwise ranking aggregation in a
crowdsourced setting. In ACM Conference on Web Search and Data Mining, pages 193?202. ACM.
[17] Chen, Y. and Suh, C. (2015). Spectral MLE: Top-K rank aggregation from pairwise comparisons. In
International Conference on Machine Learning, pages 371?380.
[18] Cheng, W., H. E. and Dembczynski, K. J. (2010). Label ranking methods based on the Plackett-Luce
model. In International Conference on Machine Learning, pages 215?222.
[19] Cooley, O., Kang, M., and Koch, C. (2016). Threshold and hitting time for high-order connectedness in
random hypergraphs. the electronic journal of combinatorics, pages 2?48.
[20] Dwork, C., Kumar, R., Naor, M., and Sivakumar, D. (2001). Rank aggregation methods for the web. In
International conference on World Wide Web, pages 613?622. ACM.
[21] Fishburn, P. (1982). Nontransitive measurable utility. Journal of Mathematical Psychology, 26(1), 31?67.

10

[22] Ford, L. R. (1957). Solution of a ranking problem from binary comparisons. American Mathematical
Monthly, pages 28?33.
[23] Graham, L. and Sugden, R. (1982). Econometric models for probabilisitic choice among products.
Economic Journal, 92(368), 805?824.
[24] Guiver, J. and Snelson, E. (2009). Bayesian inference for Plackett-Luce ranking models. In ACM
International Conference on Machine Learning, pages 377?384.
[25] Hajek, B., Oh, S., and Xu, J. (2014). Minimax-optimal inference from partial rankings. In Neural
Information Processing Systems, pages 1475?1483.
[26] Han, T. and Verd?, S. (1994). Generalizing the Fano inequality. IEEE Transactions on Information Theory,
40, 1247?1251.
[27] Heckel, R., Shah, N., Ramchandran, K., and Wainwright, M. (2016). Active ranking from pairwise
comparisons and when parametric assumptions don?t help. arXiv preprint arXiv:1606.08842.
[28] Hunter, D. R. (2004). MM algorithms for generalized Bradley-Terry models. Annals of Statistics, pages
384?406.
[29] Jamieson, K. G. and Nowak, R. (2011). Active ranking using pairwise comparisons. In Neural Information
Processing Systems, pages 2240?2248.
[30] Janson, S. (2004). Large deviations for sums of partly dependent random variables. In Random Structures
& Algorithms, pages 234?248.
[31] Jiang, X., Lim, L. H., Yao, Y., and Ye, Y. (2011). Statistical ranking and combinatorial Hodge theory.
Mathematical Programming, 127, 203?244.
[32] Luce, R. D. (1959). Individual choice behavior: A theoretical analysis. Wiley.
[33] Maystre, L. and Grossglauser, M. (2015). Fast and accurate inference of Plackett-Luce models. In Neural
Information Processing Systems, pages 172?180.
[34] McFadden, D. (1973). Conditional logit analysis of qualitative choice behavior. Frontiers in Econometrics,
pages 105?142.
[35] McFadden, D. (1980). Econometric models for probabilisitic choice among products. Journal of Business,
53(3), S13?S29.
[36] Mohajer, S., Suh, C., and Elmahdy, A. (2017). Active learning for top-K rank aggregation from noisy
comparisons. In International Conference on Machine Learning, pages 2488?2497.
[37] Negahban, S., Oh, S., and Shah, D. (2016). Rank centrality: Ranking from pair-wise comparisons.
Operations Research, 65, 266?287.
[38] Oh, S., Thekumparampil, K. K., and Xu, J. (2015). Collaboratively learning preferences from ordinal data.
In Neural Information Processing Systems, pages 1909?1917.
[39] Plackett, R. L. and Luce, R. D. (1975). The analysis of permutations. Applied Statistics, pages 193?202.
[40] Rajkumar, A. and Agarwal, S. (2014). A statistical convergence perspective of algorithms for rank
aggregation from pairwise data. In International Conference on Machine Learning, pages 118?126.
[41] Seeley, J. R. (1949). The net of reciprocal influence. Canadian Journal of Psychology, 3(4), 234?240.
[42] Shah, N. B. and Wainwright, M. J. (2015). Simple, robust and optimal ranking from pairwise comparisons.
arXiv preprint arXiv:1512.08949.
[43] Sz?r?nyi, B., Busa-Fekete, R., Paul, A., and H?llermeier, E. (2015). Online rank elicitation for PlackettLuce: A dueling bandits approach. In Neural Information Processing Systems, pages 604?612.
[44] Valiant, L. G. (1984). A theory of the learnable. Communications of the ACM, 27(11), 1134?1142.
[45] Vigna, S. (2016). Spectral ranking. Network Science, 4(4), 433?445.
[46] Walker, J. and Ben-Akiva, M. (2002). Generalized random utility model. Mathematical Social Sciences,
43(3), 303?343.
[47] Wei, T. H. (1952). The algebraic foundations of ranking theory. Ph.D. thesis, University of Cambridge.

11

"
4801,2014,How transferable are features in deep neural networks?,"Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.","How transferable are features in deep neural
networks?
Jason Yosinski,1 Jeff Clune,2 Yoshua Bengio,3 and Hod Lipson4
1
Dept. Computer Science, Cornell University
2
Dept. Computer Science, University of Wyoming
3
Dept. Computer Science & Operations Research, University of Montreal
4
Dept. Mechanical & Aerospace Engineering, Cornell University

Abstract
Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters
and color blobs. Such first-layer features appear not to be specific to a particular
dataset or task, but general in that they are applicable to many datasets and tasks.
Features must eventually transition from general to specific by the last layer of
the network, but this transition has not been studied extensively. In this paper we
experimentally quantify the generality versus specificity of neurons in each layer
of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of
higher layer neurons to their original task at the expense of performance on the
target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues
may dominate, depending on whether features are transferred from the bottom,
middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but
that transferring features even from distant tasks can be better than using random
features. A final surprising result is that initializing a network with transferred
features from almost any number of layers can produce a boost to generalization
that lingers even after fine-tuning to the target dataset.

1

Introduction

Modern deep neural networks exhibit a curious phenomenon: when trained on images, they all tend
to learn first-layer features that resemble either Gabor filters or color blobs. The appearance of these
filters is so common that obtaining anything else on a natural image dataset causes suspicion of
poorly chosen hyperparameters or a software bug. This phenomenon occurs not only for different
datasets, but even with very different training objectives, including supervised image classification
(Krizhevsky et al., 2012), unsupervised density learning (Lee et al., 2009), and unsupervised learning of sparse representations (Le et al., 2011).
Because finding these standard features on the first layer seems to occur regardless of the exact cost
function and natural image dataset, we call these first-layer features general. On the other hand, we
know that the features computed by the last layer of a trained network must depend greatly on the
chosen dataset and task. For example, in a network with an N-dimensional softmax output layer that
has been successfully trained toward a supervised classification objective, each output unit will be
specific to a particular class. We thus call the last-layer features specific. These are intuitive notions
of general and specific for which we will provide more rigorous definitions below. If first-layer
1

features are general and last-layer features are specific, then there must be a transition from general
to specific somewhere in the network. This observation raises a few questions:
? Can we quantify the degree to which a particular layer is general or specific?
? Does the transition occur suddenly at a single layer, or is it spread out over several layers?
? Where does this transition take place: near the first, middle, or last layer of the network?
We are interested in the answers to these questions because, to the extent that features within a
network are general, we will be able to use them for transfer learning (Caruana, 1995; Bengio
et al., 2011; Bengio, 2011). In transfer learning, we first train a base network on a base dataset and
task, and then we repurpose the learned features, or transfer them, to a second target network to
be trained on a target dataset and task. This process will tend to work if the features are general,
meaning suitable to both base and target tasks, instead of specific to the base task.
When the target dataset is significantly smaller than the base dataset, transfer learning can be a
powerful tool to enable training a large target network without overfitting; Recent studies have
taken advantage of this fact to obtain state-of-the-art results when transferring from higher layers
(Donahue et al., 2013a; Zeiler and Fergus, 2013; Sermanet et al., 2014), collectively suggesting that
these layers of neural networks do indeed compute features that are fairly general. These results
further emphasize the importance of studying the exact nature and extent of this generality.
The usual transfer learning approach is to train a base network and then copy its first n layers to
the first n layers of a target network. The remaining layers of the target network are then randomly
initialized and trained toward the target task. One can choose to backpropagate the errors from
the new task into the base (copied) features to fine-tune them to the new task, or the transferred
feature layers can be left frozen, meaning that they do not change during training on the new task.
The choice of whether or not to fine-tune the first n layers of the target network depends on the
size of the target dataset and the number of parameters in the first n layers. If the target dataset is
small and the number of parameters is large, fine-tuning may result in overfitting, so the features
are often left frozen. On the other hand, if the target dataset is large or the number of parameters is
small, so that overfitting is not a problem, then the base features can be fine-tuned to the new task
to improve performance. Of course, if the target dataset is very large, there would be little need to
transfer because the lower level filters could just be learned from scratch on the target dataset. We
compare results from each of these two techniques ? fine-tuned features or frozen features ? in
the following sections.
In this paper we make several contributions:
1. We define a way to quantify the degree to which a particular layer is general or specific, namely,
how well features at that layer transfer from one task to another (Section 2). We then train pairs
of convolutional neural networks on the ImageNet dataset and characterize the layer-by-layer
transition from general to specific (Section 4), which yields the following four results.
2. We experimentally show two separate issues that cause performance degradation when using transferred features without fine-tuning: (i) the specificity of the features themselves, and
(ii) optimization difficulties due to splitting the base network between co-adapted neurons on
neighboring layers. We show how each of these two effects can dominate at different layers of
the network. (Section 4.1)
3. We quantify how the performance benefits of transferring features decreases the more dissimilar
the base task and target task are. (Section 4.2)
4. On the relatively large ImageNet dataset, we find lower performance than has been previously
reported for smaller datasets (Jarrett et al., 2009) when using features computed from random
lower-layer weights vs. trained weights. We compare random weights to transferred weights?
both frozen and fine-tuned?and find the transferred weights perform better. (Section 4.3)
5. Finally, we find that initializing a network with transferred features from almost any number
of layers can produce a boost to generalization performance after fine-tuning to a new dataset.
This is particularly surprising because the effect of having seen the first dataset persists even
after extensive fine-tuning. (Section 4.1)
2

2

Generality vs. Specificity Measured as Transfer Performance

We have noted the curious tendency of Gabor filters and color blobs to show up in the first layer of
neural networks trained on natural images. In this study, we define the degree of generality of a set
of features learned on task A as the extent to which the features can be used for another task B. It
is important to note that this definition depends on the similarity between A and B. We create pairs
of classification tasks A and B by constructing pairs of non-overlapping subsets of the ImageNet
dataset.1 These subsets can be chosen to be similar to or different from each other.
To create tasks A and B, we randomly split the 1000 ImageNet classes into two groups each containing 500 classes and approximately half of the data, or about 645,000 examples each. We train
one eight-layer convolutional network on A and another on B. These networks, which we call baseA
and baseB, are shown in the top two rows of Figure 1. We then choose a layer n from {1, 2, . . . , 7}
and train several new networks. In the following explanation and in Figure 1, we use layer n = 3 as
the example layer chosen. First, we define and train the following two networks:
? A selffer network B3B: the first 3 layers are copied from baseB and frozen. The five higher
layers (4?8) are initialized randomly and trained on dataset B. This network is a control for the
next transfer network. (Figure 1, row 3)
? A transfer network A3B: the first 3 layers are copied from baseA and frozen. The five higher
layers (4?8) are initialized randomly and trained toward dataset B. Intuitively, here we copy
the first 3 layers from a network trained on dataset A and then learn higher layer features on top
of them to classify a new target dataset B. If A3B performs as well as baseB, there is evidence
that the third-layer features are general, at least with respect to B. If performance suffers, there
is evidence that the third-layer features are specific to A. (Figure 1, row 4)
We repeated this process for all n in {1, 2, . . . , 7}2 and in both directions (i.e. AnB and BnA). In
the above two networks, the transferred layers are frozen. We also create versions of the above two
networks where the transferred layers are fine-tuned:
? A selffer network B3B+ : just like B3B, but where all layers learn.
? A transfer network A3B+ : just like A3B, but where all layers learn.
To create base and target datasets that are similar to each other, we randomly assign half of the 1000
ImageNet classes to A and half to B. ImageNet contains clusters of similar classes, particularly dogs
and cats, like these 13 classes from the biological family Felidae: {tabby cat, tiger cat, Persian cat,
Siamese cat, Egyptian cat, mountain lion, lynx, leopard, snow leopard, jaguar, lion, tiger, cheetah}.
On average, A and B will each contain approximately 6 or 7 of these felid classes, meaning that
base networks trained on each dataset will have features at all levels that help classify some types
of felids. When generalizing to the other dataset, we would expect that the new high-level felid
detectors trained on top of old low-level felid detectors would work well. Thus A and B are similar
when created by randomly assigning classes to each, and we expect that transferred features will
perform better than when A and B are less similar.
Fortunately, in ImageNet we are also provided with a hierarchy of parent classes. This information
allowed us to create a special split of the dataset into two halves that are as semantically different
from each other as possible: with dataset A containing only man-made entities and B containing
natural entities. The split is not quite even, with 551 classes in the man-made group and 449 in the
natural group. Further details of this split and the classes in each half are given in the supplementary
material. In Section 4.2 we will show that features transfer more poorly (i.e. they are more specific)
when the datasets are less similar.
1
The ImageNet dataset, as released in the Large Scale Visual Recognition Challenge 2012 (ILSVRC2012)
(Deng et al., 2009) contains 1,281,167 labeled training images and 50,000 test images, with each image labeled
with one of 1000 classes.
2
Note that n = 8 doesn?t make sense in either case: B8B is just baseB, and A8B would not work because
it is never trained on B.

3

WA1

WA2

WA3

WA4

WA5

WA6

WA7

WA8

input
A

WB1

WB2

WB3

WB4

WB5

input
B

WB1

WB2

WB3

or

or

or

WA1

WA2

WA3

or

or

or

WB6

WB7

labels
A

baseA

labels
B

baseB

WB8

B3B
and
B3B+

A3B
and
A3B+

Figure 1: Overview of the experimental treatments and controls. Top two rows: The base networks
are trained using standard supervised backprop on only half of the ImageNet dataset (first row: A
half, second row: B half). The labeled rectangles (e.g. WA1 ) represent the weight vector learned for
that layer, with the color indicating which dataset the layer was originally trained on. The vertical,
ellipsoidal bars between weight vectors represent the activations of the network at each layer. Third
row: In the selffer network control, the first n weight layers of the network (in this example, n = 3)
are copied from a base network (e.g. one trained on dataset B), the upper 8 ? n layers are randomly
initialized, and then the entire network is trained on that same dataset (in this example, dataset B).
The first n layers are either locked during training (?frozen? selffer treatment B3B) or allowed to
learn (?fine-tuned? selffer treatment B3B+ ). This treatment reveals the occurrence of fragile coadaptation, when neurons on neighboring layers co-adapt during training in such a way that cannot
be rediscovered when one layer is frozen. Fourth row: The transfer network experimental treatment
is the same as the selffer treatment, except that the first n layers are copied from a network trained
on one dataset (e.g. A) and then the entire network is trained on the other dataset (e.g. B). This
treatment tests the extent to which the features on layer n are general or specific.

3

Experimental Setup

Since Krizhevsky et al. (2012) won the ImageNet 2012 competition, there has been much interest
and work toward tweaking hyperparameters of large convolutional models. However, in this study
we aim not to maximize absolute performance, but rather to study transfer results on a well-known
architecture. We use the reference implementation provided by Caffe (Jia et al., 2014) so that our
results will be comparable, extensible, and useful to a large number of researchers. Further details of
the training setup (learning rates, etc.) are given in the supplementary material, and code and parameter files to reproduce these experiments are available at http://yosinski.com/transfer.

4

Results and Discussion

We performed three sets of experiments. The main experiment has random A/B splits and is discussed in Section 4.1. Section 4.2 presents an experiment with the man-made/natural split. Section 4.3 describes an experiment with random weights.
4

0.66

Top-1 accuracy (higher is better)

0.64

0.62

0.60

0.58

baseB
selffer BnB
selffer BnB +
transfer AnB
transfer AnB +

0.56

0.54

0.52

0

1

2

3

4

5

6

7

5: Transfer + fine-tuning improves generalization

Top-1 accuracy (higher is better)

0.64

3: Fine-tuning recovers co-adapted interactions
0.62

2: Performance drops
due to fragile
co-adaptation
4: Performance
drops due to
representation
specificity

0.60

0.58

0.56

0.54

0

1

2
3
4
5
Layer n at which network is chopped and retrained

6

7

Figure 2: The results from this paper?s main experiment. Top: Each marker in the figure represents
the average accuracy over the validation set for a trained network. The white circles above n =
0 represent the accuracy of baseB. There are eight points, because we tested on four separate
random A/B splits. Each dark blue dot represents a BnB network. Light blue points represent
BnB+ networks, or fine-tuned versions of BnB. Dark red diamonds are AnB networks, and light
red diamonds are the fine-tuned AnB+ versions. Points are shifted slightly left or right for visual
clarity. Bottom: Lines connecting the means of each treatment. Numbered descriptions above each
line refer to which interpretation from Section 4.1 applies.

4.1

Similar Datasets: Random A/B splits

The results of all A/B transfer learning experiments on randomly split (i.e. similar) datasets are
shown3 in Figure 2. The results yield many different conclusions. In each of the following interpretations, we compare the performance to the base case (white circles and dotted line in Figure 2).
3
AnA networks and BnB networks are statistically equivalent, because in both cases a network is trained
on 500 random classes. To simplify notation we label these BnB networks. Similarly, we have aggregated the
statistically identical BnA and AnB networks and just call them AnB.

5

1. The white baseB circles show that a network trained to classify a random subset of 500 classes
attains a top-1 accuracy of 0.625, or 37.5% error. This error is lower than the 42.5% top-1 error
attained on the 1000-class network. While error might have been higher because the network is
trained on only half of the data, which could lead to more overfitting, the net result is that error is
lower because there are only 500 classes, so there are only half as many ways to make mistakes.
2. The dark blue BnB points show a curious behavior. As expected, performance at layer one is
the same as the baseB points. That is, if we learn eight layers of features, save the first layer of
learned Gabor features and color blobs, reinitialize the whole network, and retrain it toward the
same task, it does just as well. This result also holds true for layer 2. However, layers 3, 4, 5,
and 6, particularly 4 and 5, exhibit worse performance. This performance drop is evidence that
the original network contained fragile co-adapted features on successive layers, that is, features
that interact with each other in a complex or fragile way such that this co-adaptation could not be
relearned by the upper layers alone. Gradient descent was able to find a good solution the first
time, but this was only possible because the layers were jointly trained. By layer 6 performance
is nearly back to the base level, as is layer 7. As we get closer and closer to the final, 500-way
softmax output layer 8, there is less to relearn, and apparently relearning these one or two layers
is simple enough for gradient descent to find a good solution. Alternately, we may say that
there is less co-adaptation of features between layers 6 & 7 and between 7 & 8 than between
previous layers. To our knowledge it has not been previously observed in the literature that such
optimization difficulties may be worse in the middle of a network than near the bottom or top.
3. The light blue BnB+ points show that when the copied, lower-layer features also learn on the
target dataset (which here is the same as the base dataset), performance is similar to the base
case. Such fine-tuning thus prevents the performance drop observed in the BnB networks.
4. The dark red AnB diamonds show the effect we set out to measure in the first place: the transferability of features from one network to another at each layer. Layers one and two transfer almost
perfectly from A to B, giving evidence that, at least for these two tasks, not only are the first-layer
Gabor and color blob features general, but the second layer features are general as well. Layer
three shows a slight drop, and layers 4-7 show a more significant drop in performance. Thanks
to the BnB points, we can tell that this drop is from a combination of two separate effects: the
drop from lost co-adaptation and the drop from features that are less and less general. On layers
3, 4, and 5, the first effect dominates, whereas on layers 6 and 7 the first effect diminishes and
the specificity of representation dominates the drop in performance.
Although examples of successful feature transfer have been reported elsewhere in the literature
(Girshick et al., 2013; Donahue et al., 2013b), to our knowledge these results have been limited
to noticing that transfer from a given layer is much better than the alternative of training strictly
on the target task, i.e. noticing that the AnB points at some layer are much better than training
all layers from scratch. We believe this is the first time that (1) the extent to which transfer is
successful has been carefully quantified layer by layer, and (2) that these two separate effects
have been decoupled, showing that each effect dominates in part of the regime.
5. The light red AnB+ diamonds show a particularly surprising effect: that transferring features
and then fine-tuning them results in networks that generalize better than those trained directly on
the target dataset. Previously, the reason one might want to transfer learned features is to enable
training without overfitting on small target datasets, but this new result suggests that transferring
features will boost generalization performance even if the target dataset is large. Note that this
effect should not be attributed to the longer total training time (450k base iterations + 450k finetuned iterations for AnB+ vs. 450k for baseB), because the BnB+ networks are also trained
for the same longer length of time and do not exhibit this same performance improvement.
Thus, a plausible explanation is that even after 450k iterations of fine-tuning (beginning with
completely random top layers), the effects of having seen the base dataset still linger, boosting
generalization performance. It is surprising that this effect lingers through so much retraining.
This generalization improvement seems not to depend much on how much of the first network
we keep to initialize the second network: keeping anywhere from one to seven layers produces
improved performance, with slightly better performance as we keep more layers. The average
boost across layers 1 to 7 is 1.6% over the base case, and the average if we keep at least five
layers is 2.1%.4 The degree of performance boost is shown in Table 1.
4
We aggregate performance over several layers because each point is computationally expensive to obtain
(9.5 days on a GPU), so at the time of publication we have few data points per layer. The aggregation is

6

Table 1: Performance boost of AnB+ over controls, averaged over different ranges of layers.
layers
aggregated
1-7
3-7
5-7

4.2

mean boost
over
baseB
1.6%
1.8%
2.1%

mean boost
over
selffer BnB+
1.4%
1.4%
1.7%

Dissimilar Datasets: Splitting Man-made and Natural Classes Into Separate Datasets

As mentioned previously, the effectiveness of feature transfer is expected to decline as the base and
target tasks become less similar. We test this hypothesis by comparing transfer performance on
similar datasets (the random A/B splits discussed above) to that on dissimilar datasets, created by
assigning man-made object classes to A and natural object classes to B. This man-made/natural split
creates datasets as dissimilar as possible within the ImageNet dataset.
The upper-left subplot of Figure 3 shows the accuracy of a baseA and baseB network (white circles)
and BnA and AnB networks (orange hexagons). Lines join common target tasks. The upper of the
two lines contains those networks trained toward the target task containing natural categories (baseB
and AnB). These networks perform better than those trained toward the man-made categories, which
may be due to having only 449 classes instead of 551, or simply being an easier task, or both.
4.3

Random Weights

We also compare to random, untrained weights because Jarrett et al. (2009) showed ? quite strikingly ? that the combination of random convolutional filters, rectification, pooling, and local normalization can work almost as well as learned features. They reported this result on relatively small
networks of two or three learned layers and on the smaller Caltech-101 dataset (Fei-Fei et al., 2004).
It is natural to ask whether or not the nearly optimal performance of random filters they report carries
over to a deeper network trained on a larger dataset.
The upper-right subplot of Figure 3 shows the accuracy obtained when using random filters for the
first n layers for various choices of n. Performance falls off quickly in layers 1 and 2, and then
drops to near-chance levels for layers 3+, which suggests that getting random weights to work in
convolutional neural networks may not be as straightforward as it was for the smaller network size
and smaller dataset used by Jarrett et al. (2009). However, the comparison is not straightforward.
Whereas our networks have max pooling and local normalization on layers 1 and 2, just as Jarrett
et al. (2009) did, we use a different nonlinearity (relu(x) instead of abs(tanh(x))), different layer
sizes and number of layers, as well as other differences. Additionally, their experiment only considered two layers of random weights. The hyperparameter and architectural choices of our network
collectively provide one new datapoint, but it may well be possible to tweak layer sizes and random
initialization details to enable much better performance for random weights.5
The bottom subplot of Figure 3 shows the results of the experiments of the previous two sections
after subtracting the performance of their individual base cases. These normalized performances
are plotted across the number of layers n that are either random or were trained on a different,
base dataset. This comparison makes two things apparent. First, the transferability gap when using
frozen features grows more quickly as n increases for dissimilar tasks (hexagons) than similar tasks
(diamonds), with a drop by the final layer for similar tasks of only 8% vs. 25% for dissimilar tasks.
Second, transferring even from a distant task is better than using random filters. One possible reason
this latter result may differ from Jarrett et al. (2009) is because their fully-trained (non-random)
networks were overfitting more on the smaller Caltech-101 dataset than ours on the larger ImageNet
informative, however, because the performance at each layer is based on different random draws of the upper
layer initialization weights. Thus, the fact that layers 5, 6, and 7 result in almost identical performance across
random draws suggests that multiple runs at a given layer would result in similar performance.
5
For example, the training loss of the network with three random layers failed to converge, producing only
chance-level validation performance. Much better convergence may be possible with different hyperparameters.

7

Man-made/Natural split

Top-1 accuracy

0.7

0.5

0.6

0.4

0.5

0.3

0.4

0.2
0.1

0.3
0

Relative top-1 accuracy (higher is better)

Random, untrained filters

0.6

1

2

3

4

5

6

0.0

7

0

1

2

3

4

5

6

7

0.00
?0.05
?0.10
?0.15

reference
mean AnB, random splits
mean AnB, m/n split
random features

?0.20
?0.25
?0.30

0

1

2
3
4
5
Layer n at which network is chopped and retrained

6

7

Figure 3: Performance degradation vs. layer. Top left: Degradation when transferring between dissimilar tasks (from man-made classes of ImageNet to natural classes or vice versa). The upper line
connects networks trained to the ?natural? target task, and the lower line connects those trained toward the ?man-made? target task. Top right: Performance when the first n layers consist of random,
untrained weights. Bottom: The top two plots compared to the random A/B split from Section 4.1
(red diamonds), all normalized by subtracting their base level performance.
dataset, making their random filters perform better by comparison. In the supplementary material,
we provide an extra experiment indicating the extent to which our networks are overfit.

5

Conclusions

We have demonstrated a method for quantifying the transferability of features from each layer of
a neural network, which reveals their generality or specificity. We showed how transferability is
negatively affected by two distinct issues: optimization difficulties related to splitting networks in
the middle of fragilely co-adapted layers and the specialization of higher layer features to the original
task at the expense of performance on the target task. We observed that either of these two issues
may dominate, depending on whether features are transferred from the bottom, middle, or top of
the network. We also quantified how the transferability gap grows as the distance between tasks
increases, particularly when transferring higher layers, but found that even features transferred from
distant tasks are better than random weights. Finally, we found that initializing with transferred
features can improve generalization performance even after substantial fine-tuning on a new task,
which could be a generally useful technique for improving deep neural network performance.

Acknowledgments
The authors would like to thank Kyunghyun Cho and Thomas Fuchs for helpful discussions, Joost
Huizinga, Anh Nguyen, and Roby Velez for editing, as well as funding from the NASA Space
Technology Research Fellowship (JY), DARPA project W911NF-12-1-0449, NSERC, Ubisoft, and
CIFAR (YB is a CIFAR Fellow).
8

References
Bengio, Y. (2011). Deep learning of representations for unsupervised and transfer learning. In JMLR W&CP:
Proc. Unsupervised and Transfer Learning.
Bengio, Y., Bastien, F., Bergeron, A., Boulanger-Lewandowski, N., Breuel, T., Chherawala, Y., Cisse, M., C?ot?e,
M., Erhan, D., Eustache, J., Glorot, X., Muller, X., Pannetier Lebeuf, S., Pascanu, R., Rifai, S., Savard, F.,
and Sicard, G. (2011). Deep learners benefit more from out-of-distribution examples. In JMLR W&CP:
Proc. AISTATS?2011.
Caruana, R. (1995). Learning many related tasks at the same time with backpropagation. pages 657?664,
Cambridge, MA. MIT Press.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09.
Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. (2013a). Decaf:
A deep convolutional activation feature for generic visual recognition. Technical report, arXiv preprint
arXiv:1310.1531.
Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. (2013b). Decaf: A deep
convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531.
Fei-Fei, L., Fergus, R., and Perona, P. (2004). Learning generative visual models from few training examples:
An incremental Bayesian approach tested on 101 object categories. In Conference on Computer Vision and
Pattern Recognition Workshop (CVPR 2004), page 178.
Girshick, R., Donahue, J., Darrell, T., and Malik, J. (2013). Rich feature hierarchies for accurate object detection and semantic segmentation. arXiv preprint arXiv:1311.2524.
Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009). What is the best multi-stage architecture for
object recognition? In Proc. International Conference on Computer Vision (ICCV?09), pages 2146?2153.
IEEE.
Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., and Darrell, T. (2014).
Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093.
Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classification with deep convolutional neural
networks. In Advances in Neural Information Processing Systems 25 (NIPS?2012).
Le, Q. V., Karpenko, A., Ngiam, J., and Ng, A. Y. (2011). ICA with reconstruction cost for efficient overcomplete feature learning. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger, editors,
Advances in Neural Information Processing Systems 24, pages 1017?1025.
Lee, H., Grosse, R., Ranganath, R., and Ng, A. Y. (2009). Convolutional deep belief networks for scalable
unsupervised learning of hierarchical representations. Montreal, Canada.
Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. (2014). Overfeat: Integrated recognition, localization and detection using convolutional networks. In International Conference on Learning
Representations (ICLR 2014). CBLS.
Zeiler, M. D. and Fergus, R. (2013). Visualizing and understanding convolutional networks. Technical Report
Arxiv 1311.2901.

9

"
3086,2009,Exploring Functional Connectivities of the Human Brain using Multivariate Information Analysis,"In this study, we present a method for estimating the mutual information for a localized pattern of fMRI data. We show that taking a multivariate information approach to voxel selection leads to a decoding accuracy that surpasses an univariate inforamtion approach and other standard voxel selection methods. Furthermore,we extend the multivariate mutual information theory to measure the functional connectivity between distributed brain regions. By jointly estimating the information shared by two sets of voxels we can reliably map out the connectivities in the human brain during experiment conditions. We validated our approach on a 6-way scene categorization fMRI experiment. The multivariate information analysis is able to ?nd strong information ?ow between PPA and RSC, which con?rms existing neuroscience studies on scenes. Furthermore, by exploring over the whole brain, our method identifies other interesting ROIs that share information with the PPA, RSC scene network,suggesting interesting future work for neuroscientists.","Exploring Functional Connectivity of the Human
Brain using Multivariate Information Analysis
Barry Chai1?
Dirk B. Walther2?
Diane M. Beck2,3?
Li Fei-Fei1?
1
Computer Science Department, Stanford University, Stanford, CA 94305
2
Beckman Institute, University of Illinois at Urbana-Champaign, Urbana, IL 61801
3
Psychology Department, University of Illinois at Urbana-Champaign, Champaign, IL 61820
{bwchai,feifeili}@cs.stanford.edu {walther,dmbeck}@illinois.edu

Abstract
In this study, we present a new method for establishing fMRI pattern-based
functional connectivity between brain regions by estimating their multivariate
mutual information. Recent advances in the numerical approximation of highdimensional probability distributions allow us to successfully estimate mutual
information from scarce fMRI data. We also show that selecting voxels based
on the multivariate mutual information of local activity patterns with respect to
ground truth labels leads to higher decoding accuracy than established voxel selection methods. We validate our approach with a 6-way scene categorization fMRI
experiment. Multivariate information analysis is able to find strong information
sharing between PPA and RSC, consistent with existing neuroscience studies on
scenes. Furthermore, an exploratory whole-brain analysis uncovered other brain
regions that share information with the PPA-RSC scene network.

1 Introduction
To understand how the brain represents and processes information we must account for two complementary properties: information is represented in a distributed fashion, and brain regions are
strongly interconnected. Although heralded as a tool to address these issues, functional magnetic
resonance imaging (fMRI) initially fell short of achieving these goals because of limitations of traditional analysis methods, which treat voxels as independent. Multi-voxel pattern analysis (MVPA)
has revolutionized fMRI analysis by accounting for distributed patterns of activity rather than absolute activation levels. The analysis of functional connectivity, however, is so far mostly limited to
comparing the time courses of individual voxels. To overcome these limitations we demonstrate a
new method of pattern-based functional connectivity analysis based on mutual information of sets
of voxels. Furthermore, we show that selecting voxels based on the mutual information of local
activity with respect to ground truth outperforms other voxel selection methods.
We apply our new analysis methods to the decoding of natural scene categories from the human
brain. Human observers are able to quickly and efficiently perceive the content of natural scenes [15,
26]. It was recently shown by [23] that activity patterns in the parahippocampal place area (PPA), the
retrosplenial cortex (RSC), the lateral occipital complex (LOC), and, to some degree, primary visual
cortex (V1) contain information about the categories of natural scenes. To truly understand how
the brain categorizes natural scenes, however, it is necessary to grasp the interactions between these
regions of interest (ROIs). Our new technique for pattern-based functional connectivity enables
us to uncover shared scene category-specific information among the ROIs. When configured for
exploratory whole-brain analysis, the technique allows us to discover other brain regions that may
be involved in natural scene categorization.
Mutual information is appropriate for fMRI analysis if one considers fMRI data as a noisy communication channel in the sense of Shannon?s information theory [19]; the information contained
?
?

Barry Chai and Dirk B. Walther contributed equally to this work.
Diane M. Beck and Li Fei-Fei contributed equally to this work.

1

in a population of neurons must be communicated through hemodynamic changes and concomitant
changes in magnetization which can be measured as the blood-oxygen level dependent (BOLD)
fMRI signal, then proceed through several layers of data processing, culminating in a single time
varying value in a particular voxel. While this noisy communication concept has been embraced
by the brain machine interface community [25], information theory has, thus far, been less utilized
in the fMRI analysis community (see [8] for exceptions). This may be partly due to the numerical
difficulties in estimating the probability distributions necessary for computing mutual information.
This problem is exacerbated when patterns of voxels are considered. In this case distributions of
higher dimensionality need to be estimated from preciously few data points. Recent developments
in information theory, however, help us overcome these hurdles.
In Section 2 we review these theoretical advances and adapt them for our dual purpose of voxel
selection and pattern-based functional connectivity analysis. Following a discussion of related work
in Section 3, in Section 4 we apply our new methods to fMRI data from an experiment on distinguishing natural scene categories in the human brain. We lay conclude the paper in Section 5.

2

Multivariate mutual information for fMRI data

Information theory was originally formulated for discrete variables. In order to adapt the theory to
continuous random variables, the underlying probability distribution needs to be estimated from the
sampled data points. Previous work such as [7, 18] have used fixed bin-size histogram or Parzen
window methods for this purpose. However, these methods do not generalize to high-dimensional
data. Recently, Perez-Cruz has shown that a k-nearest-neighbor (kNN) approach to estimating information theoretic measures converges to the true information theoretic measures asymptotically with
finite k, even in higher dimensional spaces [16]. In this section we adapt this strategy to estimate
multi-voxel mutual information.
2.1

Nearest-neighbor mutual information estimate

In information theory, the randomness of a probability distribution is measured by its entropy. For a
discrete random variable x, entropy can be calculated as
n
X
H(x) = ?
p(xi ) log p(xi ).
(1)
i=1

Mutual information is intuitively defined as the reduction of the entropy of the random variable x by
the entropy of x after y is known:
I(x, y) = H(x) ? H(x|y).

(2)

The separation into entropies allow us to calculate mutual information for multivariate data. Random
variables x, y can be of arbitrary dimensions.
As shown in [24], using kNN estimation, entropies and conditional entropies can be defined as
n

H(x) = ?

1X
log pk (xi ),
n i=1

(3)

n

H(x|y) = ?

1X
pk (xi , yi )
log
,
n i=1
pk (yi )

(4)

where the summation is over n data points, each represented by xi . pk (xi ) is the kNN density
estimated at xi . pk (xi ) is defined as
pk (xi ) =

k ?(d/2 + 1)
1
.
d/2
n?1
rk (xi )d
?

(5)

where ? is the gamma function, d is the dimensionality of xi and rk (xi ) is the Euclidean distance
from xi to the k th nearest training point. pk (xi ) is the probability density function at xi , which is a
set of voxel values for a given category task(or label) in the context of our fMRI experiment.
2

2.2

fMRI multivariate information analysis

In previous work, such as [7], information theory has been used as a measure for functional connectivity of one voxel to another voxel. While such analysis is valuable for exploring connections in the
brain, it does not fully leverage the information stored in the local pattern of voxels. In this section
we propose a framework for multivariate information analysis of fMRI data for dual purposes: voxel
selection and functional connectivity.
2.2.1 Voxel selection based on mutual information with respect to ground truth label
For voxel selection we are interested in finding a subset of voxels that are highly informative for
discriminating between the ground truth labels in the experiment. This is a useful step that serves two
purposes. From a machine learning perspective, reducing the dimensionality of the brain image data
can boost classifier performance and reduce classifier variance. From a neuroscience perspective,
the locations of highly informative voxels identify functional regions involved in the experiment. To
achieve both of these goals we use a multivariate mutual information measure to analyze a localized
pattern of M voxels. This local analysis windows is moved across the brain image. At each location
we estimate the mutual information shared between the pattern of M voxels and the experiment label.
In our experiments we choose M = 7 to evaluate the smallest symmetrical pattern around a center
voxel, which consists of the center voxel and its 6 face-connected neighbors. Mutual information
between voxels V and labels L is defined as
I(V, L) = H(V ) + H(L) ? H(V, L).

(6)

Using equation 1 the entropies can be calculated by
n

I(V, L) = ?

n

n

1X
1X
1X
log pk (Vi ) ?
log pk (Li ) +
log pk (Vi , Li ),
n i=1
n i=1
n i=1

(7)

where n is the number of data-points observed, Li is the experiment label for ith data point, Vi is a
7-dimensional random variable, Vi = (vi1 ,vi2 ,vi3 ,vi4 ,vi5 ,vi6 , vi7 ) with each entry corresponding to
one of 7 voxels? values at data point i. Equation 7 can be used to compute the mutual information
of localized set of voxels Vi with respect to their ground truth label Li . We can then perform voxel
selection by selecting the locations of highest mutual information. This is useful as a preprocessing
step before applying any machine learning algorithms and as well as a way to spatially map out the
informative voxels with respect to the task.
2.2.2 Functional connectivity by shared information between distributed voxel patterns
Two distributed brain regions can be modeled as a communication channel. Measuring the mutual
information across the two regions provides an intuitive measure for their functional connectivity.
The voxel values observed in each region can be regarded as observed data from an underlying probability distribution ? the distribution that characterizes the functional region under the experiment
condition.
Previous approaches have analyzed shared information in a univariate way, computing the mutual
information between two voxels. However such univariate information analysis disregards the information stored in the local patterns of voxels. In this work we present a multivariate information
analysis that estimates shared information between two sets of voxels that leverages the information
stored in the local patterns:
I(V, S|L) = H(V |L) + H(S|L) ? H(V, S|L),

(8)

where V and S are random variables for sets of 7 voxels. L is the experiment label. Using equations
3 and 4 this can be written as
n

n

n

pk (Vi , Li )
1X
pk (Si , Li )
1X
pk (Vi , Si , Li )
1X
log
?
log
+
log
.
I(V, S|L) = ?
n i=1
pk (Li )
n i=1
pk (Li )
n i=1
pk (Li )

(9)

Equation 9 allows us to measure the functional connectivity between two distributed sets of voxels
V and S by computing the mutual information between the two sets of voxels conditioned on the
experiment task label L. We show in our experiments (sec 4.4) that by using this measurement, our
algorithm can uncover meaningful functional connectivity
patterns among regions of the brain.
3

Decoding accuracy

0.5

?

0.4

0.3

1
6
Mut. Info. (7 vox.)
Mut. Info. (1 vox.)
most active

0.1

0?

0

200

400

600

800

1000

most discr.
random
chance level

1200

1400

Number of voxels
Figure 1: Comparison of decoding accuracy3 between MI voxel selection and other standard voxel selection
methods(refer to section 4.2). The single voxel MI approach surpasses most discr.1 voxel selection but performs
on par with most active2 voxel selection. Using a pattern of 7 voxels, the MI7D approach achieves the highest
decoding accuracy. At 600 voxels, MI7D decoding accuracy3 is significantly higher than most active with pvalue < 0.05. At 1250 voxels, MI7D decoding accuracy is significantly higher than MI1D with p-value < 0.01
(This figure must be viewed in color)

3

Related work

Statistical relationships between different parts of the brain, referred to as functional connectivity,
have been computed with a number of different methods. The methods can be broadly classified as
either data-driven or model-based [10].
In data-driven approaches, no specific hypothesis of connectivity is used, but large networks of
brain regions are discovered based purely on the data. Most commonly, this is achieved with a
dimensionality-reduction procedure such as principal component analysis (PCA) or independent
component analysis (ICA). Originally applied to the analysis of PET data [5], PCA has also been
applied to fMRI data (see [12]). ICA has been gained interest for the investigation of the so-called
default network in the brain at rest [11].
Model-based approaches test a prior hypothesis about the statistical relations between a seed voxel
and a target voxel. By fixing the seed voxel and moving the target voxel all over the brain, a
connectivity map with respect to the seed voxel can be generated. The statistical relation of the
two voxels is usually modeled assuming temporal dependence between voxels in methods such as:
cross-correlation [2], coherence [21], Granger causality [1], or transfer entropy [20].
These methods compare the time courses of individual voxels. Following the same principal idea,
we model functional connectivity based on the mutual information between sets of seed and target
voxels to leverage the spatial information contained in activity patterns among voxels rather than
the temporal information between two voxels. fMRI has a higher spatial resolution than temporal resolution. We design our mutual information connectivity measure to exploit this property of
fMRI data. Yao et al. [26] have also explored pattern-based functional connectivity by modeling
the interactions between distributed sets of voxels with a generative model. We take a simpler approach by using only the multivariate information measure which allows us to explore for unknown
connections in the whole brain in a searchlight manner.
In recent years it has become apparent that patterns of fMRI activity hold more detailed information
about experimental conditions than the activation levels of individual voxels [6]. It is therefore
1

Most discri. ? Most discriminative voxels are those showing the largest difference in activity between any
pair of scene categories.
2
Most active ? Most active voxels are those showing the largest difference in activity between the fixation
condition and viewing images of any category.

4

0.17
R

L

MI

0
Figure 2: Locations of voxels with high 7D mutual information with respect to scene category label. The
known functional areas that respond to scenes and visual stimuli such as PPA, RSC, V1 are all selected, which
also explains the high decoding accuracy using the selected voxels. The brain maps shown above are based on
group analysis over 5 subjects superimposed on an MNI standard brain. (This figure must be viewed in color)

cogent to also consider the information contained in voxel patterns for the analysis of functional
connectivity. We achieve this by computing the mutual information of a pattern of locally connected
voxels at the seed location with a pattern at the target location. As with the univariate functional
connectivity analysis, this multivariate version also allows us to test hypotheses about connectivity
of brain regions as well as generate connectivity maps.
Because of the large number of voxels in the brain (many thousands, depending on resolution),
multivariate techniques usually require some kind of feature selection or dimensionality reduction.
This can be achieved by focusing on pre-defined ROIs, or by selecting voxels form the brain based
on some statistical criteria [3, 14]. Here we show that using mutual information of individual voxels
with respect to ground truth for voxel selection works at the same level as these previous methods,
but that mutual information of patterns of voxels with respect to ground truth outperforms all of the
univariate methods we tested.
Information theory has been applied to fMRI data in the context of brain machine interfaces [25],
to generate activation maps [8], for effective connectivity in patients [7], and for image registration
[17]. However, to our knowledge this is the first application to both voxel selection and functional
connectivity based on multivariate activity patterns.

4

Experiments

4.1 Data
For the experiments described in this section we use the data from the fMRI experiment on natural
scene categories by [23]. Briefly, five participants passively viewed color images belonging to six
categories of natural scenes (beaches, buildings, forests, highways, industry, and mountains). Stimuli were arranged into blocks of 10 images from the same natural scene category. Each image was
displayed sequentially for 1.6 seconds. A run was composed of 6 blocks, one for each natural scene
category, interleaved with 12 s fixation periods. Images were presented upright inverted on alternating runs, with each inverted run preserving the image and category order used in the preceding
upright run. A session contained 12 such runs, and the order of categories was randomized across
blocks. Each subject performed two blocks with a total of 24 runs. In total we have 1192 data
points per subject across all 6 categories. The data obtained from the authors in [23] contains only
localizers for V1, PPA, RSC, LOC, FFA areas. Thus we limit our seed areas to these ROIs.
4.2 Voxel selection
The goal of voxel selection is to identify the most relevant voxels for the experiment task out of the
tens of thousands of voxels in the entire brain. A quantitative evaluation of voxel selection is the
decoding accuracy3 of the selected voxels, which measures how well can the selected voxels predict
the viewing condition from the neural responses.
Fig.1 compares our mutual information-based voxel selection method to other voxel selection methods. Decoding accuracy3 using univariate kNN mutual information is comparable to most active1
5

MI 7D
MI 1D

3
Log 2 (6)

**
**

2

**

0.95**

rPPA

lPPA

**

MI

1.1**

1.5

0.84**

0.85**
0.75*

1

0.71**

0.5

lRSC

rRSC
1.16**

0

?

lPPA

rPPA

lRSC

V1

rRSC

(a) Comparing 7D and 1D mutual information within-ROI
connections

(b) 7D MI between-ROI connections

Figure 3: a) Within-ROI MI values for 7D and 1D mutual information, b) Schematic showing the significant
ROI connections found using 7D mutual information analysis. The network shows strong connections between
PPA and RSC, both ipsilaterally and contralaterally. ?? p < 10?6 , ? p < 0.01

voxel selection. Multivariate information measure is able to select the more informative voxels by
considering a local pattern of voxels jointly, leading to a boost in decoding accuracy3 .
To further understand why multivariate mutual information boosts the decoding accuracy3 , we can
look at the spatial locations of the informative voxels selected by multivariate information analysis
shown in Fig.2. The most informative voxels selected correspond to known functional regions for
scenes. In this figure we see the scene areas V1, RSC, PPA, LOC that were also identified in [23].
Interestingly, our automatic voxel selection achieves a higher decoding accuracy3 than the ROIs
selected by localizer in [23]. This may suggest that the multivariate information voxel selection is a
better segmentation of the relevant ROIs than the localizer runs.
4.3

Functional connectivity of ROIs

In the previous section, we have shown that multivariate information can effectively select informative voxels for classification. In this section, we first illustrate the increased sensitivity of a
multivariate assessment of functional connectivity within known ROIs. Then we use multivariate
information to explore connections between ROIs.
A good comparison for the functional connectivity measure is the within-ROI connectivity. Voxels
within the same ROI should exhibit high functional connectivity with each other. In Fig.3a we compared our 7D measures with equivalent one dimensional measures using within-ROI connectivity.
To this end we randomly selected 15 seed and 15 target locations within each ROI, making sure that
seed and target patterns have no voxels in common. Then we computed mutual information between
all seed and all target locations, either using individual voxels (1D case) or patterns of seven voxels
(7D case). Fig.3a shows the mean of the mutual information values for these two cases in each ROI.
In all ROIs, we find that multivariate information measure(7D) is significantly higher than the univariate measure(1D), suggesting that a pattern-based mutual information has a higher fidelity than
univariate-based mutual information in mapping out functional connections.
After having established that 7D mutual information significantly outperforms 1D mutual information we proceed to calculate the between-ROI connectivity for scene areas V1, left/right PPA, and
left/right RSC using 7D mutual information as shown in Fig.3b. Between-ROI connectivity is de3
Decoding accuracy is obtained with a leave-two-runs-out cross-validation on the our scene data. In each
fold two runs from viewing the same images upright and inverted are left out as test data. Voxel selection is
performed on the training runs using k = n/2, where n is the number of training examples in each category.
Using selected voxels, a linear SVM classifier is trained on the upright runs with C = 0.02 as in [23]. In testing
we use majority voting on the SVM prediction labels to vote for the most likely scene label for each block of
data. Decoding accuracy is the average of cross-validation accuracy over the 5 subjects.

6

right Inferior Frontal Gyrus

R

left Medial Frontal Gyrus

rRSC

left Precuneus

0.51

MI

L
lPPA
rPPA
lPPA

rPPA
Z = -13 mm

0.37

lRSC

rRSC
Z = 23 mm

Y = 52 mm

Figure 4: Connectivity map seeding from left PPA. Talairach coordinates defined in [22] are shown as the Z
and Y coordinates for axial and coronal slices respectively. The intensity of the maps shows the MI values(This
figure must be viewed in color)
left Medial Frontal Gyrus

left Medial Frontal Gyrus

left Cuneus

- 4 overlap

R

- 3 overlap

L

- 2 ovlerap

rPPA

lPPA
Y = 43 mm

rRSC

lRSC

left Cuneus
right Cuneus
right Precuneus

Z = 26 mm

Z = 29 mm

Figure 5: Overlap analysis showing areas where overlap occurs with the strongest connections from more than
two scene network ROIs. Talairach coordinates defined in [22] are shown as the Z and Y coordinates for axial
and coronal slices respectively. The color code indicates the amount of overlap.(This figure must be viewed in
color)

fined similarly as within-ROI connectivity except that seed and target locations are the chosen in
different ROIs.
A number of aspects of the connections mapped out with MI7D analysis agree with neuroscience
findings. First, it is expected that PPA and RSC should be strongly connected as part of a scene
network. Moreover, since V1 is the input to the cortical visual system, it is also likely that it should
share information with at least one member of the scene network, which in this case was the right
PPA. One novel finding from this analysis is that all of the strongest connections we discovered
included right RSC. In particular, right RSC shares strong connections with left RSC, right PPA and
left PPA, suggesting that right RSC may play a particularly important role in distinguishing natural
scene categories. More work will be needed to verify this hypothesis.
To summarize, we have verified that multivariate information analysis can reliably map out connections within and between ROIs known to be involved in processing natural scene categories. In
the next section we show how the same analysis can be extended to uncover other ROIs that share
information with this scene network in our scene classification experiment.
4.4

Functional connectivity - whole brain analysis

While it is valuable to confirm existing hypotheses about areas that represent scene categories, it
is also interesting to uncover new brain areas that might be related to scene categorization. In this
section, we show that we can use our multivariate information analysis approach to explore other
areas outside of the known ROIs that form strong connections with the known ROIs.
For each of the functional areas in the scene network, we can explore other areas connected to it.
As in section 3 we measure functional connectivity as multivariate mutual information between the
seed and candidate target areas. We fix the seed area to an ROI defined by a localizer. The candidate
area moves around the brain, at each location measuring the mutual information with respect to the
seed area.
7

4.4.1 Confirming known connections
Fig.4 shows an example of the connectivity map seeding from left PPA. Each highlighted location in
the connectivity map shows its connectivity to left PPA as measured by the multivariate information.
As shown in Fig.4, both left and right PPA are highlighted, confirming their bilateral connection.
Furthermore, we see strong connections between left PPA and left and right RSC. A minimum
cluster size of 13 is used to threshold the connectivity map. The minimum cluster size is determined
by AlphaSim in AFNI [4]. Notice in Fig.4 that the highest MI in the whole-brain analysis has MI of
0.51 whereas the within-ROI MI of left PPA in Fig.3b has a value of 1.5. The decrease in MI is due
to the smoothing of connectivity maps when we combine them across subjects.
4.4.2 Discovering new connections
Besides confirming known regions of the scene network, our connectivity maps allow us to explore
other brain areas that might be related to the scene network. In Fig.4 we not only observe known
scene network ROIs but additional areas such as the right Inferior Frontal Gyrus, left Medial Frontal
Gyrus, and left Precuneus. Interestingly, the Inferior Frontal Gyrus, typically associated with language processing [13], also showed up in a searchlight analysis for decoding accuracy in [23].
So far we have examined how the rest of the brain connects to one ROI in the scene network,
specifically we used left PPA as the example. However, to further strongly establish which regions
are functionally connected in regards to distinguishing scene category, we asked which brain areas
are strongly connected to two or more of the scene network ROIs. Areas that connect to more than
one of the scene network ROIs are particularly interesting, because having multiple connections
strengthens evidence that they play a significant role in distinguishing scene categories.
To investigate this question, we generate one connectivity map for each of the 4 scene network ROIs,
similar to Fig.4. We take the areas with the top 5 percent highest mutual information in each of the
4 maps and overlap them. Fig.5 shows this overlap analysis.
Similar to the previous analysis, the overlap analysis highlights all 4 known areas of the scene
network. Interestingly, this analysis shows that right RSC and right PPA are connected with more
regions of the scene network than left RSC and PPA. This suggests that perhaps there is a laterality
effect in the scene network that could be investigated in future studies.
Furthermore, we can also explore areas outside of the scene network with the overlap analysis.
In Fig.5, left/right Cuneus and right Precuneus, highlighted in orange, exhibit strong connections
with 3/4 of the scene network ROIs. Left Medial Frontal Gyrus is strongly connected to 2/4 of the
scene network ROIs. These exploratory areas also point to interesting future investigations for scene
category studies.

5

Conclusion

In this paper we have introduced a new method for evaluating the mutual information that patterns
of fMRI voxels share with the ground truth labels of the experiment and with patterns of voxels
elsewhere in the brain. When used as a voxel selection method for subsequent decoding of viewed
natural scene category, mutual information of patterns of voxels with respect to the ground truth
label is superior to mutual information of individual voxels.
We have shown that mutual information of voxel patterns in two ROIs is a more sensitive measure
of task-specific functional connectivity analysis than mutual information of individual voxels. We
have identified a network of regions consisting of left and right PPA and left and right RSC that
share information about the category of a natural scene viewed by the subject. Connectivity maps
generated with this method have identified left medial frontal gyrus, left/right cuenus, and right
precuneus as sharing scene-specific information with PPA and RSC. This could stimulate interesting
future work such as estimating mutual information for an even larger set of voxels and understanding
the exploratory areas highlighted by this analysis. Although we confined our experiments to data
from a scene category task, all the analysis proposed here could be used for other tasks in other
domains.
Acknoledgements
This work is funded by National Institutes of Health Grant 1 R01 EY019429 (to L.F.-F., D.M.B., D.B.W.), a
Beckman Postdoctoral Fellowship (to D.B.W.), a Microsoft Research New Faculty Fellowship (to L.F.-F.), and
the Frank Moss Gift Fund (to L.F-F.). The authors would like to thank Todd Coleman and Fernando Perez-Cruz
for the helpful discussions on entropy estimation.

8

References
[1] Granger, C. W. J. Investigating causal relations by econometric models and cross-spectral methods.
Econometrica 37, 424-438, 1969
[2] J. Cao and K. Worsley The geometry of correlation fields with an application to functional connectivity
of the brain. Ann. Appl. Probab, 9:1021C1057, 1998.
[3] D. Cox and R. Savoy Functional magnetic resonance imaging (fMRI) ?brain reading?: Detecting and
classifying distributed patterns of fMRI activity in human visual cortex. NeuroImage, 19(2):261C270,
2003.
[4] RW Cox. AFNI: Software for analysis and visualization of functional magnetic resonance neuroimages.
Computers and Biomedical Research, 29:162-173, 1996.
[5] K. J. Friston, C. D. Frith, P. F. Liddle, and R. S. Frackowiak. Functional connectivity: the principalcomponent analysis of large (PET) data sets. J Cereb Blood Flow Metab, 13(1):5C14, January 1993.
[6] J. V. Haxby, M. I. Gobbini, M. L. Furey, A. Ishai, J. L. Schouten, and P. Pietrini. Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science, 293(5539):2425C30, 2001.
Journal Article United States.
[7] H. Hinrichs, H. Heinze, and M. Schoenfeld. Causal visual interactions as revealed by an information
theoretic measure and fMRI. NeuroImage, 31(3):1051 C 1060, 2006.
[8] A. T. John, J. W. F. Iii, W. M. W. Iii, J. Kim, and A. S.?Willsky. Analysis of functional MRI data using
mutual information, 1999.
[9] Fei-Fei, L., Iyer, A., Koch, C., Perona, P. (2007). What do we perceive in a glance of a real-world scene?
Journal of Vision,7(1):10, 1-29, http://journalofvision.org/7/1/10/,doi:10.1167/7.1.10.
[10] K. Li, L. Guo, J. Nie, G. Li, and T. Liu. Review of methods for functional brain connectivity detection
using fMRI. Computerized medical imaging and graphics: The Official Journal of the Computerized
Medical Imaging Society, 33(2):131C139, March 2009.
[11] M. J. Mckeown, S. Makeig, G. G. Brown, T.-P. Jung, S. S. Kindermann, R. S. Kindermann, A. J. Bell,
and T. J. Sejnowski. Analysis of fMRI data by blind separation into independent spatial components.
Human Brain Mapping, 6:160C188, 1998.
[12] A. Meyer-Baese, A. Wismueller, and O. Lange. Comparison of two exploratory data analysis methods
for fMRI: unsupervised clustering versus independent component analysis. Information Technology in
Biomedicine, IEEE Transactions on, 8(3):387C398, Sept. 2004.
[13] Geschwind N. (1970) The organization of language and the brain. Science 170:940 944..
[14] D. Neill, A. Moore, F. Pereira, and T. Mitchell. Detecting significant multidimensional spatial clusters.
In Proceedings of Neural Information Processing Systems, 2004.
[15] M. Potter. Short-term conceptual memory for pictures. Journal of Experimental Psychology: Human
Learning and Memory, 2(5):509C522, 1976.
[16] F. Perez-Cruz. Estimation of information theoretic measures for continuous random variables. In D.
Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, NIPS, pages 1257C1264. MIT Press, 2008.
[17] Pluim, J. P. W. and Maintz, J. B. A. and Viergever, M. A. Mutual-information based registration of
medical images: a survey. IEEE Trans Med Imaging, 2003, 22:986C1004.
[18] X. U. Rui, C. H. E. N. Yen-Wei, T. A. N. G. Song-Yuan, S. Morikawa, and Y. Kurumi. Parzen-window
based normalized mutual information for medical image registration. IEICE Transactions on Information and Systems, E91-D(1):132C144, January 2008.
[19] C. E. Shannon. A Mathematical Theory of Communication. CSLI Publications, 1948.
[20] T. Schreiber. Measuring Information Transfer. Physical Review Letters, vol. 85, no. 2, pp. 461+, July
2000.
[21] F. T. Sun, L. M. Miller, and M. DEsposito. Measuring interregional functional connectivity using coherence and partial coherence analyses of fmri data. NeuroImage, 21(2):647 C 658, 2004.
[22] J. Talairach and P. Tournoux. Co-planar Stereotaxic Atlas of the Human Brain: 3-Dimensional Proportional System - an Approach to Cerebral Imaging. Thieme Medical Publishers, New York, 1988
[23] D. Walther, E. Caddigan, L. Fei-Fei*, and D. Beck* (2009), Natural scene categories revealed in
distributed patterns of activity in the human brain. The Journal of Neuroscience, 29(34):1057310581.(*indicates equal contribution)
[24] Q. Wang, S. Kulkarni, and S. Verdu. Divergence estimation of continuous distributions based on datadependent partitions. Information Theory, IEEE Transactions on, 51(9):3064C3074, Sept. 2005.
[25] B. D. Ward and Y. Mazaheri. Information transfer rate in fMRI experiments measured using mutual
information theory. Journal of Neuroscience Methods, 167(1):22 C 30, 2008. Brain-Computer Interfaces
(BCIs).
[26] B. Yao, D.B. Walther, D.M. Beck*, L. Fei-Fei*. Hierarchical Mixture of Classification Experts Uncovers
Interactions between Brain Regions. NIPS, 2009. (* indicates equal contribution)

9

"
1404,2002,"A Maximum Entropy Approach to Collaborative Filtering in Dynamic, Sparse, High-Dimensional Domains",Abstract Missing,"A Maximum Entropy Approach To
Collaborative Filtering in Dynamic, Sparse,
High-Dimensional Domains

David M. Pennock
Overture Services, Inc.
74 N. Pasadena Ave., 3rd floor
Pasadena, CA 91103,
david.pennock@overture.com

Dmitry Y. Pavlov
NEC Laboratories America
4 Independence Way
Princeton, NJ 08540,
dpavlov@nec-labs.com

Abstract
We develop a maximum entropy (maxent) approach to generating recommendations in the context of a user?s current navigation stream, suitable
for environments where data is sparse, high-dimensional, and dynamic?
conditions typical of many recommendation applications. We address
sparsity and dimensionality reduction by first clustering items based on
user access patterns so as to attempt to minimize the apriori probability that recommendations will cross cluster boundaries and then recommending only within clusters. We address the inherent dynamic nature
of the problem by explicitly modeling the data as a time series; we show
how this representational expressivity fits naturally into a maxent framework. We conduct experiments on data from ResearchIndex, a popular online repository of over 470,000 computer science documents. We
show that our maxent formulation outperforms several competing algorithms in offline tests simulating the recommendation of documents to
ResearchIndex users.

1 Introduction
Recommender systems attempt to automate the process of ?word of mouth? recommendations within a community. Typical application environments are dynamic in many respects:
users come and go, users preferences and goals change, items are added and removed, and
user navigation itself is a dynamic process. Recommendation domains are also often high
dimensional and sparse, with tens or hundreds of thousands of items, among which very
few are known to any particular user.
Consider, for instance, the problem of generating recommendations within ResearchIndex
(a.k.a., CiteSeer),1 an online digital library of computer science papers, receiving thousands
of user accesses per hour. The site automatically locates computer science papers found on
the Web, indexes their full text, allows browsing via the literature citation graph, and isolates the text around citations, among other services [8]. The archive contains over 470,000
1

http://www.researchindex.com

documents including the full text of each document, citation links between documents,
and a wealth of user access data. With so many documents, and only seven accesses per
user on average, the user-document data matrix is exceedingly sparse and thus challenging
to model. In this paper, we work with the ResearchIndex data, since it is an interesting
application domain, and is typical of many recommendation application areas [14].
There are two conceptually different ways of making recommendations. A content filtering
approach is to recommend solely based on the features of a document (e.g., showing documents written by the same author(s), or textually similar documents to ). These methods
have been shown to be good predictors [3]. Another possibility is to perform collaborative
filtering [13] by assessing the similarities between the documents requested by the current
user and the users who interacted with ResearchIndex in the past. Once the users with
browsing histories similar to that of a given user are identified, an assumption is made that
the future browsing patterns will be similar as well, and the prediction is made accordingly.
Common measures of similarity between users include Pearson correlation coefficient [13],
mean squared error [16], and vector similarity [1]. More recent work includes application
of statistical machine learning techniques, such as Bayesian networks [1], dependency networks [6], singular value decomposition [14] and latent class models [7, 12]. Most of these
recommendation algorithms are context and order independent: that is, the rank of recommendations does not depend on the context of the user?s current navigation or on recency
effects (past viewed items receive as much weight as recently viewed items).
Currently, ResearchIndex mostly employs fairly simple content-based recommenders. Our
objective was to design a superior (or at least complementary) model-based recommendation algorithm that (1) is tuned for a particular user at hand, and (2) takes into account the
identity of the currently-viewed document , so as not the lead the user too far astray from
his or her current search goal.
To overcome the sparsity and high dimensionality of the data, we cluster the documents
with an objective of maximizing the likelihood that recommendable items co-occur in the
same cluster. By marrying the clustering technique with the end goal of recommendation,
our approach appears to do a good job at maintaining high recall (sensitivity). Similar ideas
in the context of maxent were proposed recently by Goodman in [5].
We explicitly model time: each user is associated with a set of sessions, and each session
is modeled as a time sequence of document accesses. We present a maxent model that
effectively estimates the probability of the next visited document ID (DID) given the most
recently visited DID (?bigrams?) and past indicative DIDs (?triggers?). To our knowledge, this is the first application of maxent for collaborative filtering, and one of the few
published formulations that makes accurate recommendations in the context of a dynamic
user session [3, 15]. We perform offline empirical tests of our recommender and compare it
to competing models. The comparison shows our method is quite accurate, outperforming
several other less-expressive models.
The rest of the paper is organized as follows. In Section 2, we describe the log data from
ResearchIndex and how we preprocessed it. Section 3 presents the greedy algorithm for
clustering the documents and discusses how the clustering helps to decompose the original
prediction task. In Section 4, we give a high-level description of our maxent model and the
features we used for its learning. Experimental results and comparisons with other models
are discussed in Section 5. In Section 6, we draw conclusions and describe directions for
future work.

2 Preprocessing the ResearchIndex data
Each document indexed in ResearchIndex is assigned a unique document ID (DID). Whenever a user accesses the site with a cookie-enabled browser, (s)he is identified as a new or returning user and all activity is recorded on the server side with a unique user ID (UID) and a
time stamp (TID). We obtained a log file that recorded approximately 3 month worth of ResearchIndex data that can roughly be viewed as a series of requests
.



     
	

In the first processing step, we aggregated the requests by the
and broke them into
sessions. For a fixed UID, a session is defined as a sequence of document requests, with
no two consecutive requests more than seconds apart. In our experiments we chose
, so that if a user was inactive for more than 300 seconds, his next request was
considered to mark a start of a new session.






The next processing step included heuristics, such as identifying and discarding the sessions belonging to robots (they obviously contaminate the browsing patterns of human
users), collapsing all same consecutive DID accesses into a single instance of this DID (our
objective was to predict what interests the user beyond the currently requested document),
getting rid of all DIDs that occurred less than two times in the log (for two or fewer occurrences, it is hard to reliably train the system to predict them and evaluate performance),
and finally discarding sessions containing only one document.

3 Dimensionality Reduction Via Clustering
Even after the log is processed, the data still remains high-dimensional (62,240 documents),
and sparse, and hence still hard to model. To solve these problems we clustered the documents. Since our objective was to predict the instantaneous user interests, among many
possibilities of performing the clustering we chose to cluster based on user navigation patterns.

 





We scanned the processed log once and for each document
accumulated the number
was requested immediately after
; in other words, we
of times the document
computed the first-order Markov statistics or bigrams. Based on the user navigation patterns
encoded in bigrams, the greedy clustering is done as shown in the following pseudocode:

! "" $#&% ; Number of Clusters ' ;
( ' Clusters.
)+*,-
).-/1032546=# /1798;: <3! "" $#&% =#%
// max number of transitions
"" such that ! "" # >?) do
// all docs with n transitions
@;""A /CBDB""=21)+% EDFG>IH>J and A /5BDB""=21)+EDFK>LH>J and )+*MN'>O
(P )+* % A QSRTB3UV@;#""WO ;
(P )+* A QSRTB3UV@ O ;
""A /CB3B""=25)+E3FK # A /CBDB""X25)+EDFK-) * ;
// new cluster for i and j
)+*.YNY ;
@Z""A /CBDB""X25)+EDF\% [1LH>J # and # A /CB3B""=25)+E3FK>LH>J3O
# (PA /C""B3A /5BBD""=25B)+""=21E3)+FKED?F A ""QSA RT/CBDBDBU]""=@ 21)+O ;EDF ;
// j goes to cluster of i
@Z# ""A /CBDB""X25)+EDFK% >^H>J and # A /5BDB""=21)+EDF\[1LH>J3O
(P A /5BDB""=21)+EDF A QSRTBDU]@Z""WO ;
""A /CB3B""=25)+E3FK # A /CBDB""X25)+EDF ;
// i goes to cluster of j
 "" $#% IH>J ;

Input: Bigrams
Output: Set of
Algorithm:
0.
;
1. set
2. for all docs
3.
if
4.
5.
6.
7.
8.
else if
9.
10.
11.
else if
12.
13.
14.
end if
15.
16. end for

Table 1: Top features for some of the clusters.
/ 25E3)  /2CE3) B   E US/1 ""D0  2	  F  /R
 D)D4
 DR B  A A A  W 
0 /""$D)T ""$) 2  R BWE30D""=)  2 F""WB W/1) E D/CBD B""9"" /"" D) E30D )+E B14 A A A
^E F R946E3) B R E30 USE ^E
R E30D""$E B Q9/25E B A A A
Q /  E   /5B   0 DR
""$) W2   /5FF0 EDBD B  USE  )+ED0   "" Q   Q9/  E B   A A A
0 /)] B D0D4  US/1)T)+ E   F""$)  2 0 /WE D4 Q 0DE BD B""D)  ""$4 /2CE B A A A
FEWE ""D) /2CE3) B B3E R 0D""  ""$)03RTB""D) FEWE ""D) A A A
0 / 9 ""    0 !/ WE   Q /   E    D)  2   P""F1E  B   U EDFR   ""$) 2   A A A
4  "" E P""$0 E E BDB QS	0   B3E30 "" E B3E30 "" E B A A A
""

Cluster 1
Cluster 2
Cluster 3
Cluster 4
Cluster 5
Cluster 6
Cluster 7
Cluster 8

@;) O

17. if $#&% goto 1
18. Return S
The algorithm starts with empty clusters and then cycles through all documents picking the
pairs of documents that have the current highest joint visitation frequency as prompted by
a bigram frequency (lines 1 and 2). If both documents in the selected pair are unassigned, a
new cluster is allocated for them (lines 3 through 7). If one of the documents in the selected
pair has been assigned to one of the previous clusters, the second document is assigned to
the same cluster (lines 8 through 14). The algorithm repeats for a lower frequency , as
long as $#&% .

)

)

( #% "" % #

( ""%
$
%

( "" / W/CO KJ H

""

After the clustering, we can assume that if the user requests a document from the -th
cluster
,(
he' is considerably more likely
rather than
""+ to prefer
-, a next
.+ document from
,
, i.e. ) *)
from
 0/
1) . This
assumption is reasonable because by construction clusters represent densely connected (in
terms of traffic) components, and the traffic across the clusters is small compared to the
traffic within each cluster. In view of this observation, we broke individual user sessions
down into subsessions, where each subsession consisted of documents belonging to the
same cluster. The problem was thus reduced to a series of prediction problems for each
cluster.

(

 ""



@  

(P "" %



We studied the clusters by trying to find out if the documents within a cluster are topically
related. We ran code previously developed at NEC Labs [4] that uses information gain
to find the top features that distinguish each cluster from the rest. Table 1 shows the top
features for some of the created clusters. The top features are quite consistent descriptors,
suggesting that in one session a ResearchIndex user is typically interested in searching
among topically-related documents.

4 Trigger MaxEnt

@  , 2 @  O  / W/CO
/ W/
 

 W



 as a maxent distribution, where
In this paper, we model )
is
the
identity
of
the
document
that
will
be
next
requested
by
the
user
,
given
the
history
2
and the available ! for all other users. This choice of the maxent model is
natural since our intuition is that all of the previously requested documents in the user
session influence the identity of
. It is also clear that we cannot afford to build a
high-order model, because of the sparsity and high-dimensional data, so we need to restrict
ourselves to models that can be reliably estimated from the low-order statistics.

@ O

 W

Bigrams provide one type of such statistics. In order to introduce long term dependence of
on the documents
a trigger as a
3 that occurred in the history of the session,4, we+5define
2
pair of documents
in  a given cluster such that )
is substantially
different from )
. To measure the quality of triggers and in order to rank them

@

@Z/   O

 O

@    /

O

U

2

Table 2: Average number of hits and height
of predictions across the clusters for
different ranges of heights and using various models. The boxed numbers are the best
values across all models. 2
2
2
 2
 2 1%

&%
Model
Mult.
48.78
67.94
80.94
90.93
98.54
2
1 c.
1.437
2.947
4.390
5.773
7.026
Mult.
95.49
120.52
132.07
138.89
143.33
2
25 c.
1.421
2.503
3.312
3.975
4.528
Mark.
91.39
115.68
123.44
126.26
127.57
2
1 c.
1.959
3.007
3.571
3.875
4.063
Mark.
89.75
114.49
122.57
125.61
127.14
2
25 c.
1.959
3.047
3.646
3.972
4.191
Maxent 2
111.95
130.35
138.18
142.56
145.55
no sm.
1.510
2.296
2.858
3.303
3.694
Maxent
112.68
130.86
138.53
142.85
145.78
2
w. sm.
1.476
2.258
2.810
3.248
3.633
Corr.
111.02
132.87
140.96
144.99
147.34
2
1.973
2.801
3.340
3.726
4.021

J

U

J

&

U
U
U
U
U
U

we computed mutual information between events 

   



and 
	

3/

+""2

.

The set of features, together with maxent as an objective function, can be shown to lead to
the following form of the conditional maxent model

)

@  W , 2 O 





J 2 3E 7Q
@ O





@  W 32 O %$

(1)

@ @ 2 O  2 O  B IJ  AAA  ( 

(2)

 



@ 2 O is a normalization constant ensuring that the distribution sums to 1.

The set of parameters   , 2 needs to be found from the following set of equations that restrict
 O to have the same expected value for each feature as seen in
the distribution ) @
where

the training data:



@  O

)

@

,2

O   @ 32 O 



QV@

 

O

where32 the LHS represents the expectation (up
, 2 to a normalization factor) of the feature

with respect to the distribution
and the RHS is the actual frequency (up
to the same normalization factor) of this feature
in
the training data. There exist efficient
 (e.g. improved
algorithms for finding the parameters 
iterative scaling [11]) that are
 on ) are consistent.
known to converge if the constraints imposed
Under fairly general assumptions, the maxent model can also be shown to be a maximum
likelihood model [11]. Employing a Gaussian prior with a zero mean on parameters

yields a maximum aposteriori solution that has been shown to be more accurate than the related maximum likelihood solution and other smoothing techniques for maxent models [2].
We use Gaussian smoothing in our experiments with a maxent model.

5 Experimental Results and Comparisons
We compared the trigger maxent model with the following models: mixture of Markov
models (1 and 25 components), mixture of multinomials (1 and 25 components) and the

Table 3: Average time per 1000 predictions and average memory used by various models
across 1000 clusters.
Time, s Memory, KBytes
Mult.,
0.0049
0.5038
Mult., 25
0.0559
12.58
Markov, 1
0.0024
1.53
Markov, 25
0.0311
68.23
Maxent, no sm. 0.0746
90.12
Maxent, w. sm. 0.0696
90.12
Correlation
7.2013
17.26

correlation method [1]. The definitions of the models can be found in [9]. The maxent
model came in two flavors: unsmoothed and smoothed with a Gaussian prior, with 0 mean
and fixed variance 2. We did not optimize the adjustable parameters of the models (such as
the number of components for the mixture or the variance of the prior for maxent models)
or the number of clusters (1000).
We chronologically partitioned the log into roughly 8 million training requests (covering
82 days) and 2 million test requests (covering 17 days). We used the average height of
predictions on the test data as a main evaluation criteria. The 5,
height
of a prediction is
2
are available from
defined as follows. Assuming 2 that the probability estimates )
a model ) for a fixed history
and all possible values of , we first sort them in the
descending order of ) and then find the distance in terms of the number of documents to
the actually requested (which we know from the test data) from the top of this sorted list.
The height tells us how deep into the list the user must go in order to see the document that
actually interests him. The height of a perfect prediction is 0, the maximum (worst) height
for a given cluster equals the number of documents in this cluster. Since heights greater
than 20 are of little practical interest, we binned
of predictions for
 each cluster.
 the
 heights for
For binning purposes we used height ranges
. Within each
bin we also computed the average height of predictions. Thus, the best performing model
would place most of the predictions inside the bin(s) with low value(s) of
and within
those bins the averages would be as low as possible.

@

 @ Y.J3O O

O

   A AA 

Table 2 reports the average number of hits each model makes on average in each of the
bins, as well as the average height of predictions within the bin. The smoothed maxent
model has the best average height of predictions across the bins and scores roughly the
same number of hits in each of the bins as the correlation method. The mixture of Markov
models with 25 components evidently overfits on the training data and fails to outperform
a 1 component mixture. The mixture of multinomials is quite close in quality to, but still
not as good as, the maxent model with respect to both the number of hits and the height
predictions in each of the bins.
In Table 3, we present comparison of various models with respect to the average time
taken and memory required to make a prediction. The table clearly illustrates that the
maxent model (i.e., the model-based approach) is substantially more time efficient than the
correlation (i.e., the memory-based approach), even despite the fact that the model takes
on average more memory. In particular, our maxent approach is roughly two orders of
magnitude faster than the correlation.

6 Conclusions and Future Work
We have described a maxent approach to generating document recommendations in ResearchIndex. We addressed the problem of sparse, high-dimensional data by introducing a
clustering of the documents based on the user navigation patterns. A particular advantage
of our clustering is that by its definition the traffic across the clusters is small compared to
the traffic within the cluster. This advantage allowed us to decompose the original prediction problem into a set of problems corresponding to the clusters. We also demonstrated
that our clustering produces highly interpretable clusters: each cluster can be assigned a
topical name based on the top-extracted features.
We presented a number of models that can be used to solve a document prediction problem
within cluster. We showed that the maxent model that combines zero and first order Markov
terms as well as the triggers with high information content provides the best average outof-sample performance. Gaussian smoothing improved results even further.
There are several important directions to extend the work described in this paper. First,
we plan to perform ?live? testing of the clustering approach and various models in ResearchIndex. Secondly, our recent work [10] suggests that for difficult prediction problems
improvement beyond the plain maxent models can be sought by employing the mixtures of
maxent models. We also plan to look at different clustering methods for documents (e.g.,
based on the content or the link structure) and try to combine prediction results for different clusterings. Our expectation is that such combining could yield better accuracy at the
expense of longer running times. Finally, one could think of a (quite involved) EM algorithm that performs the clustering of the documents in a manner that would make prediction
within resulting clusters easier.
Acknowledgements We would like to thank Steve Lawrence for making available the ResearchIndex log data, Eric Glover for running his naming code on our clusters, Kostas
Tsioutsiouliklis and Darya Chudova for many useful discussions, and the anonymous reviewers for helpful suggestions.

References
[1] J. Breese, D. Heckerman, and C. Kadie. Empirical analysis of predictive algorithms for collaborative filtering. In Proceedings of UAI-1998, pages 43?52. San Francisco, CA: Morgan
Kaufmann Publishers, 1998.
[2] S. Chen and R. Rosenfeld. A Gaussian prior for smoothing maximum entropy models. Technical Report CMUCS -99-108, Carnegie Mellon University, 1999.
[3] D. Cosley, S. Lawrence, and D. Pennock. An open framework for practical testing of recommender systems using ResearchIindex. In International Conference on Very Large Databases
(VLDB?02), 2002.
[4] E. Glover, D. Pennock, S. Lawrence, and R. Krovetz. Inferring hierarchical descriptions. Technical Report NECI TR 2002-035, NEC Research Institute, 2002.
[5] J. Goodman. Classes for fast maximum entropy training. In Proceedings of IEEE International
Conference on Acoustics, Speech, and Signal Processing, 2001.
[6] D. Heckerman, D. Chickering, C. Meek, R. Rounthwaite, and C. Kadie. Dependency networks for density estimation, collaborative filtering, and data visualization. Journal of Machine
Learning Research, 1:49?75, 2000.
[7] T. Hofmann and J. Puzicha. Latent class models for collaborative filtering. In Proceedings of
the Sixteenth International Joint Conference on Artificial Intelligence, pages 688?693, 1999.
[8] S. Lawrence, C. L. Giles, and K. Bollacker. Digital libraries and Autonomous Citation Indexing.
IEEE Computer, 32(6):67?71, 1999.

[9] D. Pavlov and D. Pennock. A maximum entropy approach to collaborative filtering in dynamic,
sparse, high-dimensional domains. Technical Report NECI TR, NEC Research Institute, 2002.
[10] D. Pavlov, A. Popescul, D. Pennock, and L. Ungar. Mixtures of conditional maximum entropy
models. Technical Report NECI TR, NEC Research Institute, 2002.
[11] S. D. Pietra, V. D. Pietra, and J. Lafferty. Inducing features of random fields. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 19(4):380?393, April 1997.
[12] A. Popescul, L. Ungar, D. Pennock, and S. Lawrence. Probabilistic models for unified collaborative and content-based recommendation in sparse-data environments. In Proceedings of the
Seventeenth Conference on Uncertainty in Artificial Intelligence, pages 437?444, 2001.
[13] P. Resnick, N. Iacovou, M. Suchak, P. Bergstorm, and J. Riedl. GroupLens: An Open Architecture for Collaborative Filtering of Netnews. In Proceedings of ACM 1994 Conference on
Computer Supported Cooperative Work, pages 175?186, Chapel Hill, North Carolina, 1994.
ACM.
[14] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Analysis of recommender algorithms for
e-commerce. In Proceedings of the 2nd ACM Conference on Electronic Commerce, pages 158?
167, 2000.
[15] G. Shani, R. Brafman, and D. Heckerman. An MDP-based recommender system. In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence, pages 453?460,
2002.
[16] U. Shardanand and P. Maes. Social information filtering: Algorithms for automating ?word of
mouth?. In Proceedings of ACM CHI?95 Conference on Human Factors in Computing Systems,
volume 1, pages 210?217, 1995.

"
3331,2010,Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles,"Functional segregation and integration are fundamental characteristics of the human brain. Studying the connectivity among segregated regions and the dynamics of integrated brain networks has drawn increasing interest. A very controversial, yet fundamental issue in these studies is how to determine the best functional brain regions or ROIs (regions of interests) for individuals. Essentially, the computed connectivity patterns and dynamics of brain networks are very sensitive to the locations, sizes, and shapes of the ROIs. This paper presents a novel methodology to optimize the locations of an individual's ROIs in the working memory system. Our strategy is to formulate the individual ROI optimization as a group variance minimization problem, in which group-wise functional and structural connectivity patterns, and anatomic profiles are defined as optimization constraints. The optimization problem is solved via the simulated annealing approach. Our experimental results show that the optimized ROIs have significantly improved consistency in structural and functional profiles across subjects, and have more reasonable localizations and more consistent morphological and anatomic profiles.","Individualized ROI Optimization via
Maximization of Group -wise Consistency of
Structural and Functional Profiles
1, 2*

Kaiming Li, 1Lei Guo, 3Carlos Faraco, 2Dajiang Zhu, 2Fan Deng, 1Tuo Zhang, 1Xi
Jiang, 1Degang Zhang, 1Hanbo Chen, 1Xintao Hu, 3Steve Miller, 2Tianming Liu
1
School of Automation, Northwestern Polytechnical University,China;2Department of
Computer Science, the University of Georgia, USA; 3Department of Psychology, the
University of Georgia, USA; *Email:likaiming@gmail.com

Abstract
Functional segregation and integration are fundamental characteristics of the
human brain. Studying the connectivity among segregated regions and the
dynamics of integrated brain networks has drawn increasing interest. A very
controversial, yet fundamental issue in these studies is how to determine the
best functional brain regions or ROIs (regions of interests) for individuals.
Essentially, the computed connectivity patterns and dynamics of brain
networks are very sensitive to the locations, sizes, and shapes of the ROIs.
This paper presents a novel methodology to optimize the locations of an
individual's ROIs in the working memory system. Our strategy is to formulate
the individual ROI optimization as a group variance minimization problem,
in which group-wise functional and structural connectivity patterns, and
anatomic profiles are defined as optimization constraints. The optimization
problem is solved via the simulated annealing approach. Our experimental
results show that the optimized ROIs have significantly improved
consistency in structural and functional profiles across subjects, and have
more reasonable localizations and more consistent morphological and
anatomic profiles.

1

Int ro ducti o n

The human brain?s function is segregated into distinct regions and integrated via axonal fibers
[1]. Studying the connectivity among these regions and modeling their dynamics and
interactions has drawn increasing interest and effort from the brain imaging and neuroscience
communities [2-6]. For example, recently, the Human Connectome Project [7] and the 1000
Functional Connectomes Project [8] have embarked to elucidate large-scale connectivity
patterns in the human brain. For traditional connectivity analysis, a variety of models
including DCM (dynamics causal modeling), GCM (Granger causality modeling) and MVA
(multivariate autoregressive modeling) are proposed [6, 9-10] to model the interactions of the
ROIs. A fundamental issue in these studies is how to accurately identify the ROIs, which are
the structural substrates for measuring connectivity. Currently, this is still an open, urgent, yet
challenging problem in many brain imaging applications. From our perspective, the major
challenges come from uncertainties in ROI boundary definition, the tremendous variability
across individuals, and high nonlinearities within and around ROIs.
Current approaches for identifying brain ROIs can be broadly classified into four categories.
The first is manual labeling by experts using their domain knowledge. The second is a
data-driven clustering of ROIs from the brain image itself. For instance, the ReHo (regional
homogeneity) algorithm [11] has been used to identify regional homogeneous regions as ROIs.
The third is to predefine ROIs in a template brain, and warp them back to the individual space
using image registration [12]. Lastly, ROIs can be defined from the activated regions observed
during a task-based fMRI paradigm. While fruitful results have been achieved using these
approaches, there are various limitations. For instance, manual labeling is difficult to
implement for large datasets and may be vulnerable to inter-subject and intra-subject variation;

it is difficult to build correspondence across subjects using data-driven clustering methods;
warping template ROIs back to individual space is subject to the accuracy of warping
techniques and the anatomical variability across subjects.
Even identifying ROIs using task-based fMRI paradigms, which is regarded as the standard
approach for ROI identification, is still an open question. It was reported in [13] that many
imaging-related variables including scanner vender, RF coil characteristics (phase array vs.
volume coil), k-space acquisition trajectory, reconstruction algorithms, susceptibility -induced
signal dropout, as well as field strength differences, contribute to variations in ROI
identification. Other researchers reported that spatial smoothing, a common preprocessing
technique in fMRI analysis to enhance SNR, may introduce artificial localization shift s (up to
12.1mm for Gaussian kernel volumetric smoothing) [14] or generate overly smoothed
activation maps that may obscure important details [15]. For example, as shown in Fig.1a, the
local maximum of the ROI was shifted by 4mm due to the spatial smoothing process.
Additionally, its structural profile (Fig.1b) was significantly altered. Furthermore,
group-based activation maps may show different patterns from an individual's activation map;
Fig.1c depicts such differences. The top panel is the group activation map from a working
memory study, while the bottom panel is the activation map of one subject in the study. As we
can see from the highlighted boxes, the subject has less activated regions than the group
analysis result. In conclusion, standard analysis of task-based fMRI paradigm data is
inadequate to accurately localize ROIs for each individual.

Fig.1. (a): Local activation map maxima (marked by the cross) shift of one ROI due to spatial
volumetric smoothing. The top one was detected using unsmoothed data while the bottom one
used smoothed data (FWHM: 6.875mm). (b): The corresponding fibers for the ROIs in (a). The
ROIs are presented using a sphere (radius: 5mm). (c): Activation map differences between the
group (top) and one subject (bottom). The highlighted boxes show two of the missing activated
ROIs found from the group analysis.
Without accurate and reliable individualized ROIs, the validity of brain connectivity analysis,
and computational modeling of dynamics and interactions among brain networks , would be
questionable. In response to this fundamental issue, this paper presents a novel computational
methodology to optimize the locations of an individual's ROIs initialized from task-based
fMRI. We use the ROIs identified in a block-based working memory paradigm as a test bed
application to develop and evaluate our methodology. The optimization of ROI locations was
formulated as an energy minimization problem, with the goal of jointly maximizing the
group-wise consistency of functional and structural connectivity patterns and anatomic
profiles. The optimization problem is solved via the well-established simulated annealing
approach. Our experimental results show that the optimized ROIs achieved our optimization
objectives and demonstrated promising results.

2

Mat eria l s a nd Metho ds

2.1

Data acquisition and preprocessing

Twenty-five university students were recruited to participate in
this study. Each participant performed an fMRI modified version
of the OSPAN task (3 block types: OSPAN, Arithmetic, and
Baseline) while fMRI data was acquired. DTI scans were also
acquired for each participant. FMRI and DTI scans were
acquired on a 3T GE Signa scanner. Acquisition parameters were
as follows : fMRI: 64x64 matrix, 4mm slice thickness, 220mm
FOV, 30 slices, TR=1.5s, TE=25ms, ASSET=2; DTI: 128x128
matrix, 2mm slice thickness, 256mm FOV, 60 slices,
TR=15100ms, TE= variable, ASSET=2, 3 B0 images, 30
optimized gradient directions, b-value=1000). Each participant?s
fMRI data was analyzed using FSL. Individual activation map
Fig.2. working memory
reflecting the OSPAN (OSPAN > Baseline) contrast was used. In
ROIs mapped on a
total, we identified the 16 highest activated ROIs, including left
WM/GM surface
and right insula, left and right medial frontal gyrus, left and right
precentral gyrus, left and right paracingulate gyrus, left and right
dorsolateral prefrontal cortex, left and right inferior parietal lobule, left occipital pole, right
frontal pole, right lateral occipital gyrus, and left and right precuneus. Fig.2 shows the 16 ROIs
mapped onto a WM(white matter)/GM(gray matter) cortical surface. For some individuals,
there may be missing ROIs on their activation maps. Under such condition, we adapted the
group activation map as a guide to find these ROIs using linear registration.
DTI pre-processing consisted of skull removal, motion correction, and eddy current correction.
After the pre-processing, fiber tracking was performed using MEDINRIA (FA threshold: 0.2;
minimum fiber length: 20). Fibers were extended along their tangent directions to reach into
the gray matter when necessary. Brain tissue segmentation was conducted on DTI data by the
method in [16] and the cortical surface was reconstructed from the tissue maps using the
marching cubes algorithm. The cortical surface was parcellated into anatomical regions using
the HAMMER tool [17]. DTI space was used as the standard space from which to generate the
GM (gray matter) segmentation and from which to report the ROI locations on the cortical
surface. Since the fMRI and DTI sequences are both EPI (echo planar imaging) sequences,
their distortions tend to be similar and the misalignment between DTI and fMRI images is
much less than that between T1 and fMRI images [18]. Co-registration between DTI and fMRI
data was performed using FSL FLIRT [12]. The activated ROIs and tracked fibers were then
mapped onto the cortical surface for joint modeling.
2.2

Joint modeling of anatomical, structural and functional profiles

Despite the high degree of variability across subjects, there are several aspects of regularity on
which we base the proposed solution. Firstly, across subjects, the functional ROIs should have
similar anatomical locations, e.g., similar locations in the atlas space. Secondly, these ROIs
should have similar structural connectivity profiles across subjects. In other words, fibers
penetrating the same functional ROIs should have at least similar target regions across subjects.
Lastly, individual networks identified by task-based paradigms, like the working memory
network we adapted as a test bed in this paper, should have similar functional connectivity
pattern across subjects. The neuroscience bases of the above premises include: 1) structural
and functional brain connectivity are closely related [19], and cortical gyrification and
axongenesis processes are closely coupled [20]; Hence, it is reasonable to put these three types
of information in a joint modeling framework. 2)
Extensive studies have already demonstrated the
existence of a common structural and functional
architecture of the human brain [21, 22], and it
makes sense to assume that the working memory
network has similar structural and functional
connectivity patterns across individuals.
Based on these premises, we proposed to
optimize the locations of individual functional
ROIs by jointly modeling anatomic profiles,
structural connectivity patterns, and functional
connectivity patterns, as illustrated in Fig 3. The

Fig.3. ROIs optimization scheme.

goal was to minimize the group-wise variance (or maximize group-wise consistency) of these
jointly modeled profiles. Mathematically, we modeled the group-wise variance as energy E as
follows. A ROI from fMRI analysis was mapped onto the surface, and is represented by a
center vertex and its neighborhood. Suppose ??? is the ROI region j on the cortical surface of
subject i identified in Section 2.1; we find a corresponding surface ROI region ??? so that the
energy E (contains energy from n subjects, each with m ROIs) is minimized:
? = ?? (?

?? ????

+ (1 ? ?)

???

?? ????
???

)

(1)

where Ea is the anatomical constraint; Ec is the structural connectivity constraint, M Ec and

? E are the mean and standard deviation of Ec in the searching space; E f is the functional
c

connectivity constraint, M E f and ? E f are the mean and standard deviation of E f respectively;
and ? is a weighting parameter between 0 and 1. If not specified,
and m is the number of ROIs in this paper. The details of these
energy terms are provided in the following sections.
2.2.1

n is the number of subjects,

Anatomical constraint energy

Anatomical constraint energy Ea is defined to ensure that the
optimized ROIs have similar anatomical locations in the atlas
space (Fig.4 shows an example of ROIs of 15 randomly
selected subjects in the atlas space). We model the locations for
all ROIs in the atlas space using a Gaussian model (mean:
??? ,and standard deviation: ? X j for ROI j ). The model
parameters were estimated using the initial locations obtained
from Section 2.1. Let X ij be the center coordinate of region Sij

Fig.4. ROI distributions
in Atlas space.

in the atlas space, then Ea is expressed as

1
?? = { ?????1
?

(?????1)
(????>1)

(2)

? , 1 ? ? ? ?; 1 ? ? ? ?. }

(3)

where
??? ????

???? = ??? { ?

3???

Under the above definition, if any X ij is within the range of 3s X from the distribution model
j

center M X , the anatomical constraint energy will always be one; if not, there will be an
j

exponential increase of the energy which punishes the possible involvement of outliers. In
other words, this energy factor will ensure the optimized ROIs will not significantly deviate
away from the original ROIs.
2.2.2 Structural connectivity constraint energy
Structural connectivity constraint energy Ec is defined to ensure the group has similar
structural connectivity profiles for each functional ROI, since similar functional regions
should have the similar structural connectivity patterns [19],
n

m

Ec ? ?? (Cij ? M C j )Covc ?1 (Ci j ? M C j )T

(4)

i ?1 j ?1

where

Cij is the connectivity pattern vector for ROI j of subject i , M C j is the group mean
?1

for ROI j , and Covc is the inverse of the covariance matrix.
The connectivity pattern vector

Cij is a fiber target region distribution histogram. To obtain

this histogram, we first parcellate all the cortical surfaces into nine regions ( as shown in Fig.5a,
four lobes for each hemisphere, and the subcortical region) using the HAMMER algorithm

[17]. A finer parcellation is available but not used due to the relatively lower parcellation
accuracy, which might render the histogram too sensitive to the parcellation result. Then, we
extract fibers penetrating region Sij , and calculate the distribution of the fibers? target cortical
regions. Fig.5 illustrates the ideas.

Fig.5. Structural connectivity pattern descriptor. (a): Cortical surface parcellation using
HAMMER [17]; (b): Joint visualization of the cortical surface, two ROIs (blue and green
spheres), and fibers penetrating the ROIs (in red and yellow, respectively); (c): Corresponding
target region distribution histogram of ROIs in Fig.5b. There are nine bins corresponding to the
nine cortical regions. Each bin contains the number of fibers that penetrate the ROI and are
connected to the corresponding cortical region. Fiber numbers are normalized across subjects.
2.2.3

Functional connectivity constraint energy

Functional connectivity constraint energy E f is defined to ensure each individual has similar
functional connectivity patterns for the working memory system, assuming the human brain
has similar functional architecture across individuals [21].
?? = ???=1??? ? ?? ?
(5)
Here, Fi is the functional connectivity matrix for subject i , and M F is the group mean of the
dataset. The connectivity between each pair of ROIs is defined using the Pearson correlation.
The matrix distance used here is the Frobenius norm.
2.3

Energy minimization solution

The minimization of the energy defined in Section 2.2 is known as a combinatorial
optimization problem. Traditional optimization methods may not fit this problem, since there
are two noticeable characteristics in this application. First, we do not know how the energy
changes with the varying locations of ROIs. Therefore, techniques like Newton?s method
cannot be used. Second, the structure of search space is not smooth, which may lead to
multiple local minima during optimization. To address this problem, we adopt the simulated
annealing (SA) algorithm [23] for the energy minimization. The idea of the SA algorithm is
based on random walk through the space for lower energies. In these random walks, the
probability of taking a step is determined by the Boltzmann distribution,
- (E

- E )/ ( KT )

p = e i+ 1 i
(6)
if Ei ?1 ? Ei , and p ? 1 when Ei ?1 ? Ei . Here, ?? and ??+1 are the system energies at solution
configuration ? and ? + 1 respectively; ? is the Boltzmann constant; and ? is the system
temperature. In other words, a step will be taken when a lower energy is found. A step will also
be taken with probability p if a higher energy is found. This helps avoid the local minima in
the search space.

3

R esult s

Compared to structural and functional connectivity patterns, anatomical profiles are more

easily affected by variability across individuals. Therefore, the anatomical constraint energy is
designed to provide constraint only to ROIs that are obviously far away from reasonableness.
The reasonable range was statistically modeled by the localizations of ROIs warped into the
atlas space in Section 2.2.1. Our focus in this paper is the structural and functional profiles.
3.1

Optimization using anatomical and structural connectivity profile s

In this section, we use only anatomical and structural connectivity profiles to optimize the
locations of ROIs. The goal is to check whether the structural constraint energy Ec works as
expected. Fig.6 shows the fibers penetrating the right precuneus for eight subjects before (top
panel) and after optimization (bottom panel). The ROI is highlighted in a red sphere for each
subject. As we can see from the figure (please refer to the highlighted yellow arrows), after
optimization, the third and sixth subjects have significantly improved consistency with the rest
of the group than before optimization, which proves the validity of the energy function Eq.(4).

Fig.6. Comparison of structural profiles before and after optimization. Each column shows the
corresponding before-optimization (top) and after-optimization (bottom) fibers of one subject.
The ROI (right precuneus) is presented by the red sphere.
3.2

Optimization using anatomical and functional connectivity profiles

In this section, we optimize the locations of ROIs using anatomical and functional profiles,
aiming to validate the definition of functional connectivity constraint energy E f . If this energy
constraint worked well, the functional connectivity variance of the working memory system
across subjects would decrease. Fig.7 shows the comparison of the standard derivation for
functional connectivity before (left) and after (right) optimization. As we can see, the variance
is significantly reduced after optimization. This demonstrated the effectiveness of the defined
functional connectivity constraint energy.

Fig.7. Comparison of the standard derivation for functional connectivity before and after the
optimization. Lower values mean more consistent connectivity pattern cross subjects.

3.3
Consistency between optimization of functional profiles and
structural profiles

Fig.8. Optimization consistency between functional and structural profiles. Top: Functional
profile energy drop along with structural profile optimization; Bottom: Structural profile
energy drop along with functional profile optimization. Each experiment was repeated 15
times with random initial ROI locations that met the anatomical constraint.
The relationship between structure and function has been extensively studied [24], and it is
widely believed that they are closely related. In this section, we study the relationship between
functional profiles and structural profiles by looking at how the energy for one of them changes
while the energy of the other decreases. The optimization processes in Section 3.1 and 3.2 were
repeated 15 times respectively with random initial ROI locations that met the anatomical
constraint. As shown in Fig.8, in general, the functional profile energies and structural profile
energies are closely related in such a way that the functional profile energies tend to decrease
along with the structural profile optimization process, while the structural profile energies also
tend to decrease as the functional profile is optimized. This positively correlated decrease of
functional profile energy and structural profile energy not only proves the close relationship
between functional and structural profiles, but also demonstrates the consistency between
functional and structural optimization, laying down the foundation of the joint optimiza tion,
whose results are detailed in the following section.
3.4
Optimization
connectivity profiles

using

anatomical,

structural

and

functional

In this section, we used all the constraints in Eq. (1) to optimize the individual locations of all
ROIs in the working memory system. Ten runs of the optimization were performed using
random initial ROI locations that met the anatomical constraint. Weighting parameter ?
equaled 0.5 for all these runs. Starting and ending temperatures for the simulated annealing
algorithm are 8 and 0.05; Boltzmann constant K ? 1 . As we can see from Fig.9, most runs
started to converge at step 24, and the convergence energy is quite close for all runs. This
indicates that the simulated annealing algorithm provides a valid solution to our problem.
By visual inspection, most of the ROIs move to more reasonable and consistent locations after
the joint optimization. As an example, Fig.10 depicts the location movements of the ROI in
Fig. 6 for eight subjects. As we can see, the ROIs for these subjects share a similar anatomical

landmark, which appears to be the tip of the upper bank of the parieto-occipital sulcus. If the
initial ROI was not at this landmark, it moved to the landmark after the optimization, which
was the case for subjects 1, 4 and 7. The structural profiles of these ROIs are very similar to
Fig.6. The results in Fig. 10 indicate the significant improvement of ROI locations achieved by
the joint optimization procedure.

Fig.9. Convergence performance of the simulated annealing . Each run has 28 temperature
conditions.

Fig.10. The movement of right precuneus before (in red sphere) and after (in green sphere)
optimization for eight subjects. The ""C""-shaped red dash curve for each subject depicts a
similar anatomical landmark across these subjects. The yellow arrows in subject 1, 4 and 7
visualized the movement direction after optimization.

4

Co ncl usio n

This paper presented a novel computational approach to optimize the locations of ROIs
identified from task-based fMRI. The group-wise consistency of functional and structural
connectivity patterns, and anatomical locations are jointly modeled and formulated in an
energy function, which is minimized by the simulated annealing optimization algorithm.
Experimental results demonstrate the optimized ROIs have more reasonable localizations, and
have significantly improved the consistency of structural and functional connectivity profiles
and morphological and anatomic profiles across subjects. Our future work includes extending
this framework to optimize other parameters of ROIs such as size and shape, and applying and
evaluating this methodology to the optimization of ROIs identified in other brain systems such
as the visual, auditory, language, attention, and emotion networks.

5

R ef erence s

1.

Friston, K., Modalities, modes, and models in functional neuroimaging. Science, vol.326, no.5951,
399-403(2009).
Bharat B. Biswal, Toward discovery science of human brain function, PNAS March 9, 2010 vol. 107
no. 10 4734-4739.
Sporns O, Tononi G, K?tter R, The human connectome: A structural description of the human brain.
PLoS Comput Biol. 2005 Sep; 1(4): e42.
Van Dijk KR, Hedden T, Venkataraman A, Evans KC, Lazar SW, Buckner RL, Intrinsic functional
connectivity as a tool for human connectomics: theory, properties, and optimization. J Neurophysiol.
2010 Jan; 103(1): 297-321.
Hagmann P, et al., MR connectomics: Principles and challenges. J Neurosci Methods. 2010 Jan 22.
Friston K, J. et al., Dynamic causal modeling, Neuroimage, 19, 1273-1302, 2003.
http://www.humanconnectomeproject.org/
http://www.nitrc.org/projects/fcon_1000/
Goebel,R., et al., Investigating directed cortical interactions in time-resolved fMRI data using
vector autoregressive modeling and Granger causality mapping. Magnetic Resonance Imaging,
Volume 21, Issue 10, December 2003, Pages 1251-1261
Harrison L, et al., Multivariate autoregressive modeling of fMRI time series, NeuroImage, Volume
19, Issue 4, August 2003, Pages 1477-1491
Zang, Y., et al., ?Regional homogeneity approach to fMRI data analysis,? NeuroImage, 22(1): p.
394-400, 2004.
Jenkinson, M., Bannister, P., Brady, M., Smith, S., 2002. Improved optimization for the robust and
accurate linear registration and motion correction of brain images. Neuroimage 17, 825 ?841.
Friedman, L., and Glover, G.H. (2006). Report on a Multicenter fMRI Quality Assurance Protocol.
Journal of Magnetic Resonance in Imaging, 23(6):827-839.
H.J. Jo, J.M. Lee, J.H. Kim, C.H. Choi, B.M. Gu and D.H. Kang et al., Artificial shifting of fMRI
activation localized by volume- and surface-based analyses, NeuroImage 40 (3) (2008), pp.
1077?1089.
W. Ou, W.M. Wells III, and P. Golland. Combining Spatial Priors and Anatomical Information for
fMRI Detection. Medical Image Analysis, 14(3):318-331, 2010.
Tianming Liu, Hai Li, Kelvin Wong, Ashley Tarokh, Lei Guo, Stephen Wong, Brain Tissue
Segmentation Based on DTI Data, NeuroImage, 38(1):114-23, 2007.
Shen, D., et al., 2002. HAMMER: hierarchical attribute matching mechanism for elastic registration.
IEEE Trans Med Imaging 21(11), 1421-39.
Li K, et al., Cortical surface based identification of brain networks using high spatial reso lution
resting state fMRI data, International Symposium of Biomedical Imaging (ISBI) 2010.DOI:
10.1109/ISBI.2010.5490089 .
Passingham RE, et al., The anatomical basis of functional localization in the cortex. Nat Rev
Neurosci. 3(8):606-16. 2002.
Van Essen, D.: A tension-based theory of morphogenesis and compact wiring in the central nervous
system. Nature 385:313-318 (1997).
M.D. Fox and M.E. Raichle, ?Spontaneous fluctuations in brain activity observed with functional
magnetic resonance imaging?, Nat Rev Neurosci 8:700-711, 2007.
Van Dijk KR, Hedden T, Venkataraman A, Evans KC, Lazar SW, Buckner RL, Intrinsic functional
connectivity as a tool for human connectomics: theory, properties, and optimization. J Neurophysiol.
2010 Jan; 103(1): 297-321.
V. Granville, et al., Simulated annealing: A proof of convergence"". IEEE Transactions on PAMI 16
(6): 652?656. June 1994.
Honey CJ, Sporns O, Cammoun L, Gigandet X, Thiran JP, Meuli R, Hagmann P. Predicting human
resting-state functional connectivity from structural connectivity. PNAS, 106(6):2035-40. 2009.

2.
3.
4.

5.
6.
7.
8.
9.

10.
11.
12.
13.
14.

15.
16.
17.
18.

19.
20.
21.
22.

23.
24.

"
5932,2016,Deep Neural Networks with Inexact Matching for Person Re-Identification,"Person Re-Identification is the task of matching images of a person across multiple camera views. Almost all prior approaches address this challenge by attempting to learn the possible transformations that relate the different views of a person from a training corpora. Then, they utilize these transformation patterns for matching a query image to those in a gallery image bank at test time. This necessitates learning good feature representations of the images and having a robust feature matching technique. Deep learning approaches, such as Convolutional Neural Networks (CNN), simultaneously do both and have shown great promise recently. In this work, we propose two CNN-based architectures for Person Re-Identification. In the first, given a pair of images, we extract feature maps from these images via multiple stages of convolution and pooling. A novel inexact matching technique then matches pixels in the first representation with those of the second. Furthermore, we search across a wider region in the second representation for matching. Our novel matching technique allows us to tackle the challenges posed by large viewpoint variations, illumination changes or partial occlusions. Our approach shows a promising performance and requires only about half the parameters as a current state-of-the-art technique. Nonetheless, it also suffers from false matches at times. In order to mitigate this issue, we propose a fused architecture that combines our inexact matching pipeline with a state-of-the-art exact matching technique. We observe substantial gains with the fused model over the current state-of-the-art on multiple challenging datasets of varying sizes, with gains of up to about 21%.","Deep Neural Networks with Inexact Matching for
Person Re-Identification
Arulkumar Subramaniam
Indian Institute of Technology Madras
Chennai, India 600036
aruls@cse.iitm.ac.in

Moitreya Chatterjee
Indian Institute of Technology Madras
Chennai, India 600036
metro.smiles@gmail.com

Anurag Mittal
Indian Institute of Technology Madras
Chennai, India 600036
amittal@cse.iitm.ac.in

Abstract
Person Re-Identification is the task of matching images of a person across multiple
camera views. Almost all prior approaches address this challenge by attempting to
learn the possible transformations that relate the different views of a person from a
training corpora. Then, they utilize these transformation patterns for matching a
query image to those in a gallery image bank at test time. This necessitates learning
good feature representations of the images and having a robust feature matching
technique. Deep learning approaches, such as Convolutional Neural Networks
(CNN), simultaneously do both and have shown great promise recently.
In this work, we propose two CNN-based architectures for Person Re-Identification.
In the first, given a pair of images, we extract feature maps from these images via
multiple stages of convolution and pooling. A novel inexact matching technique
then matches pixels in the first representation with those of the second. Furthermore, we search across a wider region in the second representation for matching.
Our novel matching technique allows us to tackle the challenges posed by large
viewpoint variations, illumination changes or partial occlusions. Our approach
shows a promising performance and requires only about half the parameters as a
current state-of-the-art technique. Nonetheless, it also suffers from false matches at
times. In order to mitigate this issue, we propose a fused architecture that combines
our inexact matching pipeline with a state-of-the-art exact matching technique. We
observe substantial gains with the fused model over the current state-of-the-art on
multiple challenging datasets of varying sizes, with gains of up to about 21%.

1

Introduction

Successful object recognition systems, such as Convolutional Neural Networks (CNN), extract
?distinctive patterns? that describe an object (e.g. a human) in an image, when ?shown? several images
known to contain that object, exploiting Machine Learning techniques [1]. Through successive stages
of convolutions and a host of non-linear operations such as pooling, non-linear activation, etc., CNNs
extract complex yet discriminative representation of objects that are then classified into categories
using a classifier, such as softmax.

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

Figure 1: Some common challenges in Person Re-Identification.

1.1

Problem Definition

One of the key subproblems of the generic object recognition task is recognizing people. Of
special interest to the surveillance and Human-Computer interaction (HCI) community is the task
of identifying a particular person across multiple images captured from the same/different cameras,
from different viewpoints, at the same/different points in time. This task is also known as Person
Re-Identification. Given a pair of images, such systems should be able to decipher if both of them
contain the same person or not. This is a challenging task, since appearances of a person across images
can be very different due to large viewpoint changes, partial occlusions, illumination variations, etc.
Figure 1 highlights some of these challenges. This also leads to a fundamental research question:
Can CNN-based approaches effectively handle the challenges related to Person Re-Identification?
CNN-based approaches have recently been applied to this task, with reasonable success [2, 3] and are
amongst the most competitive approaches for this task. Inspired by such approaches, we explore a set
of novel CNN-based architectures for this task. We treat the problem as a classification task. During
training, for every pair of images, the model is told whether they are from the same person or not.
At test time, the posterior classification probabilities obtained from the models are used to rank the
images in a gallery image set in terms of their similarity to a query image (probe).
In this work, we propose two novel CNN-based schemes for Person Re-Identification. Our first
model hinges on the key observation that due to a wide viewpoint variation, the task of finding a
match between the pixels of a pair of images needs to be carried out over a larger region, since
?matching pixels? on the object may have been displaced significantly. Secondly, illumination
variations might cause the absolute intensities of image regions to vary, rendering exact matching
approaches ineffective. Finally, coupling these two solutions might provide a recipe for taking care
of partial occlusions as well. We call this first model of ours, Normalized X-Corr. However, the
flexibility of inexact (soft) matching over a wider search space comes at the cost of occasional false
matches. To remedy this, we propose a second CNN-based model which fuses a state-of-the-art exact
matching technique [2] with Normalized X-Corr. We hypothesize that proper training allows the two
components of the fused network to learn complimentary patterns from the data, thus aiding the final
classification. Empirical results show Normalized X-Corr to hold promise and the Fused network
outperforming all baseline approaches on multiple challenging datasets, with gains of upto 21% over
the baselines.
In the next section, we touch upon relevant prior work. We present our methodology in Section
3. Sections 4 and 5 deal with the Experiments and the discussion of the obtained Results thereof.
Finally, we conclude in Section 6, outlining some avenues worthy of exploration in the future.

2

Related Work

In broad terms, we categorize the prior work in this field into Non-Deep and Deep Learning approaches.
2.1

Non-Deep Learning Approaches

Person Re-Identification systems have two main components: Feature Extraction and Similarity
Metric for matching. Traditional approaches for Person Re-Identification either proposed useful
features, or discriminative similarity metrics for comparison, or both.
2

Typical features that have proven useful include color histograms and dense SIFT [4] features
computed over local patches in the image [5, 6]. Farenzena et al. represent image patches by
exploiting features that model appearance, chromatic content, etc [7]. Another interesting line of
work on feature representation attempts to learn a bag-of-features (a.k.a. a dictionary)-based approach
for image representation [8, 9]. Further, Prosser et al. show the effectiveness of learning a subspace
for representing the data, modeled using a set of standard features [10]. While these approaches show
promise, their performance is bounded by the ability to engineer good features. Our models, based
on deep learning, overcome this handicap by learning a representation from the data.
There is also a substantial body of work that attempts to learn an effective similarity metric for
comparing images [11, 12, 13, 14, 15, 16]. Here, the objective is to learn a distance measure that
is indicative of the similarity of the images. The Mahalanobis distance has been the most common
metric that has been adopted for matching in person re-identification [5, 17, 18, 19]. Some other
metric learning approaches attempt to learn transformations, which when applied to the feature space,
aligns similar images closer together [20]. Yet other successful metric learning approaches are an
ensemble of multiple metrics [21]. In contrast to these approaches, we jointly learn both features and
discriminative metric (using a classifier) in a deep learning framework.
Another interesting line of non-deep approaches for person re-identification have claimed novelty
both in terms of features as well as matching metrics [22, 23]. Many of them rely on weighing the
hand-engineered image features first, based on some measure such as saliency and then performing
matching [6, 24, 25]. However, this is done in a non-deep framework unlike ours.
2.2

Deep Learning based Approaches

There has been relatively fewer prior work based on deep learning for addressing the challenge of
Person Re-Identification. Most deep learning approaches exploit the CNN-framework for the task, i.e.
they first extract highly non-linear representations from the images, then they compute some measure
of similarity. Yi et al. propose a Siamese Network that takes as input the image pair that is to be
compared, performs 3 stages of convolution on them (with the kernels sharing weights), and finally
uses cosine similarity to judge their extent of match [26]. Both of our models differ by performing a
novel inexact matching of the images after two stages of convolution and then processing the output
of the matching layer to arrive at a decision.
Li et al. also adopt a two-input network architecture [3]. They take the product of the responses
obtained right after the first set of convolutions corresponding to the two inputs and process its output
to obtain a measure of similarity. Our models, on the other hand, are significantly deeper. Besides,
Normalized X-Corr stands out by retaining the matching outcome corresponding to every candidate in
the search space of a pixel rather than choosing only the maximum match. Ahmed et al. too propose
a very promising architecture for Person Re-Identification [2]. Our models are inspired from their
work and does incorporate some of their features, but we substantially differ from their approach by
performing an inexact matching over a wider search space after two stages of convolution. Further,
Normalized X-Corr has fewer parameters than Ahmed et al. [2].
Finally, our Fused model is a one of a kind deep learning architecture for Person Re-Identification.
This is because a combination (fusion) of multiple deep frameworks has hitherto remained unexplored
for this task.

3

Proposed Approach

3.1

Our Architecture

In this work, we propose two architectures for Person Re-Identification. Both of our architectures
are a type of ?Siamese?-CNN model which take as input two images for matching and outputs the
likelihood that the two images contain the same person.
3.1.1

Normalized X-Corr

The following are the principal components of the Normalized X-Corr model, as shown in Fig 2.
3

24

same
12

25

37

Shared weights

NORMALIZED
CORRELATION
+
RELU

CONV +
RELU

1x1x1500 ? 25

5
MAXPOOL
2x2

CONV

3x3x25 ? 25

37

60

10

different

17

12

25

20
20

35

74

78

500

12
MAXPOOL
2x2

CONV +
RELU

5x5x20 ? 25

37

56

156

160

28
MAXPOOL
2x2

CONV +
RELU

5x5x3 ? 20

25

Softmax
layer

25

25

56

24

1500

Fully
connected
Layer

MAXPOOL
2x2

CONV +
RELU

74

37

? 25

78

156

160

MAXPOOL
2x2
5x5x20

CONV +
RELU

5x5x3 ? 20

Cross Patch Feature
Aggregation

12

28

25

20

25

20
60

25

10

5

500

MAXPOOL
2x2

CONV

3x3x25 ? 25

35

CONV +
RELU

1x1x1500 ? 25

25

37

37

74

25

20
20

12

NORMALIZED
CORRELATION
+
RELU

17

12

12
MAXPOOL
2x2

37

CONV +
RELU

5x5x20 ? 25

78

156

160

28
MAXPOOL
2x2

CONV +
RELU

5x5x3 ? 20

24

56

Figure 2: Architecture of the Normalized X-Corr Model.

same

25

25
1500

Cross Patch Feature
Aggregation

60

Shared weights
different

37

CONV

3x3x25 ? 25

MAXPOOL
2x2

500

Softmax
layer

37

25
25

25

5

17

CONV +
RELU

1x1x1250 ? 25

10

35

MAXPOOL
2x2

CONV +
RELU

? 25

74

78

156

160

20
20

12

28
MAXPOOL
2x2
5x5x20

CONV +
RELU

5x5x3 ? 20

12

CROSS INPUT
NEIGHBORHOOD
+
RELU

37

56

24

12

1250

25

Patch summary

25

Fully connected
Layer

60

Figure 3: Architecture of the Fused Network Model.
Tied Convolution Layers: Convolutional features have been shown to be effective representation of
images [1, 2]. In order to measure similarity between the input images, the applied transformations
must be similar. Therefore, along the lines of Ahmed et al., we perform two stages of convolutions
and pooling on both the input images by passing them through two input pipelines of a ?Siamese?
Network, that share weights [2]. The first convolution layer takes as input images of size 60?160?3
and applies 20 learned filters of size 5?5?3, while the second one applies 25 learned filters of size
5?5?20. Both convolutions are followed by pooling layers, which reduce the output dimension by a
factor of 2, and ReLU (Rectified Linear Unit) clipping. This gives us 25 maps of dimension 12?37
as output from each branch which are fed to the next layer.
Normalized Correlation Layer: This is the first layer that captures the similarity of the two input
images; subsequent layers build on the output of this layer to finally arrive at a decision as to whether
the two images are of the same person or not. Different from [2, 3], we incorporate both inexact
matching and wider search. Given two corresponding input feature maps X and Y , we compute the
normalized correlation as follows. We start with every pixel of X located at (x, y), where x is along
the width and y along the height (denoted as X(x, y)). We then create two matrices. The first is a
5?5 matrix representing the 5?5 neighborhood of X(x, y), while the second is the corresponding
5?5 neighborhood of Y centered at (a, b), where 1 ? a ? 12 and y ? 2 ? b ? y + 2. Now,
markedly different from Ahmed et al. [2], we perform inexact matching over a wider search space, by
computing a Normalized Correlation between the two patch matrices. Given two matrices, E and F ,
whose elements are arranged as two N-dimensional vectors, the Normalized Correlation is given by:
PN
normxcorr(E, F ) =

? ?E )(Fi ? ?F )
,
(N ? 1).?E .?F

i=1 (Ei

where ?E , ?F denotes the means of the elements of the 2 matrices, E and F respectively, while
?E , ?F denotes their respective standard deviations (a small  = 0.01 is added to ?E and ?F to
avoid division by 0). Interestingly, Normalized Correlation being symmetric, we need to model the
computation only in one-way, thereby cutting down the number of parameters in subsequent layers.
For every pair of 5?5 matrices corresponding to a given pixel in image X, we arrange the normalized
correlation values in different feature maps. These feature maps preserve the spatial ordering of
4

pixels in X and are also 12?37 each, but their pixel values represent normalized correlation. This
gives us 60 feature maps of dimension 12?37 each. Now similarly, we perform the same operation
for all 25 pairs of maps that are input to the Normalized Correlation layer, to obtain an output of
1500, 12?37 maps. One subtle but important difference between our approach and that of Li et al.
[3] is that we preserve every correlation output corresponding to the search space of a pixel, X(x, y),
while they only keep the maximum response. We then pass these set of feature maps through a ReLU,
to discard probable noisy matches.
The mean subtraction and standard deviation normalization step incorporates illumination invariance,
a step unaccounted for in Li et al. [3]. Thus two patches which differ only in absolute intensity values
but are similar in the intensity variation pattern would be treated as similar by our models. The wider
search space, compared to Ahmed et al. [2], gives invariance to large viewpoint variation. Further,
performing inexact matching (correlation measure) over a wider search space gives us robustness to
partial occlusions. Due to partial occlusions, a part(s) (P) of a person/object visible in one view may
not be visible in others. Using a wider search space, our model looks for a part which is similar to
the missing P within a wider neighborhood of P?s original location. This is justified since typically
adjacent regions of objects in an image have regularity/similarity in appearance. e.g. Bottom and
upper half of torso. Now, since we are comparing two different parts due to the occlusion of P, we
need to perform flexible matching. Thus, inexact matching is used.
Cross Patch Feature Aggregation Layers: The Normalized Correlation layer incorporates information from the local neighborhood of a pixel. We now seek to incorporate greater context information,
to obtain a summarization effect. In order to do so, we perform 2 successive layers of convolution
(with a ReLU filtered output) followed by pooling (by a factor of 2) of the output feature maps from
the previous layer. We use 1?1?1500 convolution kernels for the first layer and 3?3?25 convolution
kernels for the second convolution layer. Finally, we get 25 maps of dimension 5?17 each.
Fully Connected Layers: The fully connected layers collate information from pixels that are very
far apart. The feature map outputs obtained from the previous layer are reshaped into one long
2125-dimensional vector. This vector is fed as input to a 500-node fully connected layer, which
connects to another fully connected layer containing 2 softmax units. The first unit outputs the
probability that the two images are same and the latter, the probability that the two are different.
One key advantage of the Normalized X-Corr model is that it has about half the number of parameters
(about 1.121 million) as the model proposed by Ahmed et al. [2] (refer supplementary section for
more details).
3.1.2

Fused Model

While the Normalized X-Corr model incorporates inexact matching over a wider search space to
handle important challenges such as illumination variations, partial occlusions, or wide viewpoint
changes, however it also suffers from occasional false matches. Upon investigation, we found that
these false matches tended to recur, especially when the background of the false matches had a similar
appearance to the person being matched (see supplementary). For such cases, an exact matching,
such as taking a difference and constraining the search window might be beneficial. We therefore
fuse the model proposed by Ahmed et al. [2] with Normalized X-Corr to obtain a Fused model, in
anticipation that it incorporates the benefits of both models. Figure 3 shows a representative diagram.
We keep the tied convolution layers unchanged like before, then we fork off two separate pipelines:
one for Normalized X-Corr and the other for Ahmed et. al.?s model [2]. The two separate pipelines
output a 2125-dimensional vector each and then they are fused in a 1000-node fully connected layer.
The outputs from the fully connected layer are then fed into a 2 unit softmax layer as before.
3.2

Training Algorithm

All the proposed architectures are trained using the Stochastic Gradient Descent (SGD) algorithm, as
in Ahmed et al. [2]. The gradient computation is fairly simple except for the Normalized Correlation
layer. Given two matrices, E (from the first branch of the Siamese network) and F (from the second
branch of the Siamese network), represented by a N-dimensional vector each, the gradient pushed
from the Normalized Correlation layer back to the convolution layers on the top branch is given by:
?normxcorr(E, F )
1
=
?Ei
(N ? 1)?E



normxcorr(E, F )(Ei ? ?E )
F i ? ?F
?
?F
?E

5


,

where Ei is the ith element of the vector representing E and other symbols have their usual meaning. Similar
notation is used for the subnetwork at the bottom. The full derivation is available in the supplementary section.

4

Experiments

Table 1: Performance of different algorithms at ranks 1, 10, and 20 on CUHK03 Labeled (left) and
CUHK03 Detected (right) Datasets.
Method
Fused Model (ours)
Norm X-Corr (ours)
Ensembles [21]
LOMO+MLAPG [5]
Ahmed et al. [2]
LOMO+XQDA [22]
Li et al. [3]
KISSME [18]
LDML [14]
eSDC [25]

4.1

r=1
72.43
64.73
62.1
57.96
54.74
52.20
20.65
14.17
13.51
8.76

r = 10
95.51
92.77
92.30
94.74
93.88
92.14
68.74
52.57
52.13
38.28

r = 20
98.40
96.78
97.20
98.00
98.10
96.25
83.06
70.03
70.81
53.44

Method
Fused Model (ours)
Norm X-Corr (ours)
LOMO+MLAPG [5]
Ahmed et al. [2]
LOMO+XQDA [22]
Li et al. [3]
KISSME [18]
LDML [14]
eSDC [25]

r=1
72.04
67.13
51.15
44.96
46.25
19.89
11.70
10.92
7.68

r = 10
96.00
94.49
92.05
83.47
88.55
64.79
48.08
47.01
33.38

r = 20
98.26
97.66
96.90
93.15
94.25
81.14
64.86
65.00
50.58

Datasets, Evaluation Protocol, Baseline Methods

We conducted experiments on the large CUHK03 dataset [3], the mid-sized CUHK01 Dataset [23], and the small
QMUL GRID dataset [27]. The datasets are divided into training and test sets for our experiments. The goal of
every algorithm is to rank images in the gallery image bank of the test set by their similarity to a probe image
(which is also from the test set). To do so, they can exploit the training set, consisting of matched and unmatched
image pairs. An oracle would always rank the ground truth match (from the gallery) in the first position. All
our experiments are conducted in the single shot setting, i.e. there is exactly one image of every person in the
gallery image bank and the results averaged over 10 test trials are reported using tables and Cumulative Matching
Characteristics (CMC) Curves (see supplementary). For all our experiments, we use a momentum of 0.9, starting
learning rate of 0.05, learning rate decay of 1 ? 10?4 , weight decay of 5 ? 10?4 . The implementation was done
in a machine with NVIDIA Titan GPUs and the code was implemented using Torch and is available online 1 . We
also conducted an ablation study, to further analyze the contribution of the individual components of our model.
CUHK03 Dataset: The CUHK03 dataset is a large collection of 13,164 images of 1360 people captured from 6
different surveillance cameras, with each person observed by 2 cameras with disjoint views [3]. The dataset
comes with manual and algorithmically labeled pedestrian bounding boxes. In this work, we conduct experiments
on both these sets. For our experiments, we follow the protocol used by Ahmed et al. [2] and randomly pick a
set of 1260 identities for training and 100 for testing. We use 100 identities from the training set for validation.
We compare the performance of both Normalized X-Corr and the Fused model with several baselines for both
labeled [2, 3, 5, 14, 18, 21, 22, 25] and detected [2, 3, 5, 14, 18, 22, 25] sets. Of these, the comparison with
Ahmed et al. [2] and with Li et al. [3] is of special interest to us since these are deep learning approaches as well.
For our models, we use mini-batch sizes of 128 and train our models for about 200,000 iterations.
CUHK01 Dataset: The CUHK01 dataset is a mid-sized collection of 3,884 images of 971 people, with each
person observed by 2 cameras with disjoint views [23]. There are 4 images of every identity. For our experiments,
we follow the protocol used by Ahmed et al. [2] and conduct 2 sets of experiments with varying training set
sizes. In the first, we randomly pick a set of 871 identities for training and 100 for testing, while in the second,
486 identities are used for testing and the rest for training. We compare the performance of both of our models
with several baselines for both 100 test identities [2, 3, 14, 18, 25] and 486 test identities [2, 8, 9, 20, 21]. For
our models, we use mini-batch sizes of 128 and train our models for about 50,000 iterations.
QMUL GRID Dataset: The QMUL underGround Re-Identification (GRID) dataset is a small and a very
challenging dataset [27]. It is a collection of only 250 people captured from 2 views. Besides, the 2 images
of every identity, there are 775 unmatched images, i.e. for these identities only 1 view is available. For our
experiments, we follow the protocol used by Liao and Li [5]. We randomly pick a set of 125 identities (who
have 2 views each) for training and leave the remaining 125 for testing. Additionally, the gallery image bank of
the test is enriched with the 775 unmatched images. This makes the ranking task even more challenging. We
compare the performance of both of our models with several baselines [11, 12, 15, 19, 22, 24]. For our models,
we use mini-batch sizes of 128 and train our models for about 20,000 iterations.
1

https://github.com/InnovArul/personreid_normxcorr

6

Table 2: Performance of different algorithms at ranks 1, 10, and 20 on CUHK01 100 Test Ids (left)
and 486 Test Ids (right) Datasets
Method
Fused Model (ours)
Norm X-Corr (ours)
Ahmed et al. [2]
Li et al. [3]
KISSME [18]
LDML [14]
eSDC [25]

4.2

r=1
81.23
77.43
65.00
27.87
29.40
26.45
22.84

r = 10
97.39
96.67
93.12
73.46
72.43
72.04
57.67

Method
Fused Model (ours)
Norm X-Corr (ours)
CPDL [8]
Ensembles [21]
Ahmed et al. [2]
Mirror-KFMA [20]
Mid-Level Filters [9]

r = 20
98.60
98.40
97.20
86.31
86.07
84.69
69.84

r=1
65.04
60.17
59.5
51.9
47.50
40.40
34.30

r = 10
89.76
86.26
89.70
83.00
80.00
75.3
65.00

r = 20
94.49
91.47
93.10
89.40
87.44
84.10
74.90

Training Strategies for the Neural Network

The large number of parameters of a deep neural network necessitate special training strategies [2]. In this work,
we adopt 3 main strategies to train our model.
Data Augmentation: For almost all the datasets, the number of negative pairs far outnumbers the number of
positive pairs in the training set. This poses a serious challenge to deep neural nets, which can overfit and get
biased in the process. Further, the positive samples may not have all the variations likely to be encountered in a
real scenario. We therefore, hallucinate positive pairs and enrich the training corpus, along the lines of Ahmed
et al. [2]. For every image in the training set of size W?H, we sample 2 images for CUHK03 (5 images for
CUHK01 & QMUL) around the original image center and apply 2D translations chosen from a uniform random
distribution in the range of [?0.05W, 0.05W ] ? [?0.05H, 0.05H]. We also augment the data with images
reflected on a vertical mirror.
Fine-Tuning: For small datasets such as QMUL, training parameter-intensive models such as deep neural
networks can be a significant challenge. One way to mitigate this issue is to fine-tune the model while training.
We start with a model pre-trained on a large dataset such as CUHK01 with 871 training identities rather than
an untrained model and then refine this pre-trained model by training on the small dataset, QMUL in our case.
During fine-tuning, we use a learning rate of 0.001.
Others: Training deep neural networks is time taking. Therefore, to speed up the training, we implemented our
code such that it spawns threads across multiple GPUs.

5

Results and Discussion

CUHK03 Dataset: Table 1 summarizes the results of the experiments on the CUHK03 Labeled dataset. Our
Fused model outperforms the existing state-of-the-art models by a wide margin, of about 10% (about 72% vs.
62%) on rank-1 accuracy, while the Normalized X-Corr model gives a 3% gain. This serves as a promising
response to our key research endeavor for an effective deep learning model for person re-identification. Further,
both models are significantly better than Ahmed et al.?s model [2]. We surmise that this is because our models
are more adept at handling variations in illumination, partial occlusion and viewpoint change.
Interestingly, we also note that the existing best performing system is a non-deep approach. This shows that
designing an effective deep learning architecture is a fairly non-trivial task. Our models? performance, viz.-a-viz.
non-deep methods, once again underscores the benefits of learning representations from the data rather than
using hand-engineered ones. A visualization of some filter responses of our model, some of the result plots, and
some ranked matching results may be found in the supplementary material.
Table 1 also presents the results on the CUHK03 Detected dataset. Here too, we see the superior performance of
our models over the existing state-of-the-art baselines. Interestingly, here our models take a wider lead over the
existing baselines (about 21%) and our models? performance rivals its own performance on the Labeled dataset.
We hypothesize that incorporating a wider search space makes our models more robust to the challenges posed
by images in which the person is not centered, such as the CUHK03 Detected dataset.
CUHK01 Dataset: Table 2 summarizes the results of the experiments on the CUHK01 dataset with 100 and
486 test identities. For the 486 test identity setting, our models were pre-trained on the training set of the larger
CUHK03 Labeled dataset and then fine-tuned on the CUHK01-486 training set, owing to the paucity of training
data. As the tables show, our models give us a gain of upto 16% over the existing state-of-the-art on the rank-1
accuracies.
QMUL GRID Dataset: QMUL GRID is a challenging dataset for person re-identification due to its small size
and the additional 775 unmatched gallery images in the test set. This is evident from the low performances of

7

Table 3: Performance of different algorithms at ranks 1, 5, 10, and 20 on the QMUL GRID Dataset
Method
Fused Model (ours)
Norm X-Corr (ours)
KEPLER [24]
LOMO+XQDA [22]
PolyMap [11]
MtMCML [19]
MRank-RankSVM [15]
MRank-PRDC [15]
LCRML [12]
XQDA [22]

r=1
19.20
16.00
18.40
16.56
16.30
14.08
12.24
11.12
10.68
10.48

r=5
38.40
32.00
39.12
33.84
35.80
34.64
27.84
26.08
25.76
28.08

r = 10
53.6
40.00
50.24
41.84
46.00
45.84
36.32
35.76
35.04
38.64

r = 20
66.4
55.2
61.44
52.40
57.60
59.84
46.56
46.56
46.48
52.56

Deep Learning Model
Yes
Yes
No
No
No
No
No
No
No
No

existing state-of-the-art algorithms. In order to train our models on this small dataset, we start with a model
trained on CUHK01 dataset with 100 test identities, then we fine-tune the models on the QMUL GRID training
set. Table 3 summarizes the results of the experiments on the QMUL GRID dataset. Here too, our Fused model
performs the best. Even though, our gain in rank-1 accuracy is a modest 1% but we believe this is significant for
a challenging dataset like QMUL.
The ablation study across multiple datasets reveals that a wider search and inexact match each buy us at least
6% individually, in terms of performance. The supplementary presents these results in more detail and also
compares the number of parameters across different models. Multi-GPU training, on the other hand gives us a
3x boost to training speed.

6

Conclusions and Future Work

In this work, we address the central research question of proposing simple yet effective deep-learning models for
Person Re-Identification by proposing two new models. Our models are capable of handling the key challenges
of illumination variations, partial occlusions and viewpoint invariance by incorporating inexact matching over a
wider search space. Additionally, the proposed Normalized X-Corr model benefits from having fewer parameters
than the state-of-the-art deep learning model. The Fused model, on the other hand, allows us to cut down on
false matches resulting from a wide matching search space, yielding superior performance.
For future work, we intend to use the proposed Siamese architectures for other matching tasks in Vision such
as content-based image retrieval. We also intend to explore the effect of incorporating more feature maps on
performance.

References
[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional
neural networks. In Advances in neural information processing systems, pages 1097?1105, 2012.
[2] Ejaz Ahmed, Michael Jones, and Tim K Marks. An improved deep learning architecture for person
re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 3908?3916, 2015.
[3] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deepreid: Deep filter pairing neural network for person
re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 152?159, 2014.
[4] David G Lowe. Distinctive image features from scale-invariant keypoints. International journal of
computer vision, 60(2):91?110, 2004.
[5] Shengcai Liao and Stan Z Li. Efficient psd constrained asymmetric metric learning for person reidentification. In Proceedings of the IEEE International Conference on Computer Vision, pages 3685?3693,
2015.
[6] Rui Zhao, Wanli Ouyang, and Xiaogang Wang. Person re-identification by salience matching. In
Proceedings of the IEEE International Conference on Computer Vision, pages 2528?2535, 2013.
[7] Michela Farenzena, Loris Bazzani, Alessandro Perina, Vittorio Murino, and Marco Cristani. Person
re-identification by symmetry-driven accumulation of local features. In Computer Vision and Pattern
Recognition (CVPR), 2010 IEEE Conference on, pages 2360?2367. IEEE, 2010.

8

[8] Sheng Li, Ming Shao, and Yun Fu. Cross-view projective dictionary learning for person re-identification. In
Proceedings of the 24th International Conference on Artificial Intelligence, AAAI Press, pages 2155?2161,
2015.
[9] Rui Zhao, Wanli Ouyang, and Xiaogang Wang. Learning mid-level filters for person re-identification. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 144?151, 2014.
[10] Bryan Prosser, Wei-Shi Zheng, Shaogang Gong, Tao Xiang, and Q Mary. Person re-identification by
support vector ranking. In BMVC, volume 2, page 6, 2010.
[11] Dapeng Chen, Zejian Yuan, Gang Hua, Nanning Zheng, and Jingdong Wang. Similarity learning on an
explicit polynomial kernel feature map for person re-identification. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 1565?1573, 2015.
[12] Jiaxin Chen, Zhaoxiang Zhang, and Yunhong Wang. Relevance metric learning for person re-identification
by exploiting global similarities. In Pattern Recognition (ICPR), 2014 22nd International Conference on,
pages 1657?1662. IEEE, 2014.
[13] Jason V Davis, Brian Kulis, Prateek Jain, Suvrit Sra, and Inderjit S Dhillon. Information-theoretic metric
learning. In Proceedings of the 24th international conference on Machine learning, pages 209?216. ACM,
2007.
[14] Matthieu Guillaumin, Jakob Verbeek, and Cordelia Schmid. Is that you? metric learning approaches for
face identification. In 2009 IEEE 12th International Conference on Computer Vision, pages 498?505.
IEEE, 2009.
[15] Chen Change Loy, Chunxiao Liu, and Shaogang Gong. Person re-identification by manifold ranking. In
2013 IEEE International Conference on Image Processing, pages 3567?3571. IEEE, 2013.
[16] Wei-Shi Zheng, Shaogang Gong, and Tao Xiang. Reidentification by relative distance comparison. IEEE
transactions on pattern analysis and machine intelligence, 35(3):653?668, 2013.
[17] Martin Hirzer, Peter M Roth, and Horst Bischof. Person re-identification by efficient impostor-based
metric learning. In Advanced Video and Signal-Based Surveillance (AVSS), 2012 IEEE Ninth International
Conference on, pages 203?208. IEEE, 2012.
[18] Martin K?stinger, Martin Hirzer, Paul Wohlhart, Peter M Roth, and Horst Bischof. Large scale metric
learning from equivalence constraints. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE
Conference on, pages 2288?2295. IEEE, 2012.
[19] Lianyang Ma, Xiaokang Yang, and Dacheng Tao. Person re-identification over camera networks using
multi-task distance metric learning. IEEE Transactions on Image Processing, 23(8):3656?3670, 2014.
[20] Ying-Cong Chen, Wei-Shi Zheng, and Jianhuang Lai. Mirror representation for modeling view-specific
transform in person re-identification. In Proc. IJCAI, pages 3402?3408. Citeseer, 2015.
[21] Sakrapee Paisitkriangkrai, Chunhua Shen, and Anton van den Hengel. Learning to rank in person reidentification with metric ensembles. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 1846?1855, 2015.
[22] Shengcai Liao, Yang Hu, Xiangyu Zhu, and Stan Z Li. Person re-identification by local maximal occurrence
representation and metric learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2197?2206, 2015.
[23] Wei Li, Rui Zhao, and Xiaogang Wang. Human reidentification with transferred metric learning. In Asian
Conference on Computer Vision, pages 31?44. Springer, 2012.
[24] Niki Martinel, Christian Micheloni, and Gian Luca Foresti. Kernelized saliency-based person reidentification through multiple metric learning. IEEE Transactions on Image Processing, 24(12):5645?
5658, 2015.
[25] Rui Zhao, Wanli Ouyang, and Xiaogang Wang. Unsupervised salience learning for person re-identification.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3586?3593,
2013.
[26] Dong Yi, Zhen Lei, Shengcai Liao, Stan Z Li, et al. Deep metric learning for person re-identification. In
ICPR, volume 2014, pages 34?39, 2014.
[27] Chen Change Loy, Tao Xiang, and Shaogang Gong. Multi-camera activity correlation analysis. In
Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 1988?1995.
IEEE, 2009.

9

"
1116,2001,The Fidelity of Local Ordinal Encoding,Abstract Missing,"The Fidelity of Local Ordinal Encoding

Javid Sadr, Sayan Mukherjee, Keith Thoresz, Pawan Sinha
Center for Biological and Computational Learning
Department of Brain and Cognitive Sciences, MIT
Cambridge, Massachusetts, 02142 USA
{sadr,sayan,thorek,sinha}@ai.mit.edu

Abstract
A key question in neuroscience is how to encode sensory stimuli
such as images and sounds. Motivated by studies of response properties of neurons in the early cortical areas, we propose an encoding
scheme that dispenses with absolute measures of signal intensity
or contrast and uses, instead, only local ordinal measures. In this
scheme, the structure of a signal is represented by a set of equalities
and inequalities across adjacent regions. In this paper, we focus
on characterizing the fidelity of this representation strategy. We
develop a regularization approach for image reconstruction from
ordinal measures and thereby demonstrate that the ordinal representation scheme can faithfully encode signal structure. We also
present a neurally plausible implementation of this computation
that uses only local update rules. The results highlight the robustness and generalization ability of local ordinal encodings for the
task of pattern classification.

1

Introduction

Biological and artificial recognition systems face the challenge of grouping together
differing proximal stimuli arising from the same underlying object. How well the
system succeeds in overcoming this challenge is critically dependent on the nature
of the internal representations against which the observed inputs are matched. The
representation schemes should be capable of efficiently encoding object concepts
while being tolerant to their appearance variations.
In this paper, we introduce and characterize a biologically plausible representation
scheme for encoding signal structure. The scheme employs a simple vocabulary
of local ordinal relations, of the kind that early sensory neurons are capable of
extracting. Our results so far suggest that this scheme possesses several desirable
characteristics, including tolerance to object appearance variations, computational
simplicity, and low memory requirements. We develop and demonstrate our ideas in
the visual domain, but they are intended to be applicable to other sensory modalities
as well.
The starting point for our proposal lies in studies of the response properties of
neurons in the early sensory cortical areas. These response properties constrain

Figure 1: (a) A schematic contrast response curve for a primary visual cortex
neuron. The response of the neuron saturates at low contrast values. (b) An
idealization of (a). This unit can be thought of as an ordinal comparator, providing
information only about contrast polarity but not its magnitude.
the kinds of measurements that can plausibly be included in our representation
scheme. In the visual domain, many striate cortical neurons have rapidly saturating
contrast response functions [1, 4]. Their tendency to reach ceiling level responses at
low contrast values render these neurons sensitive primarily to local ordinal, rather
than metric, relations. We propose to use an idealization of such units as the basic
vocabulary of our representation scheme (figure 1). In this scheme, objects are
encoded as sets of local ordinal relations across image regions. As discussed below,
this very simple idea seems well suited to handling the photometric appearance
variations that real-world objects exhibit.

Figure 2: The challenge for a representation scheme: to construct stable descriptions
of objects despite radical changes in appearance.
As figure 2 shows, variations in illumination significantly alter the individual brightness of different parts of the face, such as the eyes, cheeks, and forehead. Therefore,
absolute image brightness distributions are unlikely to be adequate for classifying
all of these images as depicting the same underlying object. Even the contrast
magnitudes across different parts of the face change greatly under different lighting
conditions. While the absolute luminance and contrast magnitude information is
highly variable across these images, Thoresz and Sinha [9] have shown that one can
identify some stable ordinal measurements. Figure 3 shows several pairs of average
brightness values over localized patches for each of the three images included in
figure 2. Certain regularities are apparent. For instance, the average brightness
of the left eye is always less than that of the forehead, irrespective of the lighting
conditions. The relative magnitudes of the two brightness values may change, but
the sign of the inequality does not. In other words, the ordinal relationship between
the average brightnesses of the <left-eye, forehead> pair is invariant under lighting
changes. Figure 3 shows several other such pair-wise invariances. It seems, therefore
that local ordinal relations may encode the stable facial attributes across different
illumination conditions. An additional advantage to using ordinal relations is their
natural robustness to sensor noise. Thus, it would seem that local ordinal representations may be well suited for devising compact representations, robust against

Figure 3: The absolute brightnesses and their relative magnitudes change under different lighting conditions but several pair-wise ordinal relationships stay invariant.
large photometric variations, for at least some classes of objects. Notably, for similar reasons, ordinal measures have also been shown to be a powerful tool for simple,
efficient, and robust stereo image matching [3].
In what follows, we address an important open question regarding the expressiveness of the ordinal representation scheme. Given that this scheme ignores absolute
luminance and contrast magnitude information, an obvious question that arises is
whether such a crude representation strategy can encode object/image structure
with any fidelity.

2

Information Content of Local Ordinal Encoding

Figure 4 shows how we define ordinal relations between an image region pa and
its immediate neighbors pb = {pa1 , . . . , pa8 }. In the conventional rectilinear grid,
when all image regions pa are considered, four of the eight relations are redundant;
we encode the remaining four as {1, 0, ?1} based on the difference in luminance
between two neighbors being positive, zero, or negative, respectively. To demonstrate the richness of information encoded by this scheme, we compare the original
image to one produced by a function that reconstructs the image using local ordinal
relationships as constraints. Our reconstruction function has the form
f (x) = w ? ?(x),
(1)
where x = {i, j} is the position of a pixel, f (x) is its intensity, ? is a map from the
input space into a high (possibly infinite) dimensional space, w is a hyperplane in
this high-dimensional space, and u ? v denotes an inner product.
Infinitely many reconstruction functions could satisfy the given ordinal constraints.
To make the problem well-posed we regularize [10] the reconstruction function subject to the ordinal constraints, as done in ordinal regression for ranking document

Department of Brain Sciences, MIT
Cambridge, Massachusetts, USA.
{sadr,sayan,thorek,sinha}@ai.mit.edu
Neighbors? relations to pixel of interest
???????????????????????????I(pa )

<
=
<
<
>
<
<
<

I(pa1 )
I(pa2 )
I(pa3 )
I(pa4 )
I(pa5 )
I(pa6 )
I(pa7 )
I(pa8 )
(1)

Figure 4: Ordinal relationships between
an image region pa and its neighbors.
???????????????????????????retrieval results [5]. Our regularization term is a norm in a Reproducing Kernel
Hilbert Space (RKHS) [2, 11]. Minimizing the norm in a RKHS subject to the
ordinal constraints corresponds to the following convex constrained quadratic optimization problem:
X
1
min ||w||2 + C
?p
(2)
?,w 2
p
subject to
?(?p )w ? (?(xpa ) ? ?(xpb )) ? |?p | ? ?p , ? p and ? ? 0,

(3)

where the function ?(y) = +1 for y ? 0 and ?1 otherwise, p is the index over
all pairwise ordinal relations between all pixels pa and their local neighbors pb (as
depicted in figure 4), ?p are slack variables which are penalized by C (the trade-off
between smoothness and ordinal constraints), and ?p take integer values {?1, 0, 1}
denoting the ordinal relation (less than, equal to, or greater than, respectively)
between pa and pb ; for the case ?p = 0 the inequality in (3) will be a strict equality.
Taking the dual of (2) subject to constraints (3) results in the following convex
quadratic optimization problem which has only box constraints:
X
1 XX
? pq
?p ?q K
(4)
max
|?p | ?p ?
?
2 p q
p
subject to
0 ? ?p ? C
?C ? ?p ? C
?C ? ?p ? 0

if
if
if

?p > 0,
?p = 0,
?p < 0,

(5)

? have
where ?p are the dual Lagrange multipliers, and the elements of the matrix K
the form
? pq
K

= (?(xpa ) ? ?(xpb )) ? (?(xqa ) ? ?(xqb ))
= K(xpa , xqa ) ? K(xpb , xqa ) ? K(xpa , xqb ) + K(xpb , xqb ),

where K(y, x) = ?(y)??(x) using the standard kernel trick [8]. In this paper we use
only Gaussian kernels K(y, x) = exp(?||x?y||2 /2? 2 ). The reconstruction function,
f (x), obtained from optimizing (4) subject to box constraints (5) has the following
form
X
f (x) =
?p (K(x, xpa ) ? K(x, xpb )) .
(6)
p

Note that in general many of the ?p values may be zero ? these terms do not
contribute to the reconstruction, and the corresponding constraints in (3) were not

300
200
100
0

0

128

255

128

255

300
200
100
0

(a)

(b)

(c)

0

(d)

Figure 5: Reconstruction results from the regularization approach. (a) Original
images. (b) Reconstructed images. (c) Absolute difference between original and
reconstruction. (d) Histogram of absolute difference.
required. The remaining ?p with absolute value less than C satisfy the inequality
constraints in (3), whereas those with absolute value at C violate them.
Figure 5 depicts two typical reconstructions performed by this algorithm. The
difference images and error histograms suggests that the reconstructions closely
match the source images.

3

Discussion

Our reconstruction results suggest that the local ordinal representation can faithfully encode image structure. Thus, even though individual ordinal relations are
insensitive to absolute luminance or contrast magnitude, a set of such relations implicitly encodes metric information. In the context of the human visual system, this
result suggests that the rapidly saturating contrast response functions of the early
visual neurons do not significantly hinder their ability to convey accurate image
information to subsequent cortical stages.
An important question that arises here is what are the strengths and limitations of
local ordinal encoding. The first key limitation is that for any choice of neighborhood size over which ordinal relations are extracted, there are classes of images for
which the local ordinal representation will be unable to encode the metric structure. For a neighborhood of size n, an image with regions of different luminance
embedded in a uniform background and mutually separated by a distance greater
than n would constitute such an image. In general, sparse images present a problem for this representation scheme, as might foveal or cortical ?magnification,? for
example. This issue could be addressed by using ordinal relations across multiple
scales, perhaps in an adaptive way that varies with the smoothness or sparseness of
the stimulus.
Second, the regularization approach above seems biologically implausible. Our intent in using this approach for reconstructions was to show via well-understood
theoretical tools the richness of information that local ordinal representations pro-

Figure 6: Reconstruction results from the relaxation approach.
vide. In order to address the neural plausibility requirement, we have developed a
simple relaxation-based approach with purely local update rules of the kind that
can easily be implemented by cortical circuitry. Each unit communicates only with
its immediate neighbors and modifies its value incrementally up or down (starting
from an arbitrary state) depending on the number of ordinal relations in the positive
or negative direction. This computation is performed iteratively until the network
settles to an equilibrium state. The update rule can be formally stated as
X
(?(Rpa ,t ? Rpb ,t ) ? ?(Ipa ? Ipb )),
(7)
Rpa ,t+1 = Rpa ,t + ?
pb

where Rpa ,t is the intensity of the reconstructed pixel pa at step t, Ipa is the intensity of the corresponding pixel in the original image, ? is a positive update
rate, and ? and pb are as described above. Figure 6 shows four examples of image
reconstructions performed using a relaxation-based approach.
A third potential limitation is that the scheme does not appear to constitute a
compact code. If each pixel must be encoded in terms of its relations with all of
its eight neighbors, where each relation takes one of three values, {?1, 0, 1}, then
what has been gained over the original image where each pixel is encoded by 8 bits?
There are three ways to address this question.
1. Eight relations per pixel is highly redundant ? four are sufficient. In fact, as
shown in figure 7, the scheme can also tolerate several missing relations.

Figure 7: Five reconstructions, shown here to demonstrate the robustness of local
ordinal encoding to missing inputs. From left to right: reconstructions based on
100%, 80%, 60%, 40%, and 20% of the full set of immediate neighbor relations.
2. An advantage to using ordinal relations is that they can be extracted and transmitted much more reliably than metric ones. These relations share the same spirit

(a)

(b)

Figure 8: A small collection of ordinal relations (a), though insufficient for high
fidelity reconstruction, is very effective for pattern classification despite significant
appearance variations. (b) Results of using a local ordinal relationship based template to detect face patterns. The program places white dots at the centers of
patches classified as faces. (From Thoresz and Sinha, in preparation.)
as loss functions used in robust statistics [6] and trimmed or Winsorized estimators.
3. The intent of the visual system is often not to encode/reconstruct images with
perfect fidelity, but rather to encode the most stable characteristics that can aid in
classification. In this context, a few ordinal relations may suffice for encoding objects
reliably. Figure 8 shows the results of using less than 20 relations for detecting faces.
Clearly, such a small set would not be sufficient for reconstructions, but it works
well for classification. Its generalization arises because it defines an equivalence
class of patterns.
In summary, the ordinal representation scheme provides a neurally plausible strategy for encoding signal structure. While in this paper we focus on demonstrating
the fidelity of this scheme, we believe that its true strength lies in defining equivalence classes of patterns enabling generalizations over appearance variations in
objects. Several interesting directions remain to be explored. These include the
study of ordinal representations across multiple scales, learning schemes for identifying subsets of ordinal relations consistent across different instances of an object,
and the relationship of this work to multi-dimensional scaling [12] and to the use
of truncated, quantized wavelet coefficients as ?signatures? for fast, multiresolution
image querying [7].

Acknowledgements
We would like to thank Gadi Geiger, Antonio Torralba, Ryan Rifkin, Gonzalo Ramos, and
Tabitha Spagnolo. Javid Sadr is a Howard Hughes Medical Institute Pre-Doctoral Fellow.

References
[1] A. Anzai, M. A. Bearse, R. D. Freeman, and D. Cai. Contrast coding by cells in
the cat?s striate cortex: monocular vs. binocular detection. Visual Neuroscience,
12:77?93, 1995.
[2] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc., 686:337?404,
1950.
[3] D. Bhat and S. Nayar. Ordinal measures for image correspondence. In IEEE Conf.
on Computer Vision and Pattern Recognition, pages 351?357, 1996.

[4] G. C. DeAngelis, I. Ohzawa, and R. D. Freeman. Spatiotemporal organization of
simple-cell receptive fields in the cat?s striate cortex. i. general characteristics and
postnatal development. J. Neurophysiology, 69:1091?1117, 1993.
[5] R. Herbrich, T. Graepel, and K. Obermeyer. Support vector learning for ordinal
regression. In Proc. of the Ninth Intl. Conf. on Artificial Neural Networks, pages
97?102, 1999.
[6] P. Huber. Robust Statistics. John Wiley and Sons, New York, 1981.
[7] C. E. Jacobs, A. Finkelstein, and D. H. Salesin. Fast multiresolution image querying.
In Computer Graphics Proc., Annual Conf. Series (SIGGRAPH 95), pages 277?286,
1995.
[8] T. Poggio. On optimal nonlinear associative recall. Biological Cybernetics, 19:201?209,
1975.
[9] K. Thoresz and P. Sinha. Qualitative representations for recognition. Vision Sciences
Society Abstracts, 1:81, 2001.
[10] A. N. Tikhonov and V. Y. Arsenin. Solutions of Ill-posed Problems. W. H. Winston,
Washington, D.C., 1977.
[11] G. Wahba. Spline Models for Observational Data. Series in Applied Mathematics,
Vol 59, SIAM, Philadelphia, 1990.
[12] F. W. Young and C. H. Null. Mds of nominal data: the recovery of metric information
with alscal. Psychometika, 53.3:367?379, 1978.

"
5304,2015,"Reflection, Refraction, and Hamiltonian Monte Carlo","Hamiltonian Monte Carlo (HMC) is a successful approach for sampling from continuous densities. However, it has difficulty simulating Hamiltonian dynamics with non-smooth functions, leading to poor performance. This paper is motivated by the behavior of Hamiltonian dynamics in physical systems like optics. We introduce a modification of the Leapfrog discretization of Hamiltonian dynamics on piecewise continuous energies, where intersections of the trajectory with discontinuities are detected, and the momentum is reflected or refracted to compensate for the change in energy. We prove that this method preserves the correct stationary distribution when boundaries are affine. Experiments show that by reducing the number of rejected samples, this method improves on traditional HMC.","Reflection, Refraction, and Hamiltonian Monte Carlo

Justin Domke
National ICT Australia (NICTA) &
Australian National University
Canberra, ACT 0200
Justin.Domke@nicta.com.au

Hadi Mohasel Afshar
Research School of Computer Science
Australian National University
Canberra, ACT 0200
hadi.afshar@anu.edu.au

Abstract
Hamiltonian Monte Carlo (HMC) is a successful approach for sampling from continuous densities. However, it has difficulty simulating Hamiltonian dynamics
with non-smooth functions, leading to poor performance. This paper is motivated
by the behavior of Hamiltonian dynamics in physical systems like optics. We introduce a modification of the Leapfrog discretization of Hamiltonian dynamics on
piecewise continuous energies, where intersections of the trajectory with discontinuities are detected, and the momentum is reflected or refracted to compensate for
the change in energy. We prove that this method preserves the correct stationary
distribution when boundaries are affine. Experiments show that by reducing the
number of rejected samples, this method improves on traditional HMC.

1

Introduction

Markov chain Monte Carlo sampling is among the most general methods for probabilistic inference.
When the probability distribution is smooth, Hamiltonian Monte Carlo (HMC) (originally called
hybrid Monte Carlo [4]) uses the gradient to simulate Hamiltonian dynamics and reduce random
walk behavior. This often leads to a rapid exploration of the distribution [7, 2]. HMC has recently
become popular in Bayesian statistical inference [13], and is often the algorithm of choice.
Some problems display piecewise smoothness, where the density is differentiable except at certain
boundaries. Probabilistic models may intrinsically have finite support, being constrained to some
region. In Bayesian inference, it might be convenient to state a piecewise prior. More complex and
highly piecewise distributions emerge in applications where the distributions are derived from other
distributions (e.g. the distribution of the product of two continuous random variables [5]) as well as
applications such as preference learning [1], or probabilistic programming [8].
While HMC is motivated by smooth distributions, the inclusion of an acceptance probability means
HMC does asymptotically sample correctly from piecewise distributions1 . However, since leapfrog
numerical integration of Hamiltonian dynamics (see [9]) relies on the assumption that the corresponding potential energy is smooth, such cases lead to high rejection probabilities, and poor performance. Hence, traditional HMC is rarely used for piecewise distributions.
In physical systems that follow Hamiltonian dynamics [6], a discontinuity in the energy can result
in two possible behaviors. If the energy decreases across a discontinuity, or the momentum is large
enough to overcome an increase, the system will cross the boundary with an instantaneous change
in momentum, known as refraction in the context of optics [3]. If the change in energy is too large to
be overcome by the momentum, the system will reflect off the boundary, again with an instantaneous
change in momentum.
1
Technically, here we assume the total measure of the non-differentiable points is zero so that, with probability one, none is ever encountered

1

6

6

6

4

4

2

2

0.

2

0.

2

0.25

4
25

0.

reflection

refraction

0.75

0.8

2

0.75

0.85

2

0

q

q2

0.25

0.8

0.95

0

0.75

q2

0.9

0

0.9

?2

0.8

0.85

0.

?2

?2

?4

?4

25

0.75

?4
0.25
0.

0.

2

?6
?6

?4

?2

0
q1

2

4

2

6

?6
?6

?4

(a)

?2

0
q1

(b)

2

4

6

?6
?6

?4

?2

0
q1

2

4

6

(c)

Figure 1: Example trajectories of baseline and reflective HMC. (a) Contours of the target distribution in
two dimensions, as defined in Eq. 18. (b) Trajectories of the rejected (red crosses) and accepted (blue dots)
proposals using baseline HMC. (c) The same with RHMC. Both use leapfrog parameters L = 25 and  = 0.1
In RHMC, the trajectory reflects or refracts on the boundaries of the internal and external polytope boundaries
and thus has far fewer rejected samples than HMC, leading to faster mixing in practice. (More examples in
supplementary material.)

Recently, Pakman and Paninski [11, 10] proposed methods for HMC-based sampling from piecewise
Gaussian distributions by exactly solving the Hamiltonian equations, and accounting for what we
refer to as refraction and reflection above. However, since Hamiltonian equations of motion can
rarely be solved exactly, the applications of this method are restricted to distributions whose logdensity is piecewise quadratic.
In this paper, we generalize this work to arbitrary piecewise continuous distributions, where each
region is a polytope, i.e. is determined by a set of affine boundaries. We introduce a modification to
the leapfrog numerical simulation of Hamiltonian dynamics, called Reflective Hamiltonian Monte
Carlo (RHMC), by incorporating reflection and refraction via detecting the first intersection of a
linear trajectory with a boundary. We prove that our method has the correct stationary distribution,
where the main technical difficulty is proving volume preservation of our dynamics to establish detailed balance. Numerical experiments confirm that our method is more efficient than baseline HMC,
due to having fewer rejected proposal trajectories, particularly in high dimensions. As mentioned,
the main advantage of this method over [11] and [10] is that it can be applied to arbitrary piecewise densities, without the need for a closed-form solution to the Hamiltonian dynamics, greatly
increasing the scope of applicability.

2

Exact Hamiltonian Dynamics

Consider a distribution P (q) ? exp(?U (q)) over Rn , where U is the potential energy.
HMC [9] is based on considering a joint distribution on momentum and position space
P (q, p) ? exp(?H(q, p)), where H(q, p) = U (q) + K(p), and K is a quadratic, meaning that
P (p) ? exp(?K(p)) is a normal distribution. If one could exactly simulate the dynamics, HMC
would proceed by (1) iteratively sampling p ? P (p), (2) simulating the Hamiltonian dynamics
dqi
?H
=
= pi
dt
?pi

dpi
?H
?U
=?
=?
dt
?qi
?qi

(1)

(2)

for some period of time , and (3) reversing the final value p. (Only needed for the proof of correctness, since this will be immediately discarded at the start of the next iteration in practice.)
Since steps (1) and (2-3) both leave the distribution P (p, q) invariant, so does a Markov chain that
alternates between the two steps. Hence, the dynamics have P (p, q) as a stationary distribution. Of
course, the above differential equations are not well-defined when U has discontinuities, and are
typically difficult to solve in closed-form.
2

3

Reflection and Refraction with Exact Hamiltonian Dynamics

Take a potential function U (q) which is differentiable in all points except at some boundaries of
partitions. Suppose that, when simulating the Hamiltonian dynamics, (q, p) evolves over time as in
the above equations whenever these equations are differentiable. However, when the state reaches a
boundary, decompose the momentum vector p into a component p? perpendicular to the boundary
and a component pk parallel to the boundary. Let ?U be the (signed) difference in potential energy
on the p
two sides of the discontinuity. If kp? k2 > 2?U then p? is instantaneously replaced by
p
0
p? := kp? k2 ? 2?U ? kp? k . That is, the discontinuity is passed, but the momentum is changed
?
in the direction perpendicular to the boundary (refraction). (If ?U is positive, the momentum will
decrease, and if it is negative, the momentum will increase.) On the other hand, if kp? k2 ? 2?U ,
then p? is instantaneously replaced by ?p? . That is, if the particle?s momentum is insufficient
to climb the potential boundary, it bounces back by reversing the momentum component which is
perpendicular to the boundary.
Pakman and Paninski [11, 10] present an algorithm to exactly solve these dynamics for quadratic
U . However, for non-quadratic U , the Hamiltonian dynamics rarely have a closed-form solution,
and one must resort to numerical integration, the most successful method for which is known as the
leapfrog dynamics.

4

Reflection and Refraction with Leapfrog Dynamics

Informally, HMC with leapfrog dynamics iterates three steps. (1) Sample p ? P (p). (2) Perform
leapfrog simulation, by discretizing the Hamiltonian equations into L steps using some small stepsize . Here, one interleaves a position step q ? q + p between two half momentum steps p ?
p ? ?U (q)/2. (3) Reverse the sign of p. If (q, p) is the starting point of the leapfrog dynamics,
and (q0 , p0 ) is the final point, accept the move with probability min(1, exp(H(p, q) ? H(p0 , q0 )))
See Algorithm 1.
It can be shown that this baseline HMC method has detailed balance with respect to P (p), even if
U (q) is discontinuous. However, discontinuities mean that large changes in the Hamiltonian may
occur, meaning many steps can be rejected. We propose a modification of the dynamics, namely,
reflective Hamiltonian Monte Carlo (RHMC), which is also shown in Algorithm 1.
The only modification is applied to the position steps: In RHMC, the first intersection of the trajectory with the boundaries of the polytope that contains q must be detected [11, 10]. The position step
is only taken up to this boundary, and reflection/refraction occurs, depending on the momentum and
change of energy at the boundary. This process continues until the entire amount of time  has been
simulated. Note that if there is no boundary in the trajectory to time , this is equivalent to baseline
HMC. Also note that several boundaries might be visited in one position step.
As with baseline HMC, there are two alternating steps, namely drawing a new momentum variable
p from P (p) ? exp(?K(p)) and proposing a move (p, q) ? (p0 , q0 ) and accepting or rejecting it
with a probability determined by a Metropolis-Hastings ratio. We can show that both of these steps
leave the joint distribution P invariant, and hence a Markov chain that also alternates between these
steps will also leave P invariant.
As it is easy to see, drawing p from P (p) will leave P (q, p) invariant, we concentrate on the second
step i.e. where a move is proposed according to the piecewise leapfrog dynamics shown in Alg. 1.
Firstly, it is clear that these dynamics are time-reversible, meaning that if the simulation takes state
(q, p) to (q0 , p0 ) it will also take state (q0 , p0 ) to (q, p). Secondly, we will show that these dynamics
are volume preserving. Formally, if D denotes the leapfrog dynamics, we will show that the absolute
value of the determinant of the Jacobian of D is one. These two properties together show that the
probability density of proposing a move from (q, p) to (q0 , p0 ) is the same of that proposing a move
from (q0 , p0 ) to (q, p). Thus, if the move (q, p) ? (q0 , p0 ) is accepted according to the standard
Metropolis-Hastings ratio, R ((q, p) ? (q0 , p0 )) = min(1, exp(H(q, p) ? H(q0 , p0 )), then detailed
balance will be satisfied. To see this, let Q denote the proposal distribution, Then, the usual proof of
correctness for Metropolis-Hastings applies, namely that
3

P (q, p)Q ((q, p) ? (q0 , p0 )) R ((q, p) ? (q0 , p0 ))
P (q0 , p0 )Q((q0 , p0 ) ? (q, p))R((q0 , p0 ) ? (q, p))
P (q, p) min(1, exp(H(q, p) ? H(q0 , p0 ))
= 1. (3)
=
P (q0 , p0 ) min(1, exp(H(q0 , p0 ) ? H(q, p))
(The final equality is easy to establish, considering the cases where H(q, p) ? H(q0 , p0 ) and
H(q0 , p0 ) ? H(q, p) separately.) This means that
detailed balance holds, and so P is a stationary distribution.

q0
0

p

x
The major difference in the analysis of RHMC, relap
tive to traditional HMC is that showing conservation
of volume is more difficult. With standard HMC
q
and leapfrog steps, volume conservation is easy to
show by observing that each part of a leapfrog step
q1 = c
is a shear transformation. This is not the case with
RHMC, and so we must resort to a full analysis of
the determinant of the Jacobian, as explored in the Figure 2: Transformation T : hq,pi ? hq0 ,p0 i
following section.
described by Lemma 1 (Refraction).
Algorithm 1: BASELINE & R EFLECTIVE HMC A LGORITHMS
input : q0 , current sample; U , potential function, L, # leapfrog steps; , leapfrog step size
output: next sample
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

begin
q ? q0 ; p ? N (0, 1)
H0 ? kpk2 /2 + U (q)
for l = 1 to L do
p ? p ? ?U (q)/2

# Half-step evolution of momentum

if BASELINE HMC then
# Full-step evolution of position:
q ? q + p
# i.e. if R EFLECTIVE HMC:
else
t0 ? 0

while hx, tx , ?U, ?i ? F IRST D ISCONTINUITY(q, p,  ? t0 , U ) 6= ? do
q?x
t0 ? t0 + tx
hp? , pk i = DECOMPOSE(p, ?)
# Perpendicular/ parallel to boundary plane ?
if kp? k2 >p2?U then
p
# Refraction
p? ? kp? k2 ? 2?U ? kp? k
?
else
p? ? ?p?
# Reflection

18
19
20
21
22
23
24
25

p ? p? + pk
q ? q + ( ? t0 )p
p ? p ? ?U (q)/2
# Half-step evolution of momentum
p ? ?p
# Not required in practice; for reversibility proof
H ? kpk2 /2 + U (q); ?H ? H ? H0
if s ? U(0, 1) < e??H return q else return q0
end
note : F IRST D ISCONTINUITY(?) returns x, the position of the first intersection of a boundary plain
with line segment [q, q + ( ? t0 )p]; tx , the time it is visited; ?U , the change in energy at the
discontinuity, and ?, the visited partition boundary. If no such point exists, ? is returned.

4

5

Volume Conservation

5.1

Refraction

In our first result, we assume without loss of generality, that there is a boundary located at the
hyperplane q1 = c. This Lemma shows that, in the refractive case, volume is conserved. The setting
is visualized in Figure 2.
Lemma 1. Let T : hq, pi ? hq0 , p0 i be a transformation in Rn that takes a unit mass located at
q := (q1 , . . . , qn ) and moves it with constant momentum p := (p1 , . . . , pn ) till it reaches a plane
q1 = c (at some point x := (c, x2 , . . . , xn ) where
 c is a constant). Subsequently the momentum is
p
0
2
changed to p =
p1 ? 2?U (x), p2 , . . . , pn (where ?U (?) is a function of x s.t. p21 > 2?U (x)).
The move is carried on for the total time period ? till it ends in q0 . For all n ? N, T satisfies the
volume preservation property.
Proof. Since for i > 1, the momentum is not affected by the collision, qi0 = qi + ? ? pi and p0i = pi .
Thus,
?qi0
?qi0
?p0i
?p0i
?j ? {2, . . . , n}s.t. j 6= i,
=
=
=
= 0.
?qj
?pj
?qj
?pj
Therefore, if we explicitly write out the Jacobian determinant |J| of the transformation T , it is
 ?q0
 

?q10
?q10
?q10
?q10

  ?q10 ?q10
1
?q10
?q10
?q10

? ? ? ?pk?1
 ?q1
  ?q
? ? ? ?pk?1
?p1
?qk
?pk

?p1
?qk
?pk
1
0
0
0
0
 ?p01

?p
?p
?p
?p
0
0
0
0
0


1
1
1
1
?p1
?p1
?p1

  ?p1 ?p1 ? ? ?
? ? ? ?pk?1

?p1
?qk
?pk
 ?q10

?q1
?p1
?pk?1
?qk
?pk


0
0
0
0
 ?q2
 
?q2
?q2
?q2
?q2

?q20
?q20
?q20
 ?q1

?
?
?


0
0
?
?
?
?p
?p
?q
?p
1
k?1
k
k

 
?pk?1
?qk
?pk



..
.
.
.


.
.
.
..
..
..
..

= .
..
..
.

.
.


0
0
0
0
0


?pk?1
?pk?1
?pk?1 
 ?pk?1 ?pk?1
?p0k?1 
?p0k?1

?
?
?
 ?q1

0
0
?
?
?
1
?p
?p
?q
?p


1
k?1
k
k
 ?q0
 
?qk
?pk
0
0
0
0

0
?qk
?qk
?qk
?qk

 
k
?qk

?
?
?
 ?q1
  0
0
???
0
1
?p1
?pk?1
?qk
?pk

?pk
0
0
0
0
 ?p0k

?p
?p
?p
?p

k
k
k


0
0
???
0
0
1 
? ? ? ?p k
?q
?p
?q
?p
1

1

k?1

k

k

(4)



Now, using standard properties of the determinant, we have that |J| = 


?q10
?q1
?p01
?q1

?q10
?p1
?p01
?p1




.


We will now explicitly calculate these four derivatives. Due to the significance of the result, we carry
out the computations in detail. Nonetheless, as this is a largely mechanical process, for brevity, we
do not comment on the derivation.
Let t1 be the time to reach x and t2 be the period between reaching x and the last point q0 . Then:
def

t1 =

c ? q1
p1

x = q + t1 p

(5)

q10 = c + p01 ? t2 (8)

def

p01 =

q

def

(6)

t2 = ? ? t1 = ? +

p21 ? 2?U (x) (9)

?t2 by (7) 1
=
?q1
p1

?q10
?q 0 ?p0
?q 0 ?t2
= 10 ? 1 + 1 ?
?q1
?p1 ?q1
?t2 ?q1
?q10
?q 0 ?p0
?q 0 ?t2
= 10 ? 1 + 1 ?
?p1
?p1 ?p1
?t2 ?p1

(8 & 10)

=

(7 & 8)

(10)

(11)

?p01
c ? q1
+ p01 ?
?p1
p21

(12)


? p21 ? 2?U (x)
?p01 (9)
1
p1 ? ??U (x)/?p1
= p 2
?
=
?p1
?p1
p01
2 p1 ? 2?U (x)
5

(7)

?p01
1
+ p01 ?
?q1
p1

t2 ?

= t2 ?

q1 ? c
p1

(13)


? p21 ? 2?U (x)
?p01
1
1 ???U (x)
?
= p 2
= 0 ?
?q1
?q1
p1
?q1
2 p1 ? 2?U (x)

(14)



c?q1
?x (5, 6) ? q + p1 p
?q
?(p/p1 )
?1
=
=
+ (c ? q1 ) ?
= (c ? q1 ) 2 ? (0, p2 , p3 , . . . , pn ) (15)
?p1
?p1
?p1
?p1
p1
?x (5, 6) ?q
p ?(c ? q1 )
q
p
p2
pn
?1
=
+
?
=
?
= (1, 0, . . . , 0) ? (1, , . . . , ) =
(0, p2 , . . . , pn )
?q1
?q1
p1
?q1
q1
p1
p1
p1
p1
(16)
?q 0 ?p0

Substituting the appropriate terms into |J| = | ?q11 ?p11 ?

?p01 ?q10
?q1 ?p1 |,

we get that





?q10 ?p01
?q10 ?p01 (11 & 12)
?p01
p01
?p01
?p01
?p0
0 c ? q1
|J| =
?
?
?
=
t2
+
?
? t2
+ p1 2
? 1
?q1 ?p1
?p1 ?q1
?q1
p1
?p1
?p1
p1
?q1
 0



0
0
p
?p1
q1 ? c ?p1 (13 & 14) 1
??U (x) q1 ? c ??U (x)
= 1
+
?
=
p1 ?
?
?
p1 ?p1
p1
?q1
p1
?p1
p1
?q1


q1 ? c ??U (x) ?x
1 ??U (x) ?x
?
+
?
?
=1?
p1
?x
?p1
p1
?x
?q1


1 ??U (x) q1 ? c
q1 ? c ?1
(15 & 16)
= 1?
?
(0,
p
,
p
,
.
.
.
,
p
)
+
?
?
(0,
p
,
.
.
.
,
p
)
= 1.
2 3
n
2
n
p1
?x
p21
p1
p1
(4)

5.2

Reflection

Now, we turn to the reflective case, and again show that volume is conserved. Again, we assume
without loss of generality that there is a boundary located at the hyperplane q1 = c.
Lemma 2. Let T : hq, pi ? hq0 , p0 i be a transformation in Rn that takes a unit mass located at
q := (q1 , . . . , qn ) and moves it with the constant momentum p := (p1 , . . . , pn ) till it reaches a plane
q1 = c (at some point x := (c, x2 , . . . , xn ) where c is a constant). Subsequently the mass is bounced
back (reflected) with momentum p0 = (?p1 , p2 , . . . , pn ) The move is carried on for a total time
period ? till it ends in q0 . For all n ? N, T satisfies the volume preservation property.
Proof. Similar to Lemma 1, for i > 1, qi0 = qi +? ?pi and p0i = pi . Therefore, for any j ? {2, . . . , n}
?q 0
?q 0
?p0
?p0
s.t. j 6= i, ?qji = ?pji = ?qji = ?pji = 0. Consequently, by equation (4), and since p01 = ?p1 ,

 

 ?q10 ?q10   ?q10 ?q10  ??q 0
 ?q1 ?p1   ?q1 ?p1 
1
|J| =  ?p0 ?p0  = 
(17)
=
1
1 

 0

?q
?1
1
?q
?p
1

1

As before, let t1 be the time to reach x and t2 be the period between reaching x and the last point
def
def
0 def
0
1
q0 . That is, t1 = c?q
p1 and t2 = ? ? t1 . It follows that q1 = c + p1 ? t2 is equal to 2c ? ? p1 ? q1 .
Hence, |J| = 1.
5.3

Reflective Leapfrog Dynamics

Theorem 1. In RHMC (Algorithm 1) for sampling from a continuous and piecewise distribution P
which has affine partitioning boundaries, leapfrog simulation preserves volume in (q, p) space.
Proof. We split the algorithm into several atomic transformations Ti . Each transformation is either
(a) a momentum step, (b) a full position step with no reflection/refraction or (c) a full or partial
position step where exactly one reflection or refraction occurs.
6

To prove that the total algorithm preserves volume, it is sufficient to show that the volume is preserved under each Ti (i.e. |JTi (q, p)| = 1) since:
|JT1 oT2 o???oTm | = |JT1 | ? |JT2 | ? ? ? |JTm |
Transformations of kind (a) and (b) are shear mappings and therefore they preserve the volume [9].
Now consider a (full or partial) position step where a single refraction occurs. If the reflective plane
is in form q1 = c, by lemma 1, the volume preservation property holds. Otherwise, as long as
the reflective plane is affine, via a rotation of basis vectors, the problem is reduced to the former
case. Since volume is conserved under rotation, in this case the volume is also conserved. With
similar reasoning, by lemma 2, reflection on a affine reflective boundary preserves volume. Thus,
since all component transformations of RHMC leapfrog simulation preserve volume, the proof is
complete.
Along with the fact that the leapfrog dynamics are time-reversible, this shows that the algorithm
satisfies detailed balance, and so has the correct stationary distribution.

6

Experiment

Compared to baseline HMC, we expect that RHMC will simulate Hamiltonian dynamics more accurately and therefore leads to fewer rejected samples. On the other hand, this comes at the expense
of slower leapfrog position steps since intersections, reflections and refractions must be computed.
To test the trade off, we compare the RHMC to baseline HMC [9] and tuned Metroplis-Hastings
(MH) with a simple isotropic Normal proposal distribution. MH is automatically tuned after [12] by
testing 100 equidistant proposal variances in interval (0, 1] and accepting a variance for which the
acceptance rate is closest to 0.24. The baseline HMC and RHMC number of steps L and step size 
are chosen to be 100 and 0.1 respectively. (Many other choices are in the Appendix.) While HMC
performance is highly standard to these parameters [7] RHMC is consistently faster.
The comparison takes place on a heavy tail piecewise model with (non-normalized) negative log
probability
?p
?
if kqk? ? 3
? q>pA q
(18)
U (q) = 1 + q> A q if 3 < kqk? ? 6
?
?+?,
otherwise
where A is a positive definite matrix. We carry out the experiment on three choices of (position
space) dimensionalites, n = 2, 10 and 50.
Due to the symmetry of the model, the ground truth expected value of q is known to be 0. Therefore,
the absolute error of the expected value (estimated by a chain q(1) , . . . , q(k) of MCMC samples) in
each dimension d = 1, . . . , n is the absolute value of the mean of d-th element of the sample vectors.
The worst mean absolute error (WMAE) over all dimensions is taken as the error measurement of
the chain.
 k

X


 1

(1)
(k)
s
WMAE q , . . . , q
=
max 
qd 
(19)

k d=1,...n 
s=1

For each algorithm, 20 Markov chains are run and the mean WMAE and 99% confidence intervals
(as error bars) versus the number of iterations (i.e. Markov chain sizes) are time (milliseconds) are
depicted in figure 2. All algorithms are implemented in java and run on a single thread of a 3.40GHz
CPU.
For each of the 20 repetitions, some random starting point is chosen uniformly and used for all three
of the algorithms. We use a diagonal matrix for A where, for each repetition, each entry on the main
diagonal is either exp(?5) or exp(5) with equal probabilities.
As the results show, even in low dimensions, the extra cost of the position step is more or less
compensated by its higher effective sample size but as the dimensionality increases, the RHMC
significantly outperforms both baseline HMC and tuned MH.
7

6

6

Baseline.HMC
Tuned.MH
Reflective.HMC

5

4

Error

Error

4

3

3

2

2

1

1

0 0
10

Baseline.HMC
Tuned.MH
Reflective.HMC

5

1

10

2

10

3

10

Iteration (dim=2, L=100, ? =0.1)

0

4

10

1

10

2

(a)
6

Baseline.HMC
Tuned.MH
Reflective.HMC

5

4

Error

Error

Baseline.HMC
Tuned.MH
Reflective.HMC

5

4

3

3

2

2

1

1

1

10

2

10

3

10

Iteration (dim=10, L=100, ? =0.1)

0

4

10

1

10

2

10

(e)
6

5

5

4

4

Baseline.HMC
Tuned.MH
Reflective.HMC

3

Error

Error

3

10

Time (dim=10, L=100, ? =0.1)

(b)
6

3

2

2

1

1

0 0
10

10

(d)

6

0 0
10

3

10

Time (dim=2, L=100, ? =0.1)

1

10

2

10

3

10

Iteration (dim=50, L=100, ? =0.1)

0

4

10

Baseline.HMC
Tuned.MH
Reflective.HMC
1

10

(c)

2

3

10

10

Time (dim=50, L=100, ? =0.1)

4

10

(f)

Figure 3: Error (worst mean absolute error per dimension) versus (a-c) iterations and (e-f) time (ms).
Tuned.HM is Metropolis Hastings with a tuned isotropic Gaussian proposal distribution. (Many more examples
in supplementary material.)

7

Conclusion

We have presented a modification of the leapfrog dynamics for Hamiltonian Monte Carlo for piecewise smooth energy functions with affine boundaries (i.e. each region is a polytope), inspired by
physical systems. Though traditional Hamiltonian Monte Carlo can in principle be used on such
functions, the fact that the Hamiltonian will often be dramatically changed by the dynamics can result in a very low acceptance ratio, particularly in high dimensions. By better preserving the Hamiltonian, reflective Hamiltonian Monte Carlo (RHMC) accepts more moves and thus has a higher
effective sample size, leading to much more efficient probabilistic inference. To use this method,
one must be able to detect the first intersection of a position trajectory with polytope boundaries.
Acknowledgements
NICTA is funded by the Australian Government through the Department of Communications and
the Australian Research Council through the ICT Centre of Excellence Program.
8

References
[1] Hadi Mohasel Afshar, Scott Sanner, and Ehsan Abbasnejad. Linear-time gibbs sampling in
piecewise graphical models. In Association for the Advancement of Artificial Intelligence,
pages 665?673, 2015.
[2] Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Handbook of Markov Chain
Monte Carlo. CRC press, 2011.
[3] Hans Adolph Buchdahl. An introduction to Hamiltonian optics. Courier Corporation, 1993.
[4] Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid monte
carlo. Physics letters B, 195(2):216?222, 1987.
[5] Andrew G Glen, Lawrence M Leemis, and John H Drew. Computing the distribution of
the product of two continuous random variables. Computational statistics & data analysis,
44(3):451?464, 2004.
[6] Donald T Greenwood. Principles of dynamics. Prentice-Hall Englewood Cliffs, NJ, 1988.
[7] Matthew D Homan and Andrew Gelman. The no-u-turn sampler: Adaptively setting path
lengths in hamiltonian monte carlo. The Journal of Machine Learning Research, 15(1):1593?
1623, 2014.
[8] David Lunn, David Spiegelhalter, Andrew Thomas, and Nicky Best. The bugs project: evolution, critique and future directions. Statistics in medicine, 28(25):3049?3067, 2009.
[9] Radford M Neal. Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte
Carlo, 2, 2011.
[10] Ari Pakman and Liam Paninski. Auxiliary-variable exact hamiltonian monte carlo samplers
for binary distributions. In Advances in Neural Information Processing Systems, pages 2490?
2498, 2013.
[11] Ari Pakman and Liam Paninski. Exact hamiltonian monte carlo for truncated multivariate
gaussians. Journal of Computational and Graphical Statistics, 23(2):518?542, 2014.
[12] Gareth O Roberts, Andrew Gelman, Walter R Gilks, et al. Weak convergence and optimal
scaling of random walk metropolis algorithms. The annals of applied probability, 7(1):110?
120, 1997.
[13] Stan Development Team. Stan Modeling Language Users Guide and Reference Manual, Version 2.5.0, 2014.

9

"
4502,2013,Low-rank matrix reconstruction and clustering via approximate message passing,"We study the problem of reconstructing low-rank matrices from their noisy observations. We formulate the problem in the Bayesian framework, which allows us to exploit structural properties of matrices in addition to low-rankedness, such as sparsity. We propose an efficient approximate message passing algorithm, derived from the belief propagation algorithm, to perform the Bayesian inference for matrix reconstruction. We have also successfully applied the proposed algorithm to a clustering problem, by formulating the problem of clustering as a low-rank matrix reconstruction problem with an additional structural property. Numerical experiments show that the proposed algorithm outperforms Lloyd's K-means algorithm.","Low-rank matrix reconstruction and clustering via
approximate message passing

Ryosuke Matsushita
NTT DATA Mathematical Systems Inc.
1F Shinanomachi Rengakan, 35,
Shinanomachi, Shinjuku-ku, Tokyo,
160-0016, Japan
matsur8@gmail.com

Toshiyuki Tanaka
Department of Systems Science,
Graduate School of Informatics, Kyoto University
Yoshida Hon-machi, Sakyo-ku, Kyoto-shi,
606-8501 Japan
tt@i.kyoto-u.ac.jp

Abstract
We study the problem of reconstructing low-rank matrices from their noisy observations. We formulate the problem in the Bayesian framework, which allows
us to exploit structural properties of matrices in addition to low-rankedness, such
as sparsity. We propose an efficient approximate message passing algorithm, derived from the belief propagation algorithm, to perform the Bayesian inference for
matrix reconstruction. We have also successfully applied the proposed algorithm
to a clustering problem, by reformulating it as a low-rank matrix reconstruction
problem with an additional structural property. Numerical experiments show that
the proposed algorithm outperforms Lloyd?s K-means algorithm.

1

Introduction

Low-rankedness of matrices has frequently been exploited when one reconstructs a matrix from its
noisy observations. In such problems, there are often demands to incorporate additional structural
properties of matrices in addition to the low-rankedness. In this paper, we consider the case where
a matrix A0 ? Rm?N to be reconstructed is factored as A0 = U0 V0? , U0 ? Rm?r , V0 ? RN ?r
(r ? m, N ), and where one knows structural properties of the factors U0 and V0 a priori. Sparseness
and non-negativity of the factors are popular examples of such structural properties [1, 2].
Since the properties of the factors to be exploited vary according to the problem, it is desirable
that a reconstruction method has enough flexibility to incorporate a wide variety of properties. The
Bayesian approach achieves such flexibility by allowing us to select prior distributions of U0 and V0
reflecting a priori knowledge on the structural properties. The Bayesian approach, however, often
involves computationally expensive processes such as high-dimensional integrations, thereby requiring approximate inference methods in practical implementations. Monte Carlo sampling methods
and variational Bayes methods have been proposed for low-rank matrix reconstruction to meet this
requirement [3?5].
We present in this paper an approximate message passing (AMP) based algorithm for Bayesian lowrank matrix reconstruction. Developed in the context of compressed sensing, the AMP algorithm reconstructs sparse vectors from their linear measurements with low computational cost, and achieves
a certain theoretical limit [6]. AMP algorithms can also be used for approximating Bayesian inference with a large class of prior distributions of signal vectors and noise distributions [7]. These
successes of AMP algorithms motivate the use of the same idea for low-rank matrix reconstruction.
The IterFac algorithm for the rank-one case [8] has been derived as an AMP algorithm. An AMP
algorithm for the general-rank case is proposed in [9], which, however, can only treat estimation of
posterior means. We extend their algorithm so that one can deal with other estimations such as the
maximum a posteriori (MAP) estimation. It is the first contribution of this paper.
1

As the second contribution, we apply the derived AMP algorithm to K-means type clustering to
obtain a novel efficient clustering algorithm. It is based on the observation that our formulation
of the low-rank matrix reconstruction problem includes the clustering problem as a special case.
Although the idea of applying low-rank matrix reconstruction to clustering is not new [10, 11], our
proposed algorithm is, to our knowledge, the first that directly deals with the constraint that each
datum should be assigned to exactly one cluster in the framework of low-rank matrix reconstruction.
We present results of numerical experiments, which show that the proposed algorithm outperforms
Lloyd?s K-means algorithm [12] when data are high-dimensional.
Recently, AMP algorithms for dictionary learning and blind calibration [13] and for matrix reconstruction with a generalized observation model [14] were proposed. Although our work has some
similarities to these studies, it differs in that we fix the rank r rather than the ratio r/m when taking
the limit m, N ? ? in the derivation of the algorithm. Another difference is that our formulation,
explained in the next section, does not assume statistical independence among the components of
each row of U0 and V0 . A detailed comparison among these algorithms remains to be made.

2
2.1

Problem setting
Low-rank matrix reconstruction

We consider the following problem setting. A matrix A0 ? Rm?N to be estimated is defined
by two matrices U0 := (u0,1 , . . . , u0,m )? ? Rm?r and V0 := (v0,1 , . . . , v0,N )? ? RN ?r as
A0 := U0 V0? , where u0,i , v0,j ? Rr . We consider the case where r ? m, N . Observations of A0
are corrupted by additive noise W ? Rm?N , whose components Wi,j are i.i.d. Gaussian random
variables following N (0, m? ). Here ? > 0 is a noise variance parameter and N (a, ? 2 ) denotes the
Gaussian distribution with mean a and variance ? 2 . The factor m in the noise variance is introduced
to allow a proper scaling in the limit where m and N go to infinity in the same order, which is
employed in deriving the algorithm. An observed matrix A ? Rm?N is given by A := A0 + W .
Reconstructing A0 and (U0 , V0 ) from A is the problem considered in this paper.
We take the Bayesian approach to address this problem, in which one requires prior distributions
of variables to be estimated, as well as conditional distributions relating observations with variables
to be estimated. These distributions need not be the true ones because in some cases they are not
available so that one has to assume them arbitrarily, and in some other cases one expects advantages
by assuming them in some specific manner in view of computational efficiencies. In this paper, we
suppose that one uses the true conditional distribution
(
)
1
1
p(A|U0 , V0 ) =
?A ? U0 V0? ?2F ,
(1)
mN exp ?
2m?
(2?m? ) 2
where ? ? ?F denotes the Frobenius norm. Meanwhile, we suppose that the assumed prior distributions of U0 and V0 , denoted by p?U and p?V , respectively, may be different from the true distributions
?
pU and pV , respectively.
We restrict p?U and p?V to distributions of the form p?U (U0 ) = i p?u (u0,i )
?
and p?V (V0 ) = j p?v (v0,j ), respectively, which allows us to construct computationally efficient
algorithms. When U ? p?U (U ) and V ? p?V (V ), the posterior distribution of (U, V ) given A is
(
)
1
?A ? U V ? ?2F p?U (U )?
pV (V ).
(2)
p?(U, V |A) ? exp ?
2m?
Prior probability density functions (p.d.f.s) p?u and p?v can be improper, that is, they can integrate to
infinity, as long as the posterior p.d.f. (2) is proper. We also consider cases where the assumed rank
r? may be different from the true rank r. We thus suppose that estimates U and V are of size m ? r?
and N ? r?, respectively.
We consider two problems appearing in the Bayesian approach. The first problem, which we call
the marginalization problem, is to calculate the marginal posterior distributions given A,
?
?
?
p?i,j (ui , vj |A) := p?(U, V |A)
duk
dvl .
(3)
k?=i
?

l?=j

These are used to calculate
the posterior mean E[U V |A] and the
?
? marginal MAP estimates
MMAP
uMMAP
:=
arg
max
p
?
(u,
v|A)dv
and
v
:=
arg
max
p?i,j (u, v|A)du. Because
u
i,j
v
i
j
2

calculation of p?i,j (ui , vj |A) typically involves high-dimensional integrations requiring high computational cost, approximation methods are needed.
The second problem, which we call the MAP problem, is to calculate the MAP estimate
arg maxU,V p?(U, V |A). It is formulated as the following optimization problem:
min C MAP (U, V ),
U,V

(4)

where C MAP (U, V ) is the negative logarithm of (2):
C MAP (U, V ) :=

m
N
?
?
1
?A ? U V ? ?2F ?
log p?u (ui ) ?
log p?v (vj ).
2m?
i=1
j=1

(5)

Because ?A ? U V ? ?2F is a non-convex function of (U, V ), it is generally hard to find the global
optimal solutions of (4) and therefore approximation methods are needed in this problem as well.
2.2

Clustering as low-rank matrix reconstruction

A clustering problem can be formulated as a problem of low-rank matrix reconstruction [11]. Suppose that v0,j ? {e1 , . . . , er }, j = 1, . . . , N , where el ? {0, 1}r is the vector whose lth component
is 1 and the others are 0. When V0 and U0 are fixed, aj follows one of the r Gaussian distributions
? 0,l , m? I), l = 1, . . . , r, where u
? 0,l is the lth column of U0 . We regard that each Gaussian
N (u
? 0,l being the center of cluster l and v0,j representing the cluster
distribution defines a cluster, u
assignment of the datum aj . One can then perform clustering on the dataset {a1 , . . . , aN } by reconstructing U0 and V0 from A = (a1 , . . . , aN ) under the structural constraint that every row of V0
should belong to {e1 , . . . , er?}, where r? is an assumed number of clusters.
Let us consider maximum likelihood estimation arg maxU,V p(A|U, V ), or equivalently, MAP esti?r?
mation with the (improper) uniform prior distributions p?u (u) = 1 and p?v (v) = r??1 l=1 ?(v?el ).
The corresponding MAP problem is
min

r ,V ?{0,1}N ??
r
U ?Rm??

?A ? U V ? ?2F

subject to vj ? {e1 , . . . , er?}.

(6)

?N ?r?
When V satisfies the constraints, the objective function ?A ? U V ? ?2F =
j=1
l=1 ?aj ?
? l ?22 I(vj = el ) is the sum of squared distances, each of which is between a datum and the center of
u
the cluster that the datum is assigned to. The optimization problem (6), its objective function, and
clustering based on it are called in this paper the K-means problem, the K-means loss function, and
the K-means clustering, respectively.
One can also use the marginal MAP estimation for clustering. If U0 and V0 follow p?U and p?V , respectively, the marginal MAP estimation is optimal in the sense that it maximizes the expectation of
accuracy with respect to p?(V0 |A). Here, accuracy is defined as the fraction of correctly assigned data
among all data. We call the clustering using approximate marginal MAP estimation the maximum
accuracy clustering, even when incorrect prior distributions are used.

3

Previous work

Existing methods for approximately solving the marginalization problem and the MAP problem
are divided into stochastic methods such as Markov-Chain Monte-Carlo methods and deterministic
ones. A popular deterministic method is to use the variational Bayesian formalism. The variational
Bayes matrix factorization [4, 5] approximates the posterior distribution p(U, V |A) as the product
VB
of two functions pVB
U (U ) and pV (V ), which are determined so that the Kullback-Leibler (KL)
VB
VB
divergence from pU (U )pV (V ) to p(U, V |A) is minimized. Global minimization of the KL divergence is difficult except for some special cases [15], so that an iterative method to obtain a local
minimum is usually adopted. Applying the variational Bayes matrix factorization to the MAP problem, one obtains the iterated conditional modes (ICM) algorithm, which alternates minimization of
C MAP (U, V ) over U for fixed V and minimization over V for fixed U .
The representative algorithm to solve the K-means problem approximately is Lloyd?s K-means algorithm [12]. Lloyd?s K-means algorithm is regarded as the ICM algorithm: It alternates minimization
of the K-means loss function over U for fixed V and minimization over V for fixed U iteratively.
3

Algorithm 1 (Lloyd?s K-means algorithm).
ntl =

N
?

I(vjt = el ),

? tl =
u

j=1

ljt+1 = arg

min

l?{1,...,?
r}

? tl ?22 ,
?aj ? u

N
1 ?
aj I(vjt = el ),
ntl j=1

(7a)

vjt+1 = elt+1 .

(7b)

j

Throughout this paper, we represent an algorithm by a set of equations as in the above. This representation means that the algorithm begins with a set of initial values and repeats the update of the
variables using the equations presented until it satisfies some stopping criteria. Lloyd?s K-means
algorithm begins with a set of initial assignments V 0 ? {e1 , . . . , er?}N . This algorithm easily gets
stuck in local minima and its performance heavily depends on the initial values of the algorithm.
Some methods for initialization to obtain a better local minimum are proposed [16].
Maximum accuracy clustering can be solved approximately by using the variational Bayes matrix
factorization, since it gives an approximation to the marginal posterior distribution of vj given A.

4
4.1

Proposed algorithm
Approximate message passing algorithm for low-rank matrix reconstruction

We first discuss the general idea of the AMP algorithm and advantages of the AMP algorithm compared with the variational Bayes matrix factorization. The AMP algorithm is derived by approximating the belief propagation message passing algorithm in a way thought to be asymptotically exact for
large-scale problems with appropriate randomness. Fixed points of the belief propagation message
passing algorithm correspond to local minima of the KL divergence between a kind of trial function
and the posterior distribution [17]. Therefore, the belief propagation message passing algorithm can
be regarded as an iterative algorithm based on an approximation of the posterior distribution, which
is called the Bethe approximation. The Bethe approximation can reflect dependence of random variables (dependence between U and V in p?(U, V |A) in our problem) to some extent. Therefore, one
can intuitively expect that performance of the AMP algorithm is better than that of the variational
Bayes matrix factorization, which treats U and V as if they were independent in p?(U, V |A).
An important property of the AMP algorithm, aside from its efficiency and effectiveness, is that
one can predict performance of the algorithm accurately for large-scale problems by using a set of
equations, called the state evolution [6]. Analysis with the state evolution also shows that required
iteration numbers are O(1) even when the problem size is large. Although we can present the state
evolution for the algorithm proposed in this paper and give a proof of its validity like [8, 18], we do
not discuss the state evolution here due to the limited space available.
We introduce a one-parameter extension of the posterior distribution p?(U, V |A) to treat the marginalization problem and the MAP problem in a unified manner. It is defined as follows:
(
)(
)?
?
p?(U, V |A; ?) ? exp ?
?A ? U V ? ?2F p?U (U )?
pV (V ) ,
2m?

(8)

which is proportional to p?(U, V |A)? , where ? > 0 is the parameter. When ? = 1, p?(U, V |A; ?)
is reduced to p?(U, V |A). In the limit ? ? ?, the distribution p?(U, V |A; ?) concentrates on the
maxima of p?(U, V |A). An algorithm for the marginalization problem on p?(U, V |A; ?) is particularized to the algorithms for the marginalization problem and for the MAP problem for the original
posterior distribution p?(U, V |A) by letting ? = 1 and ? ? ?, respectively. The AMP algorithm
for the marginalization problem on p?(U, V |A; ?) is derived in a way similar to that described in [9],
as detailed in the Supplementary Material.
In the derived algorithm, the values of variables But = (btu,1 , . . . , btu,m )? ? Rm??r , Bvt =
(btv,1 , . . . , btv,N )? ? RN ??r , ?tu ? Rr???r , ?tv ? Rr???r , U t = (ut1 , . . . , utm )? ? Rm??r ,
t ?
t
V t = (v1t , . . . , vN
) ? RN ??r , S1t , . . . , Sm
? Rr???r , and T1t , . . . , TNt ? Rr???r are calculated iteratively, where the superscript t ? N ? {0} represents iteration numbers. Variables with a negative
iteration number are defined as 0. The algorithm is as follows:
4

Algorithm 2.
But =

N
1
1 t?1 ? t
AV t ?
U
Tj ,
m?
m?
j=1

?tu =

N
N
1 ? t
1
1 ? t
(V t )? V t +
Tj ?
T , (9a)
m?
?m? j=1
m? j=1 j

uti = f (btu,i , ?tu ; p?u ),

Sit = G(btu,i , ?tu ; p?u ),
m
m
m
1
1 ? t
1 t? t
1 ? t
1 ? t
Si , ?tv =
A U ?
V
(U t )? U t +
Si ?
S ,
Bvt =
m?
m?
m?
?m? i=1
m? i=1 i
i=1
vjt+1 = f (btv,j , ?tv ; p?v ),

Tjt+1 = G(btv,j , ?tv ; p?v ).

(9b)
(9c)
(9d)

Algorithm 2 is almost symmetric in U and V . Equations (9a)?(9b) and (9c)?(9d) update quantities
related to the estimates of U0 and V0 , respectively. The algorithm requires an initial value V 0 and
begins with Tj0 = O. The functions f (?, ?; p?) : Rr??Rr???r ? Rr? and G(?, ?; p?) : Rr??Rr???r ? Rr???r ,
which have a p.d.f. p? : Rr? ? R as a parameter, are defined by
?
?f (b, ?; p?)
f (b, ?; p?) := u?
q (u; b, ?, p?)du,
G(b, ?; p?) :=
,
(10)
?b
where q?(u; b, ?, p?) is the normalized p.d.f. of u defined by
( (1
))
q?(u; b, ?, p?) ? exp ?? u? ?u ? b? u ? log p?(u) .
2

(11)

One can see that f (b, ?; p?) is the mean of the distribution q?(u; b, ?, p?) and that G(b, ?; p?) is its
covariance matrix scaled by ?. The function f (b, ?; p?) need not be differentiable everywhere;
Algorithm 2 works if f (b, ?; p?) is differentiable at b for which one needs to calculate G(b, ?; p?) in
running the algorithm.
We assume in the rest of this section the convergence of Algorithm 2, although the convergence is
?
?
?
?
?
not guaranteed in general. Let Bu? , Bv? , ??
be the converged values
u , ?v , Si , Tj , U , and V
of the respective variables. First, consider running Algorithm 2 with ? = 1. The marginal posterior
distribution is then approximated as
?
?
?v ).
?u )?
q (vj ; b?
p?i,j (ui , vj |A) ? q?(ui ; b?
v,j , ?v , p
u,i , ?u , p

(12)

?
?
?
Since u?
of q?(u; b?
?u ) and q?(v; b?
?v ), respectively, the
i and vj are the means
u,i , ?u , p
v,j , ?v , p
?
?
?
posterior mean E[U V |A] = U V p?(U, V |A)dU dV is approximated as

E[U V ? |A] ? U ? (V ? )? .

(13)

and vjMMAP are approximated as
The marginal MAP estimates uMMAP
i
?
?v ).
vjMMAP ? arg max q?(v; b?
v,j , ?v , p

?
?u ),
uMMAP
? arg max q?(u; b?
u,i , ?u , p
i

v

u

(14)

Taking the limit ? ? ? in Algorithm 2 yields an algorithm for the MAP problem (4). In this case,
the functions f and G are replaced with
[1
]
?f? (b, ?; p?)
f? (b, ?; p?) := arg min u? ?u ? b? u ? log p?(u) , G? (b, ?; p?) :=
. (15)
u
2
?b
One may calculate G? (b, ?; p?) from the Hessian of log p?(u) at u = f? (b, ?; p?), denoted by H,
(
)?1
via the identity G? (b, ?; p?) = ??H
. This identity follows from the implicit function theorem
under some additional assumptions and helps in the case where the explicit form of f? (b, ?; p?) is
not available. The MAP estimate is approximated by (U ? , V ? ).
4.2

Properties of the algorithm

Algorithm 2 has several plausible properties. First, it has a low computational cost. The computational cost per iteration is O(mN ), which is linear in the number of components of the matrix
A. Calculation of f (?, ?; p?) and G(?, ?; p?) is performed O(N + m) times per iteration. The constant
5

factor depends on p? and ?. Calculation of f for ? < ? generally involves an r?-dimensional numerical integration, although they are not needed in cases where an analytic expression of the integral
is available and cases where the variables take only discrete values. Calculation of f? involves
minimization over an r?-dimensional vector. When ? log p? is a convex function and ? is positive
semidefinite, this minimization problem is convex and can be solved at relatively low cost.
Second, Algorithm 2 has a form similar to that of an algorithm based on the variational Bayesian
matrix factorization. In fact, if the last terms on the right-hand sides of the four equations in (9a)
and (9c) are removed, the resulting algorithm is the same as an algorithm based on the variational
Bayesian matrix factorization proposed in [4] and, in particular, the same as the ICM algorithm when
? ? ?. (Note, however, that [4] only treats the case where the priors p?u and p?v are multivariate
Gaussian distributions.) Note that additional computational cost for these extra terms is O(m + N ),
which is insignificant compared with the cost of the whole algorithm, which is O(mN ).
Third, when one deals with the MAP problem, the value of C MAP (U, V ) may increase in iterations of Algorithm 2. The following proposition, however, guarantees optimality of the output of
Algorithm 2 in a certain sense, if it has converged.
?
Proposition 1. Let (U ? , V ? , S1? , . . . , Sm
, T ? , . . . , TN? ) be a fixed point of the AMP algorithm
?m 1 ?
?N
for the MAP problem and suppose that i=1 Si and j=1 Tj? are positive semidefinite. Then
U ? is a global minimum of C MAP (U, V ? ) and V ? is a global minimum of C MAP (U ? , V ).
The proof is in the Supplementary Material. The key to the proof is the following reformulation:
N
)]
[
(
)
( 1 ?
MAP
t
t?1
Tjt (U ? U t?1 )?
U = arg min C
(U, V ) ? tr (U ? U )
U
2m? j=1
t

(16)

?N
If j=1 Tjt is positive semidefinite, the second term of the minimand is the negative squared pseudometric between U and U t?1 , which is interpreted as a penalty on nearness to the temporal estimate.
?m
?N
Positive semidefiniteness of i=1 Sit and j=1 Tjt holds in almost all cases. In fact, we only have
to assume lim??? G(b, ?; p?) = G? (b, ?; p?), since G(b, ?; p?) is a scaled covariance matrix of
q?(u; b, ?, p?), which is positive semidefinite. It follows from Proposition 1 that any fixed point of the
AMP algorithm is also a fixed point of the ICM algorithm. It has two implications: (i) Execution
of the ICM algorithm initialized with the converged values of the AMP algorithm does not improve
C MAP (U t , V t ). (ii) The AMP algorithm has not more fixed points than the ICM algorithm. The
second implication may help the AMP algorithm avoid getting stuck in bad local minima.
4.3

Clustering via AMP algorithm

One can use the AMP algorithm for the MAP problem to perform the K-means clustering by letting
?r?
p?u (u) = 1 and p?v (v) = r??1 l=1 ?(v ? el ). Noting that f? (b, ?; p?v ) is piecewise constant with
respect to b and hence G? (b, ?; p?v ) is O almost everywhere, we obtain the following algorithm:
Algorithm 3 (AMP algorithm for the K-means clustering).
1
1
AV t , ?tu =
(V t )? V t , U t = But (?tu )?1 ,
m?
m?
1 ? t 1 t t
1
1
Bvt =
A U ? V S , ?tv =
(U t )? U t ? S t ,
m?
?
m?
?
[1
]
v ? ?tv v ? v ? btv,j .
vjt+1 = arg
min
v?{e1 ,...,er? } 2
But =

S t = (?tu )?1 ,

(17a)
(17b)
(17c)

It is initialized with an assignment V 0 ? {e1 , . . . , er?}N . Algorithm 3 is rewritten as follows:
ntl =

N
?
j=1

ljt+1 = arg

I(vjt = el ),

? tl =
u

N
1 ?
aj I(vjt = el ),
ntl j=1

[ 1
2m
m]
? tl ?22 + t I(vjt = el ) ? t ,
?aj ? u
nl
nl
l?{1,...,?
r } m?
min

6

(18a)
vjt+1 = elt+1 .
j

(18b)

The parameter ? appearing in
algorithm does not exist in the?
K-means clustering problem. In
?the
m
m
fact, ? appears because m?2 i=1 A2ij Sit was estimated by ? m?1 i=1 Sit in deriving Algorithm 2,
which can be justified for large-sized problems. In practice, we propose using m?2 N ?1 ?A ?
U t (V t )? ?2F as a temporary estimate of ? at tth iteration. While the AMP algorithm for the Kmeans clustering updates the value of U in the same way as Lloyd?s K-means algorithm, it performs
assignments of data to clusters in a different way. In the AMP algorithm, in addition to distances
from data to centers of clusters, the assignment at present is taken into consideration in two ways:
(i) A datum is less likely to be assigned to the cluster that it is assigned to at present. (ii) Data are
more likely to be assigned to a cluster whose size at present is smaller. The former can intuitively be
understood by observing that if vjt = el , one should take account of the fact that the cluster center
? tl is biased toward aj . The term 2m(ntl )?1 I(vjt = el ) in (18b) corrects this bias, which, as it
u
should be, is inversely proportional to the cluster size.
The AMP algorithm for maximum accuracy clustering is obtained by letting ? = 1 and p?v (v) be
a discrete distribution on {e1 , . . . , er?}. After the algorithm converges, arg maxv q?(v; vj? , ??
?v )
v ,p
gives the final cluster assignment of the jth datum and U ? gives the estimate of the cluster centers.

5

Numerical experiments

We conducted numerical experiments on both artificial and real data sets to evaluate performance
of the proposed algorithms for clustering. In the experiment on artificial data sets, we set m = 800
? 0,l , l = 1, . . . , r, were generated according to the
and N = 1600 and let r? = r. Cluster centers u
multivariate Gaussian distribution N (0, I). Cluster assignments v0,j , j = 1, . . . , N, were generated
according to the uniform distribution on {e1 , . . . , er }. For fixed ? = 0.1 and r, we generated 500
problem instances and solved them with five algorithms: Lloyd?s K-means algorithm (K-means),
the AMP algorithm for the K-means clustering (AMP-KM), the variational Bayes matrix factorization [4] for maximum accuracy clustering (VBMF-MA), the AMP algorithm for maximum accuracy
clustering (AMP-MA), and the K-means++ [16]. The K-means++ updates the variables in the same
way as Lloyd?s K-means algorithm with an initial value chosen in a sophisticated manner. For the
other algorithms, initial values vj0 , j = 1, . . . , N, were randomly generated from the same distribution as v0,j . We used the true prior distributions of U and V for maximum accuracy clustering.
We ran Lloyd?s K-means algorithm and the K-means++ until no change was observed. We ran the
AMP algorithm for the K-means clustering until either V t = V t?1 or V t = V t?2 is satisfied.
This is because we observed oscillations of assignments of a small number of data. For the other
two algorithms, we terminated the iteration when ?U t ? U t?1 ?2F < 10?15 ?U t?1 ?2F and ?V t ?
V t?1 ?2F < 10?15 ?V t?1 ?2F were met or the number of iterations exceeded 3000. We then evaluated
the following performance measures for the obtained solution (U ? , V ? ):
?N
?
? := N1 N
? ?22 ), where a
? Normalized K-means loss ?A?U ? (V ? )? ?2F /( j=1 ?aj ? a
j=1 aj .
?
N
? Accuracy maxP N ?1 j=1 I(P vj? = v0,j ), where the maximization is taken over all
r-by-r permutation matrices. We used the Hungarian algorithm [19] to solve this maximization problem efficiently.
? Number of iterations needed to converge.
We calculated the averages and the standard deviations of these performance measures over 500
instances. We conducted the above experiments for various values of r.
Figure 1 shows the results. The AMP algorithm for the K-means clustering achieves the smallest Kmeans loss among the five algorithms, while the Lloyd?s K-means algorithm and K-means++ show
large K-means losses for r ? 5. We emphasize that all the three algorithms are aimed to minimize
the same K-means loss and the differences lie in the algorithms for minimization. The AMP algorithm for maximum accuracy clustering achieves the highest accuracy among the five algorithms. It
also shows fast convergence. In particular, the convergence speed of the AMP algorithm for maximum accuracy clustering is comparable to that of the AMP algorithm for the K-means clustering
when the two algorithms show similar accuracy (r < 9). This is in contrast to the common observation that the variational Bayes method often shows slower convergence than the ICM algorithm.
7

1

1
K-means
AMP-KM
VBMF-MA
AMP-MA
K-means++

K-means
AMP-KM
VBMF-MA
AMP-MA
K-means++

0.8

0.99

Accuracy

Normalized K-means loss

0.995

0.985
0.98

0.6

0.4

0.2

0.975
0.97

2

4

6

8

10

r

12

14

16

0

18

2

4

6

8

(a)

r

12

14

16

18

(b)

2500

1
K-means
AMP-KM
VBMF-MA
AMP-MA
K-means++

2000

Number of iterations

10

0.8

Accuracy

1500

1000

500

0.6

0.4
AMP-KM
VBMF-MA
AMP-MA

0.2

0

2

4

6

8

10

r

12

14

16

0

18

0

10

20

30

Iteration number

(c)

40

50

(d)

Figure 1: (a)?(c) Performance for different r: (a) Normalized K-means loss. (b) Accuracy. (c)
Number of iterations needed to converge. (d) Dynamics for r = 5. Average accuracy at each
iteration is shown. Error bars represent standard deviations.

0.45

0.75

K-means++
AMP-KM

K-means++
AMP-KM
0.7

0.44

Accuracy

Normalized K-means loss

0.46

0.43
0.42

0.65

0.6

0.41
0.55
0.4
0.39

0

10

20

30

Number of trials

40

0.5

50

(a)

0

10

20

30

Number of trials

40

50

(b)

Figure 2: Performance measures in real-data experiments. (a) Normalized K-means loss. (b) Accuracy. The results for the 50 trials are shown in the descending order of performance for AMP-KM.
The worst two results for AMP-KM are out of the range.
In the experiment on real data, we used the ORL Database of Faces [20], which contains 400 images
of human faces, ten different images of each of 40 distinct subjects. Each image consists of 112 ?
92 = 10304 pixels whose value ranges from 0 to 255. We divided N = 400 images into r? = 40
clusters with the K-means++ and the AMP algorithm for the K-means clustering. We adopted the
initialization method of the K-means++ also for the AMP algorithm, because random initialization
often yielded empty clusters and almost all data were assigned to only one cluster. The parameter ?
was estimated in the way proposed in Subsection 4.3. We ran 50 trials with different initial values,
and Figure 2 summarizes the results.
The AMP algorithm for the K-means clustering outperformed the standard K-means++ algorithm
in 48 out of the 50 trials in terms of the K-means loss and in 47 trials in terms of the accuracy.
The AMP algorithm yielded just one cluster with all data assigned to it in two trials. The attained
minimum value of K-means loss is 0.412 with the K-means++ and 0.400 with the AMP algorithm.
The accuracies at these trials are 0.635 with the K-means++ and 0.690 with the AMP algorithm. The
average number of iterations was 6.6 with the K-means++ and 8.8 with the AMP algorithm. These
results demonstrate efficiency of the proposed algorithm on real data.
8

References
[1] P. Paatero, ?Least squares formulation of robust non-negative factor analysis,? Chemometrics and Intelligent Laboratory Systems, vol. 37, no. 1, pp. 23?35, May 1997.
[2] P. O. Hoyer, ?Non-negative matrix factorization with sparseness constraints,? The Journal of Machine
Learning Research, vol. 5, pp. 1457?1469, Dec. 2004.
[3] R. Salakhutdinov and A. Mnih, ?Bayesian probabilistic matrix factorization using Markov chain Monte
Carlo,? in Proceedings of the 25th International Conference on Machine Learning, New York, NY, Jul. 5?
Aug. 9, 2008, pp. 880?887.
[4] Y. J. Lim and Y. W. Teh, ?Variational Bayesian approach to movie rating prediction,? in Proceedings of
KDD Cup and Workshop, San Jose, CA, Aug. 12, 2007.
[5] T. Raiko, A. Ilin, and J. Karhunen, ?Principal component analysis for large scale problems with lots
of missing values,? in Machine Learning: ECML 2007, ser. Lecture Notes in Computer Science, J. N.
Kok, J. Koronacki, R. L. de Mantaras, S. Matwin, D. Mladeni?c, and A. Skowron, Eds. Springer Berlin
Heidelberg, 2007, vol. 4701, pp. 691?698.
[6] D. L. Donoho, A. Maleki, and A. Montanari, ?Message-passing algorithms for compressed sensing,?
Proceedings of the National Academy of Sciences USA, vol. 106, no. 45, pp. 18 914?18 919, Nov. 2009.
[7] S. Rangan, ?Generalized approximate message passing for estimation with random linear mixing,? in Proceedings of 2011 IEEE International Symposium on Information Theory, St. Petersburg, Russia, Jul. 31?
Aug. 5, 2011, pp. 2168?2172.
[8] S. Rangan and A. K. Fletcher, ?Iterative estimation of constrained rank-one matrices in noise,? in Proceedings of 2012 IEEE International Symposium on Information Theory, Cambridge, MA, Jul. 1?6, 2012,
pp. 1246?1250.
[9] R. Matsushita and T. Tanaka, ?Approximate message passing algorithm for low-rank matrix reconstruction,? in Proceedings of the 35th Symposium on Information Theory and its Applications, Oita, Japan,
Dec. 11?14, 2012, pp. 314?319.
[10] W. Xu, X. Liu, and Y. Gong, ?Document clustering based on non-negative matrix factorization,? in Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, Toronto, Canada, Jul. 28?Aug. 1, 2003, pp. 267?273.
[11] C. Ding, T. Li, and M. Jordan, ?Convex and semi-nonnegative matrix factorizations,? IEEE Transactions
on Pattern Analysis and Machine Intelligence, vol. 32, no. 1, pp. 45?55, Jan. 2010.
[12] S. P. Lloyd, ?Least squares quantization in PCM,? IEEE Transactions on Information Theory, vol. IT-28,
no. 2, pp. 129?137, Mar. 1982.
[13] F. Krzakala, M. M?ezard, and L. Zdeborov?a, ?Phase diagram and approximate message passing for blind
calibration and dictionary learning,? preprint, Jan. 2013, arXiv:1301.5898v1 [cs.IT].
[14] J. T. Parker, P. Schniter, and V. Cevher, ?Bilinear generalized approximate message passing,? preprint,
Oct. 2013, arXiv:1310.2632v1 [cs.IT].
[15] S. Nakajima and M. Sugiyama, ?Theoretical analysis of Bayesian matrix factorization,? Journal of Machine Learning Research, vol. 12, pp. 2583?2648, Sep. 2011.
[16] D. Arthur and S. Vassilvitskii, ?k-means++: the advantages of careful seeding,? in SODA ?07 Proceedings
of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms, New Orleans, Louisiana, Jan. 7?9,
2007, pp. 1027?1035.
[17] J. S. Yedidia, W. T. Freeman, and Y. Weiss, ?Constructing free-energy approximations and generalized
belief propagation algorithms,? IEEE Transactions on Information Theory, vol. 51, no. 7, pp. 2282?2312,
Jul. 2005.
[18] M. Bayati and A. Montanari, ?The dynamics of message passing on dense graphs, with applications to
compressed sensing,? IEEE Transactions on Information Theory, vol. 57, no. 2, pp. 764?785, Feb. 2011.
[19] H. W. Kuhn, ?The Hungarian method for the assignment problem,? Naval Research Logistics Quarterly,
vol. 2, no. 1?2, pp. 83?97, Mar. 1955.
[20] F. S. Samaria and A. C. Harter, ?Parameterisation of a stochastic model for human face identification,? in
Proceedings of 2nd IEEE Workshop on Applications of Computer Vision, Sarasota FL, Dec. 1994, pp.
138?142. [Online]. Available: http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html

9

"
2861,2008,Support Vector Machines with a Reject Option,"We consider the problem of binary classification where the classifier may abstain instead of classifying each observation. The Bayes decision rule for this setup, known as Chow's rule, is defined by two thresholds on posterior probabilities. From simple desiderata, namely the consistency and the sparsity of the classifier, we derive the double hinge loss function that focuses on estimating conditional probabilities only in the vicinity of the threshold points of the optimal decision rule. We show that, for suitable kernel machines, our approach is universally consistent. We cast the problem of minimizing the double hinge loss as a quadratic program akin to the standard SVM optimization problem and propose an active set method to solve it efficiently. We finally provide preliminary experimental results illustrating the interest of our constructive approach to devising loss functions.","Support Vector Machines with a Reject Option

Yves Grandvalet 1, 2 , Alain Rakotomamonjy 3 , Joseph Keshet 2 and St?ephane Canu 3
1
2
Heudiasyc, UMR CNRS 6599
Idiap Research Institute
Universit?e de Technologie de Compi`egne
Centre du Parc
BP 20529, 60205 Compi`egne Cedex, France CP 592, CH-1920 Martigny Switzerland
3
LITIS, EA 4108
Universit?e de Rouen & INSA de Rouen
76801 Saint Etienne du Rouvray, France

Abstract
We consider the problem of binary classification where the classifier may abstain
instead of classifying each observation. The Bayes decision rule for this setup,
known as Chow?s rule, is defined by two thresholds on posterior probabilities.
From simple desiderata, namely the consistency and the sparsity of the classifier,
we derive the double hinge loss function that focuses on estimating conditional
probabilities only in the vicinity of the threshold points of the optimal decision
rule. We show that, for suitable kernel machines, our approach is universally
consistent. We cast the problem of minimizing the double hinge loss as a quadratic
program akin to the standard SVM optimization problem and propose an active set
method to solve it efficiently. We finally provide preliminary experimental results
illustrating the interest of our constructive approach to devising loss functions.

1

Introduction

In decision problems where errors incur a severe loss, one may have to build classifiers that abstain
from classifying ambiguous examples. Rejecting these examples has been investigated since the
early days of pattern recognition. In particular, Chow (1970) analyses how the error rate may be
decreased thanks to the reject option.
There have been several attempts to integrate a reject option in Support Vector Machines (SVMs),
using strategies based on the thresholding of SVMs scores (Kwok, 1999) or on a new training criterion (Fumera & Roli, 2002). These approaches have however critical drawbacks: the former is
not consistent and the latter leads to considerable computational overheads to the original SVM
algorithm and lacks some of its most appealing features like convexity and sparsity.
We introduce a piecewise linear and convex training criterion dedicated to the problem of classification with the reject option. Our proposal, inspired by the probabilistic interpretation of SVM
fitting (Grandvalet et al., 2006), is a double hinge loss, reflecting the two thresholds in Chow?s rule.
Hence, we generalize the loss suggested by Bartlett and Wegkamp (2008) to arbitrary asymmetric
misclassification and rejection costs. For the symmetric case, our probabilistic viewpoint motivates
another decision rule. We then propose the first algorithm specifically dedicated to train SVMs with
a double hinge loss. Its implementation shows that our decision rule is at least at par with the one of
Bartlett and Wegkamp (2008).
The paper is organized as follows. Section 2 defines the problem and recalls Bayes rule for binary
classification with a reject option. The proposed double hinge loss is derived in Section 3, together
with the decision rule associated with SVM scores. Section 4 addresses implementation issues: it
formalizes the SVM training problem and details an active set algorithm specifically designed for
1

training with the double hinge loss. This implementation is tested empirically in Section 5. Finally,
Section 6 concludes the paper.

2

Problem Setting and the Bayes Classifier

Classification aims at predicting a class label y ? Y from an observed pattern x ? X . For this
purpose, we construct a decision rule d : X ? A, where A is a set of actions that typically consists
in assigning a label to x ? X . In binary problems, where the class is tagged either as +1 or ?1, the
two types of errors are: (i) false positive, where an example labeled ?1 is predicted as +1, incurring
a cost c? ; (ii) false negative, where an example labeled +1 is predicted as ?1, incurring a cost c+ .
In general, the goal of classification is to predict the true label for an observed pattern. However,
patterns close to the decision boundary are misclassified with high probability. This problem becomes especially eminent in cases where the costs, c? or c+ , are high, such as in medical decision
making. In these processes, it might be better to alert the user and abstain from prediction. This
motivates the introduction of a reject option for classifiers that cannot predict a pattern with enough
confidence. This decision to abstain, which is denoted by 0, incurs a cost, r? and r+ for examples
labeled ?1 and +1, respectively.
y

The costs pertaining to each possible decision are recapped on the righthand-side. In what follows, we assume that all costs are strictly positive:
(1)

Furthermore, it should be possible to incur a lower expected loss by
choosing the reject option instead of any prediction, that is
c? r+ + c+ r? < c? c+ .

(2)

d(x)

c? > 0 , c+ > 0 , r? > 0 , r+ > 0 .

+1

?1

+1

0

c?

0

r+

r?

?1

c+

0

Bayes? decision theory is the paramount framework in statistical decision theory, where decisions
are taken to minimize expected losses. For classification with a reject option, the overall risk is
R(d)

= c+ EXY [Y = 1, d(X) = ?1] + c? EXY [Y = ?1, d(X) = 1] +
r+ EXY [Y = 1, d(X) = 0] + r? EXY [Y = ?1, d(X) = 0] ,

(3)

where X and Y denote the random variable describing patterns and labels.
The Bayes classifier d? is defined as the minimizer of the risk R(d). Since the seminal paper of
Chow (1970), this rule is sometimes referred to as Chow?s rule:
(
+1 if P(Y = 1|X = x) > p+
?
?1 if P(Y = 1|X = x) < p?
(4)
d (x) =
0 otherwise ,
c? ? r?
r?
and p? =
.
c? ? r? + r+
c+ ? r+ + r?
Note that, assuming that (1) and (2) hold, we have 0 < p? < p+ < 1.
where p+ =

One of the major inductive principle is the empirical risk minimization, where one minimizes the
empirical counterpart of the risk (3). In classification, this principle usually leads to a NP-hard
problem, which can be circumvented by using a smooth proxy of the misclassification loss. For
example, Vapnik (1995) motivated the hinge loss as a ?computationally simple? (i.e., convex) surrogate of classification error. The following section is dedicated to the construction of such a surrogate
for classification with a reject option.

3

Training Criterion

One method to get around the hardness of learning decision functions is to replace the conditional
b
probability P(Y = 1|X = x) with its estimation P(Y
= 1|X = x), and then plug this estimation
back in (4) to build a classification rule (Herbei & Wegkamp, 2006). One of the most widespread
2

5.1

`p? ,p+ (?1, f (x))

`p? ,p+ (+1, f (x))

5.1

2.5

0

f? 0

2.5

0

f+

f (x)

f? 0

f+

f (x)

Figure 1: Double hinge loss function `p? ,p+ for positive (left) and negative examples (right), with
p? = 0.4 and p+ = 0.8 (solid: double hinge, dashed: likelihood). Note that the decision thresholds
f+ and f? are not symmetric around zero.

representative of this line of attack is the logistic regression model, which estimates the conditional
probability using the maximum (penalized) likelihood framework.
As a starting point, we consider the generalized logistic regression model for binary classification,
where
1
b
P(Y
= y|X = x) =
,
(5)
1 + exp(?yf (x))
and the function f : X ? R is estimated by the minimization of a regularized empirical risk on the
training sample T = {(xi , yi )}ni=1
n
X

`(yi , f (xi )) + ??(f ) ,

(6)

i=1

where ` is a loss function and ?(?) is a regularization functional, such as the (squared) norm of f in
a suitable Hilbert space ?(f ) = kf k2H , and ? is a regularization parameter. In the standard logistic
regression procedure, ` is the negative log-likelihoood loss
`(y, f (x)) = log(1 + exp(?yf (x))) .
This loss function is convex and decision-calibrated (Bartlett & Tewari, 2007), but it lacks an appealing feature of the hinge loss used in SVMs, that is, it does not lead to sparse solutions. This
drawback is the price to pay for the ability to estimate the posterior probability P(Y = 1|X = x)
on the whole range (0, 1) (Bartlett & Tewari, 2007).
However, the definition of the Bayes? rule (4) clearly shows that the estimation of P(Y = 1|X = x)
does not have to be accurate everywhere, but only in the vicinity of p+ and p? . This motivates the
construction of a training criterion that focuses on this goal, without estimating P(Y = 1|X = x)
on the whole range as an intermediate step. Our purpose is to derive such a loss function, without
sacrifying sparsity to the consistency of the decision rule.
Though not a proper negative log-likelihood, the hinge loss can be interpreted in a maximum a
posteriori framework: The hinge loss can be derived as a relaxed minimization of negative loglikelihood (Grandvalet et al., 2006). According to this viewpoint, minimizing the hinge loss aims
at deriving a loose approximation to the the logistic regression model (5) that is accurate only at
f (x) = 0, thus allowing to estimate whether P(Y = 1|X = x) > 1/2 or not. More generally,
one can show that, in order to have a precise estimate of P(Y = 1|X = x) = p, the surrogate loss
should be tangent to the neg-log-likelihood at f = log(p/(1 ? p)).
Following this simple constructive principle, we derive the double hinge loss, which aims at reliably
estimating P(Y = 1|X = x) at the threshold points p+ and p? . Furthermore, to encourage sparsity,
we set the loss to zero for all points classified with high confidence. This loss function is displayed in
Figure 1. Formally, for the positive examples, the double hinge loss satisfying the above conditions
can be expressed as

	
`p? ,p+ (+1, f (x)) = max ? (1 ? p? )f (x) + H(p? ), ?(1 ? p+ )f (x) + H(p+ ), 0 , (7)
3

and for the negative examples it can be expressed as

	
(8)
`p? ,p+ (?1, f (x)) = max p+ f (x) + H(p+ ), p? f (x) + H(p? ), 0 ,
where H(p) = ?p log(p) ? (1 ? p) log(1 ? p). Note that, unless p? = 1 ? p+ , there is no simple
symmetry with respect to the labels.
After training, the decision rule is defined as the plug-in estimation of (4) using the logistic regression probability estimation. Let f+ = log(p+ /(1 ? p+ )) and f? = log(p? /(1 ? p? )), the decision
rule can be expressed in terms of the function f as follows
(
+1 if f (x) > f+
?1 if f (x) < f?
(9)
dp? ,p+ (x; f ) =
0
otherwise .
The following result shows that the rule dp? ,p+ (?; f ) is universally consistent when f is learned by
minimizing empirical risk based on `p? ,p+ . Hence, in the limit, learning with the double hinge loss
is optimal in the sense that the risk for the learned decision rule converges to the Bayes? risk.
Theorem 1. Let H be a functional space that is dense in the set of continuous functions. Suppose
that we have a positive sequence {?n } with ?n ? 0 and n?2n / log n ? ?. We define fn? as
n
1X
arg min
`p? ,p+ (yi , f (xi )) + ?n kf k2H .
f ?H n
i=1
Then, limn?? R(dp? ,p+ (X; fn? )) = R(d? (X)) holds almost surely, that is, the classifier
dp? ,p+ (?; fn? ) is strongly universally consistent.
Proof. Our theorem follows directly from (Steinwart, 2005, Corollary 3.15), since `p? ,p+ is regular
(Steinwart, 2005, Definition 3.9). Besides mild regularity conditions that hold for `p? ,p+ , a loss
function is said regular if, for every ? ? [0, 1], and every t? such that
t? = arg min ? `p? ,p+ (+1, t) + (1 ? ?) `p? ,p+ (?1, t) ,
t

we have that dp? ,p+ (t? , x) agrees with d? (x) almost everywhere.
Let f1 = ?H(p? )/p? , f2 = ?(H(p+ ) ? H(p? ))/(p+ ? p? ) and f3 = H(p+ )/(1 ? p+ ) denote
the hinge locations in `p? ,p+ (?1, f (x)). Note that we have f1 < f? < f2 < f+ < f3 , and that
?
?
?1 if P(Y = 1|x) < p?
(??, f1 ] if 0 ? ? < p?
?
?
?
?
?
?
? ?1 or 0 if P(Y = 1|x) = p?
? [f1 , f2 ] if ? = p?
0 if p? < P(Y = 1|x) < p+
{f2 } if p? < ? < p+ ? dp? ,p+ (t? , x) =
t? ?
?
?
?
?
0
or
+
1
if P(Y = 1|x) = p+
[f
,
f
]
if
?
=
p
2
3
+
?
?
?
?
+1 if P(Y = 1|x) > p+
[f3 , ?) if p+ < ? ? 1
which is the desired result.
Note also that the analysis of Bartlett and Tewari (2007) can be used to show that minimizing `p? ,p+
cannot provide consistent estimates of P(Y = 1|X = x) = p for p ?
/ {p? , p+ }. This property is
desirable regarding sparsity, since sparseness does not occur when the conditional probabilities can
be unambiguously estimated .
Note on a Close Relative A double hinge loss function has been proposed recently with a different perspective by Bartlett and Wegkamp (2008). Their formulation is restricted to symmetric
classification, where c+ = c? = 1 and r+ = r? = r. In this situation, rejection may occur
only if 0 ? r < 1/2, and the thresholds on the conditional probabilities in Bayes? rule (4) are
p? = 1 ? p+ = r.
For symmetric classification, the loss function of Bartlett and Wegkamp (2008) is a scaled version
of our proposal that leads to equivalent solutions for f , but our decision rule differs. While our
probabilistic derivation of the double hinge loss motivates the decision function (9), the decision rule
of Bartlett and Wegkamp (2008) has a free parameter (corresponding to the threshold f+ = ?f? )
whose value is set by optimizing a generalization bound.
Our decision rule rejects more examples when the loss incurred by rejection is small and fewer
examples otherwise. The two rules are identical for r ' 0.24. We will see in Section 5 that this
difference has noticeable outcomes.
4

4

SVMs with Double Hinge

In this section, we show how the standard SVM optimization problem is modified when the hinge
loss is replaced by the double hinge loss. The optimization problem is first written using a compact
notation, and the dual problem is then derived.
4.1

Optimization Problem

Minimizing the regularized empirical risk (6) with the double hinge loss (7?8) is an optimization
problem akin to the standard SVM problem. Let C be an arbitrary constant, we define D = C(p+ ?
p? ), Ci = C(1 ? p+ ) for positive examples, and Ci = Cp? for negative examples. With the
introduction of slack variables ? and ?, the optimization problem can be stated as
?
n
n
X
X
1
?
2
?
?
kf
k
+
C
?
+
D
?i
min
i i
H
?
? f,b,?,? 2
i=1
i=1
(10)
s. t. yi (f (xi ) + b) ? ti ? ?i i = 1, . . . , n
?
?
?
y
(f
(x
)
+
b)
?
?
?
?
i
=
1,
.
.
.
,
n
?
i
i
i
i
?
?i ? 0 , ?i ? 0
i = 1, . . . , n ,
where, for positive examples, ti = H(p+ )/(1 ? p+ ), ?i = ?(H(p? ) ? H(p+ ))/(p? ? p+ ), while,
for negative examples ti = H(p? )/p? , ?i = (H(p? ) ? H(p+ ))/(p? ? p+ ).
For functions f belonging to a Hilbert space H endowed with a reproducing kernel k(?, ?), efficient
optimization algorithms can be drawn from the dual formulation:
?
1 T
?
? min
? G? ? ? T ? ? (t ? ? )T ?
?
? ?,? 2
s. t. yT ? = 0
(11)
?
?
0
?
?
?
C
i
=
1,
.
.
.
,
n
i
i
?
?
0 ? ?i ? ?i ? D i = 1, . . . , n .
T
where y = (y1 , . . . , yn ) , t = (t1 , . . . , tn )T and ? = (?1 , . . . , ?n )T are vectors of Rn and G is the
n ? n Gram matrix with general entry Gij = yi yj k(xi , xj ). Note that (11) is a simple quadratic
problem under box constraints. Compared to the standard SVM dual problem, one has an additional
vector to optimize, but, with the active set we developed, we only have to optimize a single vector of
Rn . The primal variablesP
f and b are then derived from the Karush-Kuhn-Tucker (KKT) conditions.
n
For f , we have: f (?) = i=1 ?i yi k(?, xi ), and b is obtained in the optimization process described
below.
4.2

Solving the Problem

To solve (11), we use an active set algorithm, following a strategy that proved to be efficient in
SimpleSVM (Vishwanathan et al., 2003). This algorithm solves the SVM training problem by a
greedy approach, in which one solves a series of small problems. First, the repartition of training
examples in support and non-support vectors is assumed to be known, and the training criterion is
optimized considering that this partition fixed. Then, this optimization results in an updated partition
of examples in support and non-support vectors. These two steps are iterated until some level of
accuracy is reached.
Partitioning the Training Set The training set is partitioned into five subsets defined by the activity of the box constraints of Problem (11). The training examples indexed by:
I0
It
IC
I?
ID

, defined by I0 = {i|?i = 0}, are such that yi (f (xi ) + b) > ti ;
, defined by It = {i|0 < ?i < Ci }, are such that yi (f (xi ) + b) = ti ;
, defined by IC = {i|?i = Ci }, are such that ?i < yi (f (xi ) + b) ? ti ;
, defined by I? = {i|Ci < ?i < Ci + D}, are such that yi (f (xi ) + b) = ?i ;
, defined by ID = {i|?i = Ci + D}, are such that yi (f (xi ) + b) ? ?i .

When example i belongs to one of the subsets described above, the KKT conditions yield that ?i
is either equal to ?i or constant. Hence, provided that the repartition of examples in the subsets I0 ,
It , IC , I? and ID is known, we only have to consider a problem in ?. Furthermore, ?i has to be
computed only for i ? It ? I? .
5

Updating Dual Variables Assuming a correct partition, Problem (11) reduces to the considerably
smaller problem of computing ?i for i ? IT = It ? I? :
?
X
1 X
?
?i si
?
?
G
?
min
?
i
j
ij
? {?i |i?IT } 2
i?I
i?IT ,j?IT X
T
(12)
X
X
?
?
(C
+
D)
y
=
0
,
C
y
+
y
?
+
s.
t.
i
i
i
i
i
i
?
i?IT

P

i?ID

i?IC

P

P
where si = ti ? j?IC Cj Gji ? j?ID (Cj + D) Gji for i ? It and si = ?i ? j?IC Cj Gji ?
P
j?ID (Cj + D) Gji for i ? I? . Note that the box constraints of Problem (11) do not appear here,
because we assumed the partition to be correct.
The solution of Problem (12) is simply obtained by solving the following linear system resulting
from the first-order optimality conditions:
? X
?
Gij ?j + yi ? = si
for i ? IT
?
?
j?I
T
X
X
X
(13)
?
yi ?i = ?
Ci yi ?
(Ci + D) yi ,
?
?
i?IT

i?IC

i?ID

where ?, which is the (unknown) Lagrange parameter associated to the equality constraint in (12),
is computed along with ?. Note that the |IT | equations of the linear system given on the first line
of (13) express that, for i ? It , yi (f (xi ) + ?) = ti and for i ? I? , yi (f (xi ) + ?) = ?i . Hence, the
primal variable b is equal to ?.
Algorithm The algorithm, described in Algorithm 1, simply alternates updates of the partition of
examples in {I0 , It , IC , I? , ID }, and the ones of coefficients ?i for the current active set IT . As for
standard SVMs, the initialization step consists in either using the solution obtained for a different
hyper-parameter, such as a higher value of C, or in picking one or several examples of each class to
arbitrarily initialize It to a non-empty set, and putting all the other ones in I0 = {1, . . . , n} \ It .
Algorithm 1 SVM Training with a Reject Option
input {xi , yi }1?i?n and hyper-parameters C, p+ , p?
initialize ? old IT = {It , I? }, IT = {I0 , IC , ID },
repeat
solve linear system (13) ? (?i )i?IT , b = ?.
if any (?i )i?IT violates the box constraints (11) then
Compute the largest ? s. t., for all i ? IT ?inew = ?iold + ?(?i ? ?iold ) obey box constraints
Let j denote the index of (?inew )i?IT at bound,
IT = IT \ {j}, IT = IT ? {j}
?jold = ?jnew
else
for all i ? IT do ?inew = ?i
if any (yi (f (xi ) + b))i?IT violates primal constraints (10) then
select i with violated constraint
IT = IT \ {i}, IT = IT ? {i}
else
exact convergence
end if
for all i ? IT do ?iold = ?inew
end if
until convergence
output f , b.
The exact convergence is obtained when all constraints are fulfilled, that is, when all examples belong to the same subset at the begining and the end of the main loop. However, it is possible to relax
the convergence criteria while having a good control on the precision on the solution by monitoring the duality gap, that is the difference between the primal and the dual objectives, respectively
provided in the definition of Problems (10) and (11).
6

Table 1: Performances in terms of average test loss, rejection rate and misclassification rate (rejection is not an error) with r+ = r? = 0.45, for the three rejection methods over four different
datasets.
Average Test Loss Rejection rate (%) Error rate (%)
Wdbc
Naive
2.9 ? 1.6
0.7
2.6
B&W?s
3.5 ? 1.8
3.9
1.8
2.9 ? 1.7
1.2
2.4
Our?s
Liver
Naive
28.9 ? 5.4
3.3
27.4
B&W?s
30.9 ? 4.0
34.5
15.4
Our?s
28.8 ? 5.1
7.9
25.2
.
Thyroid Naive
4.1 ? 2.9
0.9
3.7
B&W?s
4.4 ? 2.7
6.1
1.6
Our?s
3.7 ? 2.7
2.1
2.8
Pima
Naive
23.7 ? 1.9
7.5
20.3
B&W?s
24.7 ? 2.1
24.3
13.8
Our?s
23.1 ? 1.3
6.9
20.0

Theorem 2. Algorithm 1 converges in a finite number of steps to the exact solution of (11).
Proof. The proof follows the ones used to prove the convergence of active set methods in general,
and SimpleSVM in particular, see Propositon 1 in (Vishwanathan et al., 2003)).

5

Experiments

We compare the performances of three different rejection schemes based on SVMs. For this purpose,
we selected the datasets from the UCI repository related to medical problems, as medical decision
making is an application domain for which rejection is of primary importance. Since these datasets
are small, we repeated 10 trials for each problem. Each trial consists in splitting randomly the
examples into a training set with 80 % of examples and an independent test set. Note that the
training examples were normalized to zero-mean and unit variance before cross-validation (test sets
were of course rescaled accordingly).
In a first series of experiments, to compare our decision rule with the one proposed by Bartlett and
Wegkamp (2008) (B&W?s), we used symmetric costs: c+ = c? = 1 and r+ = r? = r. We
also chose r = 0.45, which corresponds to rather low rejection rates, in order to favour different
behaviors between these two decision rules (recall that they are identical for r ' 0.24). Besides
the double hinge loss, we also implemented a ?naive? method that consists in running the standard
SVM algorithm (using the hinge loss) and selecting a symmetric rejection region around zero by
cross-validation.
For all methods, we used Gaussian kernels. Model selection is performed by cross-validation. This
includes the selection of the kernel widths, the regularization parameter C for all methods and
additionally of the rejection thresholds for the naive method. Note that B&W?s and our decision
rules are based on learning with the double-hinge loss. Hence, the results displayed in Table 1 only
differ due to the size of the rejection region, and to the disparities that arise from the choice of
hyper-parameters that may arise in the cross-validation process (since the decision rules differ, the
cross-validation scores differ also).
Table 1 summarizes the averaged performances over the 10 trials. Overall, all methods lead to
equivalent average test losses, with an unsignificant but consistent advantage for our decision rule.
We also see that the naive method tends to reject fewer test examples than the consistent methods.
This means that, for comparable average losses, the decision rules based on the scores learned by
minimizing the double hinge loss tend to classify more accurately the examples that are not rejected,
as seen on the last column of the table.
For noisy problems such as Liver and Pima, we observed that reducing rejection costs considerably
decrease the error rate on classified examples (not shown on the table). The performances of the
two learning methods based on the double-hinge get closer, and there is still no significant gain
7

compared to the naive approach. Note however that the symmetric setting is favourable to the naive
approach, since we only have to estimate a single decision thershold. We are experimenting to see
whether the double-hinge loss shows more substantial improvements for asymmetric losses and for
larger training sets.

6

Conclusion

In this paper we proposed a new solution to the general problem of classification with a reject
option. The double hinge loss was derived from the simple desiderata to obtain accurate estimates
of posterior probabilities only in the vicinity of the decision boundaries. Our formulation handles
asymmetric misclassification and rejection costs and compares favorably to the one of Bartlett and
Wegkamp (2008).
We showed that for suitable kernels, including usual ones such as the Gaussian kernel, training a
kernel machine with the double hinge loss provides a universally consistent classifier with reject
option. Furthermore, the loss provides sparse solutions, with a limited number of support vectors,
similarly to the standard L1-SVM classifier.
We presented what we believe to be the first principled and efficient implementation of SVMs for
classification with a reject option. Our optimization scheme is based on an active set method, whose
complexity compares to standard SVMs. The dimension of our quadratic program is bounded by
the number of examples, and is effectively limited to the number of support vectors. The only
computational overhead is brought by monitoring five categories of examples, instead of the three
ones considered in standard SVMs (support vector, support at bound, inactive example).
Our approach for deriving the double hinge loss can be used for other decision problems relying
on conditional probabilities at specific values or in a limited range or values. As a first example,
one may target the estimation of discretized confidence ratings, such as the ones reported in weather
forecasts. Multi-category classification also belongs to this class of problems, since there, decisions
rely on having precise conditional probabilities within a predefined interval.
Acknowledgements
This work was supported in part by the French national research agency (ANR) through project
GD2GS, and by the IST Programme of the European Community through project DIRAC.

References
Bartlett, P. L., & Tewari, A. (2007). Sparseness vs estimating conditional probabilities: Some asymptotic
results. Journal of Machine Learning Research, 8, 775?790.
Bartlett, P. L., & Wegkamp, M. H. (2008). Classification with a reject option using a hinge loss. Journal of
Machine Learning Research, 9, 1823?1840.
Chow, C. K. (1970). On optimum recognition error and reject tradeoff. IEEE Trans. on Info. Theory, 16, 41?46.
Fumera, G., & Roli, F. (2002). Support vector machines with embedded reject option. Pattern Recognition
with Support Vector Machines: First International Workshop (pp. 68?82). Springer.
Grandvalet, Y., Mari?ethoz, J., & Bengio, S. (2006). A probabilistic interpretation of SVMs with an application
to unbalanced classification. NIPS 18 (pp. 467?474). MIT Press.
Herbei, R., & Wegkamp, M. H. (2006). Classification with reject option. The Canadian Journal of Statistics,
34, 709?721.
Kwok, J. T. (1999). Moderating the outputs of support vector machine classifiers. IEEE Trans. on Neural
Networks, 10, 1018?1031.
Steinwart, I. (2005). Consistency of support vector machine and other regularized kernel classifiers. IEEE
Trans. on Info. Theory, 51, 128?142.
Vapnik, V. N. (1995). The nature of statistical learning theory. Springer Series in Statistics. Springer.
Vishwanathan, S. V. N., Smola, A., & Murty, N. (2003). SimpleSVM. Proceedings of the Twentieth International Conference on Machine Learning (pp. 68?82). AAAI.

8

"
2149,2005,On Local Rewards and Scaling Distributed Reinforcement Learning,Abstract Missing,"On Local Rewards and Scaling Distributed
Reinforcement Learning
J. Andrew Bagnell
Robotics Institute
Carnegie Mellon University
Pittsburgh, PA 15213

Andrew Y. Ng
Computer Science Department
Stanford University
Stanford, CA 94305

dbagnell@ri.cmu.edu

ang@cs.stanford.edu

Abstract
We consider the scaling of the number of examples necessary to achieve
good performance in distributed, cooperative, multi-agent reinforcement
learning, as a function of the the number of agents n. We prove a worstcase lower bound showing that algorithms that rely solely on a global
reward signal to learn policies confront a fundamental limit: They require a number of real-world examples that scales roughly linearly in the
number of agents. For settings of interest with a very large number of
agents, this is impractical. We demonstrate, however, that there is a class
of algorithms that, by taking advantage of local reward signals in large
distributed Markov Decision Processes, are able to ensure good performance with a number of samples that scales as O(log n). This makes
them applicable even in settings with a very large number of agents n.

1

Introduction

Recently there has been great interest in distributed reinforcement learning problems where
a collection of agents with independent action choices attempts to optimize a joint performance metric. Imagine, for instance, a traffic engineering application where each traffic
signal may independently decide when to switch colors, and performance is measured by
aggregating the throughput at all traffic stops. Problems with such factorizations where the
global reward decomposes in to a sum of local rewards are common and have been studied
in the RL literature. [10]
The most straightforward and common approach to solving these problems is to apply one
of the many well-studied single agent algorithms to the global reward signal. Effectively,
this treats the multi-agent problem as a single agent problem with a very large action space.
Peshkin et al. [9] establish that policy gradient learning factorizes into independent policy
gradient learning problems for each agent using the global reward signal. Chang et al. [3]
use global reward signals to estimate effective local rewards for each agent. Guestrin et
al. [5] consider coordinating agent actions using the global reward. We argue from an
information theoretic perspective that such algorithms are fundamentally limited in their
scalability. In particular, we show in Section 3 that as a function of the number of agents
?
n, such algorithms will need to see1 ?(n)
trajectories in the worst case to achieve good
performance.
We suggest an alternate line of inquiry, pursued as well by other researchers (including
1

? notation omits logarithmic terms, similar to how big-? notation drops constant values.
Big-?

notably [10]), of developing algorithms that capitalize on the availability of local reward
signals to improve performance. Our results show that such local information can dramatically reduce the number of examples necessary for learning to O(log n). One approach
that the results suggest to solving such distributed problems is to estimate model parameters
from all local information available, and then to solve the resulting model offline. Although
this clearly still carries a high computational burden, it is much preferable to requiring a
large amount of real-world experience. Further, useful approximate multiple agent Markov
Decision Process (MDP) solvers that take advantage of local reward structure have been
developed. [4]

2

Preliminaries

We consider distributed reinforcement learning problems, modeled as MDPs, in which
there are n (cooperative) agents, each of which can directly influence only a small number
of its neighbors. More formally, let there be n agents, each with a finite state space S of
size |S| states and a finite action space A of size |A|. The joint state space of all the agents
is therefore S n , and the joint action space An . If st ? S n is the joint state of the agents at
(i)
(i)
time t, we will use st to denote the state of agent i. Similarly, let at denote the action of
agent i.
For each agent i ? {1, . . . , n}, we let neigh(i) ? {1, . . . , n} denote the subset of
agents that i?s state directly influences. For notational convenience, we assume that if
i ? neigh(j), then j ? neigh(i), and that i ? neigh(i). Thus, the agents can be viewed
as living on the vertices of a graph, where agents have a direct influence on each other?s
state only if they are connected by an edge. This is similar to the graphical games formalism of [7], and is also similar to the Dynamic Bayes Net (DBN)-MDP formalisms of [6]
and [2]. (Figure 1 depicts a DBN and an agent influence graph.) DBN formalisms allow
the more refined notion of directionality in the influence between neighbors.
More formally, each agent i is associated with a CPT (conditional probability table)
(i)
(neigh(i)) (i)
(neigh(i))
Pi (st+1 |st
, at ), where st
denotes the state of agent i?s neighbors at time
t. Given the joint action a of the agents, the joint state evolves according to
n
Y
(i)
(neigh(i)) (i)
p(st+1 |st
, at ).
(1)
p(st+1 |st , at ) =
i=1

For simplicity, we have assumed that agent i?s state is directly influenced by the states of
neigh(i) but not their actions; the generalization offers no difficulties. The initial state s1
is distributed according to some initial-state distribution D.
A policy is a map ? : S n 7? An . Writing ? out explicitly as a vector-valued function, we
have ?(s) = (?1 (s), . . . , ?n (s)), where ?i (s) : S n 7? A is the local policy of agent i. For
some applications, we may wish to consider only policies in which agent i chooses its local
action as a function of only its local state s(i) (and possibly its neighbors); in this case, ?i
can be restricted to depend only on s(i) .

Each agent has a local reward function Ri (s(i) , a(i) ), which takes values
Pnin the unit interval [0, 1]. The total payoff in the MDP at each step is R(s, a) = (1/n) i=1 R(s(i) , a(i) ).
We call this R(s, a) the global reward function, since it reflects the total reward received
by the joint set of agents. We will consider the finite-horizon setting, in which the MDP
terminates after T steps. Thus, the utility of a policy ? in an MDP M is
#
"" T n
1 XX
(i) (i)
?
Ri (st , at )|? .
U (?) = UM (?) = Es1 ?D [V (s1 )] = E
n t=1 i=1

In the reinforcement learning setting, the dynamics (CPTs) and rewards of the problem are
unknown, and a learning algorithm has to take actions in the MDP and use the resulting
observations of state transitions and rewards to learn a good policy. Each ?trial? taken by a
reinforcement learning algorithm shall consist of a T -step sequence in the MDP.

Figure 1: (Left) A DBN description of a multi-agent MDP. Each row of (round) nodes in the DBN
corresponds to one agent. (Right) A graphical depiction of the influence effects in a multi-agent
MDP. A connection between nodes in the graph implies arrows connecting the nodes in the DBN.

Our goal is to characterize the scaling of the sample complexity for various reinforcement
learning approaches (i.e., how many trials they require in order to learn a near-optimal
policy) for large numbers of agents n. Thus, in our bounds below, no serious attempt has
been made to make our bounds tight in variables other than n.

3

Global rewards hardness result

Below we show that if an RL algorithm uses only the global reward signal, then there
exists a very simple MDP?one with horizon, T = 1, only one state/trivial dynamics, and
?
two actions per agent?on which the learning algorithm will require ?(n)
trials to learn
a good policy. Thus, such algorithms do not scale well to large numbers of agents. For
example, consider learning in the traffic signal problem described in the introduction with
n = 100, 000 traffic lights. Such an algorithm may then require on the order of 100, 000
days of experience (trials) to learn. In contrast, in Section 4, we show that if a reinforcement
learning algorithm is given access to the local rewards, it can be possible to learn in such
problems with an exponentially smaller O(log n) sample complexity.
Theorem 3.1: Let any 0 < ? < 0.05 be fixed. Let any reinforcement learning algorithm L
be given that only uses the global reward signal R(s), and does not use the local rewards
Ri (s(i) ) to learn (other than through their sum). Then there exists an MDP with time
horizon T = 1, so that:
1. The MDP is very ?simple? in that it has only one state (|S| = 1, |S n | = 1); trivial
state transition probabilities (since T = 1); two actions per agent (|A| = 2); and
deterministic binary (0/1)-valued local reward functions.
2. In order for L to output a policy ?
? that is near-optimal satisfying2 U (?
?) ?
max? U (?) ? ?,it is necessary that the number of trials m be at least
0.32n + log(1/4)
?
m?
= ?(n).
log(n + 1)
Proof. For simplicity, we first assume that L is a deterministic learning algorithm, so that
in each of the m trials, its choice of action is some deterministic function of the outcomes
of the earlier trials. Thus, in each of the m trials, LP
chooses a vector of actions a ? AN ,
n
1
and receives the global reward signal R(s, a) = n i=1 R(s(i) , a(i) ). In our MDP, each
(i) (i)
local reward R(s , a ) will take values only 0 and 1. Thus, R(s, a) can take only n + 1
different values (namely, n0 , n1 , . . . , nn ). Since T = 1, the algorithm receives only one such
reward value in each trial.
Let r1 , . . . , rm be the m global reward signals received by L in the m trials. Since L is
deterministic, its output policy ?
? will be chosen as some deterministic function of these
2
For randomized algorithms we consider instead the expectation of U (?
? ) under the algorithm?s
randomization.

rewards r1 , . . . , rm . But the vector (r1 , . . . , rm ) can take on only (n + 1)m different values
(since each rt can take only n + 1 different values), and thus ?
? itself can also take only at
most (n + 1)m different values. Let ?m denote this set of possible values for ?
? . (|?m | ?
(n + 1)m ).
Call each local agent?s two actions a1 , a2 . We will generate an MDP with randomly chosen
parameters. Specifically, each local reward Ri (s(i) , a(i) ) function is randomly chosen with
equal probability to either give reward 1 for action a1 and reward 0 for action a2 ; or vice
versa. Thus, each local agent has one ?right? action that gives reward 1, but the algorithm
has to learn which of the two actions this is. Further, by choosing the right actions, the
optimal policy ? ? attains U (? ? ) = 1.
Pn
Fix any policy ?. Then UM (?) = n1 i=1 R(s(i) , ?(s(i) )) is the mean of n independent
Bernoulli(0.5) random variables (since the rewards are chosen randomly), and has expected
value 0.5. Thus, by the Hoeffding inequality, P (UM (?) ? 1?2?) ? exp(?2(0.5?2?)2 n).
Thus, taking a union bound over all policies ? ? ?M , we have
P (?? ? ?M s.t. UM (?) ? 1 ? 2?) ? |?M | exp(?2(0.5 ? 2?)2 n)
(2)
? (n + 1)m exp(?2(0.5 ? 2?)2 n)
(3)
Here, the probability is over the random MDP M . But since L outputs a policy in ?M , the
chance of L outputting a policy ?
? with UM (?
? ) ? 1 ? 2? is bounded by the chance that
there exists such a policy in ?M . Thus,
P (UM (?
? ) ? 1 ? 2?) ? (n + 1)m exp(?2(0.5 ? 2?)2 n).
(4)
By setting the right hand side to 1/4 and solving for m, we see that so long as
2(0.5 ? 2?)2 n + log(1/4)
0.32n + log(1/4)
m<
?
,
(5)
log(n + 1)
log(n + 1)
we have that P (UM (?
? ) ? 1 ? 2?) < 1/4. (The second equality above follows by taking
? < 0.05, ensuring that no policy will be within 0.1 of optimal.) Thus, under this condition,
by the standard probabilistic method argument [1], there must be at least one such MDP
under which L fails to find an ?-optimal policy.
For randomized algorithms L, we can define for each string of input random numbers
to the algorithm ? a deterministic algorithm L? . Given m samples above, the expected
performance of algorithm L? over the distribution of MDPs
Ep(M ) [L? ] ? P r(UM (L? ) ? 1 ? 2?)1 + (1 ? P r(UM (L? ) ? 1 ? 2?))(1 ? 2?)
1 3
<
+ (1 ? 2?) < 1 ? ?
4 4
Since
Ep(M ) Ep(?) [UM (L? )] = Ep(?) Ep(M ) [UM (L? )] < Ep(?) [1 ? ?]
it follows again from the probabilistic method there must be at least one MDP for which
the L has expected performance less than 1 ? ?.


4

Learning with local rewards

Assuming the existence of a good exploration policy, we now show a positive result that if
our learning algorithm has access to the local rewards, then it is possible to learn a nearoptimal policy after a number of trials that grows only logarithmically in the number of
agents n. In this section, we will assume that the neighborhood structure (encoded by
neigh(i)) is known, but that the CPT parameters of the dynamics and the reward functions
are unknown. We also assume that the size of the largest neighborhood is bounded by
maxi |neigh(i)| = B.
Definition. A policy ?explore is a (?, ?)-exploration policy if, given any i, any configuration
of states s(neigh(i)) ? S |neigh(i)| , and any action a(i) ? A, on a trial of length T the policy
?explore has at least a probability ? ? ?B of executing action a(i) while i and its neighbors
are in state s(neigh(i)) .

Proposition 4.1: Suppose the MDP?s initial state distribution is random, so that the state
(i)
si of each agent i is chosen independently from some distribution Di . Further, assume
that Di assigns probability at least ? > 0 to each possible state value s ? S. Then
the ?random? policy ? (that on each time-step chooses each agent?s action uniformly at
1
random over A) is a (?, |A|
)-exploration policy.
Proof. For any agent i, the initial state of s(neigh(i)) has has at least a ?B chance of being
any particular vector of values, and the random action policy has a 1/|A| chance of taking
any particular action from this state.

In general, it is a fairly strong assumption to assume that we have an exploration policy.
However, this assumption serves to decouple the problem of exploration from the ?sample
complexity? question of how much data we need from the MDP. Specifically, it guarantees
that we visit each local configuration sufficiently often to have a reasonable amount of data
to estimate each CPT. 3
In the envisioned procedure, we will execute an exploration policy for m trials, and
then use the resulting data we collect to obtain the maximum-likelihood estimates for the
(i)
(neigh(i)) (i)
CPT entries and the rewards. We call the resulting estimates p?(st+1 |st
, at ) and
? (i) , a(i) ).4 The following simple lemma shows that, with a number of trials that grows
R(s
only logarithmically in n, this procedure will give us good estimates for all CPTs and local
rewards.
Lemma 4.2: Let any ?0 > 0, ? > 0 be fixed. Suppose |neigh(i)| ? B for all i, and let
a (?, ?)-exploration policy be executed for m trials. Then in order to guarantee that, with
probability at least 1 ? ?, the CPT and reward estimates are ?0 -accurate:
(i)

(neigh(i))

|?
p(st+1 |st

(i)

(i)

(neigh(i))

(i)

, at ) ? p(st+1 |st
, at )| ? ?0
(i) (i)
(i) (i)
?
|R(s , a )| ? R(s , a )| ? ?0

(i)

(neigh(i))

for all i, st+1 , st

for all i, s(i) , a(i) ,

(i)

, at

(6)

it suffices that the number of trials be
m = O((log n) ? poly(

1 1
, , |S|, |A|, 1/(??B ), B, T )).
?0 ?

Proof (Sketch). Given c examples to estimate a particular CPT entry (or a reward table
entry), the probability that this estimate differs from the true value by more than ?0 can be
controlled by the Hoeffding bound:
(i)

(neigh(i))

P (|?
p(st+1 |st

(i)

(i)

(neigh(i))

, at ) ? p(st+1 |st

(i)

, at )| ? ?0 ) ? 2 exp(?2?20 c).

Each CPT has at most |A||S|B+1 entries and there are n such tables. There are also
n|S||A| possible local reward values. Taking a union bound over them, setting our probability of incorrectly estimating any CPTs or rewards to ?/2, and solving for c gives
B+1
c ? ?22 log( 4 n |A||S|
). For each agent i we see each local configurations of states and
?
0

actions (s(neigh(i)) , a(i) ) with probability ? ?B ?. For m trajectories the expected number
3
Further, it is possible to show a stronger version of our result than that stated below, showing that
a random action policy can always be used as our exploration policy, to obtain a sample complexity
bound with the same logarithmic dependence on n (but significantly worse dependencies on T and
B). This result uses ideas from the random trajectory method of [8], with the key observation that
local configurations that are not visited reasonably frequently by the random exploration policy will
not be visited frequently by any policy, and thus inaccuracies in our estimates of their CPT entries
will not significantly affect the result.
(i)
(neigh(i))
(i)
(neigh(i))
(i)
4
We let p?(st+1 |st
, at ) be the uniform distribution if (st
, at ) was never ob(i)
(i)
(i)
(i)
?
?
served in the training data, and similarly let R(s , a ) = 0 if R(s , a ) was never observed.

(s(neigh(i)) ,a(i) )

of samples we see for each CPT entry is at least m?B ?. Call Sm
the number of
samples we?ve seen of a configuration (s(neigh(i)) , a(i) ) in m trajectories. Note then that:
(s
P (Sm

(neigh(i))

,a(i) )

(s
? c) ? P (Sm

(neigh(i))

,a(i) )

(s
? E[Sm

(neigh(i))

,a(i) )

] ? c ? m?B ?).

and another application of Hoeffding?s bound ensures that:
?2
(c ? m?B ?)2 ).
mT 2
Applying again the union bound to ensure that the probability of failure here is ? ?/2 and
solving for m gives the result.

Definition. Define the radius of influence r(t) after t steps to be the maximum number of
nodes that are within t steps in the neighborhood graph of any single node.
Viewed differently, r(t) upper bounds the number of nodes in the t-th timeslice of the DBN
(as in Figure 1) which are decendants of any single node in the 1-st timeslice. In a DBN
as shown in Figure 1, we have r(t) = O(t). If the neighborhood graph is a 2-d lattice in
which each node has at most 4 neighbors, then r(t) = O(t2 ). More generally, we might
expect to have r(t) = O(t2 ) for ?most? planar neigborhood graphs. Note that, even in the
worst case, by our assumption of each node having B neighbors, we still have the bound
r(t) ? B t , which is a bound independent of the number of agents n.
(s
P (Sm

(neigh(i))

,a(i) )

(s
? E[Sm

(neigh(i))

,a(i) )

] ? c ? m?B ?) ? exp(

Theorem 4.3: Let any ? > 0, ? > 0 be fixed. Suppose |neigh(i)| ? B for all i, and let a
? be the maximum
(?, ?)-exploration policy be executed for m trials in the MDP M . Let M
likelihood MDP, estimated from data from these m trials. Let ? be a policy class, and let
?
? = arg max UM? (?)
???

? . Then to ensure that, with probability
be the best policy in the class, as evaluated on M
1 ? ?, we have that ?
? is near-optimal within ?, i.e., that
UM (?
? ) ? max UM (?) ? ?,
???

it suffices that the number of trials be:
m = O((log n) ? poly(1/?, 1/?, |S|, |A|, 1/(??B )), B, T, r(T )).

Proof. Our approach is essentially constructive: we show that for any policy, finite-horizon
value-iteration using approximate CPTs and rewards in its backups will correctly estimate
the true value function for that policy within ?/2. For simplicity, we assume that the initial
? and M ); the generalization offers no
state distribution is known (and thus the same in M
difficulties. By lemma (4.2) with m samples we can know both CPTs and rewards with the
probability required within any required ?0 .
Note also that for any MDP with the given DBN or neighborhood graph structure (including
? ) the value function for every policy ? and at each time-step has a property
both M and M
of bounded variation:
r(T )T
(i)
|V?t (s(1) , . . . s(n) ) ? V?t (s(1) , . . . s(i?1) , schanged , s(i+1) , . . . , s(n) | ?
n
This follows since a change in state can effect at most r(T ) agents? states, so the resulting
change in utility must be bounded by r(T )T /n.
To compute a bound on the error in our estimate of overall utility we compute a bound
? V? ||? . This quantity can
on the error induced by a one-step Bellman backup ||B V? ? B
be bounded in turn by considering the sequence of partially correct backup operators
?0 , . . . , B
?n where B
?i is defined as the Bellman operator for policy ? using the exact tranB
sitions and rewards for agents 1, 2, . . . , i, and the estimated transitions rewards/transitions

2500

200 agents, 20% noise is observed rewards
1

local learner
global learner

local learner
global learner

0.9

2000

number of samples necessary

0.8

0.7

performance

0.6

0.5

0.4

1500

1000

0.3

500

0.2

0.1

0

0

500

1000
1500
number of training examples

2000

2500

0

0

50

100

150

200
250
number of agents

300

350

400

Figure 2: (Left) Scaling of performance as a function of the number of trajectories seen for a global
reward and local reward algorithms. (Right) Scaling of the number of samples necessary to achieve
near optimal reward as a function of the number of agents.

for agents i + 1, . . . , n. From this definition it is immediate that the total error is equivalent
to the telescoping sum:
? V? ||? = ||B
?0 V? ? B
?1 V? + B
?1 V? ? ... + B
?n?1 V? ? B
?n V? ||?
||B V? ? B
(7)
Pn?1 ? ?
?i+1 V? ||? .
That sum is upper-bounded by the sum of term-by-term errors i=0 ||Bi V ? B
We can show that each of the terms in the sum is less than ?0 r(T )(T + 1)/n since the
?i V? ? B
?i+1 V? differ in the immediate reward contribution of agent
Bellman operators B
i + 1 by ? ?0 and differ in computing the expected value of the future value by
X
Qn
j
EQi+1 j
[
?p(si+1 |st , ?)V?t+1 (s)],
j=1

p(st+1 |st ,?)

j=i+2

t+1

p(st+1 |st ,?)

si+1

?
?
with ?p(si+1
t+1 |st , ?) ? ?0 the difference in the CPTs between Bi and Bi+1 . By the
bounded variation argument this total is then less than ?0 r(T )T |S|/n. It follows then
P ? ?
?
?
i ||Bi V ? Bi+1 V ||? ? ?0 r(T ) (T + 1)|S|. We now appeal to finite-horizon bounds
on the error induced by Bellman backups [11] to show that the ||V? ? V ||? ? T ||B V? ?
? V? ||? ? T (T + 1) ?0 r(T )|S|. Taking the expectation of V? with respect to the initial
B
state distribution D and setting m according to Lemma (4.2) with ?0 = 2|S|r(T )? T (T +1)
completes the proof.


5

Demonstration

We first present an experimental domain that hews closely to the theory in Section (3) above
to demonstrate the importance of local rewards. In our simple problem there are n = 400
independent agents who each choose an action in {0, 1}. Each agent has a ?correct? action
that earns it reward Ri = 1 with probability 0.8, and reward 0 with probability 0.2. Equally,
if the agents chooses the wrong action, it earns reward Ri = 1 with probability 0.2.
We compare two methods on this problem. Our first global algorithm uses only the global
rewards R and uses this to build a model of the local rewards, and finally solves the resulting estimated MDP exactly. The local reward functions are learnt by a least-squares
procedure with basis functions for each agent. The second algorithm also learns a local
reward function, but does so taking advantage of the local rewards it observes as opposed
to only the global signal. Figure (2) demonstrates the advantages of learning using a global
reward signal.5 On the right in Figure (2), we compute the time required to achieve 41 of
optimal reward for each algorithm, as a function of the number of agents.
In our next example, we consider a simple variant of the multi-agent S YS A DMIN6 prob5
A gradient-based model-free approach using the global reward signal was also tried, but its
performance was significantly poorer than that of the two algorithms depicted in Figure (2, left).
6
In S YS A DMIN there is a network of computers that fail randomly. A computer is more likely to
fail if a neighboring computer (arranged in a ring topology) fails. The goal is to reboot machines in
such a fashion so a maximize the number of running computers.

lem [4]. Again, we consider two algorithms: a global R EINFORCE [9] learner, and a R E INFORCE algorithm run using only local rewards, even through the local R EINFORCE algorithm run in this way is not guaranteed to converge to the globally optimal (cooperative)
solution. We note that the local algorithm learns much more quickly than using the global
reward. (Figure 3) The learning speed we observed for the global algorithm correlates well
with the observations in [5] that the number of samples needed scales roughly linearly in
the number of agents. The local algorithm continued to require essentially the same number
of examples for all sizes used (up to over 100 agents) in our experiments.
0.9

0.9
Global
Local

0.85
0.8

0.8

0.75

0.75

0.7

0.7

0.65

0.65

0.6

0.6

0.55

0.55

0.5

0.5

0.45

0.45

0.4

0

50

100

150

200

250

300

350

400

450

Global
Local

0.85

500

0.4

0

50

100

150

200

250

300

350

400

450

500

Figure 3: R EINFORCE applied to the multi-agent S YS A DMIN problem. Local refers to R EINFORCE
applied using only neighborhood (local) rewards while global refers to standard R EINFORCE (applied
to the global reward signal). (Left) shows averaged reward performance as a function of number of
iterations for 10 agents. (Right) depicts the performance for 20 agents.

References
[1]

N. Alon and J. Spencer. The Probabilistic Method. Wiley, 2000.

[2]

C. Boutilier, T. Dean, and S. Hanks. Decision theoretic planning: Structural assumptions and
computational leverage. Journal of Artificial Intelligence Research, 1999.

[3]

Y. Chang, T. Ho, and L. Kaelbling. All learning is local: Multi-agent learning in global reward
games. In Advances in NIPS 14, 2004.

[4]

C. Guestrin, D. Koller, and R. Parr. Multi-agent planning with factored MDPs. In NIPS-14,
2002.

[5]
[6]

M. Kearns and D. Koller. Efficient reinforcement learning in factored mdps. In IJCAI 16, 1999.

[7]

M. Kearns, M. Littman, and S. Singh. Graphical models for game theory. In UAI, 2001.

[8]

M. Kearns, Y. Mansour, and A. Ng. Approximate planning in large POMDPs via reusable
trajectories. (extended version of paper in NIPS 12), 1999.

[9]

L. Peshkin, K-E. Kim, N. Meleau, and L. Kaelbling. Learning to cooperate via policy search.
In UAI 16, 2000.

C. Guestrin, M. Lagoudakis, and R. Parr. Coordinated reinforcement learning. In ICML, 2002.

[10] J. Schneider, W. Wong, A. Moore, and M. Riedmiller. Distributed value functions. In ICML,
1999.
[11] R. Williams and L. Baird. Tight performance bounds on greedy policies based on imperfect
value functions. Technical report, Northeastern University, 1993.

"
6535,2017,Graph Matching via Multiplicative Update Algorithm,"As a fundamental problem in computer vision, graph matching problem can usually be formulated as a Quadratic Programming (QP) problem with doubly stochastic and discrete (integer) constraints. Since it is NP-hard, approximate algorithms are required. In this paper, we present a new algorithm, called Multiplicative Update Graph Matching (MPGM), that develops a multiplicative update technique to solve the QP matching problem. MPGM has three main benefits: (1) theoretically, MPGM solves the general QP problem with doubly stochastic constraint naturally whose convergence and KKT optimality are guaranteed. (2) Em- pirically, MPGM generally returns a sparse solution and thus can also incorporate the discrete constraint approximately. (3) It is efficient and simple to implement. Experimental results show the benefits of MPGM algorithm.","Graph Matching via Multiplicative Update Algorithm

Bo Jiang
School of Computer Science
and Technology
Anhui University, China
jiangbo@ahu.edu.cn

Jin Tang
School of Computer Science
and Technology
Anhui University, China
tj@ahu.edu.cn

Yihong Gong
School of Electronic
and Information Engineering
Xi?an Jiaotong University, China
ygong@mail.xjtu.edu.cn

Chris Ding
CSE Department,
University of Texas at
Arlington, Arlington, USA
chqding@uta.edu

Bin Luo
School of Computer Science
and Technology,
Anhui University, China
luobin@ahu.edu.cn

Abstract
As a fundamental problem in computer vision, graph matching problem can
usually be formulated as a Quadratic Programming (QP) problem with doubly
stochastic and discrete (integer) constraints. Since it is NP-hard, approximate
algorithms are required. In this paper, we present a new algorithm, called Multiplicative Update Graph Matching (MPGM), that develops a multiplicative update
technique to solve the QP matching problem. MPGM has three main benefits: (1)
theoretically, MPGM solves the general QP problem with doubly stochastic constraint naturally whose convergence and KKT optimality are guaranteed. (2) Empirically, MPGM generally returns a sparse solution and thus can also incorporate
the discrete constraint approximately. (3) It is efficient and simple to implement.
Experimental results show the benefits of MPGM algorithm.

1

Introduction

In computer vision and machine learning area, many problems of interest can be formulated by
graph matching problem. Previous approaches [3?5, 15, 16] have formulated graph matching as a
Quadratic Programming (QP) problem with both doubly stochastic and discrete constraints. Since
it is known to be NP-hard, many approximate algorithms have been developed to find approximate
solutions for this problem [8, 16, 21, 24, 20, 13].
One kind of approximate methods generally first develop a continuous problem by relaxing the discrete constraint and aim to find the optimal solution for this continuous problem. After that, they
obtain the final discrete solution by using a discretization step such as Hungarian or greedy algorithm [3, 15, 16]. Obviously, the discretization step of these methods is generally independent of the
matching objective optimization process which may lead to weak local optimum for the problem.
Another kind of methods aim to obtain a discrete solution for QP matching problem [16, 1, 24].
For example, Leordeanu et al. [16] proposed an iterative matching method (IPFP) which optimized
the QP matching problem in a discrete domain. Zhou et al. [24, 25] proposed an effective graph
matching method (FGM) which optimized the QP matching problem approximately using a convexconcave relaxation technique [21] and thus returns a discrete solution for the problem. From optimization aspect, the core optimization algorithm used in both IPFP [16] and FGM [24] is related to
Frank-Wolfe [9] algorithm and FGM [24, 25] further uses a path following procedure to alleviate the
local-optimum problem more carefully. The core of Frank-Wolfe [9] algorithm is to optimize the
quadratic problem by sequentially optimizing the linear approximations of QP problem. In addition
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

to optimization-based methods, probabilistic methods can also be used for solving graph matching
problems [3, 19, 23].
In this paper, we propose a new algorithm, called Multiplicative Update Graph Matching (MPGM),
that develops a multiplicative update technique for the general QP problem with doubly stochastic constraint. Generally, MPGM has the following three main aspects. First, MPGM solves the
general QP problem with doubly stochastic constraint directly and naturally. In MPGM algorithm,
each update step has a closed-form solution and the convergence of the algorithm is also guaranteed.
Moreover, the converged solution is guaranteed to be Karush-Kuhn-Tucker (KKT) optimality. Second, empirically, MPGM can generate a sparse solution and thus incorporates the discrete constraint
naturally in optimization. Therefore, MPGM can obtain a local optimal discrete solution for the
QP matching problem. Third, it is efficient and simple to implement. Experimental results on both
synthetic and real-world matching tasks demonstrate the effectiveness and benefits of the proposed
MPGM algorithm.

2

Problem Formulation and Related Works

Problem Formulation. Assume G = (V, E) and G? = (V ? , E ? ) are two attributed graphs to be
matched, where each node vi ? V or edge eik ? E has an attribute vector ai or rik . The aim of graph
matching problem is to establish the correct correspondences between V and V ? . For each correspondence (vi , vj? ), there is an affinity Sa (ai , a?j ) that measures how well node vi ? V matches node
vj? ? V ? . Also, for each correspondence pair (vi , vj? ) and (vk , vl? ), there is an affinity Sr (rik , r?jl )
that measures the compatibility between node pair (vi , vk ) and (vj? , vl? ). One can define an affinity
matrix W whose diagonal term Wij,ij represents Sa (ai , a?j ), and the non-diagonal element Wij,kl
contains Sr (rik , r?jl ). The one-to-one correspondences can be represented by a permutation matrix
X ? {0, 1}n?n , where n = |V | = |V ? |1 . Here, Xij = 1 implies that node vi in G corresponds to
node vj? in G? , and Xij = 0 otherwise. In this paper, we denote x = (X11 ...Xn1 , ..., X1n ...Xnn )T
as a column-wise vectorized replica of X. The graph matching problem is generally formulated as
a Quadratic Programming (QP) problem with doubly stochastic and discrete constraints [16, 3, 10],
i.e.,
x? = arg max(xT Wx) s.t. x ? P,
(1)
x

where P is defined as,
P = {x | ?i

?n
j=1

xij = 1, ?j

?n
i=1

xij = 1, xij ? {0, 1}}

(2)

The above QP problem is NP-hard and thus approximate relaxations are usually required. One
popular way is to relax the permutation domain P to the doubly stochastic domain D,
?n
?n
(3)
D = {x|?i j=1 xij = 1, ?j
i=1 xij = 1, xij ? 0}.
That is solving the following relaxed matching problem [21, 20, 10],
x? = arg max(xT Wx)
x

s.t. x ? D.

(4)

Since W is not necessarliy positive (or negative) semi-definite, thus this problem is generally not a
concave or convex problem.
Related Works. Many algorithms have been proposed to find a local optimal solution for the above
QP matching problem (Eq.(4)). One kind of popular methods is to use constraint relaxation and projection, such as GA [10] and RRWM [3]. Generally, they iteratively conduct the following two steps:
(a) searching for a solution by ignoring the doubly stochastic constraint temporarily; (b) Projecting
the current solution onto the desired doubly stochastic domain to obtain a feasible solution. Note
that the projection step (b) is generally independent of the optimization step (a) and thus may lead to
weak local optimum. Another kind of important methods is to use objective function approximation
and thus solves the problem approximately, such as Frank-Wolfe algorithm [9]. Frank-Wolfe aims
to optimize the above quadratic problem by sequentially solving the approximate linear problems.
This algorithm has been widely adopted in many recent matching methods [16, 24, 21], such as IPFP
[16] and FGM [24].
1

Here, we focus on equal-size graph matching problem. For graphs with different sizes, one can add dummy
isolated nodes into the smaller graph and transform them to equal-size case [21, 10]

2

3

Algorithm

Our aim in this paper is to develop a new algorithm to solve the general QP matching problem Eq.(4).
We call it as Multiplicative Update Graph Matching (MPGM). Formally, starting with an initial
solution vector x(0) , MPGM solves the problem Eq.(4) by iteratively updating a current solution
vector x(t) , t = 0, 1... as follows,
(t+1)

xkl

(t)

= xkl

[ 2(Wx(t) ) + ?? + ?? ]1/2
kl
l
k
,
+
?+
+
?
k
l

(5)

?
?
+
where ?+
k = (|?k | + ?k )/2, ?k = (|?k | ? ?k )/2, ?k = (|?k | + ?k )/2, ?k = (|?k | ? ?k )/2,
and the Lagrangian multipliers (?, ?) are computed as,
]
(
)
)?1 [
(
(
T)
T
T
T
diag K(t) X(t) ? X(t) diag K(t) X(t)
? =2 I ? X(t) X(t)
(
T)
? =2 diag K(t) X(t) ? X(t) ?
(6)

where K(t) , X(t) are the matrix forms of vector (Wx(t) ) and x(t) , respectively, i.e., K(t) , X(t) ?
(t)
(t)
(t)
Rn?n and Kkl = (Wx(t) )kl , Xkl = xkl . ? = (?1 , ? ? ? ?n )T ? Rn?1 , ? = (?1 , ? ? ? ?n )T ?
n?1
R
. The iteration starts with an initial x(0) and is repeated until convergence.
Complexity. The main complexity in each iteration is on computing Wx(t) . Thus, the total computational complexity for MPGM is less than O(M N 2 ), where N = n2 is the length of vector x(t)
and M is the maximum iteration. Our experience is that the algorithm converges quickly and the
average maximum iteration M is generally less than 200. Theoretically, the complexity of MPGM
is the same with RRWM [3] and IPFP [16], but obviously lower than GA [10] and FGM [24].
Comparison with Related Works. Multiplicative update algorithms have been studied in solving
matching problems [6, 13, 11, 12]. Our work is significantly different from previous works in the
following aspects. Previous works [6, 13, 11] generally first develop a kind of approximation (or
relaxation) for QP matching problem by ignoring the doubly stochastic constraint, and then aim
to find the optimum of the relaxation problem by developing an algorithm. In contrast, our work
focus on the general and challengeable QP problem with doubly stochastic constraint (Eq.(4)), and
derive a simple multiplicative algorithm to solve the problem Eq.(4) directly. Note that, the proposed
algorithm is not limited to solving QP matching problem only. It can also be used in some other QP
(or general continuous objective function) problems with doubly stochastic constraint (e.g. MAP
inference, clustering) in machine learning area. In this paper, we focus on graph matching problem.
Starting Point. To alleviate the local optima and provide a feasible starting point for MPGM algorithm, given an initial vector x(0) , we first use the simple projection x(0) = P (Wx(0) ) several times
to obtain a kind of the feasible start point for MPGM algorithm. Here P denotes the projection [22]
or normalization [20] to make x(0) satisfy the doubly stochastic constraint.

4

Theoretical Analysis

Theorem 1. Under update Eq.(5), the Lagrangian function L(x) is monotonically increasing,
L(x) = xT Wx ?

n
?
i=1

n
n
n
?
?
?
?i (
xij ? 1) ?
?j (
xij ? 1)
j=1

j=1

(7)

i=1

where ?, ? are Lagrangian multipliers.
Proof. To prove it, we use the auxiliary function approach [7, 14]. An auxiliary function function
?(x, ?
x) of Lagrangian function L(x) satisfies following,
?(x, x) = L(x), ?(x, ?x) ? L(x).

(8)

Using the auxiliary function ?(x, ?
x), we define
x(t+1) = arg max ?(x, x(t) ).
x

3

(9)

Then by construction of ?(x, ?
x), we have
L(x(t) ) = ?(x(t) , x(t) ) ? L(x(t+1) ).

(10)

This proves that L(x ) is monotonically increasing.
(t)

The main step in the following of the proof is to provide an appropriate auxiliary function and find
the global maximum for the auxiliary function. We rewrite Eq.(7) as
L(x) = xT Wx ?

n
?
i=1

=

n ?
n ?
n ?
n
?

?i (

n
?

xij ? 1) ?

j=1

n
?

?j (

j=1

Wij,kl xij xkl ?

i=1 j=1 k=1 l=1

n
?

n
?

xij ? 1)

i=1

n
n
n
?
?
?
?i (
xij ? 1) ?
?j (
xij ? 1).

i=1

j=1

j=1

(11)

i=1

We show that one auxiliary function ?(x, ?x) of L(x) is,
?(x, ?
x) =

n ?
n ?
n ?
n
?
i=1 j=1 k=1 l=1

?

n
?

n
?
j=1

(12)

?+
i

n
n
n
[?
] ?
[?
]
1 x2ij
xij
?
(
+?
xij ) ? 1 +
??
xij (1 + log
)?1
i
?
2 ?
xij
xij
j=1
i=1
j=1

?+
j

n
n
n
[?
] ?
[?
]
1 x2ij
xij
?
(
+?
xij ) ? 1 +
??
xij (1 + log
)?1 .
j
?
2 ?
xij
xij
i=1
j=1
i=1

i=1

?

(
xij xkl )
Wij,kl ?
xij ?
xkl 1 + log
?
xij ?
xkl

2

Using the inequality z ? 1 + log z and ab ? 12 (a2 + b2 )(a ? 12 ( ab + b)), one can prove that Eq.(12)
is a lower bound of Eq.(11). Thus, Z(x, ?x) is an auxiliary function of L(x). According to Eq.(9), we
need to find the global maximum of ?(x, ?x) for x. The gradient is
?xkl
?xkl
?xkl
??(x, ?
x)
xkl
xkl
= 2(W?
x)kl
? ?+
+ ??
? ?+
+ ??
k
k
l
l
?xkl
?xkl
?xkl
xkl
xkl
xkl
Note that, for graph matching problem, we have WT = W. Thus, the second derivative is
[
]
) xkl
(
?)
? 2 ?(x, x
1
? ?
+
+
= ? 2(W?
x)kl + ??
+
?
+
(?
+
?
)
?ki ?lj ? 0,
k
l
l
?xkl k
?xkl ?xij
x2kl

(13)

Therefore, ?(x, ?
x) is a concave function in x and has a unique global maximum. It can be obtained
x)
by setting the first derivative to zero ( ??(x,?
?xkl = 0), which gives
[ 2(W?x) + ?? + ?? ]1/2
kl
k
l
.
(14)
xkl = ?
xkl
+
?+
k + ?l
Therefore, we obtain the update rule in Eq.(5) by setting x(t+1) = x and x(t) = ?x. 
Theorem 2. Under update Eq.(5), the converged solution x? is Karush-Kuhn-Tucker (KKT) optimal.
Proof. The standard Lagrangian function is
L(x) = xT Wx ?

n
?
i=1

n
n
n
n ?
n
?
?
?
?
?i (
xij ? 1) ?
?j (
xij ? 1) ?
?ij xij
j=1

j=1

i=1

(15)

i=1 j=1

Here, we use the Lagrangian function to induce KKT optimal condition. Using Eq.(15), we have
?L(x)
= 2(Wx)kl ? ?k ? ?l .
?xkl

(16)

The corresponding KKT condition is
?L(x)
= 2(Wx)kl ? ?k ? ?l ? ?kl = 0
?xkl
?
?L(x)
= ?(
xkl ? 1) = 0
??k

(17)
(18)

l

?
?L(x)
= ?(
xkl ? 1) = 0
??l

(19)

?kl xkl = 0.

(20)

k

4

This leads to the following KKT complementary slackness condition,
[
]
2(Wx)kl ? ?k ? ?l xkl = 0.
(21)
?
?
Because l xkl = 1, k xkl = 1, summing over indexes k and l respectively, we obtain the following two group equations,
n
n
?
?
xkl (Wx)kl ?
?l xkl ? ?k = 0,
(22)
2
2

l=1

l=1

n
?

n
?

xkl (Wx)kl ?

k=1

?k xkl ? ?l = 0.

(23)

k=1

Eqs.(22, 23) can be equivalently reformulated as the following matrix forms,
2 diag(KXT ) ? ? ? X? = 0,

(24)

2 diag(K X) ? ? ? X ? = 0.
(25)
where k = 1, 2, ? ? ? n, l = 1, 2, ? ? ? n. K, X are the matrix forms of vector (Wx) and x, respectively,
i.e., K, X ? Rn?n and Kkl = (Wx)kl , Xkl = xkl . Thus, we can obtain the values for ? and ? as,
T

T

? = 2(I ? XT X)?1 (diag(KT X) ? XT diag(KXT ))
? = 2 diag(KX ) ? X?
On the other hand, from update Eq.(5), at convergence,
[ 2(Wx? ) + ?? + ?? ]1/2
kl
k
l
x?kl = x?kl
+
?+
k + ?l
T

(26)
(27)

(28)

Thus, we have (2(Wx? )kl ? ?k ? ?l )x?2
kl = 0, which is identical to the following KKT condition,
[
]
2(Wx? )kl ? ?k ? ?l x?kl = 0.
(29)
Substituting the values of ?k , ?l in Eq.(28) from Eqs.(26,27), we obtain update rule Eq.(5). 
Remark. Similar to the above analysis, we can also derive another similar update as,
(t+1)

xkl

?
(t)
(t) 2(Wx )kl + ?k
+
?+
k + ?l

= xkl

+ ??
l

.

(30)

The optimality and convergence of this update are also guaranteed. We omit the further discussion
of them due to the lack of space. In real application, one can use both of these two update algorithms
(Eq.(5), Eq.(30)) to obtain better results.

5

Sparsity and Discrete Solution

One property of the proposed MPGM is that it can result in a sparse optimal solution, although the
discrete binary constraint have been dropped in MPGM optimization process. This suggests that
MPGM can search for an optimal solution nearly on the permutation domain P, i.e., the boundary
of the doubly stochastic domain D. Unfortunately, here we cannot provide a theoretical proof on the
sparsity of MPGM solution, but demonstrate it experimentally.
Figure 1 (a) shows the solution x(t) across different iterations. Note that, regardless of initialization,
as the iteration increases, the solution vector x(t) of MPGM becomes more and more sparse and
converges to a discrete binary solution. Note that, in MPGM update Eq.(5), when xtkl closes to zero,
it can keep closing to zero in the following update process because of the particular multiplicative
operation. Therefore, as the iteration increases, the solution vector xt+1 is guaranteed to be more
sparse than solution vector xt . Figure 1 (b) shows the objective and sparsity2 of the solution vector
x(t) . We can observe that (1) the objective of x(t) increases and converges after some iterations,
demonstrating the convergence of MPGM algorithm. (2) The sparsity of the solution x(t) increases
and converges to the baseline, which demonstrates the ability of MPGM algorithm to maintain the
discrete constraint in the converged solution.
2
Sparsity measures the percentage of zero (close-to-zero) elements in Z. Firstly, set the threshold ? =
0.001 ? mean(Z), then renew Zij = 0 if Zij ? ?. Finally, the sparsity is defined as the percentage of zero
elements in the renewed Z.

5

Figure 1: (a) Solution vector x(t) of MPGM across different iterations (top: start from uniform
solution; middle: start from SM solution; bottom: start from RRWM solution).

6

Experiments

We have applied MPGM algorithm to several matching tasks. Our method has been compared with
some other state-of-the-art methods including SM [15], IPFP [16], SMAC [5], RRWM [3] and FGM
[24]. We implemented IPFP [16] with two versions: (1) IPFP-U that is initialized by the uniform
solution; (2) IPFP-S that is initialized by SM method [15]. In experiments, we initialize our MPGM
with uniform solution and obtain similar results when initializing with SM solution.
6.1

Synthetic Data

Similar to the works [3, 24], we have randomly generated data sets of nin 2D points as inlier nodes
for G. We obtain the corresponding nodes in graph G? by transforming the whole point set with
a random rotation and translation and then adding Gaussian noise N (0, ?) to the point positions
from graph G. In addition, we also added nout outlier nodes in both graphs respectively at random
positions. The affinity matrix W has been computed as Wij,kl = exp(??rik ? r?jl ?2F /0.0015),
where rik is the Euclidean distance between two nodes in G and similarly for r?jl .
Figure 2 summarizes the comparison results. We can note that: (1) similar to IPFP [16] and FGM
[24] which return discrete matching solutions, MPGM always generates sparse solutions on doubly
stochastic domain. (2) MPGM returns higher objective score and accuracy than IPFP [16] and FGM
[24] methods, which demonstrate that MPGM can find the sparse solution more optimal than these
methods. (3) MPGM generally performs better than the continuous domain methods including SM
[15], SMAC [5] and RRWM [3]. Comparing with these methods, MPGM incorporates the doubly
stochastic constraint more naturally and thus finds the solution more optimal than RRWM method.
(4) MPGM generally has similar time cost with RRWM [3]. We have not shown the time cost of
FGM [24] method in Fig.2, because FGM uses a hybrid optimization method and has obviously
higher time cost than other methods.
6.2

Image Sequence Data

In this section, we perform feature matching on CMU and YORK house sequences [3, 2, 18]. For
CMU ""hotel"" sequence, we have matched all images spaced by 5, 10 ? ? ? 75 and 80 frames and computed the average performances per separation gap. For YORK house sequence, we have matched
all images spaced by 1, 2 ? ? ? 8 and 9 frames and computed the average performances per separation
gap. The affinity matrix has been computed by Wij,kl = exp(??rik ? r?jl ?2F /1000), where rik is
the Euclidean distance between two points.
Figure 3 summarizes the performance results. It is noted that MPGM outperforms the other methods
in both objective score and matching accuracy, indicating the effectiveness of MPGM method. Also,
6

1

0.035

0.4
0.3

0.02

inliers nin = 20
outliers n
0.04

out

0.8
0.75
0.7

=0

0.06

0.85

0.08

Deformation noise ?

0.65

0.1

0.02

in

0.04

0.06

0.08

0.1

Deformation noise ?

0.02

0.5
0.4
2

inliers nin = 15
6

8

0.7

FGM
RRWM
SM
IPFP?U
IPFP?S
SMAC
MPGM

0.6
0.5
0.4
0.3

deformation noise ? = 0.04
4

0.2
10

2

1

1
0.9

Objective score

0.6
0.5
0.4
0.3

FGM
RRWM
SM
IPFP?U
IPFP?S
SMAC
MPGM

inliers nin = 15
outliers n

out

0.2
0.02

0.04

0.06

0.08

0.1

0.08

0.6
0.4
0.2

deformation noise ? = 0.04

0.02

0.04

0.06

0.08

0.1

8

10

0.08

0.1

Deformation noise ?

RRWM
SM
IPFP?U
IPFP?S
SMAC
MPGM

0.12
0.1
0.08
0.06
0.04

deformation noise ? = 0.04

inliers nin = 15
deformation noise ? = 0.04

0.02

0

6

8

10

2

4

6

8

0

10

2

# of outliers nout

4

6

# of outliers nout

1

0.7
FGM
RRWM
SM
IPFP?U
IPFP?S
SMAC
MPGM

0.6
0.5
0.4

0.02

inliers nin = 15
outliers nout = 5

0.6
0.4
0.2

RRWM
SM
IPFP?U
IPFP?S
SMAC
MPGM

0.05

FGM
RRWM
SM
IPFP?U
IPFP?S
SMAC
MPGM

0.8

0.2

0
0

0.1

inliers nin = 15

0.8

0.3

=5

Deformation noise ?

0.06

FGM
RRWM
SM
IPFP?U
IPFP?S
SMAC
MPGM

# of outliers nout

0.9

0.7

inliers nin = 15

4

# of outliers nout
utliers n

0.8

0.04

Deformation noise ?

0.8

Sparsity

FGM
RRWM
SM
IPFP?U
IPFP?S
SMAC
MPGM

0.6

0.005

0.8

Sparsity

Objective score

0.7

0.01

outliers nout = 0

inliers nin = 20
outliers nout = 0

1

0.9
0.8

0.02
0.015

inliers nin = 20

0

1

0.9

Accuracy

0.4
0.2

inliers n = 20
outliers nout = 0

0.025

FGM
RRWM
SM
IPFP?U
IPFP?S
SMAC
MPGM

0.6

Time

FGM
RRWM
SM
IPFP?U
IPFP?S
SMAC
MPGM

0.5

FGM
RRWM
SM
IPFP?U
IPFP?S
SMAC
MPGM

Time

0.6

RRWM
SM
IPFP?U
IPFP?S
SMAC
MPGM

0.03

0.9

0.04

Time

Accuracy

0.7

0.8

Sparsity

Objective score

0.95

0.8

Accuracy

0.04

1

1

0.9

0.03

0.02

inliers n = 15
in
outliers nout = 5

0.01

inliers n = 15
in
outliers nout = 5

0
0.04

0.06

0.08

0.02

0.1

Deformation noise ?

0.04

0.06

0.08

0

0.1

Deformation noise ?

0.02

0.04

0.06

Deformation noise ?

Figure 2: Comparison results of different methods on synthetic point sets matching
MPGM can generate sparse solutions. These are generally consistent with the results on the synthetic
data experiments and further demonstrate the benefits of MPGM algorithm.

Objective score

Accuracy

0.8
FGM
RRWM
SM
IPFP?U
IPFP?S
SMAC
MPGM

0.7
0.6
0.5
0.4

1

1

0.95

0.8

0.9

Sparsity

1
0.9

FGM
RRWM
SM
IPFP?U
IPFP?S
SMAC
MPGM

0.85
0.8
0.75
0.7

30

40

50

60

70

10

80

20

0

0.4
0.2
0

Objective score

Accuracy

0.8
0.6

FGM
RRWM
SM
IPFP?U
IPFP?S
SMAC
MPGM
2

4

6

30

40

50

60

70

10

80

20

8

1

1

0.9

0.8

0.8
0.7
0.6
0.5

FGM
RRWM
SM
IPFP?U
IPFP?S
SMAC
MPGM
2

0.6
0.4
0.2

40

50

60

70

80

FGM
RRWM
SM
IPFP?U
IPFP?S
SMAC
MPGM

0
4

6

8

Separation

Separation

30

Separation

Separation

Separation

Sparsity

20

0.4
0.2

0.65

10

FGM
RRWM
SM
IPFP?U
IPFP?S
SMAC
MPGM

0.6

2

4

6

8

Separation

Figure 3: Comparison results of different methods on CMU and YORK image sequences. Top:
CMU images; Bottom: YORK images.

6.3

Real-world Image Data

In this section, we tested our method on some real-world image datasets. We evaluate our MPGM
on the dataset [17] whose images are selected from Pascal 2007 3 . In this dataset, there are 30 pairs
of car images and 20 pairs of motorbike images. For each image pair, feature points and groundtruth matches were manually marked and each pair contains 30-60 ground-truth correspondences.
?|pi ?p? |

The affinity between two nodes is computed as Wij,ij = exp( 0.05 j ), where pi is the orientation
of normal vector at the sampled point (node) i to the contour, similarly to p?j . Also, the affinity
3

http://www.pascalnetwork.org/challenges/VOC/voc2007/workshop/index.html

7

Figure 4: Some examples of image matching on Pascal 2007 dataset (LEFT: original image pair,
MIDDLE: FGM result, RIGHT: MPGM result. Incorrect matches are marked by red lines)

Figure 5: Comparison results of different graph matching methods on the Pascal 2007 dataset
?|dik ?d? |

between two correspondences has been computed as Wij,kl = exp( 0.15 jl ), where dik denotes
the Euclidean distance between feature point i and k, similarly to d?jl . Some matching examples
are shown in Figure 4. To test the performance against outlier noise, we have randomly added 020 outlier features for each image pair. The overall results of matching accuracy across different
outlier features are summarized in Figure 5. From Figure 5, we can note that MPGM outperforms
the other competing methods including RRWM [3] and FGM [24], which further demonstrates the
effectiveness and practicality of MPGM on conducting real-world image matching tasks.

7

Conclusions and Future work

This paper presents an effective algorithm, Multiplicative Update Graph Matching (MPGM), that develops a multiplicative update technique to solve the QP matching problem with doubly stochastic
mapping constraint. The KKT optimality and convergence properties of MPGM algorithms are theoretically guaranteed. We show experimentally that MPGM solution is sparse and thus approximately
incorporates the discrete constraint in optimization naturally. In our future, the theoretical analysis
on the sparsity of MPGM needs to be further studied. Also, we will incorporate our MPGM in some
path-following strategy to find a more optimal solution for the matching problem. We will adapt the
proposed algorithm to solve some other optimization problems with doubly stochastic constraint in
machine learning and computer vision area.

Acknowledgment
This work is supported by the NBRPC 973 Program (2015CB351705); National Natural Science Foundation of China (61602001,61671018, 61572030); Natural Science Foundation of Anhui Province (1708085QF139); Natural Science Foundation of Anhui Higher Education Institutions
of China (KJ2016A020); Co-Innovation Center for Information Supply & Assurance Technology,
Anhui University; The Open Projects Program of National Laboratory of Pattern Recognition.
8

References
[1] K. Adamczewski, Y. Suh, and K. M. Lee. Discrete tabu search for graph matching. In ICCV, pages
109?117, 2015.
[2] T. S. Caetano, J. J. McAuley, L. Cheng, Q. V. Le, and A. J. Smola. Learning graph matching. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 31(6):1048?1058, 2009.
[3] M. Cho, J. Lee, and K. M. Lee. Reweighted random walks for graph matching. In European Conference
on Computer Vision, pages 492?505, 2010.
[4] D. Conte, P. Foggia, C. Sansone, and M. Vento. Thirty years of graph matching in pattern recognition.
International Journal of Pattern Recognition and Artificial Intelligence, pages 265?298, 2004.
[5] M. Cour, P. Srinivasan, and J.Shi. Balanced graph matching. In Neural Information Processing Systems,
pages 313?320, 2006.
[6] C. Ding, T. Li, and M. I. Jordan. Nonnegative matrix factorization for combinatorial optimization: Spectral clustering, graph matching and clique finding. In IEEE International Conference on Data Mining,
pages 183?192, 2008.
[7] C. Ding, T. Li, and M. I. Jordan. Convex and semi-nonnegative matrix factorization. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 32(1):45?55, 2010.
[8] O. Enqvist, K. Josephon, and F. Kahl. Optimal correspondences from pairwise constraints. In IEEE
International Conference on Computer Vision, pages 1295?1302, 2009.
[9] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval Research Logistics Quarterly,
3(1-2):95?110, 1956.
[10] S. Gold and A. Rangarajan. A graduated assignment algorithm for graph matching. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 18(4):377?388, 1996.
[11] B. Jiang, J. Tang, C. Ding, and B. Luo. A local sparse model for matching problem. In AAAI, pages
3790?3796, 2015.
[12] B. Jiang, J. Tang, C. Ding, and B. Luo. Nonnegative orthogonal graph matching. In AAAI, 2017.
[13] B. Jiang, H. F. Zhao, J. Tang, and B. Luo. A sparse nonnegative matrix factorization technique for graph
matching problem. Pattern Recognition, 47(1):736?747, 2014.
[14] D. D. Lee and H. S. Seung. Algorithms for nonnegative matrix factorization. In Neural Information
Processing Systems, pages 556?562, 2001.
[15] M. Leordeanu and M. Hebert. A spectral technique for correspondence problem using pairwise constraints.
In IEEE International Conference on Computer Vision, pages 1482?1489, 2005.
[16] M. Leordeanu, M. Hebert, and R. Sukthankar. An integer projected fixed point method for graph macthing
and map inference. In Neural Information Processing Systems, pages 1114?1122, 2009.
[17] M. Leordeanu, R. Sukthankar, and M. Hebert. Unsupervised learning for graph mathing. International
Journal of Computer Vision, 95(1):1?18, 2011.
[18] B. Luo, R. C. Wilson, and E. R. Hancock. Spectal embedding of graphs. Pattern Recognition,
36(10):2213?2230, 2003.
[19] J. J. MuAuley and T. S. Caetano. Fast matching of large point sets under occlusions. Pattern Recognition,
45(1):563?569, 2012.
[20] B. J. van Wyk and M. A. van Wyk. A pocs-based graph matching algorithm. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 16(11):1526?1530, 2004.
[21] M. Zaslavskiy, F. Bach, and J. P. Vert. A path following algorithm for the graph matching problem. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 31(12):2227?2242, 2009.
[22] R. Zass and A. Shashua. Doubly stochastic normalization for spectral clustering. In Proceedings of the
conference on Neural Information Processing Systems (NIPS), pages 1569?1576, 2006.
[23] Z. Zhang, Q. Shi, J. McAuley, W. Wei, Y. Zhang, and A. V. D. Hengel. Pairwise matching through
max-weight bipartite belief propagation. In CVPR, pages 1202?1210, 2016.
[24] F. Zhou and F. D. la Torre. Factorized graph matching. In IEEE Conference on Computer Vision and
Pattern Recognition, pages 127?134, 2012.
[25] F. Zhou and F. D. la Torre. Deformable graph matching. In IEEE Conference on Computer Vision and
Pattern Recognition, pages 127?134, 2013.

9

"
3392,2010,Dynamic Infinite Relational Model for Time-varying Relational Data Analysis,"We propose a new probabilistic model for analyzing dynamic evolutions of relational data, such as additions, deletions and split & merge, of relation clusters like communities in social networks. Our proposed model abstracts observed time-varying object-object relationships into relationships between object clusters. We extend the infinite Hidden Markov model to follow dynamic and time-sensitive changes in the structure of the relational data and to estimate a number of clusters simultaneously. We show the usefulness of the model through experiments with synthetic and real-world data sets.","Dynamic Infinite Relational Model
for Time-varying Relational Data Analysis

Katsuhiko Ishiguro Tomoharu Iwata
Naonori Ueda
NTT Communication Science Laboratories
Kyoto, 619-0237 Japan
{ishiguro,iwata,ueda}@cslab.kecl.ntt.co.jp

Joshua Tenenbaum
MIT
Boston, MA.
jbt@mit.edu

Abstract
We propose a new probabilistic model for analyzing dynamic evolutions of relational data, such as additions, deletions and split & merge, of relation clusters like
communities in social networks. Our proposed model abstracts observed timevarying object-object relationships into relationships between object clusters. We
extend the infinite Hidden Markov model to follow dynamic and time-sensitive
changes in the structure of the relational data and to estimate a number of clusters
simultaneously. We show the usefulness of the model through experiments with
synthetic and real-world data sets.

1 Introduction
Analysis of ?relational data?, such as the hyperlink structure on the Internet, friend links on social
networks, or bibliographic citations between scientific articles, is useful in many aspects. Many
statistical models for relational data have been presented [10, 1, 18]. The stochastic block model
(SBM) [11] and the infinite relational model (IRM) [8] partition objects into clusters so that the
relations between clusters abstract the relations between objects well. SBM requires specifying the
number of clusters in advance, while IRM automatically estimates the number of clusters. Similarly,
the mixed membership model [2] associates each object with multiple clusters (roles) rather than a
single cluster.
These models treat the relations as static information. However, a large amount of relational data
in the real world is time-varying. For example, hyperlinks on the Internet are not stationary since
links disappear while new ones appear every day. Human relationships in a company sometimes
drastically change by the splitting of an organization or the merging of some groups due to e.g.
Mergers and Acquisitions. One of our modeling goals is to detect these sudden changes in network
structure that occur over time.
Recently some researchers have investigated the dynamics in relational data. Tang et al.[13] proposed a spectral clustering-based model for multi-mode, time-evolving relations. Yang et al.[16]
developed the time-varying SBM. They assumed a transition probability matrix like HMM, which
governs all the cluster assignments of objects for all time steps. This model has only one transition
probability matrix for the entire data. Thus, it cannot represent more complicated time variations
such as split & merge of clusters that only occur temporarily. Fu et al.[4] proposed a time-series
extension of the mixed membership model. [4] assumes a continuous world view: roles follow a
mixed membership structure; model parameters evolve continuously in time. This model is very
general for time series relational data modeling, and is good for tracking gradual and continuous
changes of the relationships. Some works in bioinformatics [17, 5] have also adopted similar strategies. However, a continuous model approach does not necessarily best capture sudden transitions of
the relationships we are interested in. In addition, previous models assume the number of clusters is
fixed and known, which is di?cult to determine a priori.
1

In this paper we propose yet another time-varying relational data model that deals with temporal
and dynamic changes of cluster structures such as additions, deletions and split & merge of clusters. Instead of the continuous world view of [4], we assume a discrete structure: distinct clusters
with discrete transitions over time, allowing for birth, death and split & merge dynamics. More
specifically, we extend IRM for time-varying relational data by using a variant of the infinite HMM
(iHMM) [15, 3]. By incorporating the idea of iHMM, our model is able to infer clusters of objects
without specifying a number of clusters in advance. Furthermore, we assume multiple transition
probabilities that are dependent on time steps and clusters. This specific form of iHMM enables the
model to represent time-sensitive dynamic properties such as split & merge of clusters. Inference is
performed e?ciently with the slice sampler.

2 Infinite Relational Model
We first explain the infinite relational model (IRM) [8], which can estimate the number of hidden
clusters from a relational data. In IRM, Dirichlet process (DP) is used as a prior for clusters of an
unknown number, and is denoted as DP(?, G0 ) where ? > 0 is a parameter and G0 is a base measure.
We write G ? DP(?, G0 ) when a distribution G (?) is sampled from DP. In this paper, we implement
DP by using a stick-breaking process
[12], which is based on the fact that G is represented as an
?
infinite mixture of ?s: G (?) = ?
k=1 ?k ??k (?), ?k ? G 0 . ? = (?1 , ?2 , . . .) is a mixing ratio vector with
infinite elements whose sum equals one, constructed in a stochastic way:
?k = vk

k?1
?

(1 ? vl ),

vk ? Beta (1, ?) .

(1)

l=1

Here vk is drawn from a Beta distribution with a parameter ?.
The IRM is an application of the DP for relational data. Let us assume a binary two-place relation
on the set of objects D = {1, 2, . . . , N} as D ? D ? {0, 1}. For simplicity, we only discuss a two
place relation between the identical domain (D ? D). The IRM divides the set of N objects into
multiple clusters based on the observed relational data X = {xi, j ? {0, 1}; 1 ? i, j ? N}. The IRM
is able to infer the number of clusters at the same time because it uses DP as a prior distribution
of the cluster partition. Observation xi, j ? {0, 1} denotes the existence of a relation between objects
i, j ? {1, 2, . . . , N}. If there is (not) a relation between i and j, then xi, j = 1 (0). We allow asymmetric
relations xi, j , x j,i throughout the paper.
The probabilistic generative model (Fig. 1(a)) of the IRM is as follows:
?|? ? Stick (?)
zi |? ? Multinomial (?)
?k,l |?, ? ? Beta (?, ?)
(
)
xi, j |Z, H ? Bernoulli ?zi ,z j .

(2)
(3)
(4)
(5)

N
Here, Z = {zi }i=1
and H = {?k,l }?
k,l=1 . In Eq. (2) ?Stick? is the stick-breaking process (Eq. (1)). We
sample a cluster index of the object i, zi = k, k ? {1, 2, . . . , } using ? as in Eq. (3). In Eq. (4) ?k,l is
the strength of a relation between the objects in clusters k and l. Generating the observed relational
data xi, j follows Eq. (5) conditioned by the cluster assignments Z and the strengths H.

3
3.1

Dynamic Infinite Relational Model (dIRM)
Time-varying relational data

First, we define the time-varying relational data considered
in }this paper. Time-varying relational
{
data X have three subscripts t, i, and j: X = xt,i, j ? {0, 1} , where i, j ? {1, 2, . . . , N}, t ?
{1, 2, . . . , T }. xt,i, j = 1(0) indicates that there is (not) an observed relationship between objects i
and j at time step t. T is the number of time steps, and N is the number of objects. We assume
that there is no relation between objects belonging to a di?erent time step t and t0 . The time-varying
relational data X is a set of T (static) relational data for T time steps.
2

?
?0

?
?

?

?

?

?

?

zi

?

xi,j

k,l

zt,i

?

?

?

N

NXN

?

?

x t,i,j

k,l

NXN

(a)

? t,k

?

N

?

zt-1,i

zt,i

zt+1,i
N

x t,i,j

k,l

NXN

T

(b)

T

(c)

Figure 1: Graphical model of (a)IRM (Eqs.2-5), (b)?tIRM? (Eqs.7-10), and (c)dIRM (Eqs.11-15).
Circle nodes denote variables, square nodes are constants and shaded nodes indicate observations.

It is natural to assume that every object transits between di?erent clusters along with the time evolution. Observing several real world time-varying relational data, we assume there are several properties of transitions, as follows:
? P1. Cluster assignments in consecutive time steps have higher correlations.
? P2. Time evolutions of clusters are not stationary nor uniform.
? P3. The number of clusters is time-varying and unknown a priori.
P1 is a common assumption for many kinds of time series data, not limited to relational data. For
example, a member of a firm community on SNSs will belong to the same community for a long
time. A hyperlink structure in a news website may alter because of breaking news, but most of the
site does not change as rapidly every minute.
P2 tries to model occasional and drastic changes from frequent and minor modifications in relational networks. Such unstable changes are observed elsewhere. For example, human relationships
in companies will evolve every day, but a merger of departments sometimes brings about drastic
changes. On an SNS, a user community for the upcoming Olympics games may exist for a limited
time: it will not last years after the games end. This will cause an addition and deletion of a user
cluster (community). P3 is indispensable to track such changes of clusters.

3.2

Naive extensions of IRM

We attempt to modify the IRM to satisfy these properties. We first consider several straightforward
solutions based on the IRM for analyzing time-varying relational data.
The simplest way is to convert time-varying relational data X into ?static? relational data X? = { x?i, j }
? For example, we can generate X? as follows:
and apply the IRM to X.
{
x?i, j =

?
1 T1 Tt=1 xt,i, j > ?,
0 otherwise,

(6)

where ? denotes a threshold. This solution cannot represent the time changes of clustering because
it assume the same clustering results for all the time steps (z1,i = z2,i = ? ? ? = zT,i ).
We may separate the time-varying relational data X into a series of time step-wise relational data Xt
and apply the IRM for each Xt . In this case, we will have a di?erent clustering result for each time
step, but the analysis ignores the dependency of the data over time.
3

Another solution is to extend the object assignment variable zi to be time-dependent zt,i . The resulting ?tIRM? model is described as follows (Fig. 1(b)):
?|? ? Stick (?)
zt,i |? ? Multinomial (?)
?k,l |?, ? ? Beta (?, ?)
(
)
xt,i, j |Zt , H ? Bernoulli ?zt,i ,zt, j .

(7)
(8)
(9)
(10)

N
Here, Zt = {zt,i }i=1
. Since ? is shared over all time steps, we may expect that the clustering results
between time steps will have higher correlations. However, this model assumes that zt,i is conditionally independent from each other for all t given ?. This implies that the tIRM is not suitable for
modeling time evolutions since the order of time steps are ignored in the model.

3.3

dynamic IRM

To address three conditions P1?3 above, we propose a new probabilistic model called the dynamic
infinite relational model (dIRM). The generative model is given below:
?|? ? Stick (?)
(
)
?0 ? + ??k
?t,k |?0 , ?, ? ? DP ?0 + ?,
?0 + ?
(
)
zt,i |zt?1,i , ?t ? Multinomial ?t,zt?1,i
?k,l |?, ? ? Beta (?, ?)
(
)
xt,i, j |Zt , H ? Bernoulli ?zt,i ,zt, j .

(11)
(12)
(13)
(14)
(15)

Here, ?t = {?t,k : k = 1, . . . , ?}. A graphical model of the dIRM is presented in Fig. 1(c).
? in Eq. (11) represents time-average memberships (mixing ratios) to clusters. Newly introduced
?t,k = (?t,k,1 , ?t,k,2 , . . . , ?t,k,l , . . .) in Eq. (12) is a transition probability that an object remaining in the
cluster k ? {1, 2, . . .} at time t ? 1 will move to the cluster l ? {1, 2, . . .} at time t. Because of the DP,
this transition probability is able to handle infinite hidden states like iHMM [14].
The DP used in Eq. (12) has an additional term ? > 0, which is introduced by Fox et al. [3]. ?k is
a vector whose elements are zero except the kth element, which is one. Because the base measure
in Eq. (12) is biased by ? and ?k , the kth element of ?t,k prefers to take a larger value than other
elements. This implies that this DP encourages the self-transitions of objects, and we can achieve
the property P1 for time-varying relational data.
One di?erence from conventional iHMMs [14, 3] lies in P2, which is achieved by making the
transition probability ? time-dependent. ?t,k is sampled for every time step t, thus, we can model
time-varying patterns of transitions, including additions, deletions and split & merge of clusters as
extreme cases. These changes happen only temporarily, therefore, time-dependent transition probabilities are indispensable for our purpose. Note that the transition probability is also dependent
on the cluster index k, as in conventional iHMMs. Also the dIRM can automatically determine the
number of clusters thanks to DP: this enables us to hold P3.
Equation (13) generates a cluster assignment for the object i at time t, based on the cluster, where
the object was previously (zt?1,i ) and its transition probability ?. Equation (14) generates a strength
parameter ? for the pair of clusters k and l, then we obtain the observed sample xt,i, j in Eq. (15).
The di?erence between iHMMs and dIRM is two-fold. One is the time-dependent transition probability of the dIRM discussed above. The another is that the iHMMs have one hidden state sequence
s1:t to be inferred, while the dIRM needs to estimate multiple hidden state sequences z1:t,i given one
time sequence observation. Thus, we may interpret the dIRM as an extension of the iHMM, which
has N (= a number of objects) hidden sequences to handle relational data.
4

4

Inference

We use a slice sampler [15], which enables fast and e?cient sampling of the sequential hidden states.
The slice sampler introduces auxiliary variables U = {ut,i }. Given U, the number of clusters can be
reduced to a finite number during the inference, and it enables us an e?cient sampling of variables.
4.1

Sampling parameters

First, we explain the sampling of an auxiliary variable ut,i . We assume a prior of ut,i as a uniform
distribution. Also we define the joint distribution of u, z, and x:
)1??zt,i ,zt, j
(
)
(
) (
) ?z ,z (
. (16)
p xt,i, j , ut,i , ut, j , zt?1:t,i , zt?1:t, j = I ut,i < ?t,zt?1,i ,zt,i I ut, j < ?t,zt?1, j ,zt, j xt,i,t,ij t, j 1?xt,i, j
Here, I(?) is 1 if the predicate holds, otherwise zero. Using Eq. (16), we can derive the posterior of
ut,i as follows:
(
)
ut,i ? Uniform 0, ?t,zt?1,i ,zt,i .
(17)
Next, we explain the sampling of an object assignment variable zt,i . We define the following message
variable p:
(
)
pt,i,k = p zt,i = k|X1:t , U1:t , ?, H, ? .
(18)
Sampling of zt,i is similar to the forward-backward algorithm for the original HMM. First, we compute the above message variables from t = 1 to t = T (forward filtering). Next, we sample zt,i from
t = T to t = 1 using the computed message variables (backward sampling).
In forward filtering we compute the following equation from t = 1 to t = T :
) (
) ?
(
)? (
pt,i,k ? p xt,i,i |zt,i = k, H
p xt,i, j |zt,i = k, H p xt, j,i |zt,i = k, H
pt?1,i,l .

(19)

l:ut,i <?t,l,k

j,i

Note that the summation is conditioned by ut,i . The number of ls (cluster indices) that hold this
condition is limited to a certain finite number. Thus, we can evaluate the above equation.
In backward sampling, we sample zt,i from t = T to t = 1 from the equation below:
(
)
(
)
p zt,i = k|zt+1,i = l ? pt,i,k ?t+1,k,l I ut+1,i < ?t+1,k,l .

(20)

Because of I(u < ?), values of cluster indices k are limited within a finite set. Therefore, the variety
of sampled zt,i will be limited a certain finite number K given U.
Given U and Z, we have finite K-realized clusters. Thus, computing the posteriors of ?t,k and ?k,l
becomes easy and straightforward. First ? is assumed as?a K + 1-dimensional vector (mixing ratios
K
of unrepresented clusters are aggregated in ?K+1 = 1 ? k=1
?k ). mt,k,l denotes a number of objects
i such that zt?1,i = k and zt,i = l. Also, let us denote a number of xt,i, j such that zt,i = k and zt, j = l as
Nk,l . Similarly, nk,l denotes a number of xt,i, j such that zt,i = k, zt, j = l and xt,i, j = 1. Then we obtain
following posteriors:
(
)
?t,k ? Dirichlet ?0 ? + ??k + mt,k .
(21)
(
)
?k,l ? Beta ? + nk,l , ? + Nk,l ? nk,l .
(22)
mt,k is a K + 1-dimensional vector whose lth element is mt,k,l (mt,k,K+1 = 0).
We omit the derivation of the posterior of ? since it is almost the same with that of Fox et al. [3].
4.2

Sampling hyperparameters

Sampling hyperparameters is important to obtain the best results. This could be done normally by
putting vague prior distributions [14]. However, it is di?cult to evaluate the precise posteriors for
some hyperparameters [3]. Instead, we reparameterize and sample a hyperparameter in terms of
a ? (0, 1) [6]. For example, if the hyperparameter ? is assumed as Gamma-distributed, we convert ?
?
. Sampling a can be achieved from a uniform grid on (0, 1). We compute (unnormalized)
by a = 1+?
posterior probability densities at several as and choose one to update the hyperparameter.
5

IOtables data t = 1

i

Enron data t = 2

IOtables data t = 5

0
5

5

10

10

15

i

20

25

25

0

0

50

50

i

15

20

30

Enron data t = 10

0

0

i
100

100

30
5

10

15

20

25

30

150
0

5

10

j

(a)

15

20

25

0

30

50

100

j

j

(b)

(c)

150

150
0

50

100

150

j

(d)

Figure 2: Example of real-world datasets. (a)IOtables data, observations at t = 1, (b)IOtables data,
observations at t = 5, (c)Enron data, observations at t = 2, and (d)Enron data, observations at t = 10.

5

Experiments

Performance of the dIRM is compared with the original IRM [8] and its naive extension tIRM
(described in Eqs. (7-10)). To apply the IRM to time-varying relational data, we use Eq. (6) to X
with a threshold ? = 0.5. The di?erence between the tIRM (Eqs. (7-10)) and the dIRM is that
the tIRM does not incorporate the dependency between successive time steps while the dIRM does.
Hyperparameters were estimated simultaneously in all experiments.
5.1 Datasets and measurements
We prepared two synthetic datasets (Synth1 and Synth2). To synthesize datasets, we first determined
the number of time steps T , the number of clusters K, and the number of objects N. Next, we manually assigned zt,i in order to obtain cluster split & merge, additions, and deletions. After obtaining
Z, we defined the connection strengths between clusters H = {?k,l }. In this experiment, each ?k,l may
take one of two values ? = 0.1 (weakly connected) or ? = 0.9 (strongly connected). Observation X
was randomly generated according to Z and H. Synth1 is smaller (N = 16) and stable while Synth2
is much larger (N = 54), and objects actively transit between clusters.
Two real-world datasets were also collected. The first one is the National Input-Output Tables for
Japan (IOtables) provided by the Statistics Bureau of the Ministry of Internal A?airs and Communications of Japan. IOtables summarize the transactions of goods and services between industrial
sectors. We used an inverted coe?cient matrix, which is a part of the IOtables. Each element in
the matrix ei, j represents that one unit of demand in the jth sector invokes ei, j productions in the ith
sector. We generated xi, j from ei, j by binarizaion: setting xi, j = 1 if ei, j exceeds the average, and
setting xi, j = 0 otherwise. We collected data from 1985, 1990, 1995, 2000, and 2005, in 32 sectors
resolutions. Thus we obtain a time-varying relational data of N = 32 and T = 5.
The another real-world dataset is the Enron e-mail dataset [9], used in many studies including [13, 4].
We extracted e-mails sent in 2001. The number of time steps was T = 12, so the dataset was divided
into monthly transactions. The full dataset contained N = 151 persons. xt,i, j = 1(0) if there is
(not) an e-mail sent from i to j at time (month) t. We also generated a smaller dataset (N = 68) by
excluding those who send few e-mails for convenience. Quantitative measurements were computed
with this smaller dataset.
Fig. 2 presents examples of IOtables dataset ((a),(b)) and Enron dataset ((c),(d)). IOtables dataset
characterized by its stable relationships, compared to Enron dataset. In Enron dataset, the amount
of communication rapidly increases after the media reported on the Enron scandals.
We used three evaluating measurements. One is the Rand index, which computes the similarity
between true and estimated clustering results [7]. The Rand index takes the maximum value (1)
if the two clustering results completely match. We computed the Rand index between the ground
truth Zt and the estimated Z?t for each time step, and averaged the indices for T steps. We also
compute the error in the number of estimated clusters. Di?erences in the number of realized clusters
were computed between Zt and Z?t , and we calculated the average of these errors for T steps. We
6

Table 1: Computed Rand indices, numbers of erroneous clusters, and averaged test data log likelihoods.
Data
Synth1
Synth2
IOtables
Enron

IRM
0.796
0.433
-

Rand index
tIRM dIRM
0.946 0.982
0.734 0.847
-

# of erroneous clusters
IRM tIRM dIRM
1.00
0.20
0.13
3.00
0.98
0.65
-

Test log likelihood
IRM
tIRM
dIRM
-0.542 -0.508 -0.505
-0.692 -0.393 -0.318
-0.354 -0.358 -0.291
-0.120 -0.135 -0.106

calculated these measurements for the synthetic datasets. The third measure is an (approximated)
test-data log likelihood. For all datasets, we generated noisy datasets whose observation values are
inverted. The number of inverted elements was kept small so that inversions would not a?ect the
global clustering results. The ratios of inverted elements over the entire elements are set to 5% for
two synthetic data, 1% for IOtables data and 0.5% for Enron data. We made inferences on the noisy
datasets, and computed the likelihoods that ?inverted observations take the real value?. We used the
averaged log-likelihood per a observation as a measurement.
5.2

Results

First, we present the quantitative results. Table 1 lists the computed Rand index, errors in the estimated number of clusters, and test-data log likelihoods. We confirmed that dIRM outperformed
the other models in all datasets for the all measures. Particularly, dIRM showed good results in the
Synth2 and Enron datasets, where the changes in relationships are highly dynamic and unstable. On
the other hand, the dIRM did not achieve a remarkable improvement against tIRM for the Synth1
dataset whose temporal changes are small. Thus we can say that the dIRM is superior in modeling
time-varying relational data, especially for dynamic ones.
Next, we evaluate results of the real-world datasets qualitatively. Figure 3 shows the results from
IOtables data. The panel (a) illustrates the estimated ?k,l using the dIRM, and the panel (b) presents
the time evolution of cluster assignments, respectively. The dIRM obtained some reasonable and
stable industrial clusters, as shown in Fig. 3 (b). For example, dIRM groups the machine industries
into cluster 5, and infrastructure related industries are grouped into cluster 13. We believe that the
self-transition bias ? helps the model find these stable clusters. Also relationships between clusters
presented in Fig. 3 (a) are intuitively understandable. For example, demands for machine industries
(cluster 5) will cause large productions for ?iron and steel? sector (cluster 7). The ?commerce &
trade? and ?enterprise services? sectors (cluster 10) connects strongly to almost all the sectors.
There are some interesting cluster transitions. First, look at the ?finance, insurance? sector. At
t = 1, this sector belongs to cluster 14. However, the sector transits to cluster 1 afterwards, which
does not connect strongly with clusters 5 and 7. This may indicates the shift of money from these
matured industries. Next, the ?transport? sector enlarges its roll in the market by moving to cluster
14, and it causes the deletion of cluster 8. Finally, note the transitions of ?telecom, broadcast? sector.
From 1985 to 2000, this sector is in the cluster 9 which is rather independent from other clusters.
However, in 2005 the cluster separated, and telecom industry merged with cluster 1, which is a
influential cluster. This result is consistent with the rapid growth in ITC technologies and its large
impact on the world.
Finally, we discuss results on the Enron dataset. Because this e-mail dataset contains many individuals? names, we refrain from cataloging the object assignments as in the IOtables dataset. Figure
4 (a) tells us that clusters 1 ? 7 are relatively separated communities. For example, members in
cluster 4 belong to a restricted domain business such energy, gas, or pipeline businesses. Cluster 5
is a community of financial and monetary departments, and cluster 7 is a community of managers
such as vice presidents, and CFOs.
One interesting result from the dIRM is finding cluster 9. This cluster notably sends many messages
to other clusters, especially for management cluster 7. The number of objects belonging to this
cluster is only three throughout the time steps, but these members are the key-persons at that time.
7

dIRM: learned ? kl for IOtables data

1985 ( t = 1)

1990 ( t = 2)

1995 ( t = 3)

2000 ( t = 4)

2005 ( t = 5)

Cluster 1

(unborn)

finance, insurance

finance, insurance

finance, insurance

finance, insurance
telecom, broadcast

Cluster 5

machinery
electronic machinery
transport machinery
precision machinery

machinery
electronic machinery
transport machinery
precision machinery

machinery
electronic machinery
transport machinery
precision machinery

machinery
electronic machinery
transport machinery
precision machinery

machinery
electronic machinery
transport machinery
precision machinery

0.6

Cluster 7

iron and steel

iron and steel

iron and steel

iron and steel

iron and steel

0.5

Cluster 8

transport

transport

transport

(deleted)

(deleted)

Cluster 9

telecom, broadcast
consumer services

telecom, broadcast
consumer services

telecom, broadcast
consumer services

telecom, broadcast
consumer services

consumer services

Cluster 10

commerce, trades
enterprise services

commerce, trades
enterprise services

commerce, trades
enterprise services

commerce, trades
enterprise services

commerce, trades
enterprise services

Cluster 13

mining
petroleum
electric powers, gas
water, waste disposal

mining
petroleum
electric powers, gas
water, waste disposal

mining
petroleum
electric powers, gas
water, waste disposal

mining
petroleum
electric powers, gas
water, waste disposal

mining
petroleum
electric powers, gas
water, waste disposal

Cluster 14

finance, insurance

(deleted)

(deleted)

transport

transport

1

1

0.9

2
3

0.8

4
0.7

5
6

k

7
8

0.4

9
10

0.3

11
0.2

12
13

0.1

14
1

0
2

3

4

5

6

7

8

9

10 11 12 13 14

l

(b)

(a)

Figure 3: (a) Example of estimated ?k,l (strength of relationship between clusters k, l) for IOtable
data by dIRM. (d) Time-varying clustering assignments for selected clusters by dIRM.
dIRM: Learned ?kl for Enron data
?Inactive? object cluster

CEO of Enron America

The founder
COO

(a)

(b)

Figure 4: (a): Example of estimated ?k,l for Enron dataset using dIRM. (b): Number of items
belonging to clusters at each time step for Enron dataset using dIRM.

First, the CEO of Enron America stayed at cluster 9 in May (t = 5). Next, the founder of Enron was
a member of the cluster in August t = 8. The CEO of Enron resigned that month, and the founder
actually made an announcement to calm down the public. Finally, the COO belongs to the cluster in
October t = 10. This is the month that newspapers reported the accounting violations.
Fig. 4 (b) presents the time evolutions of the cluster memberships; i.e. the number of objects belonging to each cluster at each time step. In contrast to the IOtables dataset, this Enron e-mail dataset is
very dynamic, as you can see from Fig. 2(c), (d). For example, the volume of cluster 6 (inactive cluster) decreases as time evolves. This result reflects the fact that the transactions between employees
increase as the scandal is more and more revealed. On the contrary, cluster 4 is stable in membership. Thus, we can imagine that the group of energy and gas is a dense and strong community. This
is also true for cluster 5.

6 Conclusions
We proposed a new time-varying relational data model that is able to represent dynamic changes
of cluster structures. The dynamic IRM (dIRM) model incorporates a variant of the iHMM model
and represents time-sensitive dynamic properties such as split & merge of clusters. We explained a
generative model of the dIRM, and showed an inference algorithm based on a slice sampler. Experiments with synthetic and real-world time series datasets showed that the proposed model improves
the precision of time-varying relational data analysis. We will apply this model to other datasets to
study the capability and the reliability of the model. We also are interested in modifying the dIRM
to deal with multi-valued observation data.
8

References
[1] A. Clauset, C. Moore, and M. E. J. Newman. Hierarchical structure and the prediction of
missing links in networks. Nature, 453:98?101, 2008.
[2] E. Erosheva, S. Fienberg, and J. La?erty. Mixed-membership models of scientific publications.
Proceedings of the National Academy of Sciences of the United States of America (PNAS),
101(Suppl 1):5220?5227, 2004.
[3] E.B. Fox, E.B. Sudderth, M.I. Jordan, and A.S. Willsky. An HDP-HMM for systems with
state persistence. In Proceedings of the 25th International Conference on Machine Learning
(ICML), 2008.
[4] Wenjie Fu, Le Song, and Eric P. Xing. Dynamic mixed membership blockmodel for evolving
networks. In Proceedings of the 26th International Conference on Machine Learning (ICML),
2009.
[5] O. Hirose, R. Yoshida, S. Imoto, R. Yamaguchi, T. Higuchi, D. S. Chamock-Jones, C. Print,
and S. Miyano. Statistical inference of transcriptional module-based gene networks from time
course gene expression profiles by using state space models. Bioinformatics, 24(7):932?942,
2008.
[6] P. D. Ho?. Subset clustering of binary sequences, with an application to genomic abnormality
data. Biometrics, 61(4):1027?1036, 2005.
[7] L. Hubert and P. Arabie. Comparing partitions. Journal of Classification, 2(1):193?218, 1985.
[8] C. Kemp, J. B. Tenenbaum, T. L. Gri?ths, T. Yamada, and N. Ueda. Learning systems of
concepts with an infinite relational model. In Proceedings of the 21st National Conference on
Artificial Intelligence (AAAI), 2006.
[9] B. Klimat and Y. Yang. The enron corpus: A new dataset for email classification research. In
Proceedings of the European Conference on Machine Learning (ECML), 2004.
[10] D. Liben-Nowell and J. Kleinberg. The link prediction problem for social networks. In Proceedings of the Twelfth International Conference on Information and Knowledge Management,
pages 556?559. ACM, 2003.
[11] K. Nowicki and T. A. B. Snijders. Estimation and prediction for stochastic blockstructures.
Journal of the American Statistical Association, 96(455):1077?1087, 2001.
[12] J. Sethuraman. A constructive definition of dirichlet process. Statistica Sinica, 4:639?650,
1994.
[13] L. Tang, H. Liu, J. Zhang, and Z. Nazeri. Community evolution in dynamic multi-mode networks. In Proceeding of the 14th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 677?685, 2008.
[14] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet process. Journal of
The American Statistical Association, 101(476):1566?1581, 2006.
[15] J. Van Gael, Y. Saatci, Y. W. Teh, and Z. Ghahramani. Beam sampling for the infinite hidden
Markov model. In Proceedings of the 25th International Conference on Machine Learning
(ICML), 2008.
[16] T. Yang, Y. Chi, S. Zhu, Y. Gong, and R. Jin. A Bayesian approach toward finding communities and their evolutions in dynamic social networks. In Proceedings of SIAM International
Conference on Data Mining (SDM), 2009.
[17] R. Yoshida, S. Imoto, and T. Higuchi. Estimating time-dependent gene networks from time
series microarray data by dynamic linear models with markov switching. In Proceedings of
the International Conference on Computational Systems Bioinformatics, 2005.
[18] S. Zhu, K. Yu, and Y. Gong. Stochastic relational models for large-scale dyadic data using
mcmc. In Advances in Neural Information Processing Systems 21 (NIPS), 2009.

9

"
7212,1994,A model of the hippocampus combining self-organization and associative memory function,Abstract Missing,"A model of the hippocampus combining selforganization and associative memory function.
Michael E. Hasselmo, Eric Schnell
Joshua Berke and Edi Barkai

Dept. of Psychology, Harvard University
33 Kirkland St., Cambridge, MA 02138
hasselmo@katla.harvard.edu

Abstract
A model of the hippocampus is presented which forms rapid self-organized representations of input arriving via the perforant path, performs
recall of previous associations in region CA3, and performs comparison
of this recall with afferent input in region CA 1. This comparison drives
feedback regulation of cholinergic modulation to set appropriate
dynamics for learning of new representations in region CA3 and CA 1.
The network responds to novel patterns with increased cholinergic modulation, allowing storage of new self-organized representations, but
responds to familiar patterns with a decrease in acetylcholine, allowing
recall based on previous representations. This requires selectivity of the
cholinergic suppression of synaptic transmission in stratum radiatum of
regions CA3 and CAl, which has been demonstrated experimentally.

1

INTRODUCTION

A number of models of hippocampal function have been developed (Burgess et aI., 1994;
Myers and Gluck, 1994; Touretzky et al., 1994), but remarkably few simulations have
addressed hippocampal function within the constraints provided by physiological and anatomical data. Theories of the function of specific subregions of the hippocampal formation often do not address physiological mechanisms for changing dynamics between
learning of novel stimuli and recall of familiar stimuli. For example, the afferent input to
the hippocampus has been proposed to form orthogonal representations of entorhinal
activity (Marr, 1971; McNaughton and Morris, 1987; Eichenbaum and Buckingham,
1990), but simulations have not addressed the problem of when these representations

78

Michael E. Hasselmo. Eric Schnell. Joshua Berke. Edi Barkai

should remain stable, and when they should be altered. In addition, models of autoassociative memory function in region CA3 (Marr, 1971; McNaughton and Morris, 1987;
Levy, 1989; Eichenbaum and Buckingham, 1990) and heteroassociative memory function
at the Schaffer collaterals projecting from region CA3 to CAl (Levy, 1989; McNaughton,
1991) require very different activation dynamics during learning versus recall.
Acetylcholine may set appropriate dynamics for storing new information in the cortex
(Hasselmo et aI., 1992, 1993; Hasselmo, 1993, 1994; Hasselmo and Bower, 1993). Acetylcholine has been shown to selectively suppress synaptic transmission at intrinsic but
not afferent fiber synapses (Hasselmo and Bower, 1992), to suppress the neuronal adaptation of cortical pyramidal cells (Hasselmo et aI., 1994; Barkai and Hasselmo, 1994), and
to enhance long-term potentiation of synaptic potentials (Hasselmo, 1994b). Models
show that suppression of synaptic transmission during learning prevents recall of previously stored information from interfering with the storage of new information (Hasselmo
et al., 1992, 1993; Hasselmo, 1993, 1994a), while cholinergic enhancement of synaptic
modification enhances the rate of learning (Hasselmo, 1994b).
Feedback regulation of cholinergic modulation may set the appropriate level of cholinergic modulation dependent upon the novelty or familiarity of a particular input pattern.
We have explored possible mechanisms for the feedback regulation of cholinergic modulation in simulations of region CAl (Hasselmo and Schnell, 1994) and region CA3. Here
we show that self-regulated learning and recall of self-organized representations can be
obtained in a network simulation of the hippocampal formation. This model utilizes selective cholinergic suppression of synaptic transmission in stratum radiatum of region CA3,
which has been demonstrated in brain slice preparations of the hippocampus.

2

METHODS

2.1. SIMPLIFIED REPRESENTA nON OF HIPPOCAMPAL NEURONS.

In place of the sigmoid input-output functions used in many models, this model uses a
simple representation in which the output of a neuron is not explicitly constrained, but the
total network activity is regulated by feedback from inhibitory interneurons and adaptation due to intracellular calcium concentration. Separate variables represent pyramidal
cell membrane potential a, intracellular calcium concentration c, and the membrane potential of inhibitory interneurons h:

l1a i = Ai -l1 ai - J..l.C +

L Wijg(aj - e) - Hikg(h k - e h)
j

I1c?I = 'Vg(a
.i?
I
I1hk

e )- Qc
C

= IWkjg(aj-eo)-l1hk- IHk/g(h/-e)
j

where A = afferent input, ""

/

=passive decay of membrane potential, Il =strength of cal-

A Model of Hippocampus

79

cium-dependent potassium current (proportional to intracellular calcium), Wij = excitatory
recurrent synapses (longitudinal association path tenninating in stratum radiatum), gO is a
threshold linear function proportional to the amount by which membrane potential
exceeds an output threshold 00 or threshold for calcium current Oc' 'Y = strength of voltagedependent calcium current, n = diffusion constant of calcium, Wki = excitatory synapses
inhibitory interneurons, Hilc = inhibitory synapses from interneurons to pyramidal cells,
Hk}= inhibitory synapses between interneurons. This representation gives neurons adaptation characteristics similar to those observed with intracellular recording (Barkai and Hasselmo, 1994), including a prominent afterhyperpolarization potential (see Figure 1).

An

B

.... ..
~

....J

'N~JJL

C

lO

\..--14

Figure 1. Comparison of pyramidal cell model with experimental data.
In Figure I, A shows the membrane potential of a modeled pyramidal cell in response to
simulated current injection. Output of this model is a continuous variable proportional to
how much membrane potential exceeds threshold. This is analogous to the reciprocal of
interspike interval in real neuronal recordings. Note that the model displays adaptation
during current injection and afterhyperpolarization afterwards, due to the calcium-dependent potassium current. B shows the intracellularly recorded membrane potential in a pirifonn cortex pyramidal cell, demonstrating adaptation of firing frequency due to
activation of calcium-dependent potassium current. The firing rate falls off in a manner
similar to the smooth decrease in firing rate in the simplified representation. C shows an
intracellular recording illustrating long-tenn afterhyperpolarization caused by calcium
influx induced by spiking of the neuron during current injection.
2.2. NETWORK CONNECTIVITY

A schematic representation of the network simulation of the hippocampal fonnation is
shown in Figure 2. The anatomy of the hippocampal fonnation is summarized on the left
in A, and the function of these different subregions in the model is shown on the right in
B. Each of the subregions in the model contained a population of excitatory neurons with
a single inhibitory interneuron mediating feedback inhibition and keeping excitatory
activity bounded. Thus, the local activation dynamics in each region follow the equations
presented above. The connectivity of the network is further summarized in Figure 3 in the
Results section. A learning rule of the Hebbian type was utilized at all synaptic connections, with the exception of the mossy fibers from the dentate gyrus to region CA3, and the
connections to and from the medial septum. Self-organization of perforant path synapses
was obtained through decay of synapses with only pre or post-synaptic activity, and
growth of synapses with combined activity. Associative memory function at synapses

80

Michael E. Hasse/mo, Eric Schnell, Joshua Berke, Edi Barkai

arising from region CA3 was obtained through synaptic modification during cholinergic
suppression of synaptic transmission.

B

Entorhinal cortex

""""""""""""""""""""

??

?
???

: Self-organized
: representation
: r-----..L.\

?

~

Comparison

.L-______~~.-----~~~--~~

Feedback regulation of
cholinergic modulation

Regulation of
learning dynamics

Figure 2. Schematic representation of hippocampal circuitry
and the corresponding function of connections in the model.

2.3. CHOLINERGIC MODULAnON
The total output from region CAl determined the level of cholinergic modulation within
both region CA3 and CAl, with increased output causing decreased modulation. This is
consistent with experimental evidence suggesting that activity in region CAl and region
CA3 can inhibit activity in the medial septum, and thereby downregulate cholinergic modulation. This effect was obtained in the model by excitatory connections from region CAl
to an inhibitory interneuron in the medial septum, which suppressed the activity of a cholinergic neuron providing modulation to the full network. When levels of cholinergic
modulation were high, there was strong suppression of synaptic transmission at the excitatory recurrent synapses in CA3 and the Schaffer collaterals projecting from region CA3 to
CAL This prevented the spread of activity due to previous learning from interfering with
self-organization. When levels of cholinergic modulation were decreased, the strength of
synaptic transmission was increased, allowing associative recall to dominate. Cholinergic
modulation also increased the rate of synaptic modification and depolarized neurons.

2.4. TESTS OF SELF-REGULATED LEARNING AND RECALL
Simulations of the full hippocampal network evaluated the response to the sequential presentation of a series of highly overlapping activity patterns in the entorhinal cortex. Recall
was tested with interspersed presentation of degraded versions of previously presented
activity patterns. For effective recall, the pattern of activity in entorhinal cortex layer IV
evoked by degraded patterns matched the pattern evoked by the full learned version of
these patterns. The function of the full network is illustrated in Figure 3. In simulations

A Model of Hippocampus

81

focused on region CA3, activity patterns were induced sequentially in region CA3, representing afferent input from the entorhinal cortex. Different levels of external activation of
the cholinergic neuron resulted in different levels of learning of new overlapping patterns.
These results are illustrated in Figure 4.

2.5. BRAIN SLICE EXPERIMENTS
The effects in the simulations of region CA3 depended upon the cholinergic suppression
of synaptic transmission in stratum radiatum of this region The cholinergic suppression of
glutamatergic synaptic transmission in region CA3 was tested in brain slice preparations
by analysis of the influence of the cholinergic agonist carbachol on the size of field potentials elicited by stimulation of stratum radiatum. These experiments used techniques similar to previously published work in region CAl (Hasselmo and Schnell, 1994).

3 RESULTS
In the full hippocampal simulation, input of an unfamiliar pattern to entorhinal cortex
layer II resulted in high levels of acetylcholine. This allowed rapid self-organization of
the perforant path input to the dentate gyrus and region CAl. Cholinergic suppression of
synaptic transmission in region CAl prevented recall from interfering with self-organization. Instead, recurrent collaterals in region CA3 stored an autoassociative representation
of the input from the dentate gyrus to region CA3, and connections from CA3 to CA 1
stored associations between the pattern of activity in CA3 and the associated self-organized representation in region CAl.
identity
"" self-org ""matrix ~
,auto-""
,,~>
M assoc
u .....
?
?
'at)
u
u
c >.
--I.~ ?
c
?
c
?
:.a.?:i
Self-org ~
iden~ity
.9
hetero.9 hetero8
matrix
~
assoc
~ assoc
C!

~

~

?

~

111111 I ""T I' , If
,r
2 I I I II
n
I I
ld II
r I"" I 'I""'""'I II , , ,
Q)2dl' r II' I I ' l l I
~ 311111 I' I I I n
""

j

4

3d
4d
ld

2d

n,n

II
II
II

'f
I I II I n
I
II I
I
,
,
II
,
I
1I
'I

I

'I

.. ,
II f'l't?
U i
I I Itt

~

I.Ll

r""

U
I

""
H
,
I
I
U
I I 1 II r

~'

,,

1

,

'(
( U.

""

I I

I

1'1 I I
I I I n
I""'"" , 'I II I II

1 I ' ,I

I

l 11 I J1
't III I'

?""

'I . .

I

I

, I I II
I II ? .1
l "" I II
I III I I

W.

Jl 1

lU

Neuron #
Figure 3. Activity in each subregion of the full network simulation of the hippocampal
formation during presentation of a sequence of activity patterns in entorhinal cortex.

82

Michael E. Hasselmo, Eric Schnell, Joshua Berke, Edi Barkai

In Figure 3. width of the lines represents the activity of each neuron at a particular time
step. As seen here. the network forms a self-organized representation of each new pattern
consisting of active neurons in the dentate gyrus and region CAL At the same time. an
association is formed between the self-organized representation in region CAl and the
same afferent input pattern presented to entorhinal cortex layer IV. Four overlapping patterns (1-4) are presented sequentially. each of which results in learning of a separate selforganized representation in the dentate gyrus and region CAl. with an association formed
between this representation and the full input pattern in entorhinal cortex.
The recall characteristics of the network are apparent when degraded versions of the afferent input patterns are presented in the sequence (ld-4d). This degraded afferent input
weakly activates the same representations previously formed in the dentate gyrus. Recurrent excitation in region CA3 enhances this activity. giving robust recall of the full version
of this pattern. This activity then reaches CA 1. where it causes strong activation if it
matches the pattern of afferent input from the entorhinal cortex. Strong activation in
region CAl decreases cholinergic modulation. preventing formation of a new representation and allowing recall to dominate. Strong activation of the representation stored in
region CAl then activates the full representation of the pattern in entorhinal cortex layer
IV. Thus. the network can accurately recall each of many highly overlapping patterns.
The effect of cholinergic modulation on the level of learning or recall can be seen more
clearly in a simulation of auto-associative memory function in region CA3 as shown in
Figure 4. Each box shows the response of the network to sequential presentation of full
and degraded versions of two highly overlapping input patterns. The width of the black
traces represents the activity of each of 10 CA3 pyramidal cells during each simulation
step. In the top row. level of cholinergic modulation (ACh) is plotted. In A. external activation of the cholinergic neuron is absent. so there is no cholinergic suppression of synaptic transmission. In this case. the first pattern is learned and recalled properly. but
subsequent presentation of a second overlapping pattern results only in recall of the previously learned pattern. In B. with greater cholinergic suppression. recall is suppressed sufficiently to allow learning of a combination of the two input patterns. Finally. in C. strong
cholinergic suppression prevents recall. allowing learning of the new overlapping pattern
to dominate over the previously stored pattern.

A

Stored
patterns

???

??
??

?
?

-.gN.g
N

-

ACh
Inhib
Q\

.- .--.

ACh input = 0.0

..... ......

,11"",,11.

.....

B

ACh input

=0.15

C

__
ACh input

=0.3

...11'_..,.......,.. ......

111 ... 111...... 11 . . . .""'... 11 ? ?111 ??,

'

I ? ? "" '? ? _ _

'111""

.';::
~

~

?'. _""11.. ? .'.? .
.,~

. "",.

.u. . . . .... ,

0

. . . . . . . . . . 111. . . .... .

N

Figure 4. Increased cholinergic suppression of synaptic transmission in region CA3
causes greater learning of new aspects of afferent input patterns.

A Model of Hippocampus

83

Extracellular recording in brain slice preparations of hippocampal region CA3 have demonstrated that perfusion of the cholinergic agonist carbachol strongly suppresses synaptic
potentials recorded in stratum radiatum, as shown in Figure 5. In contrast, suppression of
synaptic transmission at the afferent fiber synapses arising from entorhinal cortex is much
weaker. At a concentration of 20J..tM, carbachol suppressed synaptic potentials in stratum
radiatum on average by 54.4% (n=5). Synaptic potentials elicited in stratum lacunosum
were more weakly suppressed, with an average suppression of28%.

Control

Carbachol
(20JlM)

Wash

Figure 5. Cholinergic suppression of synaptic transmission in stratum radiatum of CA3.

4

DISCUSSION

In this model of the hippocampus, self-organization at perforant path synapses forms compressed representations of specific patterns of cortical activity associated with events in
the environment. Feedback regulation of cholinergic modulation sets appropriate dynamics for learning in response to novel stimuli, allowing predominance of self-organization,
and appropriate dynamics for recall in response to familiar stimuli, allowing predominance of associative memory function. This combination of self-organization and associative memory function may also occur in neocortical structures. The selective cholinergic
suppression of feedback and intrinsic synapses has been proposed to allow self-organization of feedforward synapses while feedback synapses mediate storage of associations
between higher level representations and activity in primary cortical areas (Hasselmo,
1994b). This previous proposal could provide a physiological justification for a similar
mechanism utilized in recent models (Dayan et al., 1995). Detailed modeling of cholinergic effects in the hippocampus provides a theoretical framework for linking the considerable behavioral evidence for a role of acetylcholine in memory function (Hagan and
Morris, 1989) to the neurophysiological evidence for the effects of acetylcholine within
cortical structures (Hasselmo and Bower, 1992; 1993; Hasselmo, 1994a, 1994b).

Acknowledgements
This work supported by a pilot grant from the Massachusetts Alzheimer's Disease
Research Center and by an NIMH FIRST award MH52732-01.

References
Barkai E, Hasselmo ME (1994) Modulation of the input/output function of rat piriform
cortex pyramidal cells. J. Neurophysiol. 72: 644-658.

84

Michael E. Hasselmo, Eric Schnell, Joshua Berke, Edi Barkai

Barkai E, Bergman RE, Horwitz G, Hasselmo ME (1994) Modulation of associative memory function in a biophysical simulation of rat pirifonn cortex. J. Neurophysiol. 72:659677.
Burgess N, Reece M, O'Keefe J (1994) A model of hippocampal function. Neural Networks 7: 1065-1081.
Dayan P, Hinton GE, Neal RM and Zemel RS (1995) The Helmholtz machine. Neural
computation in press.
Eichenbaum, H. and Buckingham, J. (1990) Studies on hippocampal processing: experiment, theory and model. In: Learning and computational neuroscience: foundations of
adaptive networks, M. Gabriel and J. Moore, eds., Cambridge, MA: MIT Press.
Hagan, JJ and Morris, RGM (1989) The cholinergic hypothesis of memory: A review of
animal experiments. In Psychopharmacology of the Aging Nervous System, L.L. Iversen,
S.D. Iversen and S.H. Snyder, eds. New York: Plenum Press, p. 237-324.
Hasselmo, M.E. (1993) Acetylcholine and learning in a cortical associative memory. Neural Compo 5: 22-34.
Hasselmo ME (1994a) Runaway synaptic modification in models of cortex: Implications
for Alzheimer's disease. Neural Networks 7: 13-40.
Hasselmo ME (1994b) Neuromodulation and cortical function. Behav. Brain Res. in press
Hasselmo ME, Anderson, BP and Bower, JM (1992) Cholinergic modulation of cortical
associative memory function. J. Neurophysiol. 67(5): 1230-1246.
Hasselmo ME, Bower JM (1992) Cholinergic suppression specific to intrinsic not afferent
fiber synapses in rat pirifonn (olfactory) cortex. J. Neurophysiol. 67(5): 1222-1229.
Hasselmo ME, Bower JM (1993) Acetylcholine and memory. Trends Neurosci 16:218222.
Hasselmo ME, Barkai E, Horwitz G, Bergman RE (1993) Modulation of neuronal adaptation and cortical associative memory function. In: Computation and Neural Systems II
(Eeckman F, Bower JM, ed). Norwell, MA: Kluwer Academic Publishers.
Hasselmo ME, Schnell E (1994) Laminar selectivity of the cholinergic suppression of synaptic transmission in rat hippocampal region CAl: Computational modeling and brain
slice physiology. J. Neurosci. 14: 3898-3914.
Levy WB (1989) A computational approach to hippocampal function. In: Computational
models of learning in simple neural systems (Hawkins RD, Bower GH, ed), pp. 243-305.
Orlando, FL: Academic Press.
Myers CE and Gluck M (1994) Context, conditioning and hippocampal rerepresentation
in animal learning. Behav. Neurosci. 108: 835-847.
Marr 0 (1971) Simple memory: A theory for archicortex. Phil. Trans. Roy. Soc. B
B262:23-81
McNaughton BL (1991) Associative pattern completion in hippocampal circuits: New
evidence and new questions. Brain Res. Rev. 16:193-220.
McNaughton BL, Morris RGM (1987) Hippocampal synaptic enhancement and infonnation storage within a distributed memory system. Trends Neurosci. 10:408-415.
Touretzky OS, Wan HS and Redish AD (1994) Neural representation of space in rats and
robots. In Zurada JM and Marks RJ (eds) Computational Intelligence: Imitating life.
IEEE Press.

"
4954,2014,Deep Convolutional Neural Network for Image Deconvolution,"Many fundamental image-related problems involve deconvolution operators. Real blur degradation seldom complies with an deal linear convolution model due to camera noise, saturation, image compression, to name a few. Instead of perfectly modeling outliers, which is rather challenging from a generative model perspective, we develop a deep convolutional neural network to capture the characteristics of degradation. We note directly applying existing deep neural networks does not produce reasonable results. Our solution is to establish the connection between traditional optimization-based schemes and a neural network architecture where a novel, separable structure is introduced as a reliable support for robust deconvolution against artifacts. Our network contains two submodules, both trained in a supervised manner with proper initialization. They yield decent performance on non-blind image deconvolution compared to previous generative-model based methods.","Deep Convolutional Neural Network for Image
Deconvolution
Li Xu ?
Lenovo Research & Technology
xulihk@lenovo.com

Jimmy SJ. Ren
Lenovo Research & Technology
jimmy.sj.ren@gmail.com
Jiaya Jia
The Chinese University of Hong Kong
leojia@cse.cuhk.edu.hk

Ce Liu
Microsoft Research
celiu@microsoft.com

Abstract
Many fundamental image-related problems involve deconvolution operators. Real
blur degradation seldom complies with an ideal linear convolution model due to
camera noise, saturation, image compression, to name a few. Instead of perfectly
modeling outliers, which is rather challenging from a generative model perspective, we develop a deep convolutional neural network to capture the characteristics
of degradation. We note directly applying existing deep neural networks does not
produce reasonable results. Our solution is to establish the connection between
traditional optimization-based schemes and a neural network architecture where
a novel, separable structure is introduced as a reliable support for robust deconvolution against artifacts. Our network contains two submodules, both trained in
a supervised manner with proper initialization. They yield decent performance
on non-blind image deconvolution compared to previous generative-model based
methods.

1 Introduction
Many image and video degradation processes can be modeled as translation-invariant convolution.
To restore these visual data, the inverse process, i.e., deconvolution, becomes a vital tool in motion
deblurring [1, 2, 3, 4], super-resolution [5, 6], and extended depth of field [7].
In applications involving images captured by cameras, outliers such as saturation, limited image
boundary, noise, or compression artifacts are unavoidable. Previous research has shown that improperly handling these problems could raise a broad set of artifacts related to image content, which
are very difficult to remove. So there was work dedicated to modeling and addressing each particular
type of artifacts in non-blind deconvolution for suppressing ringing artifacts [8], removing noise [9],
and dealing with saturated regions [9, 10]. These methods can be further refined by incorporating
patch-level statistics [11] or other schemes [4]. Because each method has its own specialty as well
as limitation, there is no solution yet to uniformly address all these issues. One example is shown
in Fig. 1 ? a partially saturated blur image with compression errors can already fail many existing
approaches.
One possibility to remove these artifacts is via employing generative models. However, these models
are usually made upon strong assumptions, such as identical and independently distributed noise,
which may not hold for real images. This accounts for the fact that even advanced algorithms can
be affected when the image blur properties are slightly changed.
?
Project webpage: http://www.lxu.me/projects/dcnn/. The paper is partially supported by a grant from the
Research Grants Council of the Hong Kong Special Administrative Region (Project No. 413113).

1

(a)

( b ) Krishnan et al .

( c ) Ours

Figure 1: A challenging deconvolution example. (a) is the blurry input with partially saturated
regions. (b) is the result of [3] using hyper-Laplacian prior. (c) is our result.
In this paper, we initiate the procedure for natural image deconvolution not based on their physically
or mathematically based characteristics. Instead, we show a new direction to build a data-driven
system using image samples that can be easily produced from cameras or collected online.
We use the convolutional neural network (CNN) to learn the deconvolution operation without the
need to know the cause of visual artifacts. We also do not rely on any pre-process to deblur the image,
unlike previous learning based approaches [12, 13]. In fact, it is non-trivial to find a proper network
architecture for deconvolution. Previous de-noise neural network [14, 15, 16] cannot be directly
adopted since deconvolution may involve many neighboring pixels and result in a very complex
energy function with nonlinear degradation. This makes parameter learning quite challenging.
In our work, we bridge the gap between an empirically-determined convolutional neural network
and existing approaches with generative models in the context of pseudo-inverse of deconvolution.
It enables a practical system and, more importantly, provides an empirically effective strategy to
initialize the weights in the network, which otherwise cannot be easily obtained in the conventional
random-initialization training procedure. Experiments show that our system outperforms previous
ones especially when the blurred input images are partially saturated.

2 Related Work
Deconvolution was studied in different fields due to its fundamentality in image restoration. Most
previous methods tackle the problem from a generative perspective assuming known image noise
model and natural image gradients following certain distributions.
In the Richardson-Lucy method [17], image noise is assumed to follow a Poisson distribution.
Wiener Deconvolution [18] imposes equivalent Gaussian assumption for both noise and image gradients. These early approaches suffer from overly smoothed edges and ringing artifacts.
Recent development on deconvolution shows that regularization terms with sparse image priors are
important to preserve sharp edges and suppress artifacts. The sparse image priors follow heavy-tailed
distributions, such as a Gaussian Mixture Model [1, 11] or a hyper-Laplacian [7, 3], which could be
efficiently optimized using half-quadratic (HQ) splitting [3]. To capture image statistics with larger
spatial support, the energy is further modeled within a Conditional Random Field (CRF) framework
[19] and on image patches [11]. While the last step of HQ method is quadratic optimization, Schmidt
et al. [4] showed that it is possible to directly train a Gaussian CRF from synthetic blur data.
To handle outliers such as saturation, Cho et al. [9] used variational EM to exclude outlier regions
from a Gaussian likelihood. Whyte et al. [10] introduced an auxiliary variable in the RichardsonLucy method. An explicit denoise pass is added to deconvolution, where the denoise approach is
carefully engineered [20] or trained from noisy data [12]. The generative approaches typically have
difficulties to handle complex outliers that are not independent and identically distributed.
2

Another trend for image restoration is to leverage the deep neural network structure and big data to
train the restoration function. The degradation is therefore no longer limited to one model regarding
image noise. Burger et al. [14] showed that the plain multi-layer perceptrons can produce decent
results and handle different types of noise. Xie et al. [15] showed that a stacked denoise autoencoder (SDAE) structure [21] is a good choice for denoise and inpainting. Agostinelli et al. [22]
generalized it by combining multiple SDAE for handling different types of noise. In [23] and [16],
the convolutional neural network (CNN) architecture [24] was used to handle strong noise such as
raindrop and lens dirt. Schuler et al. [13] added MLPs to a direct deconvolution to remove artifacts.
Though the network structure works well for denoise, it does not work similarly for deconvolution.
How to adapt the architecture is the main problem to address in this paper.

3 Blur Degradation
We consider real-world image blur that suffers from several types of degradation including clipped
intensity (saturation), camera noise, and compression artifacts. The blur model is given by
y? = ?b [?(?x ? k + n)],

(1)

where ?x represents the latent sharp image. The notation ? ? 1 is to indicate the fact that ?x could
have values exceeding the dynamic range of camera sensors and thus be clipped. k is the known
convolution kernel, or typically referred to as a point spread function (PSF), n models additive
camera noise. ?(?) is a clipping function to model saturation, defined as ?(z) = min(z, zmax ),
where zmax is a range threshold. ?b [?] is a nonlinear (e.g., JPEG) compression operator.
We note that even with y? and kernel k, restoring ?x is intractable, simply because the information
loss caused by clipping. In this regard, our goal is to restore the clipped input x
?, where x
? = ?(?x).
Although solving for x? with a complex energy function that involves Eq. (1) is difficult, the generation of blurry image from an input x is quite straightforward by image synthesis according to the
convolution model taking all kinds of possible image degradation into generation. This motivates a
learning procedure for deconvolution, using training image pairs {?
xi , y?i }, where index i ? N .

4 Analysis
The goal is to train a network architecture f (?) that minimizes
1 X
kf (?
yi ) ? x?i k2 ,
2|N |

(2)

i?N

where |N | is the number of image pairs in the sample set.
We have used the recent two deep neural networks to solve this problem, but failed. One is the Stacked Sparse Denoise Autoencoder (SSDAE) [15] and the other is the convolutional neural network
(CNN) used in [16]. Both of them are designed for image denoise. For SSDAE, we use patch size
17 ? 17 as suggested in [14]. The CNN implementation is provided by the authors of [16]. We
collect two million sharp patches together with their blurred versions in training.
One example is shown in Fig. 2 where (a) is a blurred image. Fig. 2(b) and (c) show the results of
SSDAE and CNN. The result of SSDAE in (b) is still blurry. The CNN structure works relatively
better. But it suffers from remaining blurry edges and strong ghosting artifacts. This is because these
network structures are for denoise and do not consider necessary deconvolution properties. More
explanations are provided from a generative perspective in what follows.
4.1 Pseudo Inverse Kernels
The deconvolution task can be approximated by a convolutional network by nature. We consider the
following simple linear blur model
y = x ? k.
The spatial convolution can be transformed to a frequency domain multiplication, yielding
F (y) = F (x) ? F (k).
3

(a) input

(b) SSDAE [15]

(c) CNN [16]

(d) Ours

Figure 2: Existing stacked denoise autoencoder and convolutional neural network structures cannot
solve the deconvolution problem.

(a)

(b)

(c)

(d)

(e)

Figure 3: Pseudo inverse kernel and deconvolution examples.
F (?) denotes the discrete Fourier transform (DFT). Operator ? is element-wise multiplication. In
Fourier domain, x can be obtained as
x = F ?1 (F (y)/F (k)) = F ?1 (1/F (k)) ? y,
where F ?1 is the inverse discrete Fourier transform. While the solver for x is written in a form of
spatial convolution with a kernel F ?1 (1/F (k)), the kernel is actually a repetitive signal spanning
the whole spatial domain without a compact support. When noise arises, regularization terms are
commonly involved to avoid division-by-zero in frequency domain, which makes the pseudo inverse
falls off quickly in spatial domain [25].
The classical Wiener deconvolution is equivalent to using Tikhonov regularizer [2]. The Wiener
deconvolution can be expressed as
x = F ?1 (

|F (k)|2
1
{
}) ? y = k ? ? y,
F (k) |F (k)|2 + SN1 R

where SN R is the signal-to-noise ratio. k ? denotes the pseudo inverse kernel. Strong noise leads to a
large SN1 R , which corresponds to strongly regularized inversion. We note that with the introduction
of SN R, k ? becomes compact with a finite support. Fig. 3(a) shows a disk blur kernel of radius 7,
which is commonly used to model focal blur. The pseudo-inverse kernel k ? with SN R = 1E ? 4
is given in Fig. 3(b). A blurred image with this kernel is shown in Fig. 3(c). Deconvolution results
with k ? are in (d). A level of blur is removed from the image. But noise and saturation cause visual
artifacts, in compliance with our understanding of Wiener deconvolution.
Although the Wiener method is not state-of-the-art, its byproduct that the inverse kernel is with a
finite yet large spatial support becomes vastly useful in our neural network system, which manifests
that deconvolution can be well approximated by spatial convolution with sufficiently large kernels.
This explains unsuccessful application of SSDA and CNN directly to deconvolution in Fig. 2 as
follows.
? SSDA does not capture well the nature of convolution with its fully connected structures.
? CNN performs better since deconvolution can be approximated by large-kernel convolution
as explained above.
4

? Previous CNN uses small convolution kernels. It is however not an appropriate configuration in our deconvolution problem.
It thus can be summarized that using deep neural networks to perform deconvolution is by no means
straightforward. Simply modifying the network by employing large convolution kernels would lead
to higher difficulties in training. We present a new structure to update the network in what follows.
Our result in Fig. 3 is shown in (e).

5 Network Architecture
We transform the simple pseudo inverse kernel for deconvolution into a convolutional network,
based on the kernel separability theorem. It makes the network more expressive with the mapping to
higher dimensions to accommodate nonlinearity. This system is benefited from large training data.
5.1 Kernel Separability
Kernel separability is achieved via singular value decomposition (SVD) [26]. Given the inverse
kernel k ? , decomposition k ? = U SV T exists. We denote by uj and vj the j th columns of U and V ,
sj the j th singular value. The original pseudo deconvolution can be expressed as
X
sj ? uj ? (vjT ? y),
(3)
k? ? y =
j

which shows 2D convolution can be deemed as a weighted sum of separable 1D filters. In practice,
we can well approximate k ? by a small number of separable filters by dropping out kernels associated
with zero or very small sj . We have experimented with real blur kernels to ignore singular values
smaller than 0.01. The resulting average number of separable kernels is about 30 [25]. Using a
smaller SN R ratio, the inverse kernel has a smaller spatial support. We also found that an inverse
kernel with length 100 is typically enough to generate visually plausible deconvolution results. This
is important information in designing the network architecture.
5.2 Image Deconvolution CNN (DCNN)
We describe our image deconvolution convolutional neural network (DCNN) based on the separable
kernels. This network is expressed as
h3 = W3 ? h2 ; hl = ?(Wl ? hl?1 + bl?1 ), l ? {1, 2}; h0 = y?,
where Wl is the weight mapping the (l ? 1)th layer to the lth one and bl?1 is the vector value bias.
?(?) is the nonlinear function, which can be sigmoid or hyperbolic tangent.
Our network contains two hidden layers similar to the separable kernel inversion setting. The first
hidden layer h1 is generated by applying 38 large-scale one-dimensional kernels of size 121 ? 1,
according to the analysis in Section 5.1. The values 38 and 121 are empirically determined, which
can be altered for different inputs. The second hidden layer h2 is generated by applying 38 1 ? 121
convolution kernels to each of the 38 maps in h1 . To generate results, a 1 ? 1 ? 38 kernel is applied,
analogous to the linear combination using singular value sj .
The architecture has several advantages for deconvolution. First, it assembles separable kernel inversion for deconvolution and therefore is guaranteed to be optimal. Second, the nonlinear terms
and high dimensional structure make the network more expressive than traditional pseudo-inverse.
It is reasonably robust to outliers.
5.3 Training DCNN
The network can be trained either by random-weight initialization or by the initialization from the
separable kernel inversion, since they share the exact same structure.
We experiment with both strategies on natural images, which are all degraded by additive Gaussian
noise (AWG) and JPEG compression. These images are in two categories ? one with strong color
saturation and one without. Note saturation affects many existing deconvolution algorithms a lot.
5

Figure 4: PSNRs produced in different stages of our convolutional neural network architecture.

(a) Separable kernel inversion

(b) Random initialization

(c) Separable kernel initialization

(d) ODCNN output

Figure 5: Results comparisons in different stages of our deconvolution CNN.
The PSNRs are shown as the first three bars in Fig. 4. We obtain the following observations.
? The trained network has an advantage over simply performing separable kernel inversion,
no matter with random initialization or initialization from pseudo-inverse. Our interpretation is that the network, with high dimensional mapping and nonlinearity, is more expressive than simple separable kernel inversion.
? The method with separable kernel inversion initialization yields higher PSNRs than that
with random initialization, suggesting that initial values affect this network and thus can be
tuned.
Visual comparison is provided in Fig. 5(a)-(c), where the results of separable kernel inversion, training with random weights, and of training with separable kernel inversion initialization are shown.
The result in (c) obviously contains sharp edges and more details. Note that the final trained DCNN
is not equivalent to any existing inverse-kernel function even with various regularization, due to the
involved high-dimensional mapping with nonlinearities.
The performance of deconvolution CNN decreases for images with color saturation. Visual artifacts
could also be yielded due to noise and compression. We in the next section turn to a deeper structure
to address these remaining problems, by incorporating a denoise CNN module.
5.4 Outlier-rejection Deconvolution CNN (ODCNN)
Our complete network is formed as the concatenation of the deconvolution CNN module with a
denoise CNN [16]. The overall structure is shown in Fig. 6. The denoise CNN module has two
hidden layers with 512 feature maps. The input image is convolved with 512 kernels of size 16 ? 16
to be fed into the hidden layer.
The two network modules are concatenated in our system by combining the last layer of deconvolution CNN with the input of denoise CNN. This is done by merging the 1 ? 1 ? 36 kernel with 512
16 ? 16 kernels to generate 512 kernels of size 16 ? 16 ? 36. Note that there is no nonlinearity when
combining the two modules. While the number of weights grows due to the merge, it allows for a
flexible procedure and achieves decent performance, by further incorporating fine tuning.
6

64x184x38

49x49x512

64x64x38

49x49x512

184x184

kernel size
1x121

56x56

kernel size
121x1

kernel size
16x16x38

kernel size
1x1x512

kernel size
8x8x512

Outlier Rejection Sub-Network

Deconvolution Sub-Network

Restoration

Figure 6: Our complete network architecture for deep deconvolution.

5.5 Training ODCNN
We blur natural images for training ? thus it is easy to obtain a large number of data. Specifically,
we use 2,500 natural images downloaded from Flickr. Two million patches are randomly sampled
from them. Concatenating the two network modules can describe the deconvolution process and
enhance the ability to suppress unwanted structures. We train the sub-networks separately. The
deconvolution CNN is trained using the initialization from separable inversion as described before.
The output of deconvolution CNN is then taken as the input of the denoise CNN.
Fine tuning is performed by feeding one hundred thousand 184?184 patches into the whole network.
The training samples contain all patches possibly with noise, saturation, and compression artifacts.
The statistics of adding denoise CNN are also plotted in Fig. 4. The outlier-rejection CNN after fine
tuning improves the overall performance up to 2dB, especially for those saturated regions.

6 More Discussions
Our approach differs from previous ones in several ways. First, we identify the necessity of using a
relatively large kernel support for convolutional neural network to deal with deconvolution. To avoid
rapid weight-size expansion, we advocate the use of 1D kernels. Second, we propose a supervised
pre-training on the sub-network that corresponds to reinterpretation of Wiener deconvolution. Third,
we apply traditional deconvolution to network initialization, where generative solvers can guide
neural network learning and significantly improve performance.
Fig. 6 shows that a new convolutional neural network architecture is capable of dealing with deconvolution. Without a good understanding of the functionality of each sub-net and performing supervised pre-training, however, it is difficult to make the network work very well. Training the whole
network with random initialization is less preferred because the training algorithm stops halfway
without further energy reduction. The corresponding results are similarly blurry as the input images.
To understand it, we visualize intermediate results from the deconvolutional CNN sub-network,
which generates 38 intermediate maps. The results are shown in Fig. 7, where (a) is the selected
three results obtained by random-initialization training and (b) is the results at the corresponding
nodes from our better-initialized process. The maps in (a) look like the high-frequency part of the
blurry input, indicating random initialization is likely to generate high-pass filters. Without proper
starting values, its chance is very small to reach the component maps shown in (b) where sharper
edges present, fully usable for further denoise and artifact removal.
Zeiler et al. [27] showed that sparsely regularized deconvolution can be used to extract useful
middle-level representation in their deconvolution network. Our deconvolution CNN can be used to
approximate this structure, unifying the process in a deeper convolutional neural network.
7

(a)

(b)

Figure 7: Comparisons of intermediate results from deconvolution CNN. (a) Maps from random
initialization. (b) More informative maps with our initialization scheme.
kernel type Krishnan [3] Levin [7]
disk sat.
24.05dB
24.44dB
disk
25.94dB
24.54dB
motion sat.
24.07dB
23.58dB
motion
25.07dB 24.47 dB

Cho [9] Whyte [10] Schuler [13] Schmidt [4]
25.35dB 24.47dB
23.14dB
24.01dB
23.97dB 22.84dB
24.67dB
24.71dB
25.65 dB 25.54dB
24.92dB
25.33dB
24.29dB 23.65dB
25.27dB
25.49dB

Ours
26.23dB
26.01dB
27.76dB
27.92dB

Table 1: Quantitative comparison on the evaluation image set.

(a) Input

(b) Levin et al. [7]

(c) Krishnan et al. [3]

(d) EPLL [11]

(e) Cho et al. [9]

(f) Whyte et al. [10]

(g) Schuler et al. [13]

(h) Ours

Figure 8: Visual comparison of deconvolution results.

7 Experiments and Conclusion
We have presented several deconvolution results. Here we show quantitative evaluation of
our method against state-of-the-art approaches, including sparse prior deconvolution [7], hyperLaplacian prior method [3], variational EM for outliers [9], saturation-aware approach [10], learning
based approach [13] and the discriminative approach [4]. We compare performance using both disk
and motion kernels. The average PSNRs are listed in Table 1. Fig. 8 shows a visual comparison.
Our method achieves decent results quantitatively and visually. The implementation, as well as the
dataset, is available at the project webpage.
To conclude this paper, we have proposed a new deep convolutional network structure for the challenging image deconvolution task. Our main contribution is to let traditional deconvolution schemes
guide neural networks and approximate deconvolution by a series of convolution steps. Our system
novelly uses two modules corresponding to deconvolution and artifact removal. While the network
is difficult to train as a whole, we adopt two supervised pre-training steps to initialize sub-networks.
High-quality deconvolution results bear out the effectiveness of this approach.

References
[1] Fergus, R., Singh, B., Hertzmann, A., Roweis, S.T., Freeman, W.T.: Removing camera shake
from a single photograph. ACM Trans. Graph. 25(3) (2006)
8

[2] Levin, A., Weiss, Y., Durand, F., Freeman, W.T.: Understanding and evaluating blind deconvolution algorithms. In: CVPR. (2009)
[3] Krishnan, D., Fergus, R.: Fast image deconvolution using hyper-laplacian priors. In: NIPS.
(2009)
[4] Schmidt, U., Rother, C., Nowozin, S., Jancsary, J., Roth, S.: Discriminative non-blind deblurring. In: CVPR. (2013)
[5] Agrawal, A.K., Raskar, R.: Resolving objects at higher resolution from a single motion-blurred
image. In: CVPR. (2007)
[6] Michaeli, T., Irani, M.: Nonparametric blind super-resolution. In: ICCV. (2013)
[7] Levin, A., Fergus, R., Durand, F., Freeman, W.T.: Image and depth from a conventional camera
with a coded aperture. ACM Trans. Graph. 26(3) (2007)
[8] Yuan, L., Sun, J., Quan, L., Shum, H.Y.: Progressive inter-scale and intra-scale non-blind
image deconvolution. ACM Trans. Graph. 27(3) (2008)
[9] Cho, S., Wang, J., Lee, S.: Handling outliers in non-blind image deconvolution. In: ICCV.
(2011)
[10] Whyte, O., Sivic, J., Zisserman, A.: Deblurring shaken and partially saturated images. In:
ICCV Workshops. (2011)
[11] Zoran, D., Weiss, Y.: From learning models of natural image patches to whole image restoration. In: ICCV. (2011)
[12] Kenig, T., Kam, Z., Feuer, A.: Blind image deconvolution using machine learning for threedimensional microscopy. IEEE Trans. Pattern Anal. Mach. Intell. 32(12) (2010)
[13] Schuler, C.J., Burger, H.C., Harmeling, S., Sch?olkopf, B.: A machine learning approach for
non-blind image deconvolution. In: CVPR. (2013)
[14] Burger, H.C., Schuler, C.J., Harmeling, S.: Image denoising: Can plain neural networks
compete with bm3d? In: CVPR. (2012)
[15] Xie, J., Xu, L., Chen, E.: Image denoising and inpainting with deep neural networks. In:
NIPS. (2012)
[16] Eigen, D., Krishnan, D., Fergus, R.: Restoring an image taken through a window covered with
dirt or rain. In: ICCV. (2013)
[17] Richardson, W.: Bayesian-based iterative method of image restoration. Journal of the Optical
Society of America 62(1) (1972)
[18] Wiener, N.: Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications. Journal of the American Statistical Association 47(258) (1949)
[19] Roth, S., Black, M.J.: Fields of experts. International Journal of Computer Vision 82(2) (2009)
[20] Dabov, K., Foi, A., Katkovnik, V., Egiazarian, K.O.: Image restoration by sparse 3d transformdomain collaborative filtering. In: Image Processing: Algorithms and Systems. (2008)
[21] Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P.A.: Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.
Journal of Machine Learning Research 11 (2010)
[22] Agostinelli, F., Anderson, M.R., Lee, H.: Adaptive multi-column deep neural networks with
application to robust image denoising. In: NIPS. (2013)
[23] Jain, V., Seung, H.S.: Natural image denoising with convolutional networks. In: NIPS. (2008)
[24] LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document
recognition. Proceedings of the IEEE 86(11) (1998)
[25] Xu, L., Tao, X., Jia, J.: Inverse kernels for fast spatial deconvolution. In: ECCV. (2014)
[26] Perona, P.: Deformable kernels for early vision. IEEE Trans. Pattern Anal. Mach. Intell. 17(5)
(1995)
[27] Zeiler, M.D., Krishnan, D., Taylor, G.W., Fergus, R.: Deconvolutional networks. In: CVPR.
(2010)

9

"
4203,2012,How Prior Probability Influences Decision Making: A Unifying Probabilistic Model,"How does the brain combine prior knowledge with sensory evidence when making decisions under uncertainty? Two competing descriptive models have been proposed based on experimental data.  The first posits an additive offset to a decision variable, implying a static effect of the prior. However, this model is inconsistent with recent data from a motion discrimination task involving temporal integration of uncertain sensory evidence. To explain this data, a second model has been proposed which assumes a time-varying influence of the prior. Here we present a normative model of decision making that incorporates prior knowledge in a principled way.  We show that the additive offset model and the time-varying prior model emerge naturally when decision making is viewed within the framework of partially observable Markov decision processes (POMDPs).  Decision making in the model reduces to (1) computing beliefs given observations and prior information in a Bayesian manner, and (2) selecting actions based on these beliefs to maximize the  expected sum of future rewards. We show that the model can explain both  data previously explained using the additive offset model as well as more  recent data on the time-varying influence of prior knowledge on decision making.","How Prior Probability Influences Decision Making:
A Unifying Probabilistic Model

Abram L. Friesen
University of Washington
afriesen@cs.washington.edu

Yanping Huang
University of Washington
huangyp@cs.washington.edu

Michael N. Shadlen
Columbia University
Howard Hughes Medical Institute
ms4497@columbia.edu

Timothy D. Hanks
Princeton University
thanks@princeton.edu

Rajesh P. N. Rao
University of Washington
rao@cs.washington.edu

Abstract
How does the brain combine prior knowledge with sensory evidence when making
decisions under uncertainty? Two competing descriptive models have been proposed based on experimental data. The first posits an additive offset to a decision
variable, implying a static effect of the prior. However, this model is inconsistent
with recent data from a motion discrimination task involving temporal integration
of uncertain sensory evidence. To explain this data, a second model has been proposed which assumes a time-varying influence of the prior. Here we present a
normative model of decision making that incorporates prior knowledge in a principled way. We show that the additive offset model and the time-varying prior
model emerge naturally when decision making is viewed within the framework
of partially observable Markov decision processes (POMDPs). Decision making
in the model reduces to (1) computing beliefs given observations and prior information in a Bayesian manner, and (2) selecting actions based on these beliefs
to maximize the expected sum of future rewards. We show that the model can
explain both data previously explained using the additive offset model as well as
more recent data on the time-varying influence of prior knowledge on decision
making.

1

Introduction

A fundamental challenge faced by the brain is to combine noisy sensory information with prior
knowledge in order to perceive and act in the natural world. It has been suggested (e.g., [1, 2, 3, 4])
that the brain may solve this problem by implementing an approximate form of Bayesian inference.
These models however leave open the question of how actions are chosen given probabilistic representations of hidden state obtained through Bayesian inference. Daw and Dayan [5, 6] were among
the first to study decision theoretic and reinforcement learning models with the goal of interpreting
results from various neurobiological experiments. Bogacz and colleagues proposed a model that
combines a traditional decision making model with reinforcement learning [7] (see also [8, 9]).
In the decision making literature, two apparently contradictory models have been suggested to explain how the brain utilizes prior knowledge in decision making: (1) a model that adds an offset to a
1

decision variable, implying a static effect of changes to the prior probability [10, 11, 12], and (2) a
model that adds a time varying weight to the decision variable, representing the changing influence
of prior probability over time [13]. The LATER model (Linear Approach to Threshold with Ergodic
Rate), an instance of the additive offset model, incorporates prior probability as the starting point
of a linearly rising decision variable and successfully predicts changes to saccade latency when discriminating between two low contrast stimuli [10]. However, the LATER model fails to explain data
from the random dots motion discrimination task [14] in which the agent is presented with noisy,
time-varying stimuli and must continually process this data in order to make a correct choice and
receive reward. The drift diffusion model (DDM), which uses a random walk accumulation, instead
of a linear rise to a boundary, has been successful in explaining behavioral and neurophysiological
data in various perceptual discrimination tasks [14, 15, 16]. However, in order to explain behavioral
data from recent variants of random dots tasks in which the prior probability of motion direction is
manipulated [13], DDMs require the additional assumption of dynamic reweighting of the influence
of the prior over time.
Here, we present a normative framework for decision making that incorporates prior knowledge and
noisy observations under a reward maximization hypothesis. Our work is inspired by models which
cast human and animal decision making in a rational, or optimal, framework. Frazier & Yu [17]
used dynamic programming to derive an optimal strategy for two-alternative forced choice tasks
under a stochastic deadline. Rao [18] proposed a neural model for decision making based on the
framework of partially observable Markov decision processes (POMDPs) [19]; the model focuses
on network implementation and learning but assumes a fixed deadline to explain the collapsing
decision threshold seen in many decision making tasks. Drugowitsch et al. [9] sought to explain
the collapsing decision threshold by combining a traditional drift diffusion model with reward rate
maximization; their model also requires knowledge of decision time in hindsight. In this paper,
we derive a novel POMDP model from which we compute the optimal behavior for sequential
decision making tasks. We demonstrate our model?s explanatory power on two such tasks: the
random dots motion discrimination task [13] and Carpenter and Williams? saccadic eye movement
task [10]. We show that the urgency signal, hypothesized in previous models, emerges naturally as a
collapsing decision boundary with no assumption of a decision deadline. Furthermore, our POMDP
formulation enables incorporation of partial or incomplete prior knowledge about the environment.
By fitting model parameters to the psychometric function in the neutral prior condition (equal prior
probability of either direction), our model accurately predicts both the psychometric function and
the reaction times for the biased (unequal prior probability) case, without introducing additional free
parameters. Finally, the same model also accurately predicts the effect of prior probability changes
on the distribution of reaction times in the Carpenter and Williams task, data that was previously
interpreted in terms of the additive offset model.

2
2.1

Decision Making in a POMDP framework
Model Setup

We model a decision making task using a POMDP, which assumes that at any particular time step,
t, the environment is in a particular hidden state, x ? X , that is not directly observable by the
animal. The animal can make sensory measurements in order to observe noisy samples of this hidden
state. At each time step, the animal receives an observation (stimulus), st , from the environment as
determined by an emission distribution, Pr(st |x). The animal must maintain a belief over the set
of possible true world states, given the observations it has made so far: bt (x) = Pr(x|s1:t ), where
s1:t represents the sequence of stimuli that the animal has received so far, and b0 (x) represents
the animal?s prior knowledge about the environment. At each time step, the animal chooses an
action, a ? A and receives an observation and a reward, R(x, a), from the environment, depending
on the current state and the action taken. The animal uses Bayes rule to update its belief about the
environment after each observation. Through these interactions, the animal learns a policy, ?(b) ? A
for all b, which dictates the action to take for each belief state. The goal is to find an optimal policy,
? ? (b), that maximizes the animal?s total expected future reward in the task.
For example, in the random dots motion discrimination task, the hidden state, x, is composed of
both the coherence of the random dots c ? [0, 1] and the direction d ? {?1, 1} (corresponding
to leftward and rightward motion, respectively), neither of which are known to the animal. The
2

animal is shown a movie of randomly moving dots, a fraction of which are moving in the same
direction (this fraction is the coherence). The movie is modeled as a sequence of time varying
stimuli s1:t . Each frame, st , is a snapshot of the changes in dot positions, sampled from the emission
distribution st ? Pr(st |kc, d), where k > 0 is a free parameter that determines the scale of st .
In order to discriminate the direction given the stimuli, the animal uses Bayes rule to compute the
posterior probability of the static joint hidden state, Pr(x = kdc|s1:t )1 . At each time step, the animal
chooses one of three actions, a ? {AR , AL , AS }, denoting rightward eye movement, leftward eye
movement, and sampling (i.e., waiting for one more observation), respectively. When the animal
makes a correct choice (i.e., a rightward eye movement a = AR when x > 0 or a leftward eye
movement a = AL when x < 0), the animal receives a positive reward RP > 0. The animal
receives a negative reward (penalty) or no reward when an incorrect action is chosen, RN ? 0. We
assume that the animal is motivated by hunger or thirst to make a decision as quickly as possible
and model this with a unit penalty RS = ?1, representing the cost the agent needs to pay when
choosing the sampling action AS .
2.2

Bayesian Inference of Hidden State from Prior Information and Noisy Observations

In a POMDP, decisions are made based on the belief state bt (x) = Pr(x|s1:t ), which is the posterior
probability distribution over x given a sequence of observations s1:t . The initial belief b0 (x) represents the animal?s prior knowledge about x. In both the Carpenter and William?s task [10] and the
random dots motion discrimination task [13], prior information about the probability of a specific
direction (we use rightward direction here, dR , without loss of generality) is learned by the subjects,
Pr(dR ) = Pr(d = 1) = Pr(x > 0) = 1 ? Pr(dL ). Consider the random dots motion discrimination task. Unlike the traditional case where a full prior distribution is given, this direction-only prior
information provides only partial knowledge about the hidden state which also includes coherence.
In the least informative case, only Pr(dR ) is known and we model the distribution over the remaining components of x as a uniform distribution. Combining this with the direction prior, which is
Bernoulli distributed, gives a piecewise uniform distribution for the prior, b0 (x). In the general case,
we can express the distribution over coherence as a normal distribution parameterized by ?0 and ?0 ,
resulting in a piecewise normal prior over x,

Pr(dR )
x?0
(1)
b0 (x) = Z0?1 N (x | ?0 , ?0 ) ?
Pr(dL )
x < 0,
where Zt = Pr(d
R x R )(1 ? ? (0 | ?t , ?t )) + Pr(dL )? (0 | ?t , ?t ) is the normalization factor and
?(x | ?, ?) = ?? N (x | ?, ?)dx is the cumulative distribution function (CDF) of the normal
distribution. The piecewise uniform prior is then just a special case with ?0 = 0 and ?0 = ?.
We assume the emission distribution is also normally-distributed, Pr(st |x) = N (st |x, ?e2 ), which,
from Bayes? rule, results in a piecewise normal posterior distribution

Pr(dR )
x?0
?1
bt (x) = Zt N (x | ?t , ?t ) ?
(2)
Pr(dL )
x<0

 

?0
t?
st
1
t
where
?t =
+ 2 /
+ 2 ,
(3)
2
2
?0
?e
?0
?e

?1
1
t
?t2 =
+
,
(4)
?02
?e2
Pt
and the running average s?t = t0 =1 st0 /t. Consequently, the posterior distribution depends only on
s? and t, the two sufficient statistics of the sequence s1:t . For the case of a piecewise uniform prior,
?2
the variance ?t2 = te , which decreases inversely in proportion to elapsed time. Unless otherwise
mentioned, we fix ?e = 1, ?0 = ? and ?0 = 0 for the rest of this paper because we can rescale the
POMDP time step t0 = ?te to compensate.
1
In the decision making tasks that we model in this paper, the hidden state is fixed within a trial and thus
there is no transition distribution to include in the belief update equation. However, the POMDP framework is
entirely valid for time-varying states.

3

2.3

Finding the optimal policy by reward maximization

Within the POMDP framework, the animal?s goal is to find an optimal policy ? ? (bt ) that maximizes
its expected reward, starting at bt . This is encapsulated in the value function
""?
#
X
?
v (bt ) = E
r(bt+k , ?(bt+k )) | bt , ?
(5)
k=1

where the expectation is taken with respect to all future belief states (bt+1 , . . . , bt+k , . . .) given
that the animal is using ? to make decisions, and r(b, a) is the reward
R function over belief states
or, equivalently, the expected reward over hidden states, r(b, a) = x R(x, a)b(x)dx. Given the
value function, the optimal policy is simply ? ? (b) = arg max? v ? (b). In this model, the belief b is
parameterized by s?t and t, so the animal only needs to keep track of these instead of encoding the
entire posterior distribution bt (x) explicitly.
R
In our model, the expected reward r(b, a) = x R(x, a)b(x)dx is
?
RS ,
when a = AS
?
?
?1
when a = AR
r(b, a) = Zt [ RP Pr(dR ) (1 ? ?(0 | ?t , ?t )) + RN Pr(dL )?(0 | ?t , ?t ) ],
?
? ?1
Zt [ RN Pr(dR ) (1 ? ?(0 | ?t , ?t )) + RP Pr(dL )?(0 | ?t , ?t ) ],
when a = AL
(6)
where ?t and ?t are given by (3) and (4), respectively. The above equations can be interpreted as
follows. With probability Pr(dL ) ? ?(0 | ?t , ?t ), the hidden state x is less than 0, making AR an
incorrect decision and resulting in a penalty RN if chosen. Similarly, action AR is correct with
probability Pr(dR ) ? [1 ? ?(0 | ?t , ?t )] and earns a reward of RP . The inverse is true for AL . When
AS is selected, the animal simply receives an observation at a cost of RS .
Computing the value function defined in (5) involves an expectation with respect to future belief.
Therefore, we need to compute the transition probabilities over belief states, T (bt+1 |bt , a), for each
action. When the animal chooses to sample, at = AS , the animal?s belief distribution at the next
time step is computed by marginalizing over all possible observations [19]
Z
T (bt+1 |bt , AS ) = Pr(bt+1 |s, bt , AS )Pr(s|bt , AS )ds
(7)
s
1 if bt+1 (x) = Pr(s|x)bt (x)/Pr(s|bt , AS ), ?x
(8)
where
Pr(bt+1 | s, bt , AS ) =
0 otherwise;
Z
and
Pr(s | bt , AS ) =
Pr(s|x)Pr(x|b, a)dx = Ex?b [Pr(s|x)]
(9)
x

When choosing AS , the agent does not affect the world state, so, given the current state bt and
an observation s, the updated belief bt+1 is deterministic and thus Pr(bt+1 | s, bt , AS ) is a delta
function, following Bayes? rule. The probability Pr(s | bt , AS ) can be treated as a normalization
factor and is independent of hidden state2 . Thus, the transition probability function, T (bt+1 | bt , AS ),
is solely a function of the belief bt and is a stationary distribution over the belief space.
When the selected action is AL or AR , the animal stops sampling and makes an eye movement to the
left or the right, respectively. To account for these cases, we include a terminal state, ?, with zeroreward (i.e., R(?, a) = 0, ?a), and absorbing behavior, T (?|?, a) = 1, ?a. Moreover, whenever the
animal chooses AL or AR , the POMDP immediately transitions into ?: T (?|b, a ? {AL , AR }) =
1, ?b, indicating the end of a trial.
Given the transition probability between belief states T (bt+1 |bt , a) and the reward function, we can
convert our POMDP model into a Markov Decision Process (MDP) over the belief state. Standard
dynamic programming techniques (e.g., value iteration [20]) can then be applied to compute the
value function in (5). A neurally plausible method for learning the optimal policy by trial and error
using temporal difference (TD) learning was suggested in [18]. Here, we derive the optimal policy
from first principles and focus on comparisons between our model?s predictions and behavioral data.
2

Explicitly, Pr(s|bt , AS ) = Zt?1 N (s|?t , ?e2 + ?t2 )[Pr(dR ) + (1 ? 2Pr(dR ))?(0|

4

?t
2
?t
1
2
?t

+ s2
?e

+ 12
?e

, ( ?12 + ?12 )?1 ]).
t

e

3
3.1

Model Predictions
Optimal Policy

(a)

(b)

Figure 1: Optimal policy for Pr(dR ) = 0.5, and 0.9. (a?b) Optimal policy as a joint function of
s? and t. Every point in these figures represents a belief state determined by equations (2), (3) and
(4). The color of each point represents the corresponding optimal action. The boundaries ?R (t) and
?L (t) divide the belief space into three areas ?S (center), ?R (upper) and ?L (lower), respectively.
P
Model parameters: RNR?R
= 1, 000.
S
Figure 1(a) shows the optimal policy ? ? as a joint function of s? and t for the unbiased case where
the prior probability Pr(dR ) = Pr(dL ) = 0.5. ? ? partitions the belief space into three regions: ?R ,
?L , and ?S , representing the set of belief states preferring actions AR , AL and AS , respectively.
We define the boundary between AR and AS , and the boundary between AL and AS as ?R (t)
and ?L (t), respectively. Early in a trial, the model selects the sampling action AS regardless of
the value of the observed evidence. This is because the variance of the running average s? is high
for small t. Later in the trial, the model will choose AR or AL when s? is only slightly above
0 because this variance decreases as the model receives more observations. For this reason, the
width of ?S diminishes over time. This gradual decrease in the threshold for choosing one of
the non-sampling actions AR or AL has been called a ?collapsing bound? in the decision making
literature [21, 17, 22]. For this unbiased prior case, the expected reward function is symmetric,
r(bt (x), AR ) = r(Pr(x|?
st , t), AR ) = r(Pr(x| ??
st , t), AL ), and thus the decision boundaries are
also symmetric around 0: ?R (t) = ??L (t).
The optimal policy ? ? is entirely determined by the reward parameters {RP , RN , RS } and the prior
probability (the standard deviation of the emission distribution ?e only determines the temporal
resolution of the POMDP). It applies to both Carpenter and Williams? task and the random dots
task (these two tasks differ only in the interpretation of the hidden state x). The optimal action at
a specific belief state is determined by the relative, not the absolute, value of the expected future
reward. From (6), we have
r(b, AL ) ? r(b, AR ) ? RN ? RP .
(10)
Moreover, if the unit of reward is specified by the sampling penalty, the optimal policy ? ? is entirely
P
and the prior.
determined by the ratio RNR?R
S
As the prior probability becomes biased, the optimal policy becomes asymmetric. When the prior
probability, Pr(dR ), increases, the decision boundary for the more likely direction (?R (t)) shifts
towards the center (the dashed line at s? = 0 in figure 1), while the decision boundary for the opposite
direction (?L (t)) shifts away from the center, as illustrated in Figure 1(b) for prior Pr(dR = 0.9).
Early in a trial, ?S has approximately the same width as in the neutral prior case, but it is shifted
downwards to favor more sampling for dL (?
s < 0). Later in a trial, even for some belief states
with s? < 0, the optimal action is still AR , because the effect of the prior is stronger than that of the
observed data.
3.2

Psychometric function and reaction times in the random dots task

We now construct a decision model from the learned policy for the reaction time version of the
motion discrimination task [14], and compare the model?s predictions to the psychometric and
5

(a) Human SK

(b) Human LH

(c) Monkey Pr(dR ) = .8 (d) Monkey Pr(dR ) = .7

Figure 2: Comparison of Psychometric (upper panels) and Chronometric (lower panels) functions between the Model and Experiments. The dots with error bars represent experimental data
from human subject SK, and LH, and the combined results from four monkeys. Blue solid curves
are model predictions in the neutral case while green dotted curves are model predictions from the
P
= 1, 000, k = 1.45.
biased case. The R2 fits are shown in the plots. Model parameters: (a) RNR?R
S
RN ?RP
RN ?RP
(b) RS = 1, 000, ? = 1.45. (c) Pr(dR ) = 0.8, RS = 1, 000, k = 1.4. (d) Pr(dR ) = 0.7,
RN ?RP
= 1, 000, k = 1.4.
RS

chronometric functions of a monkey performing the same task [13, 14]. Recall that the belief b
is parametrized by s?t and t, so the animal only needs to know the elapsed time and compute a running average s?t of the observations in order to maintain the posterior belief bt (x). Given its current
belief, the animal selects an action from the optimal policy ? ? (bt ). When bt ? ?S , the animal
chooses the sampling action and gets a new observation st+1 . Otherwise the animal terminates
the trial by making an eye movement to the right or to the left, for s?t > ?R (t) or s?t < ?L (t),
respectively.
The performance on the task using the optimal policy can be measured in terms of both the accuracy
of direction discrimination (the so-called psychometric function), and the reaction time required to
reach a decision (the chronometric function). The hidden variable x = kdc encapsulates the unknown direction and coherence, as well as the free parameter k that determines the scale of stimulus
st . Without loss of generality, we fix d = 1 (rightward direction), and set the hidden direction dR as
the biased direction. Given an optimal policy, we compute both the psychometric and chronometric function by simulating a large number of trials (10000 trials per data point) and collecting the
reaction time and chosen direction from each trial.
The upper panels of figure 2(a) and 2(b) (blue curves) show the performance accuracy as a function
of coherence for both the model (blue solid curve) and the human subjects (blue dots) for neutral
prior Pr(dR ) = 0.5. We fit our simulation results to the experimental data by adjusting the only
P
and k. The lower panels of figure 2(a) and 2(b) (blue
two free parameters in our model: RNR?R
S
solid curves) shows the predicted mean reaction time for correct choices as a function of coherence
c for our model (blue solid curve, with same model parameters) and the data (blue dots). Note
that our model?s predicted reaction times represent the expected number of POMDP time steps
before making a rightward eye movement AR , which we can directly compare to the monkey?s
experimental data in units of real time. A linear regression is used to determine the duration ? of
a single time step and the onset of decision time tnd . This offset, tnd , can be naturally interpreted
as the non-decision residual time. We applied the experimental mean reaction time reported in [13]
with motion coherence c = 0.032, 0.064, 0.128, 0.256 and 0.512 to compute the slope and offset, ?
and tnd . Linear regression gives the unit duration per POMDP step as ? = 5.74ms , and the offset
tnd = 314.6ms, for human SK. For human LH, similar results are obtained with ? = 5.20ms and
tnd = 250.0ms. Our predicted offsets compare well with the 300ms non-decision time on average
reported in the literature [23, 24].
6

When the human subject is verbally told that the prior probability is Pr(dR ) = Pr(d = 1) = 0.8,
the experimental data is inconsistent with the predictions of the classic drift diffusion model [14]
unless an additional assumption of a dynamic bias signal is introduced. In the POMDP model we
propose, we predict both the accuracy and reaction times in the biased setting (green curves in
figure 2) with the parameters learned in the neutral case, and achieve a good fit (with the coefficients
of determination shown in fig. 2) to the experimental data reported by Hanks et al. [13]. Our model
predictions for the biased cases are a direct result of the reward maximization component of our
framework and require no additional parameter fitting.
Combined behavioral data from four monkeys is shown by the dotted curves in figure 2(c). We
fit our model parameters to the psychometric function in the neutral case, with ? = 8.20ms and
tnd = 312.50ms, and predict both the psychometric function and the reaction times in the biased
case. However, our results do not match the monkey data as well as the human data when Pr(dR ) =
0.8. This may be due to the fact that the monkeys cannot receive verbal instructions from the
experimenters and must learn an estimate of the prior during training. As a result, the monkeys?
estimate of the prior probability might be inaccurate. To test this hypothesis, we simulated our
model with Pr(dR ) = 0.7 (see figure 2(d)) and these results fit the experimental data much more
accurately (even though the actual probability was 0.8).
3.3

Reaction times in the Carpenter and Williams? task

(a)

(b)

Figure 3: Model predictions of saccadic eye movement in Carpenter & Williams? experiments [10]. (a) Saccadic latency distributions from model simulations plotted in the form of probitscale cumulative mass function, as a function of reciprocal latency. For different values of Pr(dR ),
the simulated data are well fit by straight lines, indicating that the reciprocal of latency follows a
normal distribution. The solid lines are linear functions fit to the data with the constraint that all
lines must pass through the same intercept for infinite time (see [10]). (b) Median latency plotted as
a function of log prior probability. Black dots are from experimental data and blue dots are model
predictions. The two (overlapping) straight lines are the linear least squares fits to the experimental
data and model data. These lines do not differ noticeably in either slope or offset. Model parameters:
RN ?RP
= 1, 000, k = 0.3, ?e = 0.46.
RS
In Carpenter and Williams? task, the animal needs to decide on which side d ? {?1, 1} (denoting
left or right side) a target light appeared at a fixed distance from a central fixation light. After the
sudden appearance of the target light, a constant stimulus st = s is observed by the animal, where s
can be regarded as the perceived location of the target. Due to noise and uncertainty in the nervous
system, we assume that s varies from trial to trial, centered at the location of the target light and
with standard deviation ?e (i.e., s ? N (s | k, ?e2 )), where k is the distance between the target and
the fixation light. Inference over the direction d thus involves joint inference over (d, k) where the
emission probability follows Pr(s|d, k). Then the joint state (k, d) can be one-on-one-mapped to
kd = x, where x represents the actual location of the target light. Under the POMDP framework,
Carpenter and Williams? task and the random dots task differ in the interpretation of hidden state x
and stimulus s, but they follow the same optimal policy given the same reward parameters.
Without loss of generality, we set the hidden variable x > 0 and say that the animal makes a
correct choice at a hitting time tH when the animal?s belief state reaches the right boundary. The
7

?1
saccadic latency can be computed by inverting the boundary function ?R
(s) = tH . Since, for
small t, ?R (t) behaves like a simple reciprocal function of t, the reciprocal of the reaction time is
approximately proportional to a normal distribution with t1H ? N (1/tH | k, ?e2 ). In figure 3(a),
we plot the distribution of reciprocal reaction time with different values of Pr(dR ) on a probit scale
(similar to [10]). Note that we label the y-axis using the CDF of the corresponding probit value
and the x-axis in figure 3(a) has been reversed. If the reciprocal of reaction time (with the same
prior Pr(dR ))?follows a normal distribution, each point on the graph will fall on a straight line with
y-intercept k?e2 that is independent of Pr(dR ). We fit straight lines to the points on the graph,
with the constraint that all lines should pass through the same intercept for infinite time (see [10]).
We obtain an intercept of 6.19, consistent with the intercept 6.20 obtained from experimental data
in [10]. Figure 3(b) demonstrates that the median of our model?s reaction times is a linear function
of the log of the prior probability. Increasing the prior probability lowers the decision boundary
?R (t), effectively decreasing the latency. The slope and intercept of the best fit line are consistent
with experimental data (see fig. 3(b)).

4

Summary and Conclusion

Our results suggest that decision making in the primate brain may be governed by the dual principles
of Bayesian inference and reward maximization as implemented within the framework of partially
observable Markov decision processes (POMDPs). The model provides a unified explanation for
experimental data previously explained by two competing models, namely, the additive offset model
and the dynamic weighting model for incorporating prior knowledge. In particular, the model predicts psychometric and chronometric data for the random dots motion discrimination task [13] as
well as Carpenter and Williams? saccadic eye movement task [10].
Previous models of decision making, such as the LATER model [10] and the drift diffusion
model [25, 15], have provided descriptive accounts of reaction time and accuracy data but often
require assumptions such as a collapsing bound, urgency signal, or dynamic weighting to fully explain the data [26, 21, 22, 13]. Our model provides a normative account of the data, illustrating how
the subject?s choices can be interpreted as being optimal under the framework of POMDPs.
Our model relies on the principle of reward maximization to explain how an animal?s decisions
are influenced by changes in prior probability. The same principle also allows us to predict how an
animal?s choice is influenced by changes in the reward function. Specifically, the model predicts that
P
the optimal policy ? ? is determined by the ratio RNR?R
and the prior probability Pr(dR ). Thus, a
S
testable prediction of the model is that the speed-accuracy trade-off in tasks such as the random dots
P
task is governed by the ratio RNR?R
: smaller penalties for sampling (RS ) will increase accuracy
S
and reaction time, as will larger rewards for correct choices (RP ) or greater penalties for errors
(RN ). Since the reward parameters in our model represent internal reward, our model also provides
a bridge to study the relationship between physical reward and subjective reward.
In our model of the random dots discrimination task, belief is expressed in terms of a piecewise normal distribution with the domain of the hidden variable x ? (??, ?). A piecewise beta distribution
with domain x ? [?1, 1] fits the experimental data equally well. However, the beta distribution?s
conjugate prior is the multinomial, which can limit the application of this model. For example, the
observations in the Carpenter and Williams? model cannot easily be described by a discrete value.
The belief in our model can be expressed by any distribution, even a non-parametric one, as long
as the observation model provides a faithful representation of the stimuli and captures the essential
relationship between the stimuli and the hidden world state.
The POMDP model provides a unifying framework for a variety of perceptual decision making
tasks. Our state variable x and action variable a work with arbitrary state and action spaces, ranging
from multiple alternative choices to high dimensional real value choices. The state variables can
also be dynamic, with xt following a Markov chain. Currently, we have assumed that the stimuli
are independent from one time step to the next, but most real world stimuli are temporally correlated. Our model is suitable for decision tasks with time-varying state and observations that are time
dependent within a trial (as long as they are conditional independent given the time-varying hidden
state sequence). We thus expect our model to be applicable to significantly more complicated tasks
than the ones modeled here.
8

References
[1] D. Knill and W. Richards. Perception as Bayesian inference. Cambridge University Press, 1996.
[2] R.S. Zemel, P. Dayan, and A. Pouget. Probabilistic interpretation of population codes. Neural Computation, 10(2), 1998.
[3] R.P.N. Rao. Bayesian computation in recurrent neural circuits. Neural Computation, 16(1):1?38, 2004.
[4] W.J. Ma, J.M. Beck, P.E. Latham, and A. Pouget. Bayesian inference with probabilistic population codes.
Nature Neuroscience, 9(11):1432?1438, 2006.
[5] N.D. Daw, A.C. Courville, and D.S.Touretzky. Representation and timing in theories of the dopamine
system. Neural Computation, 18(7):1637?1677, 2006.
[6] P. Dayan and N.D. Daw. Decision theory, reinforcement learning, and the brain. Cognitive, Affective and
Behavioral Neuroscience, 8:429?453, 2008.
[7] R. Bogacz and T. Larsen. Integration of reinforcement learning and optimal decision making theories of
the basal ganglia. Neural Computation, 23:817?851, 2011.
[8] C.T. Law and J. I. Gold. Reinforcement learning can account for associative and perceptual learning on a
visual-decision task. Nat. Neurosci, 12(5):655?663, 2009.
[9] J. Drugowitsch, and A. K. Churchland R. Moreno-Bote, M. N. Shadlen, and A. Pouget. The cost of
accumulating evidence in perceptual decision making. J. Neurosci, 32(11):3612?3628, 2012.
[10] R.H.S. Carpenter and M.L.L. Williams. Neural computation of log likelihood in the control of saccadic
eye movements. Nature, 377:59?62, 1995.
[11] M.C. Dorris and D.P. Munoz. Saccadic probability influences motor preparation signals and time to
saccadic initiation. J. Neurosci, 18:7015?7026, 1998.
[12] J.I. Gold, C.T. Law, P. Connolly, and S. Bennur. The relative influences of priors and sensory evidence on
an oculomotor decision variable during perceptual learning. J. Neurophysiol, 100(5):2653?2668, 2008.
[13] T.D. Hanks, M.E. Mazurek, R. Kiani, E. Hopp, and M.N. Shadlen. Elapsed decision time affects the
weighting of prior probability in a perceptual decision task. Journal of Neuroscience, 31(17):6339?6352,
2011.
[14] J.D. Roitman and M.N. Shadlen. Response of neurons in the lateral intraparietal area during a combined
visual discrimination reaction time task. Jounral of Neuroscience, 22, 2002.
[15] R. Bogacz, E. Brown, J. Moehlis, P. Hu, P. Holmes, and J.D. Cohen. The physics of optimal decision
making: A formal analysis of models of performance in two-alternative forced choice tasks. Psychological
Review, 113:700?765, 2006.
[16] R. Ratcliff and G. McKoon. The diffusion decision model: Theory and data for two-choice decision tasks.
Neural Computation, 20:127?140, 2008.
[17] P. L. Frazier and A. J. Yu. Sequential hypothesis testing under stochastic deadlines. In Advances in Neural
Information procession Systems, 20, 2007.
[18] R.P.N. Rao. Decision making under uncertainty: A neural model based on POMDPs. Frontiers in Computational Neuroscience, 4(146), 2010.
[19] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic
domains. Artificial Intelligence, 101:99?134, 1998.
[20] R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. The MIT Press, 1998.
[21] P.E. Latham, Y. Roudi, M. Ahmadi, and A. Pouget. Deciding when to decide. Soc. Neurosci. Abstracts,
740(10), 2007.
[22] A. K. Churchland, R. Kiani, and M. N. Shadlen. Decision-making with multiple alternatives. Nat.
Neurosci., 11(6), 2008.
[23] R.D. Luce. Response times: their role in inferring elementary mental organization. Oxford University
Press, 1986.
[24] M.E. Mazurek, J.D. Roitman, J. Ditterich, and M.N. Shadlen. A role for neural integrators in perceptual
decision-making. Cerebral Cortex, 13:1257?1269, 2003.
[25] J. Palmer, A.C. Huk, and M.N. Shadlen. The effects of stimulus strength on the speed and accuracy of a
perceptual decision. Journal of Vision, 5:376?404, 2005.
[26] J. Ditterich. Stochastic models and decisions about motion direction: Behavior and physiology. Neural
Networks, 19:981?1012, 2006.

9

"
6580,2017,PASS-GLM: polynomial approximate sufficient statistics for scalable Bayesian GLM inference,"Generalized linear models (GLMs)---such as logistic regression, Poisson regression, and robust regression---provide interpretable models for diverse data types. Probabilistic approaches, particularly Bayesian ones, allow coherent estimates of uncertainty, incorporation of prior information, and sharing of power across experiments via hierarchical models. In practice, however, the approximate Bayesian methods necessary for inference have either failed to scale to large data sets or failed to provide theoretical guarantees on the quality of inference. We propose a new approach based on constructing polynomial approximate sufficient statistics for GLMs (PASS-GLM). We demonstrate that our method admits a simple algorithm as well as trivial streaming and distributed extensions that do not compound error across computations. We provide theoretical guarantees on the quality of point (MAP) estimates, the approximate posterior, and posterior mean and uncertainty estimates. We validate our approach empirically in the case of logistic regression using a quadratic approximation and show competitive performance with stochastic gradient descent, MCMC, and the Laplace approximation in terms of  speed and multiple measures of accuracy---including on an advertising data set with 40 million data points and 20,000 covariates.","PASS-GLM: polynomial approximate sufficient
statistics for scalable Bayesian GLM inference

Jonathan H. Huggins
CSAIL, MIT
jhuggins@mit.edu

Ryan P. Adams
Google Brain and Princeton
rpa@princeton.edu

Tamara Broderick
CSAIL, MIT
tbroderick@csail.mit.edu

Abstract
Generalized linear models (GLMs)?such as logistic regression, Poisson regression, and robust regression?provide interpretable models for diverse data types.
Probabilistic approaches, particularly Bayesian ones, allow coherent estimates of
uncertainty, incorporation of prior information, and sharing of power across experiments via hierarchical models. In practice, however, the approximate Bayesian
methods necessary for inference have either failed to scale to large data sets or
failed to provide theoretical guarantees on the quality of inference. We propose a
new approach based on constructing polynomial approximate sufficient statistics
for GLMs (PASS-GLM). We demonstrate that our method admits a simple algorithm as well as trivial streaming and distributed extensions that do not compound
error across computations. We provide theoretical guarantees on the quality of
point (MAP) estimates, the approximate posterior, and posterior mean and uncertainty estimates. We validate our approach empirically in the case of logistic
regression using a quadratic approximation and show competitive performance
with stochastic gradient descent, MCMC, and the Laplace approximation in terms
of speed and multiple measures of accuracy?including on an advertising data set
with 40 million data points and 20,000 covariates.

1

Introduction

Scientists, engineers, and companies increasingly use large-scale data?often only available via
streaming?to obtain insights into their respective problems. For instance, scientists might be interested in understanding how varying experimental inputs leads to different experimental outputs;
or medical professionals might be interested in understanding which elements of patient histories
lead to certain health outcomes. Generalized linear models (GLMs) enable these practitioners to
explicitly and interpretably model the effect of covariates on outcomes while allowing flexible noise
distributions?including binary, count-based, and heavy-tailed observations. Bayesian approaches
further facilitate (1) understanding the importance of covariates via coherent estimates of parameter
uncertainty, (2) incorporating prior knowledge into the analysis, and (3) sharing of power across different experiments or domains via hierarchical modeling. In practice, however, an exact Bayesian
analysis is computationally infeasible for GLMs, so an approximation is necessary. While some
approximate methods provide asymptotic guarantees on quality, these methods often only run successfully in the small-scale data regime. In order to run on (at least) millions of data points and thousands of covariates, practitioners often turn to heuristics with no theoretical guarantees on quality.
In this work, we propose a novel and simple approximation framework for probabilistic inference in
GLMs. We demonstrate theoretical guarantees on the quality of point estimates in the finite-sample
setting and on the quality of Bayesian posterior approximations produced by our framework. We
show that our framework trivially extends to streaming data and to distributed architectures, with
no additional compounding of error in these settings. We empirically demonstrate the practicality
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

of our framework on datasets with up to tens of millions of data points and tens of thousands of
covariates.
Large-scale Bayesian inference. Calculating accurate approximate Bayesian posteriors for large
data sets together with complex models and potentially high-dimensional parameter spaces is a longstanding problem. We seek a method that satisfies the following criteria: (1) it provides a posterior
approximation; (2) it is scalable; (3) it comes equipped with theoretical guarantees; and (4) it
provides arbitrarily good approximations. By posterior approximation we mean that the method
outputs an approximate posterior distribution, not just a point estimate. By scalable we mean that
the method examines each data point only a small number of times, and further can be applied to
streaming and distributed data. By theoretical guarantees we mean that the posterior approximation
is certified to be close to the true posterior in terms of, for example, some metric on probability
measures. Moreover, the distance between the exact and approximate posteriors is an efficiently
computable quantity. By an arbitrarily good approximation we mean that, with a large enough
computational budget, the method can output an approximation that is as close to the exact posterior
as we wish.
Markov chain Monte Carlo (MCMC) methods provide an approximate posterior, and the approximation typically becomes arbitrarily good as the amount of computation time grows asymptotically;
thereby MCMC satisfies criteria 1, 3, and 4. But scalability of MCMC can be an issue. Conversely,
variational Bayes (VB) and expectation propagation (EP) [27] have grown in popularity due to their
scalability to large data and models?though they typically lack guarantees on quality (criteria 3
and 4). Subsampling methods have been proposed to speed up MCMC [1, 5, 6, 21, 25, 41] and
VB [18]. Only a few of these algorithms preserve guarantees asymptotic in time (criterion 4), and
they often require restrictive assumptions. On the scalability front (criterion 2), many though not
all subsampling MCMC methods have been found to require examining a constant fraction of the
data at each iteration [2, 6, 7, 30, 31, 38], so the computational gains are limited. Moreover, the
random data access required by these methods may be infeasible for very large datasets that do not
fit into memory. Finally, they do not apply to streaming and distributed data, and thus fail criterion
2 above. More recently, authors have proposed subsampling methods based on piecewise deterministic Markov processes (PDMPs) [8, 9, 29]. These methods are promising since subsampling data
here does not change the invariant distribution of the continuous-time Markov process. But these
methods have not yet been validated on large datasets nor is it understood how subsampling affects
the mixing rates of the Markov processes. Authors have also proposed methods for coalescing information across distributed computation (criterion 2) in MCMC [12, 32, 34, 35], VB [10, 11], and
EP [15, 17]?and in the case of VB, across epochs as streaming data is collected [10, 11]. (See Angelino et al. [3] for a broader discussion of issues surrounding scalable Bayesian inference.) While
these methods lead to gains in computational efficiency, they lack rigorous justification and provide
no guarantees on the quality of inference (criteria 3 and 4).
To address these difficulties, we are inspired in part by the observation that not all Bayesian models
require expensive posterior approximation. When the likelihood belongs to an exponential family,
Bayesian posterior computation is fast and easy. In particular, it suffices to find the sufficient statistics of the data, which require computing a simple summary at each data point and adding these
summaries across data points. The latter addition requires a single pass through the data and is
trivially streaming or distributed. With the sufficient statistics in hand, the posterior can then be
calculated via, e.g., MCMC, and point estimates such as the MLE can be computed?all in time independent of the data set size. Unfortunately, sufficient statistics are not generally available (except
in very special cases) for GLMs. We propose to instead develop a notion of approximate sufficient
statistics. Previously authors have suggested using a coreset?a weighted data subset?as a summary of the data [4, 13, 14, 16, 19, 24]. While these methods provide theoretical guarantees on the
quality of inference via the model evidence, the resulting guarantees are better suited to approximate
optimization and do not translate to guarantees on typical Bayesian desiderata, such as the accuracy
of posterior mean and uncertainty estimates. Moreover, while these methods do admit streaming
and distributed constructions, the approximation error is compounded across computations.
Our contributions. In the present work we instead propose to construct our approximate sufficient
statistics via a much simpler polynomial approximation for generalized linear models. We therefore
call our method polynomial approximate sufficient statistics for generalized linear models (PASSGLM). PASS-GLM satisfies all of the criteria laid of above. It provides a posterior approximation
with theoretical guarantees (criteria 1 and 3). It is scalable since is requires only a single pass over

2

the data and can be applied to streaming and distributed data (criterion 2). And by increasing the
number of approximate sufficient statistics, PASS-GLM can produce arbitrarily good approximations to the posterior (criterion 4).
The Laplace approximation [39] and variational methods with a Gaussian approximation family
[20, 22] may be seen as polynomial (quadratic) approximations in the log-likelihood space. But we
note that the VB variants still suffer the issues described above. A Laplace approximation relies on
a Taylor series expansion of the log-likelihood around the maximum a posteriori (MAP) solution,
which requires first calculating the MAP?an expensive multi-pass optimization in the large-scale
data setting. Neither Laplace nor VB offers the simplicity of sufficient statistics, including in streaming and distributed computations. The recent work of Stephanou et al. [36] is similar in spirit to ours,
though they address a different statistical problem: they construct sequential quantile estimates using
Hermite polynomials.
In the remainder of the paper, we begin by describing generalized linear models in more detail in
Section 2. We construct our novel polynomial approximation and specify our PASS-GLM algorithm
in Section 3. We will see that streaming and distributed computation are trivial for our algorithm
and do not compound error. In Section 4.1, we demonstrate finite-sample guarantees on the quality
of the MAP estimate arising from our algorithm, with the maximum likelihood estimate (MLE)
as a special case. In Section 4.2, we prove guarantees on the Wasserstein distance between the
exact and approximate posteriors?and thereby bound both posterior-derived point estimates and
uncertainty estimates. In Section 5, we demonstrate the efficacy of our approach in practice by
focusing on logistic regression. We demonstrate experimentally that PASS-GLM can be scaled
with almost no loss of efficiency to multi-core architectures. We show on a number of real-world
datasets?including a large, high-dimensional advertising dataset (40 million examples with 20,000
dimensions)?that PASS-GLM provides an attractive trade-off between computation and accuracy.

2

Background

Generalized linear models. Generalized linear models (GLMs) combine the interpretability of
linear models with the flexibility of more general outcome distributions?including binary, ordinal,
and heavy-tailed observations. Formally, we let Y ? R be the observation space, X ? Rd be the
covariate space, and ? ? Rd be the parameter space. Let D := {(xn , yn )}N
n=1 be the observed data.
We write X ? RN ?d for the matrix of all covariates and y ? RN for the vector of all observations.
We consider GLMs
PN
PN
log p(y | X, ?) = n=1 log p(yn | g ?1 (xn ? ?)) = n=1 ?(yn , xn ? ?),
where ? := g ?1 (xn ? ?) is the expected value of yn and g ?1 : R ? R is the inverse link function.
We call ?(y, s) := log p(y | g ?1 (s)) the GLM mapping function.
Examples include some of the most widely used models in the statistical toolbox. For instance, for binary observations y ? {?1}, the likelihood model is Bernoulli, p(y = 1 | ?) = ?,
?
and the link function is often either the logit g(?) = log 1??
(as in logistic regression) or the pro?1
bit g(?) = ? (?), where ? is the standard Gaussian CDF. When modeling count data y ? N, the
likelihood model might be Poisson, p(y | ?) = ?y e?? /y!, and g(?) = log(?) is the typical log link.
Other GLMs include gamma regression, robust regression, and binomial regression, all of which are
commonly used for large-scale data analysis (see Examples A.1 and A.3).
If we place a prior ?0 (d?) on the parameters, then a full Bayesian analysis aims to approximate the
(typically intractable) GLM posterior distribution ?D (d?), where
?D (d?) = R

p(y | X, ?) ?0 (d?)
.
p(y | X, ? 0 ) ?0 (d? 0 )

The maximum a posteriori (MAP) solution gives a point estimate of the parameter:
?MAP := arg max ?D (?) = arg max log ?0 (?) + LD (?),
???

(1)

???

where LD (?) := log p(y | X, ?) is the data log-likelihood. The MAP problem strictly generalizes
finding the maximum likelihood estimate (MLE), since the MAP solution equals the MLE when
using the (possibly improper) prior ?0 (?) = 1.
3

Algorithm 1 PASS-GLM inference
Require: data D, GLM mapping function ? : R ? R, degree M , polynomial basis (?m )m?N with
base measure ?
R
1: Calculate basis coefficients bm ? ??m d? using numerical integration for m = 0, . . . , M
PM
(M )
2: Calculate polynomial coefficients bm ? k=m ?k,m bm for m = 0, . . . , M
P
3: for k ? Nd with j kj ? M do
4:
Initialize tk ? 0
5: for n = 1, . . . , N doP
. Can be done with any combination of batch, parallel, or streaming
6:
for k ? Nd with j kj ? M do
7:
Update tk ? tk + (yn xn )k

m (M )
k
?D (?) = P
P
8: Form approximate log-likelihood L
k?Nd : j kj ?m k bm tk ?
?D (?) to construct approximate posterior ?
9: Use L
?D (?)

Computation and exponential families. In large part due to the high-dimensional integral implicit in the normalizing constant, approximating the posterior, e.g., via MCMC or VB, is often
prohibitively expensive. Approximating this integral will typically require many evaluations of the
(log-)likelihood, or its gradient, and each evaluation may require ?(N ) time.
Computation is much more efficient, though, if the model is in an exponential family (EF). In the EF
case, there exist functions t, ? : Rd ? Rm , such that1
log p(yn | xn , ?) = t(yn , xn ) ? ?(?) =: LD,EF (?; t(yn , xn )).
Thus, we can rewrite the log-likelihood as
PN
LD (?) = n=1 LD,EF (?; t(yn , xn )) =: LD,EF (?; t(D)),
PN
where t(D) := n=1 t(yn , xn ). The sufficient statistics t(D) can be calculated in O(N ) time,
after which each evaluation of LD,EF (?; t(D)) or ?LD,EF (?; t(D)) requires only O(1) time. Thus,
instead of K passes over N data (requiring O(N K) time), only O(N + K) time is needed. Even
for moderate values of N , the time savings can be substantial when K is large.
The Poisson distribution is an illustrative example of a one-parameter exponential family
with t(y) = (1, y,P
log y!) P
and ?(?) = (?, log ?, 1). Thus, if we have data y (there are no covariates), t(y) = (N, n yn , log yn !). In this case
Pit is easy to calculate that the maximum likelihood
estimate of ? from t(y) as t1 (y)/t0 (y) = N ?1 n yn .
Unfortunately, GLMs rarely belong to an exponential family ? even if the outcome distribution is in an exponential family, the use of a link destroys the
EF structure.
In logistic regression, we write (overloading the ? notation)
log p(yn | xn , ?) = ?logit (yn xn ? ?), where ?logit (s) := ? log(1 + e?s ). For Poisson regression
with log link, log p(yn | xn , ?) = ?Poisson (yn , xn ? ?), where ?Poisson (y, s) := ys ? es ? log y!. In
both cases, we cannot express the log-likelihood as an inner product between a function solely of
the data and a function solely of the parameter.

3

PASS-GLM

Since exact sufficient statistics are not available for GLMs, we propose to construct approximate sufficient statistics. In particular, we propose to approximate the mapping function ? with
an order-M polynomial ?M . We therefore call our method polynomial approximate sufficient
statistics for GLMs (PASS-GLM). We illustrate our method next in the logistic regression case,
where log p(yn | xn , ?) = ?logit (yn xn ? ?). The fully general treatment appears in Appendix A.
(M ) (M )
(M )
Let b0 , b1 . . . , bM be constants such that
PM
(M )
?logit (s) ? ?M (s) := m=0 bm sm .
1

Our presentation is slightly different from the standard textbook account because we have implicitly absorbed the base measure and log-partition function into t and ?.

4

Let vk :=

k

Qd

vj j for vectors v, k ? Rd . Taking s = yx ? ?, we obtain
PM
PM
(M )
(M ) P
?logit (yx ? ?) ? ?M (yx ? ?) = m=0 bm (yx ? ?)m = m=0 bm
d
Pk?N
j=1

j

=

PM

m=0

P

P
k?Nd : j kj =m

m
k



(yx)k ? k

kj =m

k k

a(k, m, M )(yx) ? ,

 (M )
where k is the multinomial coefficient and a(k, m, M ) := m
bm . Thus, ?M is an M -degree
k

d+M
polynomial approximation to ?logit (yx ? ?) with the d monomials of degree at most M serving
as sufficient statistics derived from yx. Specifically, we have a exponential family model with

m

t(yx) = ([yx]k )k

and
?(?) = (a(k, m, M )? k )k ,
P
(M )
where k is taken over all k ? Nd such that j kj ? M . We next discuss the calculation of the bm
and the choice of M .
(M )

Choosing the polynomial approximation. To calculate the coefficients bm , we choose a polynomial basis (?P
m )m?N orthogonal with respect to aR base measure ?, where ?m is degree m [37]. That
m
is, ?m (s) = j=0 ?m,j sj for some ?m,j , and ?m ?m0 d? = ?mm0 , where ?mm0 = 1 if m = m0
R
P?
and zero otherwise. If bm := ??m d?, then ?(s) = m=0 bm ?m (s) and the approximaPM
P
(M )
M
tion ?M (s) = m=0 bm ?m (s). Conclude that bm = k=m ?k,m bm . The complete PASS-GLM
framework appears in Algorithm 1.
Choices for the orthogonal polynomial basis include Chebyshev, Hermite, Leguerre, and Legendre polynomials [37]. We choose Chebyshev polynomials since they provide a uniform quality
guarantee on a finite interval, e.g., [?R, R] for some R > 0 in what follows. If ? is smooth, the
choice of Chebyshev polynomials (scaled appropriately, along with the base measure ?, based on
the choice of R) yields error exponentially small in M : sups?[?R,R] |?(s) ? ?M (s)| ? C?M for
some 0 < ? < 1 and C > 0 [26]. We show in Appendix B that the error in the approximate derivative ?0M is also exponentially small in M : sups?[?R,R] |?0 (s) ? ?0M (s)| ? C 0 ?M , where C 0 > C.
Choosing the polynomial degree. For fixed d, the number of monomials is O(M d ) while for fixed
M the number of monomials is O(dM ). The number of approximate sufficient statistics can remain
manageable when either M or d is small but becomes unwieldy if M and d are both large. Since
our experiments (Section 5) generally have large d, we focus on the small M case here.
In our experiments we further focus on the choice of logistic regression as a particularly popular
GLM example with p(yn | xn , ?) = ?logit (yn xn ? ?), where ?logit (s) := ? log(1 + e?s ). In general, the smallest and therefore most compelling choice of M a priori is 2, and we demonstrate the
reasonableness of this choice empirically in Section 5 for a number of large-scale data analyses. In
addition, in the logistic regression case, M = 6 is the next usable choice beyond M = 2. This is be(M )
cause b2k+1 = 0 for all integer k ? 1 with 2k + 1 ? M . So any approximation beyond M = 2 must
(M )

have M ? 4. Also, b4k > 0 for all integers k ? 1 with 4k ? M . So choosing M = 4k, k ? 1,
leads to a pathological approximation of ?logit where the log-likelihood can be made arbitrarily
large by taking k?k2 ? ?. Thus, a reasonable polynomial approximation for logistic regression
requires M = 2 + 4k, k ? 0. We have discussed the relative drawbacks of other popular quadratic
approximations, including the Laplace approximation and variational methods, in Section 1.

4

Theoretical Results

We next establish quality guarantees for PASS-GLM. We first provide finite-sample and asymptotic
guarantees on the MAP (point estimate) solution, and therefore on the MLE, in Section 4.1. We then
provide guarantees on the Wasserstein distance between the approximate and exact posteriors, and
show these bounds translate into bounds on the quality of posterior mean and uncertainty estimates,
in Section 4.2. See Appendix C for extended results, further discussion, and all proofs.
4.1

MAP approximation

In Appendix C, we state and prove Theorem C.1, which provides guarantees on the quality of the
MAP estimate for an arbitrary approximation L?D (?) to the log-likelihood LD (?). The approximate
5

MAP (i.e., the MAP under L?D ) is (cf. Eq. (1))
??MAP := arg max log ?0 (?) + L?D (?).
???

Roughly, we find in Theorem C.1 that the error in the MAP estimate naturally depends on the error
of the approximate log-likelihood as well as the peakedness of the posterior near the MAP. In the
latter case, if log ?D is very flat, then even a small error from using L?D in place of LD could lead
to a large error in the approximate MAP solution. We measure the peakedness of the distribution in
terms of the strong convexity constant2 of ? log ?D near ?MAP .
We apply Theorem C.1 to PASS-GLM for logistic regression and robust regression. We require the
assumption that
?M (t) ? ?(t) ?t ?
/ [?R, R],

(2)

which in the cases of logistic regression and smoothed Huber regression, we conjecture holds
for M = 2 + 4k, k ? N. For a matrix A, kAk2 denotes its spectral norm.
Corollary 4.1. For the logistic regression model, assume that k(?2 LD (?MAP ))?1 k2 ? cd/N for
some constant c > 0 and that kxn k2 ? 1 for all n = 1, . . . , N . Let ?M be the order-M Chebyshev
approximation to ?logit on [?R, R] such that Eq. (2) holds. Let ?
?D (?) denote the posterior approximation obtained by using ?M with a log-concave prior. Then there exist q
numbers r = r(R) > 1,
? = ?(M ) = O(r?M ), and ?? ?

27
?d3 c3 +54 ,

such that if R ? k?MAP k2 ? 2

cd?
??

, then

4cd?
4 4 4 2
k?MAP ? ??MAP k22 ? ? ?
c d ? + 8cd?.
?
27
The main takeaways from Corollary 4.1 are that (1) the error decreases exponentially in M thanks
to the ? term, (2) the error does not depend on the amount of data, and (3) in order for the bound
on the approximate MAP solution to hold, the norm of the true MAP solution must be sufficiently
smaller than R.
Remark 4.2. Some intuition for the assumption on the Hessian of LD , i.e., ?2 LD (?) =
PN
>
00
n=1 ?logit (yn xn ? ?)xn xn , is as follows. Typically for ? near ?MAP , the minimum eigenvalue
2
of ? LD (?) is at least N/(cd) for some c > 0. The minimum eigenvalue condition in Corollary 4.1
holds if, for example, a constant fraction of the data satisfies 0 < b ? kxn k2 ? B < ? and that
subset of the data does not lie too close to any (d ? 1)-dimensional hyperplane. This condition
essentially requires the data not to be degenerate and is similar to ones used to show asymptotic
consistency of logistic regression [40, Ex. 5.40].
The approximate MAP error bound in the robust regression case using, for example, the smoothed
Huber loss (Example A.1), is quite similar to the logistic regression result.
Corollary 4.3. For robust regression with smoothed Huber loss, assume that a constant fraction of
the data satisfies |xn ? ?MAP ? yn | ? b/2 and that kxn k2 ? 1 for all n = 1, . . . , N . Let ?M be the
order M Chebyshev approximation to ?Huber on [?R, R] such that Eq. (2) holds. Let ?
?D (?) denote
the posterior approximation obtained by using ?M with a log-concave prior. Then if R  k?MAP k2 ,
there exists r > 1 such that for M sufficiently large, k?MAP ? ??MAP k22 = O(dr?M ).
4.2

Posterior approximation

We next establish guarantees on how close the approximate and exact posteriors Rare in Wasserstein
R
distance, dW . For distributions P and Q on Rd , dW (P, Q) := supf :kf kL ?1 | f dP ? f dQ|,
where kf kL denotes the Lipschitz constant of f .3 This choice of distance is particularly useful
since, if dW (?D , ?
?D ) ? ?, then ?
?D can be used to estimate any function with bounded gradient
with error at most ? supw k?f (w)k2 . Wasserstein error bounds therefore give bounds on the mean
estimates (corresponding to f (?) = ?i ) as well as uncertainty estimates such as mean absolute deviation (corresponding to f (?) = |??i ? ?i |, where ??i is the expected value of ?i ).
Recall that a twice-differentiable function f : Rd ? R is %-strongly convex at ? if the minimum eigenvalue
of the Hessian of f evaluated at ? is at least % > 0.
3
2
The Lipschitz constant of function f : Rd ? R is kf kL := supv,w?Rd k?(v)??(w)k
.
kv?wk2
2

6

CovType

ChemReact

0
-1
-2
-3
-4
-4

0

0.3

1.0

0.2

0.5

0.1

0.0 6

-1

?(t)

-2

4

2

0

ynxn,

2

MAP

4

2.0
1.5

-2

2

0

4

2

4

4

2 0

ynxn,

2

MAP

4

6

CodRNA
0.2

?2(t)

1.0

0

6 0.0 6

?(t) Webspam

?2(t)

-3
-4
-2
-4

1.5

0.1

0.5
0.0 6

4

(a)

2

0

ynxn,

2

MAP

4

6 0.0

12

4

4

ynxn,

12

MAP

20

(b)

Figure 1: Validating the use of PASS-GLM with M = 2. (a) The second-order Chebyshev approximation to ? = ?logit on [?4, 4] is very accurate, with error of at most 0.069. (b) For a variety of
datasets, the inner products hyn xn , ?MAP i are mostly in the range of [?4, 4].
Our general result (Theorem C.3) is stated and proved in Appendix C. Similar to Theorem C.1,
the result primarily depends on the peakedness of the approximate posterior and the error of the
approximate gradients. If the gradients are poorly approximated then the error can be large while
if the (approximate) posterior is flat then even small gradient errors could lead to large shifts in
expected values of the parameters and hence large Wasserstein error.
We apply Theorem C.3 to PASS-GLM for logistic regression and Poisson regression. We give
simplified versions of these corollaries in the main text and defer the more detailed versions to
Appendix C. For logistic regression we assume M = 2 and ? = Rd since this is the setting we
use for our experiments. The result is similar in spirit to Corollary 4.1, though more straightforward
? ?R
since M = 2. Critically, we see in this result how having small error depends on |yn xn ? ?|
with high probability. Otherwise the second term in the bound will be large.
Corollary 4.4. Let ?2 be the second-order Chebyshev approximation to ?logit on [?R, R] and
? denote the posterior approximation obtained by using ?2 with a Gauslet ?
?D (?) = N(? | ??MAP , ?)
R
PN
? and let ?1
sian prior ?0 (?) = N(? | ?0 , ?0 ). Let ?? := ??D (d?), let ?1 := N ?1 n=1 hyn xn , ?i,
? ? ?1 , where n ? Unif{1, . . . , N }.
be the subgaussianity constant of the random variable hyn xn , ?i
? 2 ? cd/N , and that kxn k2 ? 1 for all n = 1, . . . , N . Then
Assume that |?1 | ? R, that k?k
with ?02 := k?0 k2 , we have



?
dW (?D , ?
?D ) = O dR4 + d?0 exp ?12 ?0?2 ? 2 ?0?1 (R ? |?1 |)
.
? < R, so that ?2 is a good
The main takeaway from Corollary 4.4 is that if (a) for most n, |hxn , ?i|
approximation to ?logit , and (b) the approximate posterior concentrates quickly, then we get a highquality approximate posterior. This result matches up with the experimental results (see Section 5
for further discussion).
For Poisson regression, we return to the case of general M . Recall that in the Poisson regression
model that the expectation of yn is ? = exn ?? . If yn is bounded and has non-trivial probability of
being greater than zero, we lose little by restricting xn ? ? to be bounded. Thus, we will assume that
the parameter space is bounded. As in Corollaries 4.1 and 4.3, the error is exponentially small in M
PN
and, as long as k n=1 xn x>
n k2 grows linearly in N , does not depend on the amount of data.
Corollary 4.5. Let fM (s) be the order-M Chebyshev approximation to et on the interval [?R, R], and let ?
?D (?) denote the posterior approximation obtained by using the approximation
log p?(yn | xn , ?) := yn xn ? ? ? fM (xn ? ?) ? log yn ! with a log-concave prior on ? = BR (0). If
PN
00
inf s?[?R,R] fM
(s) ? %? > 0, k n=1 xn x>
n k2 = ?(N/d), and kxn k2 ? 1 for all n = 1, . . . , N ,
then

dW (?D , ?
?D ) = O d?
%?1 M 2 eR 2?M .
7

10.0

time?(sec)

100.0

0.32
0.1
0.032
0.01
1.0

10.0

time?(sec)

100.0

time?(sec)

100.0

1.0
0.32
0.1

0.1
1.0

1.0

10.0

time?(sec)

100.0

0.32
0.1
0.032
0.1

(a) W EBSPAM

1.0

10.0

time?(sec)

100.0

(b) C OV T YPE

Negative?Test?LL

0.14
0.12
0.01
1.0

0.1

1.0

time?(sec)

10.0

average?mean?error

0.1
1.0

1.0
3.2

0.16

0.32
0.1
0.032
0.01
0.01
1.0

0.1

1.0

time?(sec)

10.0

0.1
0.01
0.1

1.0

time?(sec)

average?variance?error

time?(sec)

100.0

average?mean?error

10.0

average?variance?error

1.0

1.0

1.0

Negative?Test?LL

Negative?Test?LL

0.62
0.1

average?mean?error

0.64

average?variance?error

average?variance?error

PASS?LR2
Laplace
0.6
SGD
True?Posterior
MALA
0.5

0.66

average?mean?error

Negative?Test?LL

0.68

0.6
0.5
0.4
0.3
0.2
0.01

100.0

1.0

100.0

1.0

100.0

10.0
3.2
1.0

time?(sec)
4.0
2.5
1.6

10.0 1.0

(c) C HEM R EACT

1.0

time?(sec)

time?(sec)

(d) C OD RNA

Figure 2: Batch inference results. In all metrics smaller is better.
Note that although %??1 does depend on R and M , as M becomes large it converges to eR . Observe
that if we truncate a prior on Rd to be on BR (0), by making R and M sufficiently large, the Wasserstein distance between ?D and the PASS-GLM posterior approximation ?
?D can be made arbitarily
small. Similar results could be shown for other GLM likelihoods.

5

Experiments

In our experiments, we focus on logistic regression, a particularly popular GLM example.4 As
discussed in Section 3, we choose M = 2 and call our algorithm PASS-LR2. Empirically, we observe that M = 2 offers a high-quality approximation of ? on the interval [?4, 4] (Fig. 1a). In
fact sups?[?4,4] |?2 (s) ? ?(s)| < 0.069. Moreover, we observe that for many datasets, the inner
products yn xn ? ?MAP tend to be concentrated within [?4, 4], and therefore a high-quality approximation on this range is sufficient for our analysis. In particular, Fig. 1b shows histograms of
yn xn ? ?MAP for four datasets from our experiments. In all but one case, over 98% of the data points
satisfy |yn xn ? ?MAP | ? 4. In the remaining dataset (C OD RNA), only ?80% of the data satisfy this
condition, and this is the dataset for which PASS-LR2 performed most poorly (cf. Corollary 4.4).
5.1

Large dataset experiments

In order to compare PASS-LR2 to other approximate Bayesian methods, we first restrict our attention
to datasets with fewer than 1 million data points. We compare to the Laplace approximation and the
adaptive Metropolis-adjusted Langevin algorithm (MALA). We also compare to stochastic gradient
descent (SGD) although SGD provides only a point estimate and no approximate posterior. In all
experiments, no method performs as well as PASS-LR2 given the same (or less) running time.
Datasets. The C HEM R EACT dataset consists of N = 26,733 chemicals, each with d = 100 properties. The goal is to predict whether each chemical is reactive. The W EBSPAM corpus consists
of N = 350,000 web pages and the covariates consist of the d = 127 features that each appear in
at least 25 documents. The cover type (C OV T YPE) dataset consists of N = 581,012 cartographic
observations with d = 54 features. The task is to predict the type of trees that are present at each observation location. The C OD RNA dataset consists of N = 488,565 and d = 8 RNA-related features.
The task is to predict whether the sequences are non-coding RNA.
Fig. 2 shows average errors of the posterior mean and variance estimates as well as negative test loglikelihood for each method versus the time required to run the method. SGD was run for between
1 and 20 epochs. The true posterior was estimated by running three chains of adaptive MALA for
50,000 iterations, which produced Gelman-Rubin statistics well below 1.1 for all datasets.
4

Code is available at https://bitbucket.org/jhhuggins/pass-glm.

8

PASS?LR2?(area?=?0.696)
SGD?(area?=?0.725)

10.0

0.75

speedup

True?Positive?Rate

1.00
0.50
0.25
0.00
0.00

7.5
5.0
2.5

0.25

0.50

0.75

False?Positive?Rate

0

1.00

(a)

10

cores

20

(b)

Figure 3: (a) ROC curves for streaming inference on 40 million C RITEO data points. SGD and
PASS-LR2 had negative test log-likelihoods of, respectively, 0.07 and 0.045. (b) Cores vs. speedup
(compared to one core) for parallelization experiment on 6 million examples from the C RITEO
dataset.
Speed. For all four datasets, PASS-LR2 was an order of magnitude faster than SGD and 2?3 orders
of magnitude faster than the Laplace approximation. Mean and variance estimates. For C HEM R EACT, W EBSPAM, and C OV T YPE, PASS-LR2 was superior to or competitive with SGD, with
MALA taking 10?100x longer to produce comparable results. Laplace again outperformed all other
methods. Critically, on all datasets the PASS-LR2 variance estimates were competitive with Laplace
and MALA. Test log-likelihood. For C HEM R EACT and W EBSPAM, PASS-LR2 produced results
competitive with all other methods. MALA took 10?100x longer to produce comparable results.
For C OV T YPE, PASS-LR2 was competitive with SGD but took a tenth of the time, and MALA took
1000x longer for comparable results. Laplace outperformed all other methods, but was orders of
magnitude slower than PASS-LR2. C OD RNA was the only dataset where PASS-LR2 performed
poorly. However, this performance was expected based on the yn xn ? ?MAP histogram (Fig. 1a).
5.2

Very large dataset experiments using streaming and distributed PASS-GLM

We next test PASS-LR2, which is streaming without requiring any modifications, on a subset of 40
million data points from the Criteo terabyte ad click prediction dataset (C RITEO). The covariates are
13 integer-valued features and 26 categorical features. After one-hot encoding, on the subset of the
data we considered, d ? 3 million. For tractability we used sparse random projections [23] to reduce
the dimensionality to 20,000. At this scale, comparing to the other fully Bayesian methods from
Section 5.1 was infeasible; we compare only to the predictions and point estimates from SGD. PASSLR2 performs slightly worse than SGD in AUC (Fig. 3a), but outperforms SGD in negative test loglikelihood (0.07 for SGD, 0.045 for PASS-LR2). Since PASS-LR2 estimates a full covariance, it
was about 10x slower than SGD. A promising approach to speeding up and reducing memory usage
of PASS-LR2 would be to use a low-rank approximation to the second-order moments.
To validate the efficiency of distributed computation with PASS-LR2, we compared running times
on 6M examples with dimensionality reduced to 1,000 when using 1?22 cores. As shown in Fig. 3b,
the speed-up is close to optimal: K cores produces a speedup of about K/2 (baseline 3 minutes
using 1 core). We used Ray to implement the distributed version of PASS-LR2 [28].5

6

Discussion

We have presented PASS-GLM, a novel framework for scalable parameter estimation and Bayesian
inference in generalized linear models. Our theoretical results provide guarantees on the quality of
point estimates as well as approximate posteriors derived from PASS-GLM. We validated our approach empirically with logistic regression and a quadratic approximation. We showed competitive
performance on a variety of real-world data, scaling to 40 million examples with 20,000 covariates,
and trivial distributed computation with no compounding of approximation error.
There a number of important directions for future work. The first is to use randomization methods
along the lines of random projections and random feature mappings [23, 33] to scale to larger M
and d. We conjecture that the use of randomization will allow experimentation with other GLMs for
which quadratic approximations are insufficient.
5

https://github.com/ray-project/ray

9

Acknowledgments
JHH and TB are supported in part by ONR grant N00014-17-1-2072, ONR MURI grant N00014-11-1-0688,
and a Google Faculty Research Award. RPA is supported by NSF IIS-1421780 and the Alfred P. Sloan Foundation.

References
[1] S. Ahn, A. Korattikara, and M. Welling. Bayesian posterior sampling via stochastic gradient Fisher
scoring. In International Conference on Machine Learning, 2012.
[2] P. Alquier, N. Friel, R. Everitt, and A. Boland. Noisy Monte Carlo: convergence of Markov chains with
approximate transition kernels. Statistics and Computing, 26:29?47, 2016.
[3] E. Angelino, M. J. Johnson, and R. P. Adams. Patterns of scalable Bayesian inference. Foundations and
R in Machine Learning, 9(2-3):119?247, 2016.
Trends
[4] O. Bachem, M. Lucic, and A. Krause. Practical coreset constructions for machine learning. arXiv.org,
Mar. 2017.
[5] R. Bardenet, A. Doucet, and C. C. Holmes. Towards scaling up Markov chain Monte Carlo: an adaptive
subsampling approach. In International Conference on Machine Learning, pages 405?413, 2014.
[6] R. Bardenet, A. Doucet, and C. C. Holmes. On Markov chain Monte Carlo methods for tall data. Journal
of Machine Learning Research, 18:1?43, 2017.
[7] M. J. Betancourt. The fundamental incompatibility of Hamiltonian Monte Carlo and data subsampling.
In International Conference on Machine Learning, 2015.
[8] J. Bierkens, P. Fearnhead, and G. O. Roberts. The zig-zag process and super-efficient sampling for
Bayesian analysis of big data. arXiv.org, July 2016.
[9] A. Bouchard-C?ot?e, S. J. Vollmer, and A. Doucet. The bouncy particle sampler: A non-reversible rejectionfree Markov chain Monte Carlo method. arXiv.org, pages 1?37, Jan. 2016.
[10] T. Broderick, N. Boyd, A. Wibisono, A. C. Wilson, and M. I. Jordan. Streaming variational Bayes. In
Advances in Neural Information Processing Systems, Dec. 2013.
[11] T. Campbell, J. Straub, J. W. Fisher, III, and J. P. How. Streaming, distributed variational inference for
Bayesian nonparametrics. In Advances in Neural Information Processing Systems, 2015.
[12] R. Entezari, R. V. Craiu, and J. S. Rosenthal. Likelihood inflating sampling algorithm. arXiv.org, May
2016.
[13] D. Feldman, M. Faulkner, and A. Krause. Scalable training of mixture models via coresets. In Advances
in Neural Information Processing Systems, pages 2142?2150, 2011.
[14] W. Fithian and T. Hastie. Local case-control sampling: Efficient subsampling in imbalanced data sets.
The Annals of Statistics, 42(5):1693?1724, Oct. 2014.
[15] A. Gelman, A. Vehtari, P. Jyl?anki, T. Sivula, D. Tran, S. Sahai, P. Blomstedt, J. P. Cunningham, D. Schiminovich, and C. Robert. Expectation propagation as a way of life: A framework for Bayesian inference on
partitioned data. arXiv.org, Dec. 2014.
[16] L. Han, T. Yang, and T. Zhang. Local uncertainty sampling for large-scale multi-class logistic regression.
arXiv.org, Apr. 2016.
[17] L. Hasenclever, S. Webb, T. Lienart, S. Vollmer, B. Lakshminarayanan, C. Blundell, and Y. W. Teh.
Distributed Bayesian learning with stochastic natural-gradient expectation propagation and the posterior
server. Journal of Machine Learning Research, 18:1?37, 2017.
[18] M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley. Stochastic variational inference. Journal of Machine
Learning Research, 14:1303?1347, 2013.
[19] J. H. Huggins, T. Campbell, and T. Broderick. Coresets for scalable Bayesian logistic regression. In
Advances in Neural Information Processing Systems, May 2016.
[20] T. Jaakkola and M. I. Jordan. A variational approach to Bayesian logistic regression models and their
extensions. In Sixth International Workshop on Artificial Intelligence and Statistics, volume 82, 1997.

10

[21] A. Korattikara, Y. Chen, and M. Welling. Austerity in MCMC land: Cutting the Metropolis-Hastings
budget. In International Conference on Machine Learning, 2014.
[22] A. Kucukelbir, R. Ranganath, A. Gelman, and D. M. Blei. Automatic variational inference in Stan. In
Advances in Neural Information Processing Systems, June 2015.
[23] P. Li, T. J. Hastie, and K. W. Church. Very sparse random projections. In SIGKDD Conference on
Knowledge Discovery and Data Mining, 2006.
[24] M. Lucic, M. Faulkner, A. Krause, and D. Feldman. Training mixture models at scale via coresets.
arXiv.org, Mar. 2017.
[25] D. Maclaurin and R. P. Adams. Firefly Monte Carlo: Exact MCMC with subsets of data. In Uncertainty
in Artificial Intelligence, Mar. 2014.
[26] J. C. Mason and D. C. Handscomb. Chebyshev Polynomials. Chapman and Hall/CRC, New York, 2003.
[27] T. P. Minka. Expectation propagation for approximate Bayesian inference. In Uncertainty in Artificial
Intelligence. Morgan Kaufmann Publishers Inc, Aug. 2001.
[28] R. Nishihara, P. Moritz, S. Wang, A. Tumanov, W. Paul, J. Schleier-Smith, R. Liaw, M. Niknami, M. I.
Jordan, and I. Stoica. Real-time machine learning: The missing pieces. In Workshop on Hot Topics in
Operating Systems, 2017.
[29] A. Pakman, D. Gilboa, D. Carlson, and L. Paninski. Stochastic bouncy particle sampler. In International
Conference on Machine Learning, Sept. 2017.
[30] N. S. Pillai and A. Smith. Ergodicity of approximate MCMC chains with applications to large data sets.
arXiv.org, May 2014.
[31] M. Pollock, P. Fearnhead, A. M. Johansen, and G. O. Roberts. The scalable Langevin exact algorithm:
Bayesian inference for big data. arXiv.org, Sept. 2016.
[32] M. Rabinovich, E. Angelino, and M. I. Jordan. Variational consensus Monte Carlo. arXiv.org, June 2015.
[33] A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In Advances in Neural Information Processing Systems, pages 1313?1320, 2009.
[34] S. L. Scott, A. W. Blocker, F. V. Bonassi, H. A. Chipman, E. I. George, and R. E. McCulloch. Bayes and
big data: The consensus Monte Carlo algorithm. In Bayes 250, 2013.
[35] S. Srivastava, V. Cevher, Q. Tran-Dinh, and D. Dunson. WASP: Scalable Bayes via barycenters of subset
posteriors. In International Conference on Artificial Intelligence and Statistics, 2015.
[36] M. Stephanou, M. Varughese, and I. Macdonald. Sequential quantiles via Hermite series density estimation. Electronic Journal of Statistics, 11(1):570?607, 2017.
[37] G. Szeg?o. Orthogonal Polynomials. American Mathematical Society, 4th edition, 1975.
[38] Y. W. Teh, A. H. Thiery, and S. Vollmer. Consistency and fluctuations for stochastic gradient Langevin
dynamics. Journal of Machine Learning Research, 17(7):1?33, Mar. 2016.
[39] L. Tierney and J. B. Kadane. Accurate approximations for posterior moments and marginal densities.
Journal of the American Statistical Association, 81(393):82?86, 1986.
[40] A. W. van der Vaart. Asymptotic Statistics. University of Cambridge, 1998.
[41] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In International
Conference on Machine Learning, 2011.

11

"
3405,2010,Batch Bayesian Optimization via Simulation Matching,"Bayesian optimization methods are often used to optimize unknown functions that are costly to evaluate. Typically, these methods sequentially select inputs to be evaluated one at a time based on a posterior over the unknown function that is updated after each evaluation. There are a number of effective sequential policies for selecting the individual inputs. In many applications, however, it is desirable to perform multiple evaluations in parallel, which requires selecting batches of multiple inputs to evaluate at once. In this paper, we propose a novel approach to batch Bayesian optimization, providing a policy for selecting batches of inputs with the goal of optimizing the function as efficiently as possible. The key idea is to exploit the availability of high-quality and efficient sequential policies, by using Monte-Carlo simulation to select input batches that closely match their expected behavior. To the best of our knowledge, this is the first batch selection policy for Bayesian optimization. Our experimental results on six benchmarks show that the proposed approach significantly outperforms two baselines and can lead to large advantages over a top sequential approach in terms of performance per unit time.","Batch Bayesian Optimization
via Simulation Matching

Javad Azimi, Alan Fern, Xiaoli Z. Fern
School of EECS, Oregon State University
{azimi, afern, xfern}@eecs.oregonstate.edu

Abstract
Bayesian optimization methods are often used to optimize unknown functions that
are costly to evaluate. Typically, these methods sequentially select inputs to be
evaluated one at a time based on a posterior over the unknown function that is
updated after each evaluation. In many applications, however, it is desirable to
perform multiple evaluations in parallel, which requires selecting batches of multiple inputs to evaluate at once. In this paper, we propose a novel approach to
batch Bayesian optimization, providing a policy for selecting batches of inputs
with the goal of optimizing the function as efficiently as possible. The key idea is
to exploit the availability of high-quality and efficient sequential policies, by using
Monte-Carlo simulation to select input batches that closely match their expected
behavior. Our experimental results on six benchmarks show that the proposed approach significantly outperforms two baselines and can lead to large advantages
over a top sequential approach in terms of performance per unit time.

1

Introduction

We consider the problem of maximizing an unknown function f (x) when each evaluation of the
function has a high cost. In such cases, standard optimization techniques such as empirical gradient
methods are not practical due to the high number of function evaluations that they demand. Rather,
Bayesian optimization (BO) methods [12, 4] have demonstrated significant promise in their ability
to effectively optimize a function given only a small number of evaluations. BO gains this efficiency
by leveraging Bayesian models that take into account all previously observed evaluations in order
to better inform future evaluation choices. In particular, typical BO methods continually maintain a
posterior over f (x) that is used to select the next input to evaluate. The result of the evaluation is
then used to update the posterior and the process repeats. There are a number of well established
policies for selecting the next input to evaluate given the current posterior. We will refer to such
policies as sequential policies to stress the fact that they select one input at a time.
In many applications it is possible and desirable to run multiple function evaluations in parallel.
This is the case, for example, when the underlying function corresponds to a controlled laboratory
experiment where multiple experimental setups are examined simultaneously, or when the underlying function is the result of a costly computer simulation and multiple simulations can be run across
different processors in parallel. In such cases, existing sequential policies are not sufficient. Rather,
batch mode BO is more appropriate, where policies select a batch of multiple inputs to be evaluated
at once. To the best of our knowledge and as noted in [4], there is no established work on BO that
considers the batch selection problem, except for a brief treatment in [21]. The main contribution of
this work is to propose an approach to batch BO and to demonstrate its effectiveness.
The key motivation behind our approach comes from the fact that the sequential mode of BO has a
fundamental advantage over BO in batch mode. This is because in sequential mode, each function
evaluation is immediately used to obtain a more accurate posterior of f (x), which in turn will allow
1

a selection policy to make more informed choices about the next input. Given an effective sequential
selection policy, our goal is then to design a batch policy that approximates its behavior.
In particular, our batch policy attempts to select a batch that ?matches? the expected behavior of a
sequential policy as closely as possible. The approach generates Monte-Carlo simulations of a sequential policy given the current posterior, and then derives an optimization problem over possible
batches aimed at minimizing the loss between the sequential policy and the batch. We consider two
variants of this optimization problem that yield a continuous weighted k-means problem and a combinatorial weighted k-medoid problem. We solve the k-means variant via k-means clustering and
show that the k-medoid variant corresponds to minimizing a non-increasing supermodular function,
for which there is an efficient approximation algorithm [9].
We evaluate our approach on a collection of six functions and compare it to random and another
baseline batch policy based on submodular maximization. The results show that our approach significantly outperforms these baselines and can lead to large advantages over a top sequential approach
in terms of performance per unit time.

2

Problem Setup

Let X ? Rn be an n-dimensional input space, where we will often refer to elements of X as an
experiment and assume that each dimension i is bounded in [Ai , Bi ]. We assume an unknown realvalued function f : X ? R, which represents the expected value of the dependent variable after
running an experiment. For example, f (x) might correspond to the result of a wet-lab experiment
or a computer simulation with input parameters x. Conducting an experiment x produces a noisy
outcome y = f (x) + , where  is a noise term that might be 0 in some applications.
Our objective is to find an experiment x ? X that approximately maximizes f by requesting a
limited number of experiments and observing their outcomes. Furthermore we are interested in
applications where (1) running experiments is costly (e.g. in terms of laboratory or simulation time);
and (2) it is desirable to run k > 1 experiments in parallel. This motivates the problem of selecting
a sequence of batches, each containing k experiments, where the choice of a batch can depend on
the results observed from all previous experiments. We will refer to the rule for selecting a batch
based on previous experiments as the batch policy. The main goal of this paper is to develop a batch
policy that optimizes the unknown function as efficiently as possible.
Due to the high cost of experiments, traditional optimization techniques such as empirical gradient
ascent are not practical for our setting, due to their high demands on the number of experiments.
Rather, we build on Bayesian optimization (BO) [10, 12, 4], which leverages Bayesian modeling
in an attempt to achieve more efficient optimization. In particular, BO maintains a posterior over
the unknown function based on previously observed experiments, e.g. represented via a Gaussian
Process (GP) [19]. This posterior is used to select the next experiment to be run in a way that attempts
to trade-off exploring new parts of the experimental space and exploiting parts that look promising.
While the BO literature has provided a number of effective policies, they are all sequential policies,
where only a single experiment is selected and run at a time. Thus, the main novelty of our work is
in defining a batch policy in the context of BO, which is described in the next section.

3

Simulation Matching for Batch Selection

Given a data set D of previously observed experiments, which induces a posterior distribution over
the unknown function, we now consider how to select the next batch of k experiments. A key issue in
making this choice is to manage the trade-off between exploration and exploitation. The policy must
attempt to explore by requesting experiments from unexplored parts of the input space, at the same
time also attempt to optimize the unknown function via experiments that look promising given the
current data. While, under most measures, optimizing this trade-off is computationally intractable,
there are a number of heuristic sequential policies from the BO literature that are computationally
efficient and perform very well in practice. For example, one such policy selects the next experiment
to be the one that has the ?maximum expected improvement? according to the current posterior
[14, 10]. The main idea behind our approach is to leverage such sequential policies by selecting a
batch of k > 1 experiments that ?closely matches? the sequential policy?s expected behavior.
More formally, let ? be a sequential policy. Given a data set D of prior experimental results, ? returns
the next experiment x ? X to be selected. As is standard in BO, we assume we have a posterior
2

density P (f | D) over the unknown function f , such as a Gaussian Process. Given this density we
can define a density over the outcomes of executing policy ? for k steps, each outcome consisting
of a set of k selected experiments. Let S?k be the random variable denoting the set of k experiments
resulting from such k-step executions, which has a well defined density over all possible sets given
the posterior of f . Importantly, it is generally straightforward to use Monte Carlo simulation to
sample values of S?k .1 Our batch policy is based on generating a number of samples of S?k , which
are used to define an objective for optimizing a batch of k experiments. Below we describe this
objective and a variant, followed by a description of how we optimize the proposed objectives.
3.1 Batch Objective Function
Our goal is to select a batch B of k experiments that best ?matches the expected behavior? of a base
sequential policy ? conditioned on the observed data D. More precisely, we consider a batch B to
be a good match for a policy execution if B contains an experiment that is close to the best of the k
experiments selected by the policy. To specify this objective we first introduce some notation. Given
a function f and a set of experiments S, we define x? (f, S) = arg maxx?S f (x) to be the maximizer
of f in S. Also, for any experiment x and set B we define nn(x, B) = arg minx0 ?B k x ? x0 k to
be the nearest neighbor of x in set B. Our objective can now be written as selecting a batch B that
minimizes




OBJ(B) = ES?k Ef |S?k k x? (f, S?k ) ? nn(x? (f, S?k ), B) k2 | D | D .
Note that this nested expectation is the result of decomposing the joint posterior over S?k and f as
P (f, S?k | D) = P (f | S?k , D) ? P (S?k | D). If we assume that the unknown function f (x) is
Lipschitz continuous then minimizing this objective can be viewed as minimizing an upper bound
on the expected performance difference between the sequential policy and the selected batch. Here
the performance of a policy or a batch is equal to the output value of the best selected experiment.
We will approximate this objective by replacing the outer expectation over S?k with a sample average
over n samples {S1 , . . . , Sn } of S?k as follows, recalling that each Si is a set of k experiments:


1X
OBJ(B) ?
Ef |Si k x? (f, Si ) ? nn(x? (f, Si ), B) k2 | D
n i
1XX
=
Pr(x = x? (f, Si ) | D, Si )? k x ? nn(x, B) k2
n i
x?Si
1XX
=
?i,x ? k x ? nn(x, B) k2
(1)
n i
x?Si

The second step follows by noting that x? (f, Si ) must be one of the k experiments in Si .
We now define our objective as minimizing (1) over batch B. The objective corresponds to a
weighted k-means clustering problem, where we must select B to minimize the weighted distortion between the simulated points and their closest points in B. The weight on each simulated
experiment ?i,x corresponds to the probability that the experiment x ? Si achieves the maximum
value of the unknown f among the experiments in Si , conditioned on D and the fact that S?k = Si .
We refer to this objective as the k-means objective.
We also consider a variant of this objective where the goal is to find a B that minimizes
(1) under
S
the constraint that B is restricted to experiments in the simulations, i.e. B ? i Si s.t. |B| = k.
This objective corresponds to the weighted k-medoid clustering problem, which is often considered
to improve robustness to outliers in clustering. Accordingly we will refer to this objective as the
k-medoid objective and note that given a fixed set of simulations this corresponds to a discrete
optimization problem.
3.2 Optimization Approach
The above k-means and k-medoid objectives involve the weights ?i,x = P (x = x?i (f ) | D, S?k =
Si ), for each x ? Si . In general these weights will be difficult to compute exactly, particularly
1
For example, this can be done by starting with D and selecting the first experiment x1 using ? and then
using P (f | D) to simulate the result y1 of experiment x1 . This simulated experiment is added to D and the
process repeats for k ? 1 additional experiments.

3

Algorithm 1 Greedy Weighted k-Medoid Algorithm
Input:S = {(x1 , w1 ), . . . , (xm , wm )}, k
Output:B
B ? {x1 , . . . , xm } // initialize batch to all data points
while |B| > k do P
m
x ? arg minx?B j=1 wj ? k xj ? nn(xj , B \ x) k // point that influences objective the least
B ?B\x
end while
return B

due to the conditioning on the set Si . In this work, we approximate those weights by dropping the
conditioning on Si , for which it is then possible to derive a closed form when the posterior over f is
represented as a Gaussian Process (GP). We have found that this approach leads to good empirical
performance. In particular, instead of using the weights ?i,x we use the weights ?
? i,x = P (x =
x?i (f ) | D). When the posterior over f is represented as a GP, as in our experiments, the joint
distribution over experimental outcomes in Si = {xi,1 , . . . , xi,k } is normally distributed. That is,
the random vector hf (xi,1 ), . . . , f (xi,k )i ? N (?, ?), where the mean ? and covariance ? have
standard closed forms given by the GP conditioned on D. From this, it is clear that for a GP the
computation of ?
? i,x is equivalent to computing the probability that the ith component of a normally
distributed vector is larger than the other components. A closed form solution for this probability is
given by the following proposition.

Proposition 1. If (y1 , y2 , . . . , yk ) ? N ?y , ?y then for any i ? {1, . . . , k},
P (yi ? y1 , yi ? y2 , . . . , yi ? yk ) =

k?1
Y

(1 ? ?(??j ))

(2)

j=1

? 1
where ?(.) is standard normal cdf, ? = (?1 , ?2 , ? ? ?, ?k?1 ) = A?y A0 2 A?y , such that A ?
R(k?1)?k is a sparse matrix that for any j = 1, 2, ? ? ?, k ? 1 we have Aj,i = 1, and for any 1 ? p < i
we have Ap,p = ?1 , and for any i < p ? k we have Ap?1,p = ?1.
Using this approach to compute the weights we can now consider optimizing the k-means and kmedoid objectives from (1), both of which are known to be NP-hard problems. For the k-means
objective we solveSforSthe set B by simply applying the k-means clustering algorithm [13] to the
weighted data set i x?Si {(x, ?
? i,x )}. The k cluster centers are returned as our batch B.
The k-medoid objective is well known [22] and the weighted k-medoid clustering algorithm [11]
has been shown to perform well and be robust to outliers in the data. While we have experimented
with this algorithm and obtained good results, we have achieved results that are as good or better
using an alternative greedy algorithm that provides certain approximation guarantees. Pseudo-code
for this algorithm is shown in Figure 1. The input to the algorithm is the set of weighted experiments
and the batch size k. The algorithm initializes the batch B to include all of the input experiments,
which achieves the minimum objective value of zero. The algorithm then iteratively removes one
experiment from B at a time until |B| = k, each time removing the element whose removal results
in the smallest increase in the k-medoid objective.
This greedy algorithm is motivated by theoretical results on the minimization of non-increasing,
supermodular set functions.
Definition 1. Suppose S is a finite set, f : 2S ? R+ is a supermodular set function if for all
B1 ? B2 ? S and {x} ? S \ B2 , it holds that f (B1 ) ? f (B1 ? {x}) ? f (B2 ) ? f (B2 ? {x}).
Thus, a set function is supermodular if adding an element to a smaller set provides no less improvement than adding the element to a larger set. Also, a set function is non-increasing if for any set
S and element x if f (S) ? f (S ? {x}). It can be shown that our k-medoid objective function of
(1) is bothSa non-increasing and supermodular function of B and achieves a minimum value of zero
for B = i Si . It follows that we can obtain an approximation guarantee for the described greedy
algorithm in [9].
4

Theorem 1. [9] Let f be a monotonic non-increasing supermodular function over subsets of the
finite set S, |S| = m and f (S) = 0. Let B be the set of the elements returned by the greedy
algorithm 1 s.t |B| = k, q = m ? k and B ? = argminB 0 ?S,|B 0 |=k f (B 0 ), then


q
1
q+t
et ? 1
f (B) ?
? 1 f (B ? ) ?
f (B ? )
(3)
t
q
t
where t is the steepness parameter [9] of function f .
Notice that the approximation bound involves the steepness parameter t of f , which characterizes
the rate of decrease of f . This is unavoidable since it is known that achieving a constant factor
approximation guarantee is not possible unless P=NP [17]. Further this bound has been shown to be
tight for any t [9]. Note that this is in contrast to guarantees for greedy maximization of submodular
functions [7] for which there are constant factor guarantees. Also note that the greedy algorithm
we use is qualitatively different from the one used for submodular maximization, since it greedily
removes elements from B rather than greedily adding elements to B.

4

Implementation Details and Baselines

GP Posterior. Our batch selection approach described above requires that we maintain a posterior
over the unknown function f . For this purpose we use a zero-mean GP prior with a zero-mean
Gaussian noise model with variance equal to 0.01. The GP covariance is specified by a Gaussian
1
2
kernel K(x, x0 ) = ? exp ? 2w
where ymax is the
k x ? x0 k2 , with signal variance ? = ymax
maximum value of the unknown function. In all of our experiments we used a simple rule of thumb
Pd
to set the kernel width w to 0.01 i=1 li where li is the input space length in dimension i. We have
found this rule to work well for a variety of problems. An alternative would be to use a validationbased approach for selecting the kernel parameters. In the BO setting, however, we have found this
to be unreliable since the number of data points is relatively small.
Base Sequential Policy. Our batch selection approach also requires a base sequential policy ? to be
used for simulation matching. This policy must be able to select the next experiment given any set
of prior experimental observations D. In our experiments, we use a policy based on the Maximum
Expected Improvement (MEI) heuristic [14, 10] which is a very successful sequential policy for BO
and has been shown to converge in the limit to the global optimum. Given data D the MEI policy
simply selects the next experiment to be the one that maximizes the expected improvement over the
current set of experiments with respect to maximizing the unknown function. More formally, let y ?
be the value of the best/largest experimental outcome observed so far in D. The MEI value of an
experiment x is given by MEI(x) = Ef [max{f (x) ? y ? , 0} | D]. For our GP posterior over f we
?
??(x)
can derive a closed form for this given by: u = y ?(x)
where y ? is our best currently observed
value. For any given example x, the MEI can be computed as follows:
MEI(x)

=

?(x) [?u?(?u) + ?(u)] ,

u=

y ? ? ?(x)
?(x)

where ? and ? are the standard normal cumulative distribution and density functions and ?(x) and
?(x) are the mean and variance of f (x) according to the GP given D, which have simple closed
forms. Note that we have also evaluated our simulation-matching approach with an alternative
sequential policy known as Maximum Probability of Improvement [16, 10]. The results (not shown
in this paper) are similar to those obtained from MEI, showing that our general approach works well
for different base policies.
The computation of the MEI policy requires maximizing MEI(x) over the input space X . In general, this function does not have a unique local maximum and various strategies have been tried for
maximizing it. In our experiments, we (approximately) maximize the MEI function using the DIRECT black-box optimization procedure, which has shown good optimization performance as well
as computational efficiency in practice.
Baseline Batch Policies. To the best of our knowledge there is no well-known batch policy for
Bayesian optimization. However, in our experiments we will compare against two baselines. The
first baseline is random selection, where a batch of k random experiments is returned at each step. Interestingly, in the case of batch active learning for classification, the random batch selection strategy
5

Function
Cosines
Rosenbrock
Michalewicz

Table 1: Benchmark Functions.
Mathematical representation
1 ? (u2 + v 2 ? 0.3cos(3?u) ? 0.3cos(3?v))
u = 1.6x ? 0.5, v = 1.6y ? 0.5
10 ? 100(y ? x2 )2 ? (1 ? x)2
  2 20
P5
i.x
? i=1 sin(xi ). sin ? i

has been surprisingly effective and is often difficult to outperform with more sophisticated strategies
[8]. However, as our experiments will show, our approach will dominate random.
Our second, more sophisticate, baseline is based on selecting a batch of experiments whose expected
maximum output is the largest. More formally, we consider selecting a size k batch B that maximizes the objective Ef [maxx?B f (x) | D], which we will refer to as the EMAX objective. For
our GP prior, each set B = {x1 , . . . , xk } can be viewed as defining a normally distributed vector hf (x1 ), . . . , f (xk )i ? N (?, ?). Even in this case, finding the optimal set B is known to be
NP-hard. However, for the case where f is assumed to be non-negative, the EMAX objective is
a non-negative, submodular, non-decreasing function of B. Together these properties imply that a
simple greedy algorithm can achieve an approximation ratio of 1 ? e?1 [7]. The algorithm starts
with an empty B and greedily adds experiments to B, each time selecting the one that improves the
EMAX objective the most. Unfortunately, in general there is no closed form solution for evaluating
the EMAX objective, even in our case of normally distributed vectors [20]. Therefore, to implement the greedy algorithm, which requires many evaluations of the EMAX objective, we use Monte
Carlo sampling, where for a given set B we sample the corresponding normally distributed vector
and average the maximum values across the samples.

5

Experimental Results

In this section we evaluate our proposed batch BO approach and the baseline approaches on six
different benchmarks.
5.1 Benchmark Functions
We consider three well-known synthetic benchmark functions: Cosines and Rosenbrock [1, 5],
which are over [0, 1]2 , and Michalewicz [15], which is over [0, ?]5 . Table 1 gives the formulas
for each of these functions. Two additional benchmark functions Hydrogen and FuelCell, which
range over [0, 1]2 , are derived from real-world experimental data sets. In both cases, the benchmark function was created by fitting regression models to data sets resulting from real experiments.
The Hydrogen data set is the result of data collected as part of a study on biosolar hydrogen production [6], where the goal was to maximize the hydrogen production of a particular bacteria by
optimizing the PH and Nitrogen levels of the growth medium. The FuelCell data set was collected
as part of a study investigating the influence of anodes? nano-structure on the power output of microbial fuel cells [3]. The experimental inputs include the average area and average circularity of
the nano-particles [18]. Contour plots of the four 2-d functions are shown in Figure 1.
The last benchmark function is derived from the Cart-Pole [2] problem, which is a commonly used
reinforcement learning problem. The goal is to optimize the parameters of a controller for a wheeled
cart with the objective of balancing a pole. The controller is parameterized by four parameters
giving a 4-d space of experiments in [1, ?1]4 . Given a setting for these parameters, the benchmark
function is implemented by using the standard Cart-Pole simulator to return the reward received for
the controller.
5.2 Results
Figures 2 and 3 show the performance of our methods on all six benchmark functions for batch sizes
5 and 10 respectively. Each graph contains 5 curves, each corresponding to a different BO approach
(see below). Each curve is the result of taking an average of 100 independent runs. The x-axis of
each graph represents the total number of experiments and the y-axis represents the regret values,
where the regret of a policy at a particular point is the difference between the best possible output
value (or an upper bound if the value is not known) and the best value found by the policy. Hence the
regret is always positive and smaller values are preferred. Each run of a policy initializes the data set
to contain 5 randomly selected experiments for the 2-d functions and 20 random initial experiments
for the higher dimensional functions.
6

1

1

1

0.9

0.9

0.9

0.8

0.8

0.8

0.7

0.7

0.7

0.6

0.6

0.6

0.5

0.5

0.5

0.4

0.4

0.4

0.3

0.3

0.3

0.2

0.2

0.2

0.1

0.1

0

0

0.1

0.2

0.3

Fuel Cell

0.4

0.5

0.6

0.7

0.8

0.9

0

1

0.1

0

0.1

Hydrogen

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

0

1

0

0.1

Cosines

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Rosenbrock

Figure 1: The contour plots for the four 2?dimension proposed test functions.

0.65

0.3

Sequential
k?medoid
k?means
EMAX
Random

0.6
0.55
0.5

Sequential
k?medoid
k?means
EMAX
Random

0.6

0.5

0.45
0.4
0.35

Regret

0.2

Regret

Regret

Sequential
k?medoid
k?means
EMAX
Random

0.25

0.15

0.4

0.3

0.1

0.3

0.2

0.05

0.25
0.2
10

15

20
25
# of Experimets

30

0
10

35

15

20
25
# of Experimets

Fuel Cell

20
25
# of Experimets

0.35

Cosines

700

Regret

0.15

300

0.05
200
25

35

2.6

500

0.1

30

2.7

600

400

20
25
# of Experimets

Sequential
k?medoid
k?means
EMAX
Random

2.8

0.2

35

3

800

0.3
0.25

30

2.9

Regret

0.4

15

15

Hydrogen
Sequential
k?medoid
k?means
EMAX
Random

0.45

Regret

0.1
10

35

900

0.5

0
10

30

2.5
2.4
2.3

Sequential
k?medoid
k?means
EMAX
Random

50

2.2
2.1

75

Rosenbrock

100
125
# of Experimets

150

175

2
25

200

30

35

Cart-Pole

40

45
50
55
60
# of Experimets

65

70

75

80

Michalewicz

Figure 2: Performance evaluation with batch size 5.
0.25

0.65
Sequential
k?medoid
k?means
EMAX
Random

0.6
0.55

0.6

Sequential
k?medoid
k?means
EMAX
Random

0.2

0.5
0.45

0.4

0.15

Regret

Regret

Regret

0.5
0.45

0.25

0.3

0.05

0.2

0.25

0.15
20

25
# of Experimets

30

0
15

35

20

Fuel Cell

30

0.1
15

35

20

Hydrogen
Sequential
k?medoid
k?means
EMAX
Random

0.3
0.25

25
# of Experimets

30

35

Cosines
3
Sequential
k?medoid
k?means
EMAX
Random

2.9

800

2.8
700

2.7
600

0.15

500

0.1

400

0.05

300

20

25
# of Experimets

Rosenbrock

30

35

200
30

Regret

0.2

Regret

Regret

25
# of Experimets

900

0.35

0
15

0.4
0.35
0.3

0.1

0.35

0.2
15

Sequential
k?medoid
k?means
EMAX
Random

0.55

2.6
2.5
2.4

Sequential
k?medoid
k?means
EMAX
Random

60

2.3
2.2
90
120
# of Experimets

150

180

200

2.1
30

40

Cart-Pole
Figure 3: Performance evaluation with batch size 10.
7

50
60
# of Experimets

Michalewicz

70

80

Each graph gives curves for four batch approaches including our baselines Random and EMAX,
along with our proposed approaches based on the k-means and k-medoid objectives, which are
optimized by weighted k-means clustering and the greedy Algorithm 1 respectively. In addition, for
reference we plot the performance of the base Sequential MEI BO policy (k = 1) on each graph.
Note that since the batch approaches request either 5 or 10 experiments at a time, their curves only
contain data points at those intervals. For example, for the batch size 5 results the first point on a
batch curve corresponds to 10 experiments, including the initial 5 experiments and the first requested
batch. The next point on the batch curve is for 15 experiments which includes the next requested
batch and so on. Rather the Sequential policy has a point at every step since it requests experiments
one at a time. It is important to realize that we generally expect a good sequential policy to do better,
or no worse, than a batch policy with respect to performance per number of experiments. Thus, the
Sequential curve can be typically viewed as an upper performance bound and provides an indication
of how much loss is incurred when moving to a batch setting in terms of efficiency per experiment.
Comparison to Baselines. The major observation from our results is that for all benchmarks and
for both batch sizes the proposed k-means and k-medoid approaches significantly outperform the
baselines. This provides strong validation for our proposed simulation-matching approach to batch
selection.
k-means vs. k-medoid. In most cases, the k-means and k-medoid approaches perform similarly.
However, for both batch sizes k-medoid often does shows a small improvement over k-means and
appears to have a significant advantage in FuelCell. The only exception is in Hydrogen where kmeans shows a small advantage over k-medoid for small numbers of experiments. Overall, both
approaches appear to be effective and in these domains k-medoid has a slight edge.
Batch vs. Sequential. The advantage of Sequential over our batch approaches varies with the benchmark. However, in most cases, our proposed batch approaches catch up to Sequential in a relatively
small number of experiments and in some cases, the batch policies are similar to Sequential from
the start. The main exception is Cart-Pole for batch size 10, where the batch policies appear to be
significantly less efficient in terms of performance versus number of experiments. Generally, we see
that the difference between our batch policies and Sequential is larger for batch size 10 than batch
size 5, which is expected, since larger batch sizes imply that less information per experiment is used
in making decisions.
It is clear, however, that if we evaluate the performance of our batch policies in terms of experimental time, then there is a very significant advantage over Sequential. In particular, the amount of
experimental time for a policy is approximately equal to the number of requested batches, assuming
that the batch size is selected to allow for all selected experiments to be run in parallel. This means,
for example, that for the batch size 5 results, 5 time steps for the batch approaches correspond to
30 total experiments (5 initial + 5 batches). We can compare this point to the first point on the
Sequential curve, which also corresponds to 5 time steps (5 experiments beyond the initial 5). In all
cases, the batch policies yield a very large improvement in regret reduction per unit time, which is
the primary motivation for batch selection.

6

Summary and Future Work

In this paper we introduced a novel approach to batch BO based on the idea of simulation matching.
The key idea of our approach is to design batches of experiments that approximately match the
expected performance of high-quality sequential policies for BO. We considered two variants of
the matching problem and showed that both approaches significantly outperformed two baselines
including random batch selection on six benchmark functions. For future work we plan to consider
the general idea of simulation matching for other problems, such as active learning, where there are
also good sequential policies and batch selection is often warranted. In addition, we plan to consider
less myopic approaches for selecting each batch and the problem of batch size selection, where there
is a choice about batch size that must take into account the current data and experimental budget.

Acknowledgments
The authors acknowledge the support of the NSF under grants IIS-0905678.
8

References
[1] B. S. Anderson, A. W. More, and D. Cohn. A nonparametric approach to noisy and costly optimization.
In ICML, 2000.
[2] A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements that can solve difficult
learning control problems. 13:835?846, 1983.
[3] D. Bond and D. Lovley. Electricity production by geobacter sulfurreducens attached to electrodes. Applications of Environmental Microbiology, 69:1548?1555, 2003.
[4] E. Brochu, M. Cora, and N. de Freitas. A tutorial on Bayesian optimization of expensive cost functions,
with application to active user modeling and hierarchical reinforcement learning. Technical Report TR2009-23, Department of Computer Science, University of British Columbia, 2009.
[5] M. Brunato, R. Battiti, and S. Pasupuleti. A memory-based rash optimizer. In AAAI-06 Workshop on
Heuristic Search, Memory Based Heuristics and Their applications, 2006.
[6] E. H. Burrows, W.-K. Wong, X. Fern, F. W. Chaplen, and R. L. Ely. Optimization of ph and nitrogen
for enhanced hydrogen production by synechocystis sp. pcc 6803 via statistical and machine learning
methods. Biotechnology Progress, 25:1009?1017, 2009.
[7] M. F. G Nemhauser, L Wolsey. An analysis of the approximations for maximizing submodular set functions. Mathematical Programmingn, 14:265?294, 1978.
[8] Y. Guo and D. Schuurmans. Discriminative batch mode active learning. Proceedings of Advances in
Neural Information Processing Systems (NIPS2007), 6, 2007.
[9] V. P. Il?ev. An approximation guarantee of the greedy descent algorithm for minimizing a supermodular
set function. Discrete Applied Mathematics, 114(1-3):131?146, 2001.
[10] D. Jones. A taxonomy of global optimization methods based on response surfaces. Journal of Global
Optimization, 21:345?383, 2001.
[11] L. Kaufman and P. J. Rousseeuw. Clustering by means of medoids. Statistical data analysis based on L1
norm, pages 405?416, 1987.
[12] D. Lizotte. Practical Bayesian optimization. PhD thesis, University of Alberta, 2008.
[13] S. Lloyd. Least squares quantization in PCM. IEEE Transactions on Information Theory, 28(2):129?137,
1982.
[14] M. Locatelli. Bayesian algorithms for one-dimensional globaloptimization. J. of Global Optimization,
10(1):57?76, 1997.
[15] Z. Michalewicz. Genetic algorithms + data structures = evolution programs (2nd, extended ed.).
Springer-Verlag New York, Inc., New York, NY, USA, 1994.
[16] A. Moore and J. Schneider. Memory-based stochastic optimization. In NIPS, 1995.
[17] G. Nemhauser and L. Wolsey. Integer and combinatorial optimization. Wiley New York, 1999.
[18] D. Park and J. Zeikus. Improved fuel cell and electrode designs for producing electricity from microbial
degradation. Biotechnol.Bioeng., 81(3):348?355, 2003.
[19] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT, 2006.
[20] A. M. Ross. Computing Bounds on the Expected Maximum of Correlated Normal Variables . Methodology and Computing in Applied Probability, 2008.
[21] M. Schonlau. Computer Experiments and Global Optimization. PhD thesis, University of Waterloo, 1997.
[22] H. D. Vinod. Integer programming and the theory of grouping. Journal of the American Statistical
Association, 64(326):506?519, 1969.

9

"
3512,2010,A Reduction from Apprenticeship Learning to Classification,"We provide new theoretical results for apprenticeship learning, a variant of reinforcement learning in which the true reward function is unknown, and the goal is to perform well relative to an observed expert. We study a common approach to learning from expert demonstrations: using a classification algorithm to learn to imitate the expert's behavior. Although this straightforward learning strategy is widely-used in practice, it has been subject to very little formal analysis. We prove that, if the learned classifier has error rate $\eps$, the difference between the value of the apprentice's policy and the expert's policy is $O(\sqrt{\eps})$. Further, we prove that this difference is only $O(\eps)$ when the expert's policy is close to optimal. This latter result has an important practical consequence: Not only does imitating a near-optimal expert result in a better policy, but far fewer demonstrations are required to successfully imitate such an expert. This suggests an opportunity for substantial savings whenever the expert is known to be good, but demonstrations are expensive or difficult to obtain.","A Reduction from Apprenticeship Learning to
Classification
Umar Syed?
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
usyed@cis.upenn.edu

Robert E. Schapire
Department of Computer Science
Princeton University
Princeton, NJ 08540
schapire@cs.princeton.edu

Abstract
We provide new theoretical results for apprenticeship learning, a variant of reinforcement learning in which the true reward function is unknown, and the goal
is to perform well relative to an observed expert. We study a common approach
to learning from expert demonstrations: using a classification algorithm to learn
to imitate the expert?s behavior. Although this straightforward learning strategy
is widely-used in practice, it has been subject to very little formal analysis. We
prove that, if the learned classifier has error rate ?, the difference
? between the
value of the apprentice?s policy and the expert?s policy is O( ?). Further, we
prove that this difference is only O(?) when the expert?s policy is close to optimal.
This latter result has an important practical consequence: Not only does imitating
a near-optimal expert result in a better policy, but far fewer demonstrations are
required to successfully imitate such an expert. This suggests an opportunity for
substantial savings whenever the expert is known to be good, but demonstrations
are expensive or difficult to obtain.

1

Introduction

Apprenticeship learning is a variant of reinforcement learning, first introduced by Abbeel & Ng [1]
(see also [2, 3, 4, 5, 6]), designed to address the difficulty of correctly specifying the reward function
in many reinforcement learning problems. The basic idea underlying apprenticeship learning is that
a learning agent, called the apprentice, is able to observe another agent, called the expert, behaving
in a Markov Decision Process (MDP). The goal of the apprentice is to learn a policy that is at least
as good as the expert?s policy, relative to an unknown reward function. This is a weaker requirement
than the usual goal in reinforcement learning, which is to find a policy that maximizes reward.
The development of the apprenticeship learning framework was prompted by the observation that,
although reward functions are often difficult to specify, demonstrations of good behavior by an
expert are usually available. Therefore, by observing such a expert, one can infer information about
the true reward function without needing to specify it.
Existing apprenticeship learning algorithms have a number of limitations. For one, they typically
assume that the true reward function can be expressed as a linear combination of a set of known features. However, there may be cases where the apprentice is unwilling or unable to assume that the
rewards have this structure. Additionally, most formulations of apprenticeship learning are actually
harder than reinforcement learning; apprenticeship learning algorithms typically invoke reinforcement learning algorithms as subroutines, and their performance guarantees depend strongly on the
quality of these subroutines. Consequently, these apprenticeship learning algorithms suffer from the
same challenges of large state spaces, exploration vs. exploitation trade-offs, etc., as reinforcement
?

Work done while the author was a student at Princeton University.

1

learning algorithms. This fact is somewhat contrary to the intuition that demonstrations from an
expert ? especially a good expert ? should make the problem easier, not harder.
Another approach to using expert demonstrations that has received attention primarily in the empirical literature is to passively imitate the expert using a classification algorithm (see [7, Section 4] for
a comprehensive survey). Classification is the most well-studied machine learning problem, and it
is sensible to leverage our knowledge about this ?easier? problem in order to solve a more ?difficult?
one. However, there has been little formal analysis of this straightforward learning strategy (the
main recent example is Ross & Bagnell [8], discussed below). In this paper, we consider a setting
in which an apprentice uses a classification algorithm to passively imitate an observed expert in an
MDP, and we bound the difference between the value of the apprentice?s policy and the value of
the expert?s policy in terms of the accuracy of the learned classifier. Put differently, we show that
apprenticeship learning can be reduced to classification. The idea of reducing one learning problem
to another was first proposed by Zadrozny & Langford [9].
Our main contributions in this paper are a pair of theoretical results. First, we?show that the difference between the value of the apprentice?s policy and the expert?s policy is O( ?),1 where ? ? (0, 1]
is the error of the learned classifier. Secondly, and perhaps more interestingly, we extend our first
result to prove that the difference in policy values is only O(?) when the expert?s policy is close to
optimal. Of course, if one could perfectly imitate the expert, then naturally a near-optimal expert
policy is preferred. But our result implies something further: that near-optimal experts are actually
easier to imitate, in the sense that fewer demonstration are required to achieve the same performance
guarantee. This has important practical consequences. If one is certain a priori that the expert is
demonstrating good behavior, then our result implies that many fewer demonstrations need to be collected than if this were not the case. This can yield substantial savings when expert demonstrations
are expensive or difficult to obtain.

2

Related Work

Several authors have reduced reinforcement learning to simpler problems. Bagnell et al [10] described an algorithm for constructing a good nonstationary policy from a sequence of good ?onestep? policies. These policies are only concerned with maximizing reward collected in a single
time step, and are learned with the help of observations from an expert. Langford & Zadrozny
[11] reduced reinforcement learning to a sequence of classification problems (see also Blatt & Hero
[12]), but these problems have an unusual structure, and the authors are only able to provide a small
amount of guidance as to how data for these problems can be collected. Kakade & Langford [13]
reduced reinforcement learning to regression, but required additional assumptions about how easily
a learning algorithm can access the entire state space. Importantly, all this work makes the standard
reinforcement learning assumptions that the true rewards are known, and that a learning algorithm
is able to interact directly with the environment. In this paper we are interested in settings where
the reward function is not known, and where the learning algorithm is limited to passively observing
an expert. Concurrently to this work, Ross & Bagnell [8] have described an approach to reducing
imitation learning to classification, and some of their analysis resembles ours. However, their framework requires somewhat more than passive observation of the expert, and is focused on improving
the sensitivity of the reduction to the horizon length, not the classification error. They also assume
that the expert follows a deterministic policy, and assumption we do not make.

3

Preliminaries

We consider a finite-horizon MDP, with horizon H. We will allow the state space S to be infinite,
but assume that the action space A is finite. Let ? be the initial state distribution, and ? the transition
function, where ?(s, a, ?) specifies the next-state distribution from state s ? S under action a ? A.
The only assumption we make about the unknown reward function R is that 0 ? R(s) ? Rmax for
all states s ? S, where Rmax is a finite upper bound on the reward of any state.
1
The big-O notation is concealing a polynomial dependence on other problem parameters. We give exact
bounds in the body of the paper.

2

We introduce some notation and definitions regarding policies. A policy ? is stationary if it is a
mapping from states to distributions over actions. In this case, ?(s, a) denotes the probability of
taking action a in state s. Let ? be the set of all stationary policies. A policy ? is nonstationary if it
belongs to the set ?H = ? ? ? ? ? (H times) ? ? ? ? ? . In this case, ?t (s, a) denotes the probability of
taking action a in state s at time t. Also, if ? is nonstationary, then ?t refers to the stationary policy
that is equal to the tth component of ?. A (stationary or nonstationary) policy ? is deterministic if
each one of its action distributions is concentrated on a single action. If a deterministic policy ? is
stationary, then ?(s) is the action taken in state s, and if ? is nonstationary, the ?t (s) is the action
taken in state s at time t.
We define the value function Vt? (s) for a nonstationary policy ? at time t as follows in the usual
manner:
""H
#

X

?
Vt (s) , E
R(st? )  st = s, at? ? ?t? (st? , ?), st? +1 ? ?(st? , at? , ?) .
t? =t

So Vt? (s) is the expected cumulative reward for following policy ? when starting at state s and time
step t. Note that there are several value functions per nonstationary policy, one for each time step t.
The value of a policy is defined to be V (?) , E[V1? (s) | s ? ?(?)], and an optimal policy ? ? is one
that satisfies ? ? , arg max? V (?).
We write ? E to denote the (possibly nonstationary) expert policy, and VtE (s) as an abbreviation for
E
Vt? (s). Our goal is to find a nonstationary apprentice policy ? A such that V (? A ) ? V (? E ). Note
that the values of these policies are with respect to the unknown reward function.

Let Dt? be the distribution on state-action pairs at time t under policy ?. In other words, a sample
(s, a) is drawn from Dt? by first drawing s1 ? ?(?), then following policy ? for time steps 1 through
t, which generates a trajectory (s1 , a1 , . . . , st , at ), and then letting (s, a) = (st , at ). We write DtE as
E
an abbreviation for Dt? . In a minor abuse of notation, we write s ? Dt? to mean: draw state-action
pair (s, a) ? Dt? , and discard a.

4

Details and Justification of the Reduction

Our goal is to reduce apprenticeship learning to classification, so let us describe exactly how this
reduction is defined, and also justify the utility of such a reduction.
In a classification problem, a learning algorithm is given a training set h(x1 , y1 ), . . . , (xm , ym )i,
where each labeled example (xi , yi ) ? X ? Y is drawn independently from a distribution D on X ?
Y. Here X is the example space and Y is the finite set of labels. The learning algorithm is also given
the definition of a hypothesis class H, which is a set of functions mapping X to Y. The objective
of the learning algorithm is to find a hypothesis h ? H such that the error Pr(x,y)?D (h(x) 6= y) is
small.
For our purposes, the hypothesis class H is said to be PAC-learnable if there exists a learning
algorithm A such that, whenever A is given a training set of size m = poly( 1? , 1? ), the algorithm
? ? H such that, with probability at least 1 ? ?,
runs for poly( 1? , 1? ) 
steps and outputs
a hypothesis h

?
6= y ? ??
+ ?. Here ??
= inf h?H Pr(x,y)?D (h(x) 6= y) is the
we have Pr(x,y)?D h(x)
H,D

H,D
poly( 1? , 1? )

error of the best hypothesis in H. The expression
will typically also depend on other
quantities, such as the number of labels |Y| and the VC-dimension of H [14], but this dependence is
not germane to our discussion.

The existence of PAC-learnable hypothesis classes is the reason that reducing apprenticeship learning to classification is a sensible endeavor. Suppose that the apprentice observes m independent

trajectories from the expert?s policy ? E , where the ith trajectory is a sequence si1 , ai1 , . . . , siH , aiH .
The key is to note that each (sit , ait ) can be viewed as an independent sample from the distribution
DtE . Now consider a PAC-learnable hypothesis class H, where H contains a set of functions map1 1
, ? ), then for each time step
ping the state space S to the finite action space A. If m = poly( H?
?
t, the apprentice can use a PAC learning algorithm for
 H to learn
 a hypothesis ht ? H such that,
1
? t (s) 6= a ? ?? E + ?. And by the union
, we have Pr(s,a)?DtE h
with probability at least 1 ? H?
H,D
t

3

bound, this inequality holds for all t with probability at least 1 ? ?. If each ??H,DE + ? is small, then a
t
? t for all t. This policy uses the learned
natural choice for the apprentice?s policy ? A is to set ?tA = h
classifiers to imitate the behavior of the expert.
In light of the preceding discussion, throughout the remainder of this paper we make the following
assumption about the apprentice?s policy.
Assumption 1. The apprentice policy ? A is a deterministic policy that satisfies
Pr(s,a)?DtE (?tA (s) 6= a) ? ? for some ? > 0 and all time steps t.
As we have shown, an apprentice policy satisfying Assumption 1 with small ? can be found with
high probability, provided that expert?s policy is well-approximated by a PAC-learnable hypothesis
class and that the apprentice is given enough trajectories from the expert. A reasonable intuition is
that the value of the policy ? A in Assumption 1 is nearly as high as the value of the policy ? E ; the
remainder of this paper is devoted to confirming this intuition.

5

Guarantee for Any Expert

If the error rate ? in Assumption 1 is small, then the apprentice?s policy ? A closely imitates the
expert?s policy ? E , and we might hope that this implies that V (? A ) is not much less than V (? E ).
This is indeed the case, as the next theorem shows.
?
Theorem 1. If Assumption 1 holds, then V (? A ) ? V (? E ) ? 2 ?H 2 Rmax .
In a typical classification problem, it is assumed that the training and test examples are drawn from
the same distribution. The main challenge in proving Theorem 1 is that this assumption does not hold
for the classification problems to which we have reduced the apprenticeship learning problem. This
is because, although each state-action pair (sit , ait ) appearing in an expert trajectory is distributed
according to DtE , a state-action pair (st , at ) visited by the apprentice?s policy may not follow this
distribution, since the behavior of the apprentice prior to time step t may not exactly match the
expert?s behavior. So our strategy for proving Theorem 1 will be to show that these differences do
not cause the value of the apprentice policy to degrade too much relative to the value of the expert?s
policy.
Before proceeding, we will show that Assumption 1 implies a condition that is, for our purposes,
more convenient.
?t (s) 6= a) ? ?, then for
Lemma 1. Let ?
? be a deterministic nonstationary policy. If Pr(s,a)?DtE (?

?
E
?t (s)) ? 1 ? ?1 ? 1 ? ?1
all ?1 ? (0, 1] we have Prs?DtE ?t (s, ?

?t (s)) ? 1 ? ?1 <
Proof. Fix any ?1 ? (0, 1], and suppose for contradiction that Prs?DtE ?tE (s, ?
?t (s)) ? 1 ? ?1 , and that s is bad otherwise. Then
1 ? ??1 . Say that a state s is good if ?tE (s, ?
?t (s) = a | s is good)
?t (s) = a) = Prs?DtE (s is good) ? Pr(s,a)?DtE (?
Pr(s,a)?DtE (?
+ Prs?DtE (s is bad) ? Pr(s,a)?DtE (?
?t (s) = a | s is bad)

? Prs?DtE (s is good) ? 1 + (1 ? Prs?DtE (s is good)) ? (1 ? ?1 )

= 1 ? ?1 (1 ? Prs?DtE (s is good))
<1??

?t (s) = a | s is bad) ? 1 ? ?1 , and the second
where the first inequality holds because Pr(s,a)?DtE (?
?
inequality holds because Prs?DtE (s is good) < 1 ? ?1 . This chain of inequalities clearly contradicts
the assumption of the lemma.
The next two lemmas are the main tools used to prove Theorem 1. In the proofs of these lemmas, we
write sa to denote a trajectory, where sa = (?
s1 , a
?1 , . . . , s?H , a
?H ) ? (S ? A)H . Also, let dP? denote
PH
the probability measure induced on trajectories by following policy ?, and let R(sa) = t=1 R(?
st )
4

denote the sum of the rewards of the states in trajectory sa. Importantly, using these definitions we
have
Z
V (?) =

R(sa)dP? .

sa

The next lemma proves that if a deterministic policy ?almost? agrees with the expert?s policy ? E in
every state and time step, then its value is not much worse the value of ? E .
Lemma 2. Let ?
? be a deterministic nonstationary policy. If for all states s and time steps t we have
?t (s)) ? 1 ? ? then V (?
? ) ? V (? E ) ? ?H 2 Rmax .
?tE (s, ?

? ? that is, ?
? (?
st ) = a
?t for all time steps
Proof. Say a trajectory sa is good if it is ?consistent? with ?
t ? and that sa is bad otherwise. We have
Z
R(sa)dP?E
V (? E ) =
Zsa
Z
R(sa)dP?E +
R(sa)dP?E
=
sa good
sa bad
Z
R(sa)dP?E + ?H 2 Rmax
?
sa good
Z
R(sa)dP?? + ?H 2 Rmax
?
sa good

= V (?
? ) + ?H 2 Rmax

where the first inequality holds because, by the union bound, P?E assigns at most an ?H fraction
of its measure to bad trajectories, and the maximum reward of a trajectory is HRmax . The second
inequality holds because good trajectories are assigned at least as much measure by P?? as by P?E ,
because ?
? is deterministic.
The next lemma proves a slightly different statement than Lemma 2: If a policy exactly agrees with
the expert?s policy ? E in ?almost? every state and time step, then its value is not much worse the
value of ? E .
Lemma 3. Let ?
? be a nonstationary policy.
If for all time steps t we have
?t (s, ?) = ?tE (s, ?) ? 1 ? ? then V (?
? ) ? V (? E ) ? ?H 2 Rmax .
Prs?DtE ?

st , ?) = ?
?t (?
st , ?) for all time steps t, and that sa is bad
Proof. Say a trajectory sa is good if ?tE (?
otherwise. We have
Z
V (?
?) =
R(sa)dP??
sa
Z
Z
R(sa)dP?? +
R(sa)dP??
=
sa good
sa bad
Z
Z
R(sa)dP?E +
R(sa)dP??
=
sa good
sa bad
Z
Z
Z
R(sa)dP?E ?
R(sa)dP?E +
R(sa)dP??
=
sa
sa bad
sa bad
Z
? V (? E ) ? ?H 2 Rmax +
R(sa)dP??
sa bad

? V (? E ) ? ?H 2 Rmax .

The first inequality holds because, by the union bound, P?E assigns at most an ?H fraction of
its measure to bad trajectories, and the maximum reward of a trajectory is HRmax . The second
inequality holds by our assumption that all rewards are nonnegative.
We are now ready to combine the previous lemmas and prove Theorem 1.
5

Proof of Theorem 1. Since the apprentice?s policy ? A satisfies Assumption 1, by Lemma 1 we can
choose any ?1 ? (0, 1] and have

Prs?DtE ?tE (s, ?tA (s)) ? 1 ? ?1 ? 1 ? ??1 .

Now construct a ?dummy? policy ?
? as follows: For all time steps t, let ?
?t (s, ?) = ?tE (s, ?) for any
E
A
A
state s where ?t (s, ?t (s)) ? 1 ? ?1 . On all other states, let ?
?t (s, ?t (s)) = 1. By Lemma 2
V (? A ) ? V (?
? ) ? ?1 H 2 Rmax
and by Lemma 3
?
V (?
? ) ? V (? E ) ? H 2 Rmax .
?1
Combining these inequalities yields


?
A
E
V (? ) ? V (? ) ? ?1 +
H 2 Rmax .
?1
?
Since ?1 was chosen arbitrarily, we set ?1 = ?, which maximizes this lower bound.

6

Guarantee for Good Expert

Theorem 1 makes no assumptions about the value of the expert?s policy. However, in many cases it
may be reasonable to assume that the expert is following a near-optimal policy (indeed, if she is not,
then we should question the decision to select her as an expert). The next theorem shows that the
dependence of V (? A ) on the classification error ? is significantly better when the expert is following
a near-optimal policy.

Theorem 2. If Assumption 1 holds, then V (? A ) ? V (? E ) ? 4?H 3 Rmax + ??E , where ??E ,
V (? ? ) ? V (? E ) is the suboptimality of the expert?s policy ? E .
?
Note that the bound in Theorem 2 varies with ? and not with ?. We can interpret this bound as
follows: If our goal is to learn an apprentice policy whose value is within ??E of the expert policy?s
value, we can double our progress towards that goal by halving the classification error rate. On the
other hand, Theorem 2 suggests that the error rate must be reduced by a factor of four.
To see why a near-optimal expert policy should yield a weaker dependence on ?, consider an expert
policy ? E that is an optimal policy, but in every state s ? S selects one of two actions as1 and
as2 uniformly at random. A deterministic apprentice policy ? A that closely imitates the expert will
either set ? A (s) = as1 or ? A (s) = as2 , but in either case the classification error will not be less than
1
E
s
s
2 . However, since ? is optimal, both actions a1 and a2 must be optimal actions for state s, and so
A
the apprentice policy ? will be optimal as well.
Our strategy for proving Theorem 2 is to replace Lemma 2 with a different result ? namely, Lemma
6 below ? that has a much weaker dependence on the classification error ? when ??E is small.
To help us prove Lemma 6, we will first need to define several useful policies. The next several
definitions will be with respect to an arbitrary nonstationary base policy ? B ; in the proof of Theorem
2, we will make a particular choice for the base policy.
Fix a deterministic nonstationary policy ? B,? that satisfies
?tB (s, ?tB,? (s)) ? 1 ? ?
for some ? ? (0, 1] and all states s and time steps t. Such a policy always exists by letting ? = 1, but
if ? is close to zero, then ? B,? is a deterministic policy that ?almost? agrees with ? B in every state
and time step. Of course, depending on the choice of ? B , a policy ? B,? may not exist for small ?,
but let us set aside that concern for the moment; in the proof of Theorem 2, the base policy ? B will
be chosen so that ? can be as small as we like.
Having thus defined ? B,? , we define ? B\? as follows: For all states s ? S and time steps t, if
?tB (s, ? B,? (s)) < 1, then let
?
0
if ?tB,? (s) = a
?
?
?
B\?
?t (s, a) =
?tB (s, a)
?
?
otherwise
? P
B
?
a? 6=? B,? (s) ?t (s, a )
t

6

B\?

for all actions a ? A, and otherwise let ?t

(s, a) =

1
|A|

B\?
?t (s, ?)

for all a ? A. In other words, in each

state s and time step t, the distribution
is obtained by proportionally redistributing the
probability assigned to action ?tB,? (s) by the distribution ?tB (s, ?) to all other actions. The case
where ?tB (s, ?) assigns all probability to action ?tB,? (s) is treated specially, but as will be clear from
B\?
the proof of Lemma 4, it is actually immaterial how the distribution ?t (s, ?) is defined in these
cases; we choose the uniform distribution for definiteness.
Let ? B+ be a deterministic policy defined by

i
h B

?
?tB+ (s) = arg max E Vt+1
(s? )  s? ? ?(s, a, ?)
a

for all states s ? S and time steps t. In other words, ?tB+ (s) is the best action in state s at time t,
assuming that the policy ? B is followed thereafter.
The next definition requires the use of mixed policies. A mixed policy consists of a finite set of
deterministic nonstationary policies, along with a distribution over those policies; the mixed policy
is followed by drawing a single policy according to the distribution in the initial time step, and
following that policy exclusively thereafter. More formally, a mixed policy is defined by a set of
for some finite N , where each component policy ? i is a deterministic
ordered pairs {(? i , ?(i))}N
PN i=1
nonstationary policy, i=1 ?(i) = 1 and ?(i) ? 0 for all i ? [N ].

We define a mixed policy ?
? B,?,+ as follows: For each component policy ? i and each time step t,
B,?
i
i
either ?t = ?t or ?t = ?tB+ . There is one component policy for each possible choice; this yields
N = 2|H| component policies. And the probability ?(i) assigned to each component policy ? i is
?(i) = (1 ? ?)k(i) ?H?k(i) , where k(i) is the number of times steps t for which ?ti = ?tB,? .
Having established these definitions, we are now ready to prove several lemmas that will help us
prove Theorem 2.
Lemma 4. V (?
? B,?,+ ) ? V (? B ).
B,?,+

B

Proof. The proof will be by backwards induction on t. Clearly VH??
(s) = VH? (s) for all states
?
s, since the value function VH for any policy ? depends only on the reward function R. Now suppose
?B
?
? B,?,+
(s) for all states s. Then for all states s
(s) ? Vt+1
for induction that Vt+1

h
i
B,?,+
?
? B,?,+ ?  ?
Vt??
(s) = R(s) + E Vt+1
(s )  a ? ?
?tB,?,+ (s, ?), s? ? ?(s, a? , ?)

h B
i

?
? R(s) + E Vt+1
(s? )  a? ? ?
?tB,?,+ (s, ?), s? ? ?(s, a? , ?)


h B
i
h B
i


?
?
= R(s) + (1 ? ?)E Vt+1
(s? )  s? ? ?(s, ?tB,? (s), ?) + ?E Vt+1
(s? )  s? ? ?(s, ?tB+ (s), ?)

h B
i

?
? R(s) + ?tB (s, ?tB,? (s)) ? E Vt+1
(s? )  s? ? ?(s, ?tB,? (s), ?)



h B
i

?
(s? )  s? ? ?(s, ?tB+ (s), ?)
+ 1 ? ?tB (s, ?tB,? (s)) ? E Vt+1

h B
i

?
? R(s) + ?tB (s, ?tB,? (s)) ? E Vt+1
(s? )  s? ? ?(s, ?tB,? (s), ?)



h B
i

B\?
?
+ 1 ? ?tB (s, ?tB,? (s)) ? E Vt+1
(s? )  a? ? ?t (s, ?), s? ? ?(s, a? , ?)

h B
i

?
= R(s) + E Vt+1
(s? )  a? ? ?tB (s), s? ? ?(s, a? , ?)
B

= Vt? (s).

The first equality holds for all policies ?, and follows straightforwardly from the definition of Vt? .
The rest of the derivation uses, in order: the inductive hypothesis; the definition of ?
? B,?,+ ; property
B+
B,?
?B
of ?
and the fact that ?t (s) is the best action with respect to Vt+1 ; the fact that ?tB+ (s) is the
B
?B
; the definition of ? B\? ; the definition of Vt? (s).
best action with respect to Vt+1
Lemma 5. V (?
? B,?,+ ) ? (1 ? ?H)V (? B,? ) + ?HV (? ? ).
7

Proof. Since ?
? B,?,+ is a mixed policy, by the linearity of expectation we have
V (?
? B,?,+ ) =

N
X

?(i)V (? i )

i=1

i

B,?,+

where each ? is a component policy of ?
?
and ?(i) is its associated probability. Therefore
X
V (?
? B,?,+ ) =
?(i)V (? i )
i

? (1 ? ?)H V (? B,? ) + (1 ? (1 ? ?)H )V (? ? )
? (1 ? ?H)V (? B,? ) + ?HV (? ? ).

Here we used the fact that probability (1 ? ?)H ? 1 ? ?H is assigned to a component policy that is
identical to ? B,? , and the value of any component policy is at most V (? ? ).
Lemma 6. If ? <

1
H,

then V (? B,? ) ? V (? B ) ?

?H
1??H ?? B .

Proof. Combining Lemmas 4 and 5 yields
(1 ? ?H)V (? B,? ) + ?HV (? ? ) ? V (? B ).

And via algebraic manipulation we have

(1 ? ?H)V (? B,? ) + ?HV (? ? ) ? V (? B )

? (1 ? ?H)V (? B,? ) ? (1 ? ?H)V (? B ) + ?HV (? B ) ? ?HV (? ? )

? (1 ? ?H)V (? B,? ) ? (1 ? ?H)V (? B ) ? ?H??B
?H
? V (? B,? ) ? V (? B ) ?
? B.
1 ? ?H ?
In the last line, we were able to divide by (1 ? ?H) without changing the direction of the inequality
1
because of our assumption that ? < H
.
We are now ready to combine the previous lemmas and prove Theorem 2.
Proof of Theorem 2. Since the apprentice?s policy ? A satisfies Assumption 1, by Lemma 1 we can
1
choose any ?1 ? (0, H
) and have

Prs?DtE ?tE (s, ?tA (s)) ? 1 ? ?1 ? 1 ? ??1 .

As in the proof of Theorem 1, let us construct a ?dummy? policy ?
? as follows: For all time steps
t, let ?
?t (s, ?) = ?tE (s, ?) for any state s where ?tE (s, ?tA (s)) ? 1 ? ?1 . On all other states, let
?
?t (s, ?tA (s)) = 1. By Lemma 3 we have
?
(1)
V (?
? ) ? V (? E ) ? H 2 Rmax .
?1

? ) = V (? ? ) ? ??? and rearranging yields
Substituting V (? E ) = V (? ? ) ? ??E and V (?
?
??? ? ??E + H 2 Rmax .
?1

(2)

Now observe that, if we set the base policy ? B = ?
? , then by definition ? A is a valid choice for
1
B,?1
. And since ?1 < H we have
?
?1 H
???
1 ? ?1 H


?
?1 H
??E + H 2 Rmax
? V (?
?) ?
1 ? ?1 H
?1


?
?
?
1H
? V (? E ) ? H 2 Rmax ?
??E + H 2 Rmax
?1
1 ? ?1 H
?1

V (? A ) ? V (?
?) ?

where we used Lemma 6, (2) and (1), in that order. Letting ?1 =
8

1
2H

proves the theorem.

(3)

References
[1] Pieter Abbeel and Andrew Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the 21st International Conference on Machine Learning, 2004.
[2] P Abbeel and A Y Ng. Exploration and apprenticeship learning in reinforcement learning. In
Proceedings of the 22nd International Conference on Machine Learning, 2005.
[3] Nathan D. Ratliff, J. Andrew Bagnell, and Martin A. Zinkevich. Maximum margin planning.
In Proceedings of the 23rd International Conference on Machine Learning, 2006.
[4] Umar Syed and Robert E. Schapire. A game-theoretic approach to apprenticeship learning. In
Advances in Neural Information Processing Systems 20, 2008.
[5] J. Zico Kolter, Pieter Abbeel, and Andrew Ng. Hierarchical apprenticeship learning with application to quadruped locomotion. In Advances in Neural Information Processing Systems 20,
2008.
[6] Umar Syed and Robert E. Schapire. Apprenticeship learning using linear programming. In
Proceedings of the 25th International Conference on Machine Learning, 2008.
[7] Brenna D. Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot
learning from demonstration. Robotics and Autonomous Systems, 57(5):469?483, 2009.
[8] St?ephane Ross and J. Andrew Bagnell. Efficient reduction for imitation learning. In AISTATS,
2010.
[9] Bianca Zadrozny, John Langford, and Naoki Abe.
Cost-sensitive learning by costproportionate example weighting. In Proceedings of the Third IEEE International Conference
on Data Mining, 2003.
[10] J. Andrew Bagnell, Sham Kakade, Andrew Y. Ng, and Jeff Schneider. Policy search by dynamic programming. In Advances in Neural Information Processing Systems 15, 2003.
[11] John Langford and Bianca Zadrozny. Relating reinforcement learning performance to classification performance. In Proceedings of the 22nd International Conference on Machine Learning, 2005.
[12] Doron Blatt and Alfred Hero. From weighted classification to policy search. In Advances in
Neural Information Processing Systems 18, pages 139?146, 2006.
[13] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Proceedings 19th International Conference on Machine Learning, 2002.
[14] V. N. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of
events to their probabilities. Theory of Probability and Its Applications, 16:264?280, 1971.

9

"
962,2000,Sparse Greedy Gaussian Process Regression,Abstract Missing,"Sparse Greedy
Gaussian Process Regression

Alex J. Smola?
RSISE and Department of Engineering
Australian National University
Canberra, ACT, 0200

Peter Bartlett
RSISE
Australian National University
Canberra, ACT, 0200

Alex.Smola@anu.edu.au

Peter.Bartlett@anu.edu.au

Abstract
We present a simple sparse greedy technique to approximate the
maximum a posteriori estimate of Gaussian Processes with much
improved scaling behaviour in the sample size m. In particular,
computational requirements are O(n 2 m), storage is O(nm), the
cost for prediction is 0 (n) and the cost to compute confidence
bounds is O(nm), where n ?: m. We show how to compute a
stopping criterion, give bounds on the approximation error, and
show applications to large scale problems.

1

Introduction

Gaussian processes have become popular because they allow exact Bayesian analysis
with simple matrix manipulations, yet provide good performance. They share with
Support Vector machines and Regularization Networks the concept of regularization
via Reproducing Kernel Hilbert spaces [3], that is, they allow the direct specification
of the smoothness properties of the class of functions under consideration. However,
Gaussian processes are not always the method of choice for large datasets, since they
involve evaluations of the covariance function at m points (where m denotes the
sample size) in order to carry out inference at a single additional point. This may
be rather costly to implement - practitioners prefer to use only a small number of
basis functions (Le. covariance function evaluations).
Furthermore, the Maximum a Posteriori (MAP) estimate requires computation,
storage, and inversion of the full m x m covariance matrix Kii = k( Xi, Xi) where
Xl! ... ,xm are training patterns. While there exist techniques [2, 8] to reduce the
computational cost of finding an estimate to O(km 2 ) rather than O(m 3 ) when
the covariance matrix contains a significant number of small eigenvalues, all these
methods still require computation and storage of the full covariance matrix. None
of these methods addresses the problem of speeding up the prediction stage (except
for the rare case when the integral operator corresponding to the kernel can be
diagonalized analytically [8]).
We devise a sparse greedy method, similar to those proposed in the context of
wavelets [4], solutions of linear systems [5] or matrix approximation [7] that finds
?Supported by the DFG (Sm 62-1) and the Australian Research Council.

an approximation of the MAP estimate by expanding it in terms of a small subset
of kernel functions k (Xi, .). Briefly, the technique works as follows: given a set of
(already chosen) kernel functions, we seek the additional function that increases
the posterior probability most. We add it to the set of basis functions and repeat
until the maximum is approximated sufficiently well. A similar approach for a tight
upper bound on the posterior probability gives a stopping criterion.

2

Gaussian Process Regression

Consider a finite set X = {Xl.'"" xm} of inputs. In Gaussian Process Regression,
we assume that for any such set there is a covariance matrix K with elements
Kij = k( Xi, Xj). We assume that for each input X there is a corresponding output
y(x), and that these outputs are generated by
y(x) = t(x) +
(1)
are both normal random variables, with
rv N(O, ( 2 ) and
where t(x) and
t = (t(Xl), ... , t(xm))T rv N(O, K). We can use Bayes theorem to determine the
distribution of the output y(x) at a (new) input x. Conditioned on the data (X,y),
the output y(x) is normally distributed. It follows that the mean of this distribution
is the maximum a posteriori probability (MAP) estimate of y. We are interested in
estimating this mean, and also the variance.

e

e

e

It is possible to give an equivalent parametric representation of y that is more convenient for our purposes. We may assume that the vector y = (y(Xl)"""" ,y(xm))T
of outputs is generated by

e

y=Ka+e,

(2)

where a rv N(O, K- 1 ) and rv N(O, ( 2 1). Consequently the posterior probability
p(aly, X) over the latent variables a is proportional to
exp(-2;21Iy - Ka11 2) exp(-!a TKa)
(3)
and the conditional expectation of y(x) for a (new) location X is E[y(x)ly,X] =
k T aopt, where k T denotes the vector (k( Xl. x), ... , k (x m , x)) and aopt is the value
of a that maximizes (3). Thus, it suffices to compute aopt before any predictions
are required. The problem of choosing the MAP estimate of a is equivalent to the
problem of minimizing the negative log-posterior,
(4)
minimize [-y T Ka + !a T (a 2 K + KT K) a]
aEW""

(ignoring constant terms and rescaling by ( 2 ). It is easy to show that the mean of
the conditional distribution of y(x) is k T (K +( 21)-ly, and its variance is k(x, x) +
a 2 - k T (K + ( 21)-lk (see, for example, [2]).

3

Approximate Minimization of Quadratic Forms

For Gaussian process regression, searching for an approximate solution to (4) relies
on the assumption that a set of variables whose posterior probability is close to that
of the mode of the distribution will be a good approximation for the MAP estimate.
The following theorem suggests a simple approach to estimating the accuracy of an
approximate solution to (4). It uses an idea from [2] in a modified, slightly more
general form.
Theorem 1 (Approximation Bounds for Quadratic Forms) Denote by K E
lRmxm a positive semidefinite matrix, y, a E lRm and define the two quadratic forms

Q(a)

:= -y T

Ka

1
+ _aT
(a 2 K + KT K)a,

2

(5)

Q*(a) := -y Ta

1
+ _aT
(a 2 1 + K)a.

(6)

2

Suppose Q and Q* have minima Qmin and Q:nn. Then for all a, a* E IRffl we have

_~IIYI12 -

Qmin

:::::

Q*(a*):::::

Q;',.in

:::::a-2(_~IIYI12_Q(a)),

with equalities throughout when Q(a)

(7)

a 2 Q*(a*),

Q(a):::::

(8)

= Qmin and Q*(a*) = Q;',.in.

Hence, by minimizing Q* in addition to Q we can bound Q's closeness to the
optimum and vice versa.
Proof The minimum of Q(a) is obtained for aopt
minimizes Q*), hence
Qmin

=

1

T

-2""Y K(K

+a 2 1) -1 y

*

and Qmin

=

=

(K
1

+ a 21)-1y

T

(which also

2-1

-2""Y (K +a 1)

y.

(9)

This allows us to combine Qmin and Q;',.in to Qmin + a 2 Q;',.in = _~llyI12. Since by
definition Q (a) ::::: Qmin for all a (and likewise Q* (a*) ::::: Q;',.in for all a*) we may
solve Qmin + a 2 Q;',.in for either Q or Q* to obtain lower bounds for each of the two
quantities. This proves (7) and (8).
?
Equation (7) is useful for computing an approximation to the MAP solution,
whereas (8) can be used to obtain error bars on the estimate. To see this, note that
in calculating the variance, the expensive quantity to compute is -kT (K +a 21)-1k.
However, this can be found by solving
(10)
minimize [-k Ta + ~a T (a 2 1 + K) a] ,
aEIRm

and the expression inside the parentheses is Q*(a) with y = k (see (6)). Hence, an
approximate minimizer of (10) gives an upper bound on the error bars, and lower
bounds can be obtained from (8) .
2
1
2
I
.
?11
h
.
(
*) .- 2(Q(a)+u Q*(a*)+2liYli) ?
h
n practice we W1 use t e quantly gap a, a .- -Q(a)+u2Q * (a*)+~liYli2 ,I.e. t e
relative size of the difference between upper and lower bound as stopping criterion.

4

A Sparse Greedy Algorithm

The central idea is that in order to obtain a faster algorithm, one has to reduce
the number of free variables. Denote by P E IRffl xn with m ::::: nand m,n E N an
extension matrix (Le. p T is a projection) with p T P = 1. We will make the ansatz
ap := P[3 where [3 E IRn
(11)
and find solutions [3 such that Q(ap) (or Q*(ap)) is minimized. The solution is
[3opt

=

(pT

(a 2 K + K T K)

p) -1 p T K T y.

(12)

Clearly if Pis ofrank m, this will also be the solution of (4) (the minimum negative
log posterior for all a E IRffl ). In all other cases, however, it is an approximation.
Computational Cost of Greedy Decompositions
For a given P E IRffl xn let us analyze the computational cost involved in the estimation procedures. To compute (12) we need to evaluate pT Ky which is O(nm),
(KP)T(KP) which is O(n 2 m) and invert an n x n matrix, which is O(n 3 ). Hence the
total cost is O(n 2 m). Predictions then cost only k T a which is O(n). Using P also
to minimize Q*(P[3*) costs no more than O(n 3 ), which is needed to upper-bound
the log posterior.

For error bars, we have to approximately minimize (10) which can done for a = P(3
at O(n 3 ) cost. If we compute (PKpT)-l beforehand, this can be done by at O(n 2 )
and likewise for upper bounds. We have to minimize -k T K P(3 + !(3T pT ((72 K +
KT K)P(3 which costs O(n 2 m) (once the inverse matrices have been computed,
one may, however, use them to compute error bars at different locations, too, thus
costing only O(n 2 )). The lower bounds on the error bars may not be so crucial,
since a bad estimate will only lead to overly conservative confidence intervals and
not have any other negative effect. Finally note that all we ever have to compute
and store is K P, i.e. the m x n submatrix of K rather than K itself. Table 1
summarizes the scaling behaviour of several optimization algorithms.
Exact
Solution
Memory
Initialization
Pred. Mean
Error Bars

Conjugate
Gradient [2]

O(m~)

O(m~)

O(m;j)

O(nm:l)

g~:~)

g~~~2)

Optimal Sparse
Decomposition
O(nm)
O(n:lm)
O(n2
O(n 2 m) or O(n 2 )

Sparse Greedy
Approximation
O(nm)
o (K.n:lm)
O(n)
O(K.n 2 m) or O(n 2 )

Table 1: Computational Cost of Optimization Methods. Note that n <t:: m and also
note that the n used in Conjugate Gradient, Sparse Decomposition, and Sparse
Greedy Approximation methods will differ, with neG ::; nSD ::; nSGA since the
search spaces are more restricted. K. = 60 gives near-optimal results.
Sparse Greedy Approxhnation
Several choices for P are possible, including choosing the principal components
of K [8], using conjugate gradient descent to minimize Q [2], symmetric Cholesky
factorization [1], or using a sparse greedy approximation of K [7]. Yet these methods
have the disadvantage that they either do not take the specific form of y into account
[8, 7] or lead to expansions that cost O(m) for prediction and require computation
and storage of the full matrix [8, 2].
If we require a sparse expansion of y (x) in terms of k( Xi, x) (i.e. many ai in y = k T a
will be 0) we must consider matrices P that are a collection of unit vectors ei (here
(ei)j = Oij). We use a greedy approach to find a good approximation. First, for
n = 1, we choose P = ei such that Q(P(3) is minimal. In this case we could
permit ourselves to consider all possible indices i E {I, ... m} and find the best one
by trying out all of them. Next assume that we have found a good solution P(3
where P contains n columns. In order to improve this solution, we may expand
P into the matrix P new := [Pold, ei] E lRmx (n+1) and seek the best ei such that
P new minimizes min,8 Q(Pnew (3). (Performing a full search over all possible n + 1
out of m indices would be too costly.) This greedy approach to finding a sparse
approximate solution is described in Algorithm 1. The algorithm also maintains an
approximate minimum of Q*, and exploits the bounds of Theorem 1 to determine
when the approximation is sufficiently accurate. (N ote that we leave unspecified
how the subsets M ~ I, M* ~ I* are chosen. Assume for now that we choose
M = I, M* = I*, the full set of indices that have not yet been selected.) This
method is very similar to Matching Pursuit [4] or iterative reduced set Support
Vector algorithms [6], with the difference that the target to be approximated (the
full solution a) is only given implicitly via Q( a).

Approximation Quality
Natarajan [5] studies the following Sparse Linear Approximation problem: Given
A E lRmxn , b E lRm , E > 0, find X E lRn with minimal number of nonzero entries
such that IIAx - bl1 2 ::; E.

If we define A := (a 2K + KTK)~ and b := A- 1Ky, then we may write Q(o) =

!llb - Aol1 2 + c where c is a constant independent of o. Thus the problem of
sparse approximate minimization of Q(o) is a special case of Natarajan's problem
(where the matrix A is square, symmetric, and positive definite). In addition, the
algorithm considered by Natarajan in [5J involves sequentially choosing columns of
A to maximally decrease IIAx - bll. This is clearly equivalent to the sparse greedy
algorithm described above. Hence, it is straightforward to obtain the following
result from Theorem 2 in [5J.

Theorem 2 (Approximation Rate) Algorithm 1 achieves Q(o) ::; Q(oopt) + E
when a has

n::; I8n*~E/4)ln(IIA-1KYII)
).1

E

non-zero components, where n*(E/4) is the minimal number of nonzero components
in vectors a for which Q(o) ::; Q(oopt) + E/4, A = (a 2K + KTK)1/2, and).l is
the minimum of the magnitudes of the singular values of A, the matrix obtained by
normalizing the columns of A.
Randomized Algorithms for Subset Selection
Unfortunately, the approximation algorithm considered above is still too expensive
for large m since each search operation involves O(m) indices. Yet, if we are satisfied
with finding a relatively good index rather than the best, we may resort to selecting
a random subset of size K ?: m. In Algorithm 1, this corresponds to choosing
M ~ I, M* ~ 1* as random subsets of size K. In fact, a constant value of K will
typically suffice. To see why, we recall a simple lemma from [7J: the cumulative
distribution function of the maximum of m i.i.d. random variables 6, ...
is
FO m , where F(?) is the cdf of
Thus, in order to find a column to add to P
that is with probability 0.95 among the best 0.05 of all such columns, a random
subsample of size ilogO.05/log0.951 = 59 will suffice.

ei.

,em

Algorithm 1 Sparse Greedy Quadratic Minimization.
Require: Training data X = {Xl, ... , Xm}, Targets y, Noise a 2, Precision E
Initialize index sets I,1* = {I, ... ,m}j S, S* = 0.
repeat
Choose M ~ I, M* ~ I*.
Find arg milliEM Q ([P, eiJ,Bopt), argmilli""EM"" Q* OP*, ei"" J,B~Pt)?
Move i from I to S, i* from 1* to S*.
Set P:= [P,eiJ, P*:= [P*,ei""J.
until Q(P,Bopt} + a2Q*(P,B~Pt) + !llyl12 ::; HIQ(P,Bopt} I+ la2Q*(P,B~Pt) +! IIYl121
Output: Set of indices S, ,Bopt, (pTKP)-t, and (pT(KTK +a2K)p)-1.
Numerical Considerations
The crucial part is to obtain the values of Q(P,Bopt} cheaply (with P = [Pold, eiJ),
provided we solved the problem for Pold. From (12) one can see that all that needs
to be done is a rank-I update on the inverse. In the following we will show that this
can be obtained in O(mn) operations, provided the inverse of the smaller subsystem
is known. Expressing the relevant terms using Pold and k i we obtain
pTKT y
[Pold,eiJTK T y = (PoidKT y,kJ y)
Poid (KT K + a 2K) Pold pJd (KT + a 21) ~
pT (KTK +a 2K) P
[
kJ(K +a 21)Pold
kJki + a2Kii

1

Thus computation of the terms costs only O(nm), given the values for Pold' Furthermore, it is easy to verify that we can write the inverse of a symmetric positive
semidefinite matrix as
(13)
where 'Y := (C + BT A -1 B)-1. Hence, inversion of pT (KT K + a 2 K) P costs only
O(n 2 ). Thus, to find P of size m x n takes O(ltn 2 m) time. For the error bars,
(p T KP)-1 will generally be a good starting value for the minimization of (10),
so the typical cost for (10) will be O(Tmn) for some T < n, rather than O(mn 2 ).
Finally, for added numerical stability one may want to use an incremental Cholesky
factorization in (13) instead of the inverse of a matrix.

5

Experiments and Discussion

We used the Abalone dataset from the VCI Repository to investigate the properties
of the algorithm. The dataset is of size 4177, split into 4000 training and 177 testing
split to analyze the numerical performance, and a (3000,1177) split to assess the
generalization error (the latter was needed in order to be able to invert (and keep in
memory) the full matrix K + a 2 1 for a comparison). The data was rescaled to zero
mean and unit variance coordinate-wise. Finally, the gender encoding in Abalone
(male/female/infant) was mapped into {(I, 0, 0), (0, 1, 0), (0,0, I)}.
IIx-x'1I2

In all our experiments we used Gaussian kernels k(x, x') = exp( -~) as covariance kernels. Figure 1 analyzes the speed of convergence for different It.

1O-' O'--------,L----,L------,o'---------""c---,L------'-,-------,-L---,L------'-,------,-!
20

40

60

80
100
120
Number of Ilerahons

140

160

180

200

Figure 1: Speed of Convergence.
We plot the size of the gap between upper and lower bound of the
log posterior (gap( a, a*)) for the
first 4000 samples from the Abalone
dataset (a 2 = 0.1 and 2w 2 = 10).
From top to bottom: subsets of size
1, 2, 5, 10, 20, 50, 100, 200. The
results were averaged over 10 runs.
The relative variance of the gap size
was less than 10%.
One can see that that subsets of size
50 and above ensure rapid convergence.

For the optimal parameters (2a 2 = 0.1 and 2w 2 = 10, chosen after [7]) the average
test error of the sparse greedy approximation trained until gap(a, a*) < 0.025 on a
(3000,1177) split (the results were averaged over ten independent choices of training
sets.) was 1.785 ? 0.32, slightly worse than for the GP estimate (1.782 ? 0.33).
The log posterior was -1.572.105 (1 ? 0.005), the optimal value -1.571 . 105 (1 ?
0.005). Hence for all practical purposes full inversion of the covariance matrix and
the sparse greedy approximation have statistically indistinguishable generalization
performance.
In a third experiment (Table 2) we analyzed the number of basis functions needed
to minimize the log posterior to gap(a, a*) < 0.025, depending on different choices
of the kernel width a. In all cases, less than 10% of the kernel functions suffice to

find a good minimizer of the log posterior, for the error bars, even less than 2% are
sufficient. This is a dramatic improvement over previous techniques.
Kernel width 2w:&
Kernels for log-posterior
Kernels for error bars

1
373
79?61

2
287
49?43

5
255
26?27

10
257
17?16

20
251
12?9

50
270
8?5

Table 2: Number of basis functions needed to minimize the log posterior on the
Abalone dataset (4000 training samples), depending on the width of the kernel w.
Also, number of basis functions required to approximate k T (K + 0-21)- l k which is
needed to compute the error bars. We averaged over the remaining 177 test samples.
To ensure that our results were not dataset specific and that the algorithm scales well
we tested it on a larger synthetic dataset of size 10000 in 20 dimensions distributed
according to N(O, 1). The data was generated by adding normal noise with variance
0- 2 = 0.1 to a function consisting of 200 randomly chosen Gaussians of width 2w 2 =
40 and normally distributed coefficients and centers.
We purposely chose an inadequate Gaussian process prior (but correct noise level)
of Gaussians with width 2w 2 = 10 in order to avoid trivial sparse expansions. After
500 iterations (i.e. after using 5% of all basis functions) the size of the gap(cr, cr?)
was less than 0.023 (note that this problem is too large to be solved exactly).
We believe that sparse greedy approximation methods are a key technique to scale
up Gaussian Process regression to sample sizes of 10.000 and beyond. The techniques presented in the paper, however, are by no means limited to regression.
Work on the solutions of dense quadratic programs and classification problems is in
progress. The authors thank Bob Williamson and Bernhard Sch6lkopf.

References
[1] S. Fine and K Scheinberg. Efficient SVM training using low-rank kernel representation.
Technical report, IBM Watson Research Center, New York, 2000.
[2] M. Gibbs and D .J .C . Mackay. Efficient implementation of gaussian processes. Technical
report, Cavendish Laboratory, Cambridge, UK, 1997.
[3] F. Girosi. An equivalence between sparse approximation and support vector machines.
Neural Computation, 10(6):1455-1480, 1998.
[4] S. Mallat and Z. Zhang. Matching Pursuit in a time-frequency dictionary. IEEE
Transactions on Signal Processing, 41:3397-3415, 1993.
[5] B. K Natarajan. Sparse approximate solutions to linear systems. SIAM Journal of
Computing, 25(2) :227-234, 1995.
[6] B. Sch6lkopf, S. Mika, C. Burges, P . Knirsch, K-R. Miiller, G . Ratsch, and A. Smola.
Input space vs. feature space in kernel-based methods. IEEE Transactions on Neural
Networks, 10(5):1000 - 1017, 1999.
[7] A.J . Smola and B. Sch6lkopf. Sparse greedy matrix approximation for machine learning. In P. Langley, editor, Proceedings of the 17th International Conference on Machine
Learning, pages 911 - 918, San Francisco, 2000. Morgan Kaufman.
[8] C .KI. Williams and M. Seeger. The effect of the input density distribution on kernelbased classifiers. In P. Langley, editor, Proceedings of the Seventeenth International
Conference on Machine Learning, pages 1159 - 1166, San Francisco, California, 2000.
Morgan Kaufmann.

"
3541,2011,History distribution matching method for predicting effectiveness of HIV combination therapies,"This paper presents an approach that predicts the effectiveness of HIV combination therapies by simultaneously addressing several problems affecting the available HIV clinical data sets: the different treatment backgrounds of the samples, the uneven representation of the levels of therapy experience, the missing treatment history information, the uneven therapy representation and the unbalanced therapy outcome representation. The computational validation on clinical data shows that, compared to the most commonly used approach that does not account for the issues mentioned above, our model has significantly higher predictive power. This is especially true for samples stemming from patients with longer treatment history and samples associated with rare therapies. Furthermore, our approach is at least as powerful for the remaining samples.","History distribution matching method for predicting
effectiveness of HIV combination therapies

Jasmina Bogojeska
Max-Planck Institute for Computer Science
Campus E1 4
66123 Saarbr?ucken, Germany
jasmina@mpi-inf.mpg.de

Abstract
This paper presents an approach that predicts the effectiveness of HIV combination therapies by simultaneously addressing several problems affecting the available HIV clinical data sets: the different treatment backgrounds of the samples, the
uneven representation of the levels of therapy experience, the missing treatment
history information, the uneven therapy representation and the unbalanced therapy outcome representation. The computational validation on clinical data shows
that, compared to the most commonly used approach that does not account for
the issues mentioned above, our model has significantly higher predictive power.
This is especially true for samples stemming from patients with longer treatment
history and samples associated with rare therapies. Furthermore, our approach is
at least as powerful for the remaining samples.

1

Introduction

According to [18], more than 33 million people worldwide are infected with the human immunodeficiency virus (HIV), for which there exists no cure. HIV patients are treated by administration of
combinations of antiretroviral drugs, which succeed in suppressing the virus much longer than the
monotherapies based on a single drug. Eventually, the drug combinations also become ineffective
and need to be replaced. On such occasion, the very large number of potential therapy combinations
makes the manual search for an effective therapy increasingly impractical. The search is particulary
challenging for patients in the mid to late stages of antiretroviral therapy because of the accumulated
drug resistance from all previous therapies. The availability of large clinical data sets enables the
development of statistical methods that offer an automated procedure for predicting the outcome
of potential antiretroviral therapies. An estimate of the therapy outcome can assist physicians in
choosing a successful regimen for an HIV patient.
However, the HIV clinical data sets suffer from several problems. First of all, the clinical data
comprise therapy samples that originate from patients with different treatment backgrounds. Also
the various levels of therapy experience ranging from therapy-na??ve to heavily pretreated are represented with different sample abundances. Second, the samples on different combination therapies
have widely differing frequencies. In particular, many therapies are only represented with very few
data points. Third, the clinical data do not necessarily have the complete information on all administered HIV therapies for all patients and the information on whether all administered therapies is
available or not is also missing for many of the patients. Finally, the imbalance between the effective and the ineffective therapies is increasing over time: due to the knowledge acquired from HIV
research and clinical practice the quality of treating HIV patients has largely increased in the recent
years rendering the amount of effective therapies in recently collected data samples much larger
than the amount of ineffective ones. These four problems create bias in the data sets which might
negatively affect the usefulness of the derived statistical models.
1

In this paper we present an approach that addresses all these problems simultaneously. To tackle the
issues of the uneven therapy representation and the different treatment backgrounds of the samples,
we use information on both the current therapy and the patient?s treatment history. Additionally, our
method uses a distribution matching approach to account for the problems of missing information in
the treatment history and the growing gap between the abundances of effective and ineffective HIV
therapies over time. The performance of our history distribution matching approach is assessed by
comparing it with two common reference methods in the so called time-oriented validation scenario,
where all models are trained on data from the more distant past, while their performance is assessed
on data from the more recent past. In this way we account for the evolving trends in composing drug
combination therapies for treating HIV patients.
Related work. Various statistical learning methods, including artificial neural networks, decision
trees, random forests, support vector machines (SVMs) and logistic regression [19, 11, 14, 10, 16,
1, 15], have been used to predict the effectiveness of HIV combination therapies from clinical data.
None of these methods considers the problems affecting the available clinical data sets: different
treatment backgrounds of the samples, uneven representations of therapies and therapy outcomes,
and incomplete treatment history information. Some approaches [2, 4] deal with the uneven therapy
representation by training a separate model for each combination therapy on all available samples
with properly derived sample weights. The weights reflect the similarities between the target therapy
and all training therapies. However, the therapy-specific approaches do not address the bias originating from the different treatment backgrounds of the samples, or the missing treatment history
information.

2

Problem setting

Let z denote a therapy sample that comprises the viral genotype g represented as a binary vector indicating the occurrence of a set of resistance-relevant mutations, the therapy combination z encoded
as a binary vector that indicates the individual drugs comprising the current therapy, the binary vector h representing the drugs administered in all known previous therapies, and the label y indicating
the success (1) or failure (?1) of the therapy z. Let D = {(g1 , z1 , h1 , y1 ), . . . , (gm , zm , hm , ym )}
denote the training set and let s refer to the therapy sample of interest. Let start(s) refer to the point
of time when the therapy s was started and patient(s) refer to the patient identifier corresponding
to the therapy sample s. Then:
r(s) = {z | (start(z) ? start(s)) and (patient(z) = patient(s))}
denotes the complete treatment data associated with the therapy sample s and will be referred to as
therapy sequence. It contains all known therapies administered to patient(s) not later than start(s)
ordered by their corresponding starting times. We point out that each therapy sequence also contains
the current therapy, i.e., the most recent therapy in the therapy sequence r(s) is s. Our goal is to train
a model f (g, s, h) that addresses the different types of bias associated with the available clinical data
sets when predicting the outcome of the therapy s. In the rest of the paper we denote the set of input
features (g, s, h) by x.

3

History distribution matching method

The main idea behind the history distribution matching method we present in this paper is that the
predictions for a given patient should originate from a model trained using samples from patients
with treatment backgrounds similar as the one of the target patient. The details of this method are
summarized in Algorithm 1. In what follows, we explain each step of this algorithm.
3.1

Clustering based on similarities of therapy sequences

Clustering partitions a set of objects into clusters, such that the objects within each cluster are more
similar to one another than to the objects assigned to a different cluster [7]. In the first step of
Algorithm 1, all available training samples are clustered based on the pairwise dissimilarity of their
corresponding therapy sequences. In the following, we first describe a similarity measure for therapy
sequences and then present the details of the clustering.
2

Algorithm 1: History distribution matching method
1. Cluster the training samples by using the pairwise dissimilarities of their corresponding
therapy sequences.
2. For each (target) cluster:
? Compute sample weights that match the distribution of all available training
samples to the distribution of samples in the target cluster.
? Train a sample-weighted logistic regression model using the sample weights
computed in the previous distribution matching step.

Similarity of therapy sequences. In order to quantify the pairwise similarity of therapy sequences
we use a slightly modified version of the alignment similarity measure introduced in [5]. It adapts
sequence alignment techniques [13] to the problem of aligning therapy sequences by considering the
specific therapies given to a patient, their respective resistance-relevant mutations, the order in which
they were applied and the length of the therapy history. The alphabet used for the therapy sequence
alignment comprises all distinct drug combinations making up the clinical data set. The pairwise
similarities between the different drug combinations are quantified with the resistance mutations
kernel [5], which uses the table of resistance-associated mutations of each drug afforded by the
International AIDS society [8]. First, binary vectors indicating resistance-relevant mutations for the
set of drugs occurring in a combination are calculated for each therapy. Then, the similarity score
of two therapies of interest is computed as normalized inner product between their corresponding
resistance mutation vectors. In this way, the therapy similarity also accounts for the similarity of the
genetic fingerprint of the potential latent virus populations of the compared therapies. Each therapy
sequence ends with the current (most recent) therapy ? the one that determines the label of the sample
and the sequence alignment is adapted such that the most recent therapies are always matched.
Therefore, it also accounts for the problem of uneven representation of the different therapies in the
clinical data. It has one parameter that specifies the linear gap cost penalty.
For the history distribution matching method, we modified the alignment similarity kernel described
in the paragraph above such that it also takes the importance of the different resistance-relevant mutations into account. This is achieved by updating the resistance mutations kernel, where instead of
using binary vectors that indicate the occurrence of a set of resistance-relevant mutations, we use
vectors that indicate their importance. If two or more drugs from a certain drug group, that comprise a target therapy share a resistance mutation, then we consider its maximum importance score.
Importance scores for the resistance-relevant mutations are derived from in-vivo experiments and
can be obtained from the Stanford University HIV Drug Resistance Database [12]. Furthermore, we
want to keep the cluster similarity measure parameter-free, such that in the process of model selection the clustering Step 1 in Algorithm 1 is decoupled from the Step 2 and is computed only once.
This is achieved by computing the alignments with zero gap costs and ensures time-efficient model
selection procedure. However, in this case only the similarities of the matched therapies comprising
the two compared therapy sequences contribute to the similarity score and thus the differing lengths
of the therapy sequences are not accounted for. Having a clustering similarity measure that addresses
the differing therapy lengths is important for tackling the uneven sample representation with respect
to the level of therapy experience. In order to achieve this we normalize each pairwise similarity
score with the length of the longer therapy sequence. This yields pairwise similarity values in the
interval [0, 1] which can easily be converted to dissimilarity values in the same range by subtracting
them from 1.

Clustering. Once we have a measure of dissimilarity of therapy sequences, we cluster our data
using the most popular version of K-medoids clustering [7], referred to as partitioning around
medoids (PAM) [9]. The main reason why we choose this approach instead of the simpler K-means
clustering [7] is that it can use any precomputed dissimilarity matrix. We select the number of
clusters with the silhouette validation technique [17], which uses the so-called silhouette value to
assess the quality of the clustering and select the optimal number of clusters.
3

3.2

Cluster distribution matching

The clustering step of our method groups the training data into different bins based on their therapy
sequences. However, the complete treatment history is not necessarily available for all patients in
our clinical data set. Therefore, by restricting the prediction model for a target sample only to the
data from its corresponding cluster, the model might ignore relevant information from the other
clusters. The approach we use to deal with this issue is inspired by the multi-task learning with
distribution matching method introduced in [2].
In our current problem setting, the goal is to train a prediction model fc : x ? y for each cluster
c of similar treatment sequences, where x denotes the input features and y denotes the label. The
straightforward approach to achieve this is to train a prediction model by using only the samples
in cluster c. However, since the available treatment history for some samples might be incomplete,
totally excluding the samples from all other clusters (6= c) ignores relevant information about the
model fc . Furthermore, the cluster-specific tasks are related and the samples from the other clusters
? especially those close to the cluster boundaries of cluster c ? also carry valuable information
for the model fc . Therefore, we use a multi-task learning approach where a separate model is
trained for each cluster by not only using the training samples from the target cluster, but also the
available training samples from the remaining clusters with appropriate sample-specific weights.
These weights are computed by matching the distribution of all samples to the distribution of the
samples of the target cluster and they thereby reflect the relevance of each sample for the target
cluster. In this way, the model for the target cluster uses information from the input features to
extract relevant knowledge from the other clusters.
More formally, let D = {(x1 , y1 , c1 ), . . . , (xm , ym , cm )} denote the training data, where ci denotes
the cluster associated with the training sample (xi ,P
yi ) in the history-based clustering. The training
data are governed by the joint training distribution c p(c)p(x, y|c). The most accurate model for a
given target cluster t minimizes the loss with respect to the conditional probability p(x, y|t) referred
to as the target distribution. In [2] it is shown that:
E(x,y)?p(x,y|t) [`(ft (x))] = E(x,y)?Pc p(c)p(x,y|c) [rt (x, y)`(ft (x))],
where:

p(x, y|t)
.
p(c)p(x,
y|c)
c

rt (x, y) = P

(1)
(2)

In
P other words, by using sample-specific weights rt (x, y) that match the training distribution
c p(c)p(x, y|c) to the target distribution p(x, y|t) we can minimize the expected loss with respect
to the target distribution by minimizing the expected loss with respect to the training distribution.
The weighted training data are governed by the correct target distribution p(x, y|t) and the sample
weights reflect the relevance of each training sample for the target model. The weights are derived
based on information from the input features. If a sample was assigned to the wrong cluster due to
the incompleteness of the treatment history, by matching the training to the target distribution it can
still receive high sample weight for the model of its correct cluster.
In order to avoid the estimation of the high-dimensional densities p(x, y|t) and p(x, y|c) in Equation 2, we follow the example of [3, 2] and compute the sample weights rt (x, y) using a discriminative model for a conditional distribution with a single variable:
rt (x, y) =

p(t|x, y)
,
p(t)

(3)

where p(t|x, y) quantifies the probability that a sample (x, y) randomly drawn from the training set
D belongs to the target cluster t. p(t) is the prior probability which can easily be estimated from the
training data.
As in [2], p(t|x, y) is modeled for all clusters jointly using a kernelized version of multi-class logistic
regression with a feature mapping that separates the effective from the ineffective therapies:


?(y, +1)x
?(x, y) =
,
(4)
?(y, ?1)x
where ? is the Kronecker delta (?(a, b) = 1, if a = b, and ?(a, b) = 0, if a 6= b). In this way, we can
train the cluster-discriminative models for the effective and the ineffective therapies independently,
4

and thus, by proper time-oriented model selection address the increasing imbalance in their representation over time. Formally, the multi-class model is trained by maximizing the log-likelihood
over the training data using a Gaussian prior on the model parameters:
X
arg max
log(p(ci |xi , yi , v)) + vT ??1 v,
v

(xi ,yi ,ci )?Dc

where v are the model parameters (a concatenation of the cluster specific parameters vc ), and ? is
the covariance matrix of the Gaussian prior.
3.3

Sample-weighted logistic regression method

As described in the previous subsection, we use a multi-task distribution matching procedure to
obtain sample-specific weights for each cluster, which reflect the relevance of each sample for the
corresponding cluster. Then, a separate logistic regression model that uses all available training data
with the proper sample weights is trained for each cluster. More formally, let t denote the target
cluster and let rt (x, y) denote the weight of the sample (x, y) for the cluster t. Then, the prediction
model for the cluster t that minimizes the loss over the weighted training samples is given by:
X
1
arg min
rt (xi , y)? ? `(ft (xi ), yi ) + ?wtT wt ,
(5)
wt |D|
(xi ,yi )?D

where wt are the model parameters, ? is the regularization parameter, ? is a smoothing parameter
for the sample-specific weights and `(f (x, wt ), y) = ln(1 + exp(?ywtT x)) is the loss of linear
logistic regression.
All in all, our method first clusters the training data based on their corresponding therapy sequences
and then learns a separate model for each cluster by using relevant data from the remaining clusters.
By doing so it tackles the problems of the different treatment backgrounds of the samples and the
uneven sample representation in the clinical data sets with respect to the level of therapy experience.
Since the alignment kernel considers the most recent therapy and the drugs comprising this therapy
are encoded as a part of the input feature space, our method also deals with the differing therapy
abundances in the clinical data sets. Once we have the models for each cluster, we use them to
predict the label of a given test sample x as follows: First of all, we use the therapy sequence of the
target sample to calculate its dissimilarity to the therapy sequences of each of the cluster centers.
Then, we assign the sample x to the cluster c with the closest cluster center. Finally, we use the
logistic regression model trained for cluster c to predict the label y for the target sample x.

4
4.1

Experiments and results
Data

The clinical data for our model are extracted from the EuResist [16] database that contains information on 93014 antiretroviral therapies administered to 18325 HIV (subtype B) patients from several
countries in the period from 1988 to 2008. The information employed by our model is extracted
from these data: the viral sequence g assigned to each therapy sample is obtained shortly before
the respective therapy was started (up to 90 days before); the individual drugs of the currently administered therapy z; all available (known) therapies administered to each patient h, r(z); and the
response to a given therapy quantified with a label y (success or failure) based on the virus load values (copies of viral RNA per ml blood plasma) measured during its course (for more details see [4]
and the Supplementary material). Finally, our training set comprises 6537 labeled therapy samples
from 690 distinct therapy combinations.
4.2

Validation setting

Time-oriented validation scenario. The trends of treating HIV patients change over time as a
result of the gathered practical experience with the drugs and the introduction of new antiretroviral
drugs. In order to account for this phenomenon we use the time-oriented validation scenario [4]
which makes a time-oriented split when selecting the training and the test set. First, we order all
5

available training samples by their corresponding therapy starting dates. We then make a timeoriented split by selecting the most recent 20% of the samples as the test set and the rest as the
training set. For the model selection we split the training set further in a similar manner. We take
the most recent 25% of the training set for selecting the best model parameters (see Supplementary
material) and refer to this set as tuning set. In this way, our models are trained on the data from the
more distant past, while their performance is measured on the data from the more recent past. This
scenario is more realistic than other scenarios since it captures how a given model would perform on
the recent trends of combining the drugs. The details of the data sets resulting from this scenario are
given in Table 1, where one can also observe the large gap between the abundances of the effective
and ineffective therapies, especially for the most recent data.
Table 1: Details on the data sets generated in the time-oriented validation scenario.
Data set
Sample count
Success rate

training
3596
69%

tuning
1634
79%

test
1307
83%

The search for an effective HIV therapy is particulary challenging for patients in the mid to late
stages of antiretroviral therapy when the number of therapy options is reduced and effective therapies are increasingly hard to find because of the accumulated drug resistance mutations from all
previous therapies. The therapy samples gathered in the HIV clinical data sets are associated with
patients whose treatment histories differ in length: while some patients receive their first antiretroviral treatment, others are heavily pretreated. These different sample groups, from treatment na??ve to
heavily pretreated, are represented unevenly in the HIV clinical data with fewer samples associated
to therapy-experienced patients (see Figure 1 (a) in the Supplementary material). In order to assess
the ability of a given target model to address this problem, we group the therapy samples in the test
set into different bins based on the number of therapies administered prior to the therapy of interest
? the current therapy (see Table 1 in the Supplementary material). Then, we assess the quality of a
given target model by reporting its performance for each of the bins. In this way we can assess the
predictive power of the models in dependence on the level of therapy experience.
Another important property of an HIV model is its ability to address the uneven representation of
the different therapies (see Figure 1 (b) in the Supplementary material). In order to achieve this we
group the therapies in the test set based on the number of samples they have in the training set, and
then we measure the model performance on each of the groups. The details on the sample counts in
each of the bins are given in Table 2 of the Supplementary material. In this manner we can evaluate
the performance of the models for the rare therapies. Due to the lack of data and practical experience
for the rare HIV combination therapies, predicting their efficiency is more challenging compared to
estimating the efficiency of the frequent therapies.
Reference methods. In our computational experiments we compare the results of our history distribution matching approach, denoted as transfer history clustering validation scenario, to those of
three reference approaches, namely the one-for-all validation scenario, the history-clustering validation scenario, and the therapy-specific validation scenario. The one-for-all method mimics the
most common approaches in the field [16, 1, 19] that train a single model (here logistic regression)
on all available therapy samples in the data set. The information on the individual drugs comprising
the target (most recent) therapy and the drugs administered in all its available preceding therapies
are encoded in a binary vector and supplied as input features. The history-clustering method implements a modified version of Algorithm 1 that skips the distribution matching step. In other words, a
separate model is trained for each cluster by using only the data from the respective cluster. We introduce this approach to assess the importance of the distribution matching step. The therapy-specific
scenario implements the drugs kernel therapy similarity model described in [4]. It represents the
approaches that train a separate model for each combination therapy by using not only the samples from the target therapy but also the available samples from similar therapies with appropriate
sample-importance weights.
Performance measures. The performance of all considered methods is assessed by reporting their
corresponding accuracies (ACC) and AUCs (Area Under the ROC Curve). The accuracy reflects the
ability of the methods to make correct predictions, i.e., to discriminate between successful and failing HIV combination therapies. With the AUC we are able to assess the quality of the ranking based
6

on the probability of therapy success. For this reason, we carry out the model selection based on
both accuracy and AUC and then use accuracy or AUC, respectively, to assess the model performance. In order to compare the performance of two methods on a separate test set, the significance
of the difference of two accuracies as well as their standard deviations are calculated based on a
paired t-test. The standard deviations of the AUC values and the significance of the difference of
two AUCs used for the pairwise method comparison are estimated as described in [6].
4.3

Experimental results

According to the results from the silhouette validation technique [17] displayed in Figure 2 in the
Supplementary material, the first clustering step of Algorithm 1 divides our training data into two
clusters ? one comprises the samples with longer therapy sequences (with average treatment history
length of 5.507 therapies), and the other one those with shorter therapy sequences (with average
treatment history length of 0.308 therapies). Thus, the transfer history distribution matching method
trains two models, one for each cluster. The clustering results are depicted in Figure 3 in the Supplementary material. In what follows, we first present the results of the time-oriented validation
scenario stratified for the length of treatment history, followed by the results stratified for the abundance of the different therapies. In both cases we report both the accuracies and the AUCs for all
considered methods.

0.80

The computational results for the transfer history method and the three reference methods stratified
for the length of the therapy history are summarized in Figure 1, where (a) depicts the accuracies,
and (b) depicts the AUCs. For samples with a small number (? 5) of previously administered therapies, i.e., with short treatment histories, all considered models have comparable accuracies. For
test samples from patients with longer (> 5) treatment histories, the transfer history clustering approach achieves significantly better accuracy (p-values ? 0.004) compared to those of the reference
methods. According to the paired difference test described in [6], the transfer history approach has
significantly better AUC performance for test samples with longer (> 5) treatment histories compared to the one-for-all (p-value = 0.043) and the history-clustering (p-value = 0.044) reference
methods. It also has better AUC performance compared to the one of the therapy-specific model, yet
this improvement is not significant (p-value = 0.253). Furthermore, the transfer history approach
achieves better AUCs for test samples with less than five previously administered therapies compared to all reference methods. However, the improvement is only significant for the one-for-all
method (p-value = 0.007). The corresponding p-values for the history-clustering method and the
therapy-specific method are 0.080 and 0.178, respectively.

0.75
0.45

0.5

0.50

0.55

0.6

0.60

0.65

AUC

0.8
0.7

ACC

transfer history clustering
history clustering
therapy specific
one?for?all

0.70

0.9

transfer history clustering
history clustering
therapy specific
one?for?all

0?5

>5

0?5

Number of preceding treatments

>5

Number of preceding treatments

(a)

(b)

Figure 1: Accuracy (a) and AUC (b) results of the different models obtained on the test set in the
time-oriented validation scenario. Error bars indicate the standard deviations of each model. The
test samples are grouped based on their corresponding number of known previous therapies.
The experimental results, stratified for the abundance of the therapies summarizing the accuracies
and AUCs for all considered methods, are depicted in Figure 2 (a) and (b), respectively. As can
7

1.0

be observed from Figure 2 (a), all considered methods have comparable accuracies for the test
therapies with more than seven samples. The transfer history method achieves significantly better
accuracy (p-values ? 0.0001) compared to all reference methods for the test therapies with few
(0 ? 7) available training samples. Considering the AUC results in Figure 2 (b), the transfer history
approach outperforms all the reference models for the rare test therapies (with 0 ? 7 training samples) with estimated p-values of 0.05 for the one-for-all, 0.042 for the therapy-specific and 0.1 for
the history-clustering model. The one-for-all and the therapy-specific models have slightly better
AUC performance compared to the transfer history and the history-clustering approaches for test
therapies with 8 ? 30 available training samples. However, according to the paired difference test
described in [6], the improvements are not significant with p-values larger than 0.141 for all pairwise comparisons. Moreover, considering the test therapies with more than 30 training samples the
transfer history approach significantly outperforms the one-for-all approach with estimated p-value
of 0.037. It also has slightly better AUC performance than the history-clustering model and the
therapy-specific model, however these improvements are not significant with estimated p-values of
0.064 and 0.136, respectively.

0.8

transfer history clustering
history clustering
therapy specific
one?for?all

0.7

AUC

0.5

0.5

0.6

0.6

0.7

ACC

0.8

0.9

transfer history clustering
history clustering
therapy specific
one?for?all

0?7

8?30

>30

0?7

Number of available training samples

8?30

>30

Number of available training samples

(a)

(b)

Figure 2: Accuracy (a) and AUC (b) results of the different models obtained on the test set in the
time-oriented validation scenario. Error bars indicate the standard deviations of each model. The
test samples are grouped based on the number of available training examples for their corresponding
therapy combinations.

5

Conclusion

This paper presents an approach that simultaneously considers several problems affecting the available HIV clinical data sets: the different treatment backgrounds of the samples, the uneven representation of the different levels of therapy experience, the missing treatment history information,
the uneven therapy representation and the unbalanced therapy outcome representation especially
pronounced in recently collected samples. The transfer history clustering model has its prime advantage for samples stemming from patients with long treatment histories and for samples associated
with rare therapies. In particular, for these two groups of test samples it achieves significantly better
accuracy than all considered reference approaches. Moreover, the AUC performance of our method
for these test samples is also better than all reference methods and significantly better compared
to the one-for-all method. For the remaining test samples both the accuracy and the AUC performance of the transfer history method are at least as good as the corresponding performances of all
considered reference methods.
Acknowledgments
We gratefully acknowledge the EuResist EEIG for providing the clinical data. We thank Thomas Lengauer
for the helpful comments and for supporting this work. We also thank Levi Valgaerts for the constructive
suggestions. This work was funded by the Cluster of Excellence (Multimodal Computing and Interaction).

8

References
[1] A. Altmann, M. D?aumer, N. Beerenwinkel, E. Peres, Y. Sch?ulter, A. B?uch, S. Rhee,
A. S?onnerborg, WJ. Fessel, M. Shafer, WR. Zazzi, R. Kaiser, and T. Lengauer. Predicting
response to combination antiretroviral therapy: retrospective validation of geno2pheno-THEO
on a large clinical database. Journal of Infectious Diseases, 199:999?1006, 2009.
[2] S. Bickel, J. Bogojeska, T. Lengauer, and T. Scheffer. Multi-task learning for HIV therapy
screening. In Proceedings of the International Conference on Machine Learning, 2008.
[3] S. Bickel, M. Br?uckner, and T. Scheffer. Discriminative learning for differing training and test
distributions. In Proceedings of the International Conference on Machine Learning, 2007.
[4] J. Bogojeska, S. Bickel, A. Altmann, and T. Lengauer. Dealing with sparse data in predicting
outcomes of HIV combination therapies. Bioinformatics, 26:2085?2092, 2010.
[5] J. Bogojeska, D. St?ockel, M. Zazzi, R. Kaiser, F. Incardona, M. Rosen-Zvi, and T. Lengauer.
History-alignment models for bias-aware prediction of virological response to HIV combination therapy. submitted, 2011.
[6] J. Hanley and B. McNeil. A method of comparing the areas under receiver operating characteristic curves derived from the same cases. Radiology, 148:839?843, 1983.
[7] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer,
2009.
[8] VA. Johnson, F. Brun-Vezinet, B. Clotet, HF. G?unthrad, DR. Kuritzkes, D. Pillay, JM.
Schapiro, and DD. Richman. Update of the drug resistance mutations in HIV-1: December
2008. Topics in HIV Medicine, 16:138?145, 2008.
[9] L. Kaufman and PJ. Rousseeuw. Finding Groups in Data. An introduction to cluster analysis.
John Wiley and Sons, Inc., 1990.
[10] B. Larder, D. Wang, A. Revell, J. Montaner, R. Harrigan, F. De Wolf, J. Lange, S. Wegner,
L. Ruiz, MJ. Prez-Elas, S. Emery, J. Gatell, A. DArminio Monforte, C. Torti, M. Zazzi, and
C. Lane. The development of artificial neural networks to predict virological response to combination HIV therapy. Antiviral Therapy, 12:15?24, 2007.
[11] RH. Lathrop and MJ. Pazzani. Combinatorial optimization in rapidly mutating drug-resistant
viruses. Journal of Combinatorial Optimization, 3:301?320, 1999.
[12] TF. Liu and Shafer RW. Web resources for HIV type 1 genotypic-resistance test interpretation.
Clinical Infectious Diseases, 42, 2006.
[13] S. Needleman and C. Wunsch. A general method applicable to the search for similarities in the
amino acid sequence of two proteins. Journal of Molecular Biology, 48(3):443?453, 1970.
[14] DA. Ouattara. Mathematical analysis of the HIV-1 infection: parameter estimation, therapies
effectiveness and therapeutical failures. In Engineering in Medicine and Biology Society, 2005.
[15] M. Prosperi, A. Altmann, M. Rosen-Zvi, E. Aharoni, G. Borgulya, F. Bazso, A. S?onnerborg,
E. Sch?ulter, D. Struck, G. Ulivi, A. Vandamme, J. Vercauteren, and M. Zazzi. Investigation of
expert rule bases, logistic regression, and non-linear machine learning techniques for predicting
response to antiretroviral treatment. Antiviral Therapy, 14:433?442, 2009.
[16] M. Rosen-Zvi, A. Altmann, M. Prosperi, E. Aharoni, H. Neuvirth, A. S?onnerborg, E. Sch?ulter,
D. Struck, Y. Peres, F. Incardona, R. Kaiser, M. Zazzi, and T. Lengauer. Selecting anti-HIV
therapies based on a variety of genomic and clinical factors. Proceedings of the ISMB, 2008.
[17] P. J. Rousseeuw. Silhouettes: a graphical aid to the interpretation and validation of cluster
analysis. Journal of Computational and Applied Mathematics, 20:53?65, 1987.
[18] UNAIDS/WHO. Report on the global aids epidemic: 2010. 2010.
[19] D. Wang, BA. Larder, A. Revell, R. Harrigan, and J. Montaner. A neural network model
using clinical cohort data accurately predicts virological response and identifies regimens with
increased probability of success in treatment failures. Antiviral Therapy, 8:U99?U99, 2003.

9

"
4277,2013,Correlations strike back (again): the case of associative memory retrieval,"It has long been recognised that statistical dependencies in neuronal activity need to be taken into account when decoding stimuli encoded in a neural population. Less studied, though equally pernicious, is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an auto-associative memory. We show that activity-dependent learning generically produces such correlations, and failing to take them into account in the dynamics of memory retrieval leads to catastrophically poor recall. We derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of synaptic plasticity rules. These dynamics involve well-studied circuit motifs, such as forms of feedback inhibition and experimentally observed dendritic nonlinearities. We therefore show how addressing the problem of synaptic correlations leads to a novel functional account of key biophysical features of the neural substrate.","Correlations strike back (again): the case of
associative memory retrieval

Cristina Savin1
cs664@cam.ac.uk

Peter Dayan2
dayan@gatsby.ucl.ac.uk

M?at?e Lengyel1
m.lengyel@eng.cam.ac.uk
1

Computational & Biological Learning Lab, Dept. Engineering, University of Cambridge, UK
2
Gatsby Computational Neuroscience Unit, University College London, UK

Abstract
It has long been recognised that statistical dependencies in neuronal activity need
to be taken into account when decoding stimuli encoded in a neural population.
Less studied, though equally pernicious, is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an
auto-associative memory. We show that activity-dependent learning generically
produces such correlations, and failing to take them into account in the dynamics
of memory retrieval leads to catastrophically poor recall. We derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of
synaptic plasticity rules. These dynamics involve well-studied circuit motifs, such
as forms of feedback inhibition and experimentally observed dendritic nonlinearities. We therefore show how addressing the problem of synaptic correlations leads
to a novel functional account of key biophysical features of the neural substrate.

1

Introduction

Auto-associative memories have a venerable history in computational neuroscience. However, it is
only rather recently that the statistical revolution in the wider field has provided theoretical traction
for this problem [1]. The idea is to see memory storage as a form of lossy compression ? information
on the item being stored is mapped into a set of synaptic changes ? with the neural dynamics during
retrieval representing a biological analog of a corresponding decompression algorithm. This implies
there should be a tight, and indeed testable, link between the learning rule used for encoding and the
neural dynamics used for retrieval [2].
One issue that has been either ignored or trivialized in these treatments of recall is correlations
among the synapses [1?4] ? beyond the perfect (anti-)correlations emerging between reciprocal
synapses with precisely (anti-)symmetric learning rules [5]. There is ample experimental data for
the existence of such correlations: for example, in rat visual cortex, synaptic connections tend to
cluster together in the form of overrepresented patterns, or motifs, with reciprocal connections being
much more common than expected by chance, and the strengths of the connections to and from
each neuron being correlated [6]. The study of neural coding has indicated that it is essential to
treat correlations in neural activity appropriately in order to extract stimulus information well [7?
9]. Similarly, it becomes pressing to examine the nature of correlations among synaptic weights in
auto-associative memories, the consequences for retrieval of ignoring them, and methods by which
they might be accommodated.
1

Here, we consider several well-known learning rules, from simple additive ones to bounded synapses
with metaplasticity, and show that, with a few significant exceptions, they induce correlations between synapses that share a pre- or a post-synaptic partner. To assess the importance of these dependencies for recall, we adopt the strategy of comparing the performance of decoders which either
do or do not take them into account [10], showing that they do indeed have an important effect on
efficient retrieval. Finally, we show that approximately optimal retrieval involves particular forms
of nonlinear interactions between different neuronal inputs, as observed experimentally [11].

2

General problem formulation

We consider a network of N binary neurons that enjoy all-to-all connectivity.1 As is conventional,
and indeed plausibly underpinned by neuromodulatory interactions [12], we assume that network
dynamics do not play a role during storage (with stimuli being imposed as patterns of activity on the
neurons), and that learning does not occur during retrieval.
To isolate the effects of different plasticity rules on synaptic correlations from other sources of
correlations, we assume that the patterns of activity inducing the synaptic changes have no particular
structure, i.e. their distribution factorizes. For further simplicity, we take these activity patterns to
be binary with pattern density f , i.e. a prior over patterns defined as:
Y
Pstore (x) =
Pstore (xi )
Pstore (xi ) = f xi ? (1 ? f )1?xi
(1)
i

During recall, the network is presented with a cue, x
?, which is a noisy or partial version of one
of the originally stored patterns. Network dynamics should complete this partial pattern, using the
information in the weights W (and the cue). We start by considering arbitrary dynamics; later we
impose the critical constraint for biological realisability that they be strictly local, i.e. the activity of
neuron i should depend exclusively on inputs through incoming synapses Wi,? .
Since information storage by synaptic plasticity is lossy, recall is inherently a probabilistic inference
problem [1, 13] (Fig. 1a), requiring estimation of the posterior over patterns, given the information
in the weights and the recall cue:
? ) ? Pstore (x) ? Pnoise (?
P (x|W, x
x|x) ? P(W|x)

(2)

This formulation has formed the foundation of recent work on constructing efficient autoassociative
recall dynamics for a range of different learning rules [2?4]. In this paper, we focus on the last term
P(W|x), which expresses the probability of obtaining W as the synaptic weight matrix when x is
stored along with T ? 1 random patterns (sampled from the prior, Eq. 1). Critically, this is where
we diverge from previous analyses that assumed this distribution was factorised, or only trivially
correlated due to reciprocal synapses being precisely (anti-)symmetric [1, 2, 4]. In contrast, we
explicitly study the emergence and effects of non-trivial correlations in the synaptic weight matrixdistribtion, because almost all synaptic plasticity rules induce statistical dependencies between the
synaptic weights of each neuron (Fig. 1a, d).
The inference problem expressed by Eq. 2 can be translated into neural dynamics in several ways
? dynamics could be deterministic, attractor-like, converging to the most likely pattern (a MAP
estimate) of the distribution of x [2], or to a mean-field approximate solution [3]; alternatively, the
dynamics could be stochastic, with the activity over time representing samples from the posterior,
and hence implicitly capturing the uncertainty associated with the answer [4]. We consider the latter.
Since we estimate performance by average errors, the optimal response is the mean of the posterior,
which can be estimated by integrating the activity of the network during retrieval.
We start by analysing the class of additive learning rules, to get a sense for the effect of correlations on retrieval. Later, we focus on multi-state synapses, for which learning rules are described
by transition probabilities between the states [14]. These have been used to capture a variety of
important biological constraints such as bounds on synaptic strengths and metaplasticity, i.e. the
fact that synaptic changes induced by a certain activity pattern depend on the history of activity at
the synapse [15]. The two classes of learning rule are radically different; so if synaptic correlations
matter during retrieval in both cases, then the conclusion likely applies in general.
1
Complete connectivity simplifies the computation of the parameters for the optimal dynamics for cascadelike learning rules considered in the following, but is not necessary for the theory.

2

1

covariance rule
simple Hebb rule
cortical data (Song 2005)

0.5
0

d

e
error (%)

1

corr

c

error (%)

b
corr

a

0.5
0

10 control
exact (considering correlations)
simple (ignoring correlations)

5
0

25

30

50

100

50

100

N

20
10 control
0

25

N

Figure 1: Memory recall as inference and additive learning rules. a. Top: Synaptic weights,
W, arise by storing the target pattern x together with T ?1 other patterns, {x(t) }t=1...T?1 . During
? , is a noisy version of the target pattern. The task of recall is to infer x given W
recall, the cue, x
and x
? (by marginalising out {x(t) }). Bottom: The activity of neuron i across the stored patterns is
a source of shared variability between synapses connecting it to neurons j and k. b-c. Covariance
rule: patterns of synaptic correlations and recall performance for retrieval dynamics ignoring or
considering synaptic correlations; T = 5. d-e. Same for the simple Hebbian learning rule. The
control is an optimal decoder that ignores W.

3

Additive learning rules

Local additive learning rules assume that synaptic changes induced by different activity patterns
combine additively; such that storing a sequence of T patterns from Pstore (x), results in weights
P
(t)
(t)
Wij = t ?(xi , xj ), with function ?(xi , xj ) describing the change in synaptic strength induced
by presynaptic activity xj and postsynaptic activity xi . We consider a generalized Hebbian form for
this function, with ? (xi , xj ) = (xi ? ?)(xj ? ?). This class includes, for example, the covariance
rule (? = ? = f ), classically used in Hopfield networks, or simple Hebbian learning (? = ? = 0).
As synaptic changes are deterministic, the only source of uncertainty in the distribution P(W|x)
is the identity of the other stored patterns. To estimate this, let us first consider the distribution of
the weights after storing one random pattern from Pstore (x). The mean ? and covariance C of the
weight change induced by this event can be computed as:2
Z
Z

? = Pstore (x)?| (x)dx,
C = Pstore (x) ?| (x) ? ?| (x)T dx ? ? ? ?T
(3)
Since the rule is additive and the patterns are independent, the mean and covariance scale linearly
with the number of intervening patterns. Hence, the distribution over possible weight values at
recall, given that pattern x is stored along with T ? 1 other, random, patterns has mean ?W =
?(x) + (T ? 1) ? ?, and covariance CW = (T ? 1) ? C. Most importantly, because the rule is
additive, in the limit of many stored patterns (and in practice even for modest values of T ), the
distribution P(W|x) approaches a multivariate Gaussian that is characterized completely by these
two quantities; moreover, its covariance is independent of x.
For retrieval dynamics based on Gibbs sampling, the key quantity is the log-odds ratio


P(xi = 1|x?i , W, x
?)
(4)
Ii = log
P(xi = 0|x?i , W, x
?)
for neuron i, which could be represented by the total current entering the unit. This would translate
into a probability of firing given by the sigmoid activation function f (Ii ) = 1/(1 + e?Ii ).
The total current entering a neuron is a sum of two terms: one term from the external input of the
form c1 ? x
?i + c2 (with constants c1 and c2 determined by parameters f and r [16]), and one term
from the recurrent input, of the form:

T
 
T



1
(0)
(0)
(1)
(1)
Iirec =
(5)
W| ? ?W
C?1 W| ? ?W ? W| ? ?W
C?1 W| ? ?W
2(T ?1)
2
For notational convenience, we use a column-vector form of the matrix of weight changes ?, and the
weight matrix W, marked by subscript | .

3

(0/1)

where ?W = ?| (x(0/1) )+(T?1)? and x(0/1) is the vector of activities obtained from x in which
the activity of neuron i is set to 0, or 1, respectively.
It is easy to see that for the covariance rule, ? (xi , xj ) = (xi ? f )(xj ? f ), synapses sharing
a single pre- or post-synaptic partner happen to be uncorrelated (Fig. 1b). Moreover, as for any
(anti-)symmetric additive learning rule, reciprocal connections are perfectly correlated (Wij = Wji ).
The (non-degenerate part of the) covariance matrix in this case becomes diagonal, and the total
current in optimal retrieval reduces to simple linear dynamics :
Ii =

1
2
(T ? 1) ?W

X

Wij xj ?

j

|

{z

}

recurrent input


X
(1 ? 2f )2 X
1 ? 2f
Wij ? f 2
xj ? f
2
2
j
j
| {z }
{z
}
| {z }
|
feedback inhibition

homeostatic term

(6)

constant

2
where ?W
is the variance of a synaptic weight resulting from storing a single pattern. This term
includes a contribution from recurrent excitatory input, dynamic feedback inhibition (proportional
to the total population activity) and a homeostatic term that reduces neuronal excitability as function
of the net strength of its synapses (a proxy for average current the neuron expects to receive) [17].
Reassuringly, the optimal decoder for the covariance rule recovers a form for the input current that is
closely related to classic Hopfield-like [5] dynamics (with external field [1, 18]): feedback inhibition
is needed only when the stored patterns are not balanced (f 6= 0.5); for the balanced case, the
homeostatic term can be integrated in the recurrent current, by rewriting neural activities as spins.
In sum, for the covariance rule, synapses are fortuitously uncorrelated (except for symmetric pairs
which are perfectly correlated), and thus simple, classical linear recall dynamics suffice (Fig. 1c).

The covariance rule is, however, the exception rather than the rule. For example, for simple Hebbian
learning, ? (xi , xj ) = xi ?xj , synapses sharing a pre- or post-synaptic partner are correlated (Fig. 1d)
and so the covariance matrix C is no longer diagonal. Interestingly, the final expression of the
recurrent current to a neuron remains strictly local (because of additivity and symmetry), and very
similar to Eq. 6, but feedback inhibition becomes a non-linear function of the total activity in the
network [16]. In this case, synaptic correlations have a dramatic effect: using the optimal non-linear
dynamics ensures high performance, but trying to retrieve information using a decoder that assumes
synaptic independence (and thus uses linear dynamics) yields extremely poor performance, which
is even worse than the obvious control of relying only on the information in the recall cue and the
prior over patterns (Fig. 1e).
For the generalized Hebbian case, ? (xi , xj ) = (xi ??)(xj ??) with ? 6= ?, the optimal decoder becomes even more complex, with the total current including additional terms accounting for pairwise
correlations between any two synapses that have neuron i as a pre- or post-synaptic partner [16].
Hence, retrieval is no longer strictly local3 and a biological implementation will require approximating the contribution of non-local terms as a function of locally available information, as we discuss
in detail for palimpsest learning below.

4

Palimpsest learning rules

Though additive learning rules are attractive for their analytical tractability, they ignore several important aspects of synaptic plasticity, e.g. they assume that synapses can grow without bound. We
investigate the effects of bounded weights by considering another class of learning rules, which assumes synaptic efficacies can only take binary values, with stochastic transitions between the two
underpinned by paired cascades of latent internal states [14] (Fig. 2). These learning rules, though
very simple, capture an important aspect of memory ? the fact that memory is leaky, and information
about the past is overwritten by newly stored items (usually referred to as the palimpsest property).
Additionally, such rules can account for experimentally observed synaptic metaplasticity [15].
3
For additive learning rules, the current to neuron i always depends only on synapses local to a neuron, but
these can also include outgoing synapses of which the weight, W?i , should not influence its dynamics. We refer
to such dynamics as ?semi-local?. For other learning rules, the optimal current to neuron i may depend on all
connections in the network, including Wjk with j, k 6= i (?non-local? dynamics).

4

R1

D P

post

R2

R3

c

- -

0
1

D
P

P D
D P
0 1
pre

0.4

cortex data (Song 2005)

d

correlated synapses

20
10

0.2
0

0

0.4

20

error (%)

b

correlation coefficient

a

0.2
0
0.6

pseudostorage

10
0
20

0.3

*

10

0

exact approx

0

simple
dynamics

*

corr-dependent
dynamics

Figure 2: Palimpsest learning. a. The cascade model. Colored circles are latent states (V ) that
belong to two different synaptic weights (W ), arrows are state transitions (blue: depression, red:
potentiation) b. Different variants of mapping pre- and post-synaptic activations to depression (D)
and potentiation (P): R1?postsynaptically gated, R2?presynaptically gated, R3?XOR rule. c. Correlation structure induced by these learning rules. c. Retrieval performance for each rule.
Learning rule
Learning is stochastic and local, with changes in the state of a synapse Vij being determined only by
the activation of the pre- and post-synaptic neurons, xj and xi . In general, one could define separate
transition matrices for each activity pattern, M(xi , xj ), describing the probability of a synaptic state
transitioning between any two states Vij to Vij0 following an activity pattern, (xi , xj ). For simplicity,
we define only two such matrices, for potentiation, M+ , and depression, M? , respectively, and then
map different activity patterns to these events. In particular, we assume Fusi?s cascade model [14]4
and three possible mappings (Fig. 2b [16]): 1) a postsynaptically gated learning rule, where changes
occur only when the postsynaptic neuron is active, with co-activation of pre- and post- neuron leading to potentiation, and to depression otherwise5 ; 2) a presynaptically gated learning rule, typically
assumed when analysing cascades[20, 21]; and 3) an XOR-like learning rule which assumes potentiation occurs whenever the pre- and post- synaptic activity levels are the same, with depression
otherwise. The last rule, proposed by Ref. 22, was specifically designed to eliminate correlations
between synapses, and can be viewed as a version of the classic covariance rule fashioned for binary
synapses.
Estimating the mean and covariance of synaptic weights
At the level of a single synapse, the presentation of a sequence of uncorrelated patterns from
Pstore (x) corresponds to a Markov random walk, P
defined by a transition matrix M, which averages over possible neural activity patterns: M = xi ,xj Pstore (xi ) ? Pstore (xj ) ? M(xi , xj ). The
distribution over synaptic states t steps after the initial encoding can be calculated by starting from
the stationary distribution of the weights ? V 0 (assuming a large number of other patterns have previously been stored; formally, this is the eigenvector of M corresponding to eigenvalue ? = 1), then
storing the pattern (xi , xj ), and finally t ? 1 other patterns from the prior:
t?1

? V (xi , xj , t) = M

? M(xi , xj ) ? ? V 0 ,

(7)

?lV

with the distribution over states given as a column vector,
= P(Vij = l|xi , xj ), l ? {1 . . . 2n},
where n is the depth of the cascade. Lastly, the distribution over weights, P(Wij |xi , xj ), can be
derived as ? W = MV ?W ? ? V , where MV ?W is a deterministic map from states to observed
weights (Fig. 2a).
As in the additive case, the states of synapses sharing a pre- or post- synaptic partner will be correlated (Figs. 1a, 2c). The degree of correlations for different synaptic configurations can be estimated
by generalising the above procedure to computing the joint distribution of the states of pairs of
synapses, which we represent as a matrix ?. E.g. for a pair of synapses sharing a postsynaptic
partner (Figs. 1b, d, and 2c), element (u, v) is ?uv = P(Vpost,pre1 = u, Vpost,pre2 = v). Hence, the
presentation of an activity pattern (xpre1 , xpre2 , xpost ) induces changes in the corresponding pair of
4

Other models, e.g. serial [19], could be used as well without qualitatively affecting the results.
One could argue that this is the most biologically relevant as plasticity is often NMDA-receptor dependent,
and hence it requires postsynaptic depolarisation for any effect to occur.
5

5

incoming synapses to neuron post as ?(1) = M(xpost , xpre1 ) ? ?(0) ? M(xpost , xpre2 )T , where ?(0)
is the stationary distribution corresponding to storing an infinite number of triplets from the pattern
distribution [16].
Replacing ? V with ? (which is now a function of the triplet (xpre1 , xpre2 , xpost )), and the multiplication by M with the slightly more complicated operator above, we can estimate the evolution of
the joint distribution over synaptic states in a manner very similar to Eq. 7:
X
? i ) ? ?(t?1) ? M(x
? i )T ,
Pstore (xi ) ? M(x
(8)
?(t) =
xi
P
? i) =
where M(x
xj Pstore (xj )M(xi , xj ). Also as above, the final joint distribution over states
can be mapped into a joint distribution over synaptic weights as MV ?W ? ?(t) ? MT
V ?W . This
approach can be naturally extended to all other correlated pairs of synapses [16].
The structure of correlations for different synaptic pairs varies significantly as a function of the
learning rule (Fig. 2c), with the overall degree of correlations depending on a range of factors.
Correlations tend to decrease with cascade depth and pattern sparsity. The first two variants of the
learning rule considered are not symmetric, and so induce different patterns of correlations than the
additive rules above. The XOR rule is similar to the covariance rule, but the reciprocal connections
are no longer perfectly correlated (due to metaplasticity), which means that it is no longer possible
to factorize P(W|x). Hence, assuming independence at decoding seems bound to introduce errors.
Approximately optimal retrieval when synapses are independent
If we ignore synaptic correlations, the evidence from the weights factorizes, P(W|x) =
Q
3
i,j P(Wij |xi , xj ), and so the exact dynamics would be semi-local . We can further approximate
the contribution of the outgoing weights by its mean, which recovers the same simple dynamics
derived for the additive case:


X
X
X
P(xi = 1|x?i , W, x
?)
Ii = log
= c1
Wij xj + c2
Wij + c3
xj + c4 x?i + c5 (9)
j
j
j
P(xi = 0|x?i , W, x
?)
The parameters c. depend on the prior over x, the noise model, the parameters of the learning rule
and t. Again, the optimal decoder is similar to previously derived attractor dynamics; in particular,
for stochastic binary synapses with presynaptically gated learning the optimal dynamics require
dynamic inhibition only for sparse patterns, and no homeostatic term, as used in [21] .
To validate these dynamics, we remove synaptic correlations by a pseudo-storage procedure in which
synapses are allowed to evolve independently according to transition matrix M, rather than changing
as actual intermediate patterns are stored. The dynamics work well in this case, as expected (Fig. 2d,
blue bars). However, when storing actual patterns drawn from the prior, performance becomes extremely poor, and often worse than the control (Fig. 2d, gray bars). Moreover, performance worsens
as the network size increases (not shown). Hence, ignoring correlations is highly detrimental for this
class of learning rules too.
Approximately optimal retrieval when synapses are correlated
To accommodate synaptic correlations, we approximate P(W|x) with a maximum entropy distribution with the same marginals and covariance structure, ignoring the higher order moments.6
Specifically, we assume the evidence from the weights has the functional form:
X

1X
1
exp
kij (x, t) ? Wij +
J(ij)(kl) (x, t) ? Wij Wkl
(10)
P(W|x, t) =
ij
ijkl
Z(x, t)
2
We use the TAP mean-field method [23] to find parameters k and J and the partition function, Z,
for each possible activity pattern x, given the mean and covariance for the synaptic weights matrix,
computed above7 [16].
6
This is just a generalisation of the simple dynamics which assume a first order max entropy model; moreover, the resulting weight distribution is a binary analog of the multivariate normal used in the additive case,
allowing the two to be directly compared.
7
Here, we ask whether it is possible to accommodate correlations in appropriate neural dynamics at all,
ignoring the issue of how the optimal values for the parameters of the network dynamics would come about.

6

a

b

no corr
corr

5

0.05

0.5

0.01

0
0

0

0

?5
?10

?0.05

d

12

6

0
?2

0

2

4

6

8

10

number of coactive inputs

12

10

20

0

10

0

10

20

e

10
5
0
?5
?10

?0.01

20

0

2

4

6

8

10

12

number of coactive inputs

normalized EPSP

0

TIP

20

MIDDLE

10

postsynaptic current

c

postsynaptic current

0

BASE

?0.5

1.0
0.8
0.6
0.4
0.2
0.0

0

1

2 3 4 5 6
number of inputs

7

Figure 3: Implications for neural dynamics. a. R1: parameters for Iirec ; linear modulation by
network activity, nb . b. R2: nonlinear modulation of pairwise term by network activity (cf. middle
panel in a); other parameters have
P linear dependences on nb . c. R1: Total current as
Pfunction of
number of coactivated inputs, j Wij xj ; lines: different levels of neural excitability j Wij , line
widths scale with frequency of occurrence in a sample run. d. Same for R2. e. Nonlinear integration
in dendrites, reproduced from [11], cf. curves in c.
Exact retrieval dynamics based on Eq. 10, but not respecting locality constraints, work substantially
better in the presence of synaptic correlations, for all rules (Fig. 2d, yellow bars). It is important to
note that for the XOR rule, which was supposed to be the closest analog to the covariance rule and
hence afford simple recall dynamics [22], error rates stay above control, suggesting that it is actually
a case in which even dependencies beyond 2nd-order correlation would need to be considered.
As in the additive case, exact recall dynamics are biologically implausible, as the total current to
the neuron depends on the full weight matrix. It is possible to approximate the dynamics using
strictly local information by replacing the nonlocal term by its mean, which, however,
is no longer a
P
constant, but rather a linear function of the total activity in the network, nb = j6=i xj [16]. Under
this approximation, the current from recurrent connections corresponding to the evidence from the
weights becomes:
 X

P(W|x(1) )
1X
4
J4
=
k
(x)Wij Wik ? Z 4
(11)
(x)W
+
Iirec = log
ij
ij
jk (ij)(ik)
j
2
P(W|x(0) )
where i is the index of the neuron to be updated, and x(0/1) activity vector has the to-be-updated
neuron?s activity set to 1 or 0, respectively, and all other components given by the
 current network

4
4
state. The functions kij
(x) = kij (x(1) )?kij (x(0) ), J(ij)(kl)
(x) = J(ij)(kl) x(1) ?J(ij)(kl) x(0) ,


and Z 4 = log Z x(1) ? log Z x(0) depend on the local activity at the indexed synapses,
modulated by the number of active neurons in the network, nb . This approximation is again consistent with our previous analysis, i.e. in the absence of synaptic correlations, the complex dynamics
recover the simple case presented before. Importantly, this approximation also does about as well as
exact dynamics (Fig. 2d, red bars).
For post-synaptically gated learning, comparing the parameters of the dynamics in the case of independent versus correlated synapses (Fig. 3a) reveals a modest modulation of the recurrent input by
the total activity. More importantly, the net current to the postsynaptic P
neuron depends non-linearly
(formally, quadratically) on the number of co-active inputs, nW 1 = j xj Wij , (Fig. 3c), which
is reminiscent of experimentally observed dendritic non-linearities [11] (Fig. 3e). Conversely, for
the presynaptically gated learning rule, approximately optimal dynamics predict a non-monotonic
modulation of activity by lateral inhibition (Fig. 3b), but linear neural integration (Fig. 3d).8 Lastly,
retrieval based on the XOR rule has the same form as the simple dynamics derived for the factorized
case [16]. However, the total current has to be rescaled to compensate for the correlations introduced
by reciprocal connections.
8
The difference between the two rules emerges exclusively because of the constraint of strict locality of the
approximation, since the exact form of the dynamics is essentially the same for the two.

7

additive
cascade

RULE
covariance
simple Hebbian
generalized Hebbian
presyn. gated
postsyn. gated
XOR

EXACT DYNAMICS
strictly local, linear
strictly local, nonlinear
semi-local, nonlinear
nonlocal, nonlinear
nonlocal, nonlinear
beyond correlations

NEURAL IMPLEMENTATION
linear feedback inh., homeostasis
nonlinear feedback inh.
nonlinear feedback inh.
nonlinear feedback inh., linear dendritic integr.
linear feedback inh., non-linear dendritic integr.
?

Table 1: Results summary: circuit adaptations against correlations for different learning rules.

5

Discussion

Statistical dependencies between synaptic efficacies are a natural consequence of activity dependent
synaptic plasticity, and yet their implications for network function have been unexplored. Here, in
the context of an auto-associative memory network, we investigated the patterns of synaptic correlations induced by several well-known learning rules and their consequent effects on retrieval. We
showed that most rules considered do indeed induce synaptic correlations and that failing to take
them into account greatly damages recall. One fortuitous exception is the covariance rule, for which
there are no synaptic correlations. This might explain why the bulk of classical treatments of autoassociative memories, using the covariance rule, could achieve satisfying capacity levels despite
overlooking the issue of synaptic correlations [5, 24, 25].
In general, taking correlations into account optimally during recall requires dynamics in which there
are non-local interactions between neurons. However, we derived approximations that perform well
and are biologically realisable without such non-locality (Table 1). Examples include the modulation of neural responses by the total activity of the population, which could be mediated by feedback
inhibition, and specific dendritic nonlinearities. In particular, for the post-synaptically gated learning rule, which may be viewed as an abstract model of hippocampal NMDA receptor-dependent
plasticity, our model predicts a form of non-linear mapping of recurrent inputs into postsynaptic
currents which is similar to experimentally observed dendritic integration in cortical pyramidal cells
[11]. In general, the tight coupling between the synaptic plasticity used for encoding (manifested
in patterns of synaptic correlations) and circuit dynamics offers an important route for experimental
validation [2].
None of the rules governing synaptic plasticity that we considered perfectly reproduced the pattern
of correlations in [6]; and indeed, exactly which rule applies in what region of the brain under which
neuromodulatory influences is unclear. Furthermore, results in [6] concern the neocortex rather
than the hippocampus, which is a more common target for models of auto-associative memory.
Nonetheless, our analysis has shown that synaptic correlations matter for a range of very different
learning rules that span the spectrum of empirical observations.
Another strategy to handle the negative effects of synaptic correlations is to weaken or eliminate
them. For instance, in the palimpsest synaptic model [14], the deeper the cascade, the weaker the
correlations, and so metaplasticity may have the beneficial effect of making recall easier. Another,
popular, idea is to use very sparse patterns [21], although this reduces the information content of
each one. More speculatively, one might imagine a process of off-line synaptic pruning or recoding,
in which strong correlations are removed or the weights adjusted so that simple recall methods will
work.
Here, we focused on second-order correlations. However, for plasticity rules such as XOR, we
showed that this does not suffice. Rather, higher-order correlations would need to be considered,
and thus, presumably higher-order interactions between neurons approximated. Finally, we know
from work on neural coding of sensory stimuli that there are regimes in which correlations either
help or hurt the informational quality of the code, assuming that decoding takes them into account.
Given our results, it becomes important to look at the relative quality of different plasticity rules,
assuming realizable decoding ? it is not clear whether rules that strive to eliminate correlations will
be bested by ones that do not.
Acknowledgments This work was supported by the Wellcome Trust (CS, ML), the Gatsby Charitable Foundation (PD), and the European Union Seventh Framework Programme (FP7/2007?2013)
under grant agreement no. 269921 (BrainScaleS) (ML).
8

References
1. Sommer, F.T. & Dayan, P. Bayesian retrieval in associative memories with storage errors. IEEE transactions on neural networks 9, 705?713 (1998).
2. Lengyel, M., Kwag, J., Paulsen, O. & Dayan, P. Matching storage and recall: hippocampal spike timingdependent plasticity and phase response curves. Nature Neuroscience 8, 1677?1683 (2005).
3. Lengyel, M. & Dayan, P. Uncertainty, phase and oscillatory hippocampal recall. Advances in Neural
Information Processing (2007).
4. Savin, C., Dayan, P. & Lengyel, M. Two is better than one: distinct roles for familiarity and recollection in
retrieving palimpsest memories. in Advances in Neural Information Processing Systems, 24 (MIT Press,
Cambridge, MA, 2011).
5. Hopfield, J.J. Neural networks and physical systems with emergent collective computational abilities.
Proc. Natl. Acad. Sci. USA 76, 2554?2558 (1982).
6. Song, S., Sj?ostr?om, P.J., Reigl, M., Nelson, S. & Chklovskii, D.B. Highly nonrandom features of synaptic
connectivity in local cortical circuits. PLoS biology 3, e68 (2005).
7. Dayan, P. & Abbott, L. Theoretical Neuroscience (MIT Press, 2001).
8. Averbeck, B.B., Latham, P.E. & Pouget, A. Neural correlations, population coding and computation.
Nature Reviews Neuroscience 7, 358?366 (2006).
9. Pillow, J.W. et al. Spatio-temporal correlations and visual signalling in a complete neuronal population.
Nature 454, 995?999 (2008).
10. Latham, P.E. & Nirenberg, S. Synergy, redundancy, and independence in population codes, revisited.
Journal of Neuroscience 25, 5195?5206 (2005).
11. Branco, T. & H?ausser, M. Synaptic integration gradients in single cortical pyramidal cell dendrites. Neuron
69, 885?892 (2011).
12. Hasselmo, M.E. & Bower, J.M. Acetylcholine and memory. Trends Neurosci. 16, 218?222 (1993).
13. MacKay, D.J.C. Maximum entropy connections: neural networks. in Maximum Entropy and Bayesian
Methods, Laramie, 1990 (eds. Grandy, Jr, W.T. & Schick, L.H.) 237?244 (Kluwer, Dordrecht, The Netherlands, 1991).
14. Fusi, S., Drew, P.J. & Abbott, L.F. Cascade models of synaptically stored memories. Neuron 45, 599?611
(2005).
15. Abraham, W.C. Metaplasticity: tuning synapses and networks for plasticity. Nature Reviews Neuroscience
9, 387 (2008).
16. For details, see Supplementary Information.
17. Zhang, W. & Linden, D. The other side of the engram: experience-driven changes in neuronal intrinsic
excitability. Nature Reviews Neuroscience (2003).
18. Engel, A., Englisch, H. & Sch?utte, A. Improved retrieval in neural networks with external fields. Europhysics Letters (EPL) 8, 393?397 (1989).
19. Leibold, C. & Kempter, R. Sparseness constrains the prolongation of memory lifetime via synaptic metaplasticity. Cerebral cortex (New York, N.Y. : 1991) 18, 67?77 (2008).
20. Amit, Y. & Huang, Y. Precise capacity analysis in binary networks with multiple coding level inputs.
Neural computation 22, 660?688 (2010).
21. Huang, Y. & Amit, Y. Capacity analysis in multi-state synaptic models: a retrieval probability perspective.
Journal of computational neuroscience (2011).
22. Dayan Rubin, B. & Fusi, S. Long memory lifetimes require complex synapses and limited sparseness.
Frontiers in Computational Neuroscience (2007).
23. Thouless, D.J., Anderson, P.W. & Palmer, R.G. Solution of ?Solvable model of a spin glass?. Philosophical
Magazine 35, 593?601 (1977).
24. Amit, D., Gutfreund, H. & Sompolinsky, H. Storing infinite numbers of patterns in a spin-glass model of
neural networks. Phys Rev Lett 55, 1530?1533 (1985).
25. Treves, A. & Rolls, E.T. What determines the capacity of autoassociative memories in the brain? Network
2, 371?397 (1991).

9

"
1228,2001,Probabilistic principles in unsupervised learning of visual structure: human data and a model,Abstract Missing,"Probabilistic principles in unsupervised learning
of visual structure: human data and a model
Shimon Edelman, Benjamin P. Hiles
& Hwajin Yang
Department of Psychology
Cornell University, Ithaca, NY 14853
se37,bph7,hy56  @cornell.edu

Nathan Intrator
Institute for Brain and Neural Systems
Box 1843, Brown University
Providence, RI 02912
Nathan Intrator@brown.edu

Abstract
To find out how the representations of structured visual objects depend
on the co-occurrence statistics of their constituents, we exposed subjects
to a set of composite images with tight control exerted over (1) the conditional probabilities of the constituent fragments, and (2) the value of Barlow?s criterion of ?suspicious coincidence? (the ratio of joint probability
to the product of marginals). We then compared the part verification response times for various probe/target combinations before and after the
exposure. For composite probes, the speedup was much larger for targets that contained pairs of fragments perfectly predictive of each other,
compared to those that did not. This effect was modulated by the significance of their co-occurrence as estimated by Barlow?s criterion. For
lone-fragment probes, the speedup in all conditions was generally lower
than for composites. These results shed light on the brain?s strategies for
unsupervised acquisition of structural information in vision.

1 Motivation
How does the human visual system decide for which objects it should maintain distinct
and persistent internal representations of the kind typically postulated by theories of object
recognition? Consider, for example, the image shown in Figure 1, left. This image can be
represented as a monolithic hieroglyph, a pair of Chinese characters (which we shall refer
to as  and  ), a set of strokes, or, trivially, as a collection of pixels. Note that the second
option is only available to a system previously exposed to various combinations of Chinese
characters. Indeed, a principled decision whether to represent this image as  , 
or otherwise can only be made on the basis of prior exposure to related images.
According to Barlow?s [1] insight, one useful principle is tallying suspicious coincidences:
two candidate fragments  and  should be combined into a composite object 	 if the
probability of their joint appearance 

 is much higher than 

 , which is
the probability expected in the case of their statistical independence. This criterion may be
compared to the Minimum Description Length (MDL) principle, which has been previously
discussed in the context of object representation [2, 3]. In a simplified form [4], MDL calls
for representing 	 explicitly as a whole if 


 , just as the principle
of suspicious coincidences does.





While the Barlow/MDL criterion

 
  
 certainly indicates a suspicious coincidence, there are additional probabilistic considerations that may be used
in setting the degree of association between  and  . One example is the possi

ble perfect predictability of  from  and vice versa, as measured by
, then  and  are perfectly predictive of each

   
   . If


other and should really be coded by a single symbol, whereas the MDL criterion may suggest merely that some association between the representation of  and that of  be estab

lished. In comparison, if  and  are not perfectly predictive of each other (
),
there is a case to be made in favor of coding them separately to allow for a maximally
expressive representation, whereas MDL may actually suggest a high degree of association


  

  ). In this study we investigated whether the human
(if
visual system uses a criterion based on

 alongside MDL while learning (in an unsupervised manner) to represent composite objects.









	




	




	  








	

AB

Figure 1: Left: how many objects are contained in image  ? Without prior knowledge, a
reasonable answer, which embodies a holistic bias, should be ?one? (Gestalt effects, which
would suggest two convex ?blobs? [5], are beyond the scope of the present discussion).
Right: in this set of ten images,  appears five times as a whole; the other five times
a fragment wholly contained in 	 appears in isolation. This statistical fact provides
grounds for considering  to be composite, consisting of two fragments (call the upper
one  and the lower one  ), because 
 
, but 
  
.

 

     

To date, psychophysical explorations of the sensitivity of human subjects to stimulus statistics tended to concentrate on means (and sometimes variances) of the frequency of various
stimuli (e.g., [6]. One recent and notable exception is the work of Saffran et al. [7], who
showed that infants (and adults) can distinguish between ?words? (stable pairs of syllables
that recur in a continuous auditory stimulus stream) and non-words (syllables accidentally
paired with each other, the first of which comes from one ?word? and the second ? from
the following one). Thus, subjects can sense (and act upon) differences in transition probabilities between successive auditory stimuli. This finding has been recently replicated, with
infants as young as 2 months, in the visual sequence domain, using successive presentation
of simple geometric shapes with controlled transition probabilities [8]. Also in the visual
domain, Fiser and Aslin [9] presented subjects with geometrical shapes in various spatial
configurations, and found effects of conditional probabilities of shape co-occurrences, in a
task that required the subjects to decide in each trial which of two simultaneously presented
shapes was more familiar.
The present study was undertaken to investigate the relevance of the various notions of
statistical independence to the unsupervised learning of complex visual stimuli by human
subjects. Our experimental approach differs from that of [9] in several respects. First,
instead of explicitly judging shape familiarity, our subjects had to verify the presence of a
probe shape embedded in a target. This objective task, which produces a pattern of response
times, is arguably better suited to the investigation of internal representations involved in
object recognition than subjective judgment. Second, the estimation of familiarity requires
the subject to access in each trial the representations of all the objects seen in the experi-

ment; in our task, each trial involved just two objects (the probe and the target), potentially
sharpening the focus of the experimental approach. Third, our experiments tested the pre
 , and MDL, or Barlow?s
dictions of two distinct notions of stimulus independence:
ratio.


	

2 The psychophysical experiments
In two experiments, we presented stimuli composed of characters such as those in Figure 1
to nearly 100 subjects unfamiliar with the Chinese script. The conditional probabilities
of the appearance of individual characters were controlled. The experiments involved two
types of probe conditions: P TYPE=Fragment, or 
 (with
	 as the
 (with
	 as
reference condition), and P TYPE=Composite, or 	
reference). In this notation (see Figure 2, left),  and  are ?familiar? fragments with controlled minimum conditional probability

 , and   are novel (low-probability)
fragments.

  
 	 


	

  

Each of the two experiments consisted of a baseline phase, followed by training exposure
(unsupervised learning), followed in turn by the test phase (Figure 2, right). In the baseline
and test phases, the subjects had to indicate whether or not the probe was contained in the
target (a task previously used by Palmer [5]). In the intervening training phase, the subjects
merely watched the character triplets presented on the screen; to ensure their attention, the
subjects were asked to note the order in which the characters appeared.
V

ABZ

VW

baseline/test

ABZ

target

reference

mask
probe

A

ABZ

AB

ABZ
4

test

3
2
1

probe

target

probe

Fragment

target
unsupervised training

Composite

Figure 2: Left: illustration of the probe and target composition for the two levels of P TYPE
(Fragment and Composite). For convenience, the various categories of characters that
appeared in the experiment are annotated here by Latin letters:  ,  stand for characters
with controlled



   
   , and   stand for characters that
appeared only once throughout an experiment. In experiment 1, the training set was con



structed with
for some pairs, and
for others; in experiment 2,
Barlow?s suspicious coincidence ratio was also controlled. Right top: the structure of a
part verification trial (same for baseline and test phases). The probe stimulus was followed
by the target (each presented for
; a mask was shown before and after the target).
The subject had to indicate whether or not the former was contained in the latter (in this
example, the correct answer is yes). A sequence consisting of 64 trials like this one was
presented twice: before training (baseline phase) and after training (test phase). For ?positive? trials (i.e., probe contained in target), we looked at the S PEEDUP following training,


  ; negative trials were discarded. Right bottom: the
defined as 

structure of a training trial (the training phase, placed between baseline and test, consisted
of 80 such trials). The three components of the stimulus appeared one by one for
to make sure that the subject attended to each, then together for
. The subject was
required to note whether the sequence unfolded in a clockwise or counterclockwise order.

 
	  


	    






	

 	 


   



 
  
 


!   


 




The logic behind the psychophysical experiments rested on two premises. First, we knew
from earlier work [5] that a probe is detected faster if it is represented monolithically (that
is, considered to be a good ?object? in the Gestalt sense). Second, we hypothesized that a
composite stimulus would be treated as a monolithic object to the extent that its constituent
characters are predictable from each other, as measured by a high conditional probability,

 , and/or by a high suspicious coincidence ratio, . The main prediction following
from these premises is that the S PEEDUP (the difference in response time between baseline
and test phases) for a composite probe should reflect the mutual predictability of the probe?s
constituents in the training set. Thus, our hypothesis ? that statistics of co-occurrence
determine the constituents in terms of which structured objects are represented ? would
be supported if the S PEEDUP turns out to be larger for those composite probes whose
constituents tend to appear together in the training set. The experiments, therefore, hinged
on a comparison of the patterns of response times in the ?positive? trials (in which the
probe actually is embedded in the target; see Figure 2, left) before and after exposure to the
training set.


	

400
Composite
Fragment

analog of speedup

0.3

speedup, ms

300
200
100

Composite
Fragment

0.2
0.1
0

?0.1

0
0.4

minCP
0.6

0.8

?0.2
0.4

1

0.6

0.8

minCP
1

  

Figure 3: Left: unsupervised learning of statistically defined structure by human subjects,
 ). The dependent variable S PEED - UP is defined as the difference in
experiment 1 (
between baseline and test phases (least-squares estimates of means and standard errors,
computed by the LSMEANS option of SAS procedure MIXED [10]). The S PEED - UP for


composite probes (solid line) with
exceeded that in the other conditions by
. Right: the results of a simulation of experiment 1 by a model derived from
about
the one described in [4]. The model was exposed to the same 80 training images as the
human subjects. The difference of reconstruction errors for probe and target served as the
analog of RT; baseline measurements were conducted on half-trained networks.




	

   


 

2.1 Experiment 1
Fourteen subjects, none of them familiar with the Chinese writing system, participated in
this experiment in exchange for course credit. Among the stimuli, two characters 

  . Alternatively,  could
could be paired, in which case we had 
 
be unpaired, with 
 
, 
  
(in this experiment, we held the suspicious
coincidence ratio


  

  constant at   ). For the paired 



  
   
the minimum conditional probability
and the
two characters were perfectly predictable from each other, whereas for the unpaired 
, and they were not. In the latter case 	 probably should not be represented


as a whole.


	

  

 
    




 
 
 


	    



 

   

 
 

   
      ! 




As expected, we found the value of S PEED - UP to be strikingly different for composite
probes with
(
) compared to the other three conditions (about
);


see Figure 3, left. A mixed-effects repeated measures analysis of variance (SAS procedure
  
MIXED [10]) for S PEED - UP revealed a marginal effect of P TYPE (   
) and a significant interaction P TYPE 

 interaction (   
 
 ).


	

 
   !


	


	


 principle: S PEEDUP was generThis behavior conforms to the predictions of the
ally higher for composite probes, and disproportionately higher for composite probes with
. The subjects in experiment 1 proved to be sensitive to the



 measure
of independence in learning to associate object fragments together. Note that the suspi

 

    .
cious coincidence ratio was the same in both cases,

 over and above the (constant-valued) MDLThus, the visual system is sensitive to
related criterion, according to which the propensity to form a unified representation of two
fragments,  and  , should be determined by [1, 4].




	






	

200

200

150
100
50
0.8
minCP
minCP=0.5

150
100
50
0
0.4

1

250

250

200

200

speedup, ms

speedup, ms

0.6

150
100
50
0
0



r=8.33
250
speedup, ms

speedup, ms

r=1.13
250

0
0.4


	

5
r

0.6

0.8
minCP
minCP=1.0

1

150
100
50
0
0

10

5
r

  

10


	

Figure 4: Human subjects, experiment 2 (
 ). The effect of

 found in experiment 1 was modulated in a complicated fashion by the effect of the suspicious coincidence
ratio (see text for discussion).
2.2 Experiment 2


	


 together.
In the second experiment, we studied the effects of varying both and
Because these two quantities are related (through the Bayes theorem), they cannot be manipulated independently. To accommodate this constraint, some subjects saw two sets of
 , in the first sesstimuli, with



  and with





	

  

 


	

 

  


	

  

  


	

 

 

 and with
sion and other two sets, with






  ,
in the second session; for other subjects, the complementary combinations were used in
each session. Eighty one subjects unfamiliar with the Chinese script participated in this
experiment for course credit.

The results (Figure 4) showed that S PEEDUP was consistently higher for composite probes.
Thus, the association between probe constituents was strengthened by training in each of
the four conditions. S PEEDUP was also generally higher for the high suspicious coinci  , and disproportionately higher for composite probes in the
dence ratio case,
 case, indicating a complicated synergy between the two mea,


sures of dependence,

 and . A mixed-effects repeated measures analysis of variance (SAS procedure MIXED [10]) for S PEED - UP revealed significant main effects of
 ) and (    
  

P TYPE (   
), as well as
 ) and 
two significant two-way interactions, 

 (    

 ). There was also a marginal three-way interaction,
P TYPE (   
 


  P TYPE (     
 ).


   

	
 !  

   

	
 !       
       

	


	

 

!





        
  

The findings of these two psychophysical experiments can be summarized as follows: (1)
an individual complex visual shape (a Chinese character) is detected faster than a composite stimulus (a pair of such characters) when embedded in a 3-character scene, but this
advantage is narrowed with practice; (2) a composite attains an ?objecthood? status to the
extent that its constituents are predictable from each other, as measured either by the conditional probability,

 , or by the suspicious coincidence ratio, ; (3) for composites,
the strongest boost towards objecthood (measured by response speedup following unsuper
 is high and is low, or vice versa. The nature of
vised learning) is obtained when
this latter interaction is unclear, and needs further study.


	


	

3 An unsupervised learning model and a simulated experiment
The ability of our subjects to construct representations that reflect the probability of cooccurrence of complex shapes has been replicated by a pilot version of an unsupervised
learning model, derived from the work of [4]. The model (Figure 5) is based on the following observation: an auto-association network fed with a sequence of composite images
in which some fragment/location combinations are more likely than others develops a nonuniform spatial distribution of reconstruction errors. Specifically, smaller errors appear in
those locations where the image fragments recur. This information can be used to form a
spatial receptive field for the learning module, while the reconstruction error can signal its
relevance to the current input [11, 12].

    

In the simplified pilot model, the spatial receptive field (labeled in Figure 5, left, as ?relevance mask?) consists of four weights, one per quadrant:  
, 	
 
 
     . During
 the

    
,
unsupervised training, the weights are updated by setting  
where  is the reconstruction error in trial , and and are learning constants. In a
simulation of experiment 1, a separate module with its own four-weight ?receptive field?
was trained for each of the composite stimuli shown to the human subjects. 1 The Euclidean
distance between probe and target representations at the output of the model served as the
analog of response time, allowing us to compare the model?s performance with that of the
humans. We found the same differential effects of

 for Fragment and Composite probes in the real and simulated experiments; compare Figure 3, left (humans) with
Figure 3, right (model).










	

1

The full-fledged model, currently under development, will have a more flexible receptive field
structure, and will incorporate competitive learning among the modules.

input

error

input

adapt

?
erri
relevance
mask (RF)

ensemble of modules

auto?
associator

reconstructed

Figure 5: Left: the functional architecture of a fragment module. The module consists of
two adaptive components: a reconstruction network, and a relevance mask, which assigns
different weights to different input pixels. The mask modulates the input multiplicatively,
determining the module?s receptive field. Given a sequence of images, several such modules working in parallel learn to represent different categories of spatially localized patterns
(fragments) that recur in those images. The reconstruction error serves as an estimate of
the module?s ability to deal with the input ([11, 12]; in the error image, shown on the right,
white corresponds to high values). Right: the Chorus of Fragments (CoF) is a bank of
such fragment modules, each tuned to a particular shape category, appearing in a particular
location [13, 4].

4 Discussion
Human subjects have been previously shown to be able to acquire, through unsupervised
learning, sensitivity to transition probabilities between syllables of nonsense words [7] and
between digits [14], and to co-occurrence statistics of simple geometrical figures [9]. Our
results demonstrate that subjects can also learn (presumably without awareness; cf. [14]) to
treat combinations of complex visual patterns differentially, depending on the conditional
probabilities of the various combinations, accumulated during a short unsupervised training
session.

   
 


 

In our first experiment, the criterion of suspicious coincidence between the occurrences
and 
 
conditions: in each case, we
of  and  was met in both 
  
had

  
  
     . Yet, the subjects? behavior indicated a significant
holistic bias: the representation they form tends to be monolithic (	 ), unless imperfect
mutual predictability of the potential fragments ( and  ) provides support for representing them separately. We note that a similar holistic bias, operating in a setting where a
single encounter with a stimulus can make a difference, is found in language acquisition:
an infant faced with an unfamiliar word will assume it refers to the entire shape of the most
salient object [15]. In our second experiment, both the conditional probabilities as such,
and the suspicious coincidence ratio were found to have the predicted effects, yet these
two factors interacted in a complicated manner, which requires a further investigation.





Our current research focuses on (1) the elucidation of the manner in which subjects process
statistically structured data, (2) the development of the model of structure learning outlined
in the preceding section, and (3) an exploration of the implications of this body of work for
wider issues in vision, such as the computational phenomenology of scene perception [16].

References
[1] H. B. Barlow. Unsupervised learning. Neural Computation, 1:295?311, 1989.
[2] R. S. Zemel and G. E. Hinton. Developing population codes by minimizing description length. Neural Computation, 7:549?564, 1995.
[3] E. Bienenstock, S. Geman, and D. Potter. Compositionality, MDL priors, and object
recognition. In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, Neural Information
Processing Systems, volume 9. MIT Press, 1997.
[4] S. Edelman and N. Intrator. A productive, systematic framework for the representation
of visual structure. In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in
Neural Information Processing Systems 13, pages 10?16. MIT Press, 2001.
[5] S. E. Palmer. Hierarchical structure in perceptual representation. Cognitive Psychology, 9:441?474, 1977.
[6] M. J. Flannagan, L. S. Fried, and K. J. Holyoak. Distributional expectations and
the induction of category structure. Journal of Experimental Psychology: Learning,
Memory and Cognition, 12:241?256, 1986.
[7] J. R. Saffran, R. N. Aslin, and E. L. Newport. Statistical learning by 8-month-old
infants. Science, 274:1926?1928, 1996.
[8] N. Z. Kirkham, J. A. Slemmer, and S. P. Johnson. Visual statistical learning in infancy:
Evidence for a domain general learning mechanism. Cognition, -:?, 2002. in press.
[9] J. Fiser and R. N. Aslin. Unsupervised statistical learning of higher-order spatial
structures from visual scenes. Psychological Science, 6:499?504, 2001.
[10] SAS. User?s Guide, Version 8. SAS Institute Inc., Cary, NC, 1999.
[11] D. Pomerleau. Input reconstruction reliability estimation. In C. L. Giles, S. J. Hanson, and J. D. Cowan, editors, Advances in Neural Information Processing Systems,
volume 5, pages 279?286. Morgan Kaufmann Publishers, 1993.
[12] I. Stainvas and N. Intrator. Blurred face recognition via a hybrid network architecture.
In Proc. ICPR, volume 2, pages 809?812, 2000.
[13] S. Edelman and N. Intrator. (Coarse Coding of Shape Fragments) + (Retinotopy) 
Representation of Structure. Spatial Vision, 13:255?264, 2000.
[14] G. S. Berns, J. D. Cohen, and M. A. Mintun. Brain regions responsive to novelty in
the absence of awareness. Science, 276:1272?1276, 1997.
[15] B. Landau, L. B. Smith, and S. Jones. The importance of shape in early lexical
learning. Cognitive Development, 3:299?321, 1988.
[16] S. Edelman. Constraints on the nature of the neural representation of the visual world.
Trends in Cognitive Sciences, 6:?, 2002. in press.

"
1165,2001,Estimating Car Insurance Premia: a Case Study in High-Dimensional Data Inference,Abstract Missing,"Estimating Car Insurance Premia:
a Case Study in High-Dimensional Data
Inference

Nicolas Chapados, Yoshua Bengio, Pascal Vincent, Joumana
Ghosn, Charles Dugas, Ichiro Takeuchi, Linyan Meng
University of Montreal, dept. IRQ, CP 6128, Succ. Centre-Ville, Montreal, Qc, Canada, H3C3J7
{chapadosJbengioy,vincentp,ghosnJdugas,takeuchi,mengl}~iro.umontreal.ca

Abstract
Estimating insurance premia from data is a difficult regression
problem for several reasons: the large number of variables, many of
which are .discrete, and the very peculiar shape of the noise distribution, asymmetric with fat tails, with a large majority zeros and a
few unreliable and very large values. We compare several machine
learning methods for estimating insurance premia, and test them
on a large data base of car insurance policies. We find that function approximation methods that do not optimize a squared loss,
like Support Vector Machines regression, do not work well in this
context. Compared methods include decision trees and generalized
linear models. The best results are obtained with a mixture of
experts, which better identifies the least and most risky contracts,
and allows to reduce the median premium by charging more to the
most risky customers.

1

Introduction

The main mathematical problem faced by actuaries is that of estimating how much
each insurance contract is expected to cost. This conditional expected claim amount
is called the pure premium and it is the basis of the gross premium charged to the
insured. This expected value is conditionned on information available about the
insured and about the contract, which we call input profile here. This regression
problem is difficult for several reasons: large number of examples, -large number
variables (most of which are discrete and multi-valued), non-stationarity of the
distribution, and a conditional distribution of the dependent variable which is very
different from those usually encountered in typical applications .of machine learning
and function approximation. This distribution has a mass at zero: the vast majority
of the insurance contracts do not yield any claim. This distribution is also strongly
asymmetric and it has fat tails (on one side only, corresponding to the large claims).

In this paper we study and compare several learning algorithms along with methods
traditionally used by actuaries for setting insurance premia. The study is performed
on a large database of automobile insurance policies. The methods that were tried

are the following: the constant (unconditional) predictor as a benchmark, linear
regression, generalized linear models (McCullagh and NeIder, 1989), decision tree
models (CHAID (Kass, 1980)), support vector machine regression (Vapnik, 1998),
multi-layer neural networks, mixtures of neural network experts, and the current
premium structure of the insurance company.

In a variety of practical applications, we often find data distributions with an asymmetric heavy tail extending out towards more positive values. Modeling data with
such an asymmetric heavy-tail distribution is essentially difficult because outliers, which are sampled from the tail of the distribution, have a strong influence
on parameter estimation. When the distribution is symmetric (around the mean),
the problems caused by outliers can be reduced using robust estimation techniques
(Huber, 1982; F.R.Hampel et al., 1986; Rousseeuw and Leroy, 1987) which basically
intend to ignore or downweight outliers. Note that these techniques do not work
for an asymmetric distribution: most outliers are on the same side of the mean,
so downweighting them introduces a strong bias on its estimation: the conditional
expectation would be systematically underestimated.
There is another statistical difficulty, due to the large number of variables (mostly
discrete) and the fact that many interactions exist between them. Thus the traditional actuarial methods based on tabulating average claim amounts for combinations of values are quickly hurt by the curse of dimensionality, unless they
make hurtful independence assumptions (Bailey and Simon, 1960). Finally, there
is a computational difficulty: we had access to a large database of ~ 8 x 106 examples, and the training effort and numerical stability of some algorithms can be
burdensome for such a large number of training examples.
This paper is organized as follows: we start by describing the mathematical criteria
underlying insurance premia estimation (section 2), followed by a brief review of the
learning algorithms that we consider in this study, including our best-performing
mixture of positive-output neural networks (section 3). We then highlight our most
important experimental results (section 4), and in view of them conclude with an examination of the prospects for applying statistical learning algorithms to insurance
modeling (section 5).

2

Mathematical Objectives

The first goal of insurance premia modeling is to estimate the expected claim amount
for a given insurance contract for a future one-year period (here we consider that the
amount is 0 when no claim is filed). Let X E Rm denote the customer and contract
input profile, a vector representing all the information known about the customer
and the proposed insurance policy before the beginning of the contract. Let A E R+
denote the amount that the customer claims during the contract period; we shall
assume that A is non-negative. Our objective is to estimate this claim amount,
which is the pure premium Ppure of a given contract x: 1
Ppure(X)

== E[AIX == x].

(1)

The Precision Criterion. In practice, of course, we have no direct access to the
quantity (1), which we must estimate. One possible criterion is to seek the most
precise estimator, which minimizes the mean-squared error (MSE) over a data set
D == {(xl,a?)}r=l. Let P == {p(?;8)} be a function class parametrized by the
IThe pure premium is distinguished from the premium actually charged to the customer, which must account for the risk remaining with the insurer, the administrative
overhead, desired profit, and other business costs.

parameter vector (). The MSE criterion produces the most precise function (on
average) within the class, as measured with respect to D:
L

()*

= argm:n ~ L(P(Xi; (}) -

ai)2.

(2)

i=1

Is it an appropriate criterion and why? First one should note that if PI and P2 are
two estimators of E[AIX]' then the MSE criterion is a good indication of how close
they are to E[AIX], since by the law of iterated expectations,

E[(PI(X) - A)2] - E[(P2(X) - A)2]

== E[(PI(X) - E[AIX])2]

-E[(P2(X) - E[AIX])2],
and of course the expected MSE is minimized when p(X) == E[AIX].
The Fairness Criterion. However, in insurance policy pricing, the precision criterion is not the sole part of the picture; just as important is that the estimated
premia do not systematically discriminate against specific segments of the population. We call this objective the fairness criterion. We define the bias of the premia
b(P) to be the difference between the average premium and the average incurred
amount, in a given population P:
1
(3)
b(P) = 1FT
p(Xi) - ai,

L

(xi,ai)EP

where IPI denotes the cardinality of the set P, and p(.) is some premia estimation
function. A possible fairness criterion would be based on minimizing the norm of
the bias over every subpopulation Q of P. From a practical standpoint, such a
minimization would be extremely difficult to carry out. Furthermore, the bias over
small subpopulations is hard to estimate with statistical significance. We settle
instead for an approximation that gives good empirical results. After training a
model to minimize the MSE criterion (2), we define a finite number of disjoint
subsets (subpopulations) of the test set P, PkC P, P k n Pj:f;k == 0, and verify that
the absolute bias is not significantly different from zero. The subsets Pk can be
chosen at convenience; in our experiments, we considered 10 subsets of equal-size
delimited by the deciles of the test set premium distribution. In this way, we verify
that, for example, for the group of contracts with a premium between the 5th and
the 6th decile, the average premium matches the average claim amount.

3

Models Evaluated

An important requirement for any model of insurance premia is that it should produce positive premia: the company does not want to charge negative money to its
customers! To obtain positive outputs neural networks we have considered
using an exponential activation function at the output layer but this created numerical difficulties (when the argument of the exponential is large, the gradient is
huge). fustead, we have successfully used the ""softplus"" activation function (Dugas
et al., 2001):
softplus(s) == log(1 + e 8 )
where s is the weighted sum of an output neuron, and softplus(s) is the corresponding predicted premium. Note that this function is convex, monotone increasing, and
can be considered as a smooth version of the ""positive part"" function max(O, x).
The best model that we obtained is a mixture of experts in which the experts
are positive outputs neural networks. The gater network (Jacobs et al., 1991)
has softmax outputs to obtain positive w~ights summing to one.

X

10-3

Distribution of (claim - prediction) in each prediction quintile

2

1.8

1.6

1.4
1.2

0.8

0.6

0.4

0.2

oL-.._..I....=~-L-~-l...-_----L_----l-==:::=:~::=::::::r:::===:?==~
-3000

-2000

-1000

1000
2000
claim - prediction

3000

4000

5000

6000

Proportion of non-zero claims in each prediction quintile
0.25 r - - - r - - - - - - - , r - - - - - - - - - - , - - - - - - , . - - - - - - - - , - - - ,

0.15

0.1

0.05

3
quintile

Figure 1: A view of the conditional distribution of the claim amounts in the out-ofsample test set. Top: probability density of (claim amount - conditional expectation) for
5 quintiles of the conditional expectation, excluding zero-claim records. The mode moves
left for increasing conditional expectation quintiles. Bottom: proportion of non-zero claim
records per quintile of the prediction.

The mixture model was compared to other models. The constant model only
has intercepts as free parameters. The linear model corresponds to a ridge linear
regression (with weight decay chosen with the validation set). Generalized linear
models (GLM) estimate the conditional expectation from j(x) == eb+w1x with
parameters b and w. Again weight decay is used and tuned on the validation set.
There are many variants of GLMs and they are popular for building insurance
models, since they provide positive outputs, interpretable parameters, and can be
associated to parametric models of the noise.
Decision trees are also used by practitioners in the insurance industry, in particular
the CHAID-type models (Kass, 1980; Biggs, Ville and Suen, 1991), which use
statistical criteria for deciding how to split nodes and when to stop growing the tree.
We have compared our models with a CHAID implementation based on (Biggs, Ville
and Suen, 1991), adapted for regression purposes using a MANOVA analysis. The
threshold parameters were selected based on validation set MSE.
Regression Support Vector Machines (SVM) (Vapnik, 1998) were also evaluated

Mean-Squared Error

67.1192

.......................................................................................... :;....:-.:--'* .....

67.0851

....

-*""------

~~--:-~-:-~~: :.~:-.:- - -~-~--:.- -.~~--'---~ :
.. ..

. . -..

. . _..

..

Test

.

56.5744
Validation

56.5416
56.1108

.
Training

56.0743

Figure 2: MSE results for eight models. Models have been sorted in ascending order
of test results. The training, validation and test curves have been shifted closer together
for visualization purposes (the significant differences in MSE between the 3 sets are due
to ""outliers""). The out-of-sample test performance of the Mixture model is significantly
better than any of the other. Validation based model selection is confirmed on test results.
CondMean is a constructive greedy version of GLM.

but yielded disastrous results for two reasons: (1) SVM regression optimizes an L 1 like criterion that finds a solution close to the conditional median, whereas the
MSE criterion is minimized for the conditional mean, and because the distribution
is highly asymmetric the conditional median is far from the conditional mean; (2)
because the output variable is difficult to predict, the required number of support
vectors is huge, also yielding poor generalization. Since the median is actually 0
for our data, we tried to train the SVM using only the cases with positive claim
amounts, and compared the performance to that obtained with the GLM and the
neural network. The SVM is still way off the mark because of the above two reasons.
Figure 1 (top) illustrates the fat tails and asymetry of the conditional distribution
of the claim amounts.
.
Finally, we compared the best statistical model with a proprietary table-based and
rule-based premium estimation method that was provided to us as the benchmark
against which to judge improvements.

4

Experimental Results

Data from five kinds of losses were included in the study (Le. a sub-premium was
estimated for each type of loss), but we report mostly aggregated results showing
the error on the total estimated premium. The input variables contain information
about the policy (e.g., the date to deal with inflation, deductibles and options), the
car, and the driver (e.g., about past claims, past infractions, etc...). Most variables
are subject to discretization and binning. Whenever possible, the bins are chosen
such that they contain approximately the same number of observations. For most
models except CHAID, the discrete variables are one-hot encoded. The number of
input random variables is 39, all discrete except one, but using one-hot encoding this
results in an input vector x of length m == 266. An overall data set containing about

Table 1: Statistical comparison of the prediction accuracy difference between several
individual learning models and the best Mixture model. The p-value is given under the
null hypothesis oino difference between Model #1 and the best Mixture model. Note that
all differences are statistically significant.
Model #1
Model #2
Constant
Mixture
CHAID
Mixture
GLM
Mixture
Softplus NN Mixture
Linear
Mixture
Mixture
NN

Mean MSE Diff.
3.40709e-02
2.35891e-02
7.54013e-03
6.71066e-03
5.82350e-03
5.23885e-03

Std. Error
3.32724e-03
2.57762e-03
1.15020e-03
1.09351e-03
1.32211e-03
1.41112e-03

Z

p-value

10.2400
9.1515
6.5555
6.1368
4.4047
3.7125

0
0
2.77e-ll
4.21e-l0
5.30e-06
1.02e-04

Table 2: MSE difference between benchmark and Mixture models across the 5 claim
categories (kinds of losses) and the total claim amount. In all cases except category 1, the
IvIixture model is statistically significantly (p < 0.05) more precise than the benchmark
model.
Claim Category
(Kind of Loss)
Category 1
Category 2
Category 3
Category 4
Category 5
Total claim amount

MSE Difference
Benchmark minus Mixture
20669.53
1305.57
244.34
1057.51
1324.31
60187.60

95% Confidence Interval
Lower
Higher
(-4682.83 - 46021.89 )
(1032.76 1578.37 )
(6.12 482.55 )
(623.42 1491.60 )
(1077.95 1570.67 )
( 7743.96 - 112631.24)

8 million examples is randomly permuted and split into a training set, validation
set and test set, respectively of size 50%, 25% and 25% of the total. The validation
set is used to select among models (includi~g the choice of capacity), and th~ test
set is used for final statistical comparisons. Sample-wise paired statistical tests are
used to reduce the effect of huge per-sample variability.
Figure 1 is an attempt at capturing the shape of the conditional distribution of claim
amounts given input profiles, by considering the distributions of claim amounts in
different quantiles of the prediction (pure premium), on the test set. The top figure
excludes the point mass of zero claims and rather shows the difference between the
claim amount and the estimated conditional expectation (obtained with the mixture
model). The bottom histogram shows that the fraction of claims increases nicely
for the higher predicted pure premia.
Table 1 and Figure 2 summarize the comparison between the test MSE of the different tested models. NN is a neural network with linear output activation whereas
Softplus NNhas the softplus output activations. The Mixture is the mixture of softplus neural networks. This result identifies the mixture model with softplus neural
networks as the best-performing of the tested statistical models. Our conjecture is
that the mixture model works better because it is more robust to the effect of ""outliers"" (large claims). Classical robust regression methods (Rousseeuw and Leroy,
1987) work by discarding or downweighting outliers: they cannot be applied here
because the claims distribution is highly asymmetric (the extreme values are always
large ones, the claims being all non-negative). Note that the capacity of each model
has been tuned on the validation set. Hence, e.g. CHAID could have easily yielded
lower training error, but at the price of worse generalization.

x10

4

Rule-Based minus UdeM Mixture

2,...-------,------,-----r-------.-------,-----.---------.----.......,
Mean = -1.5993e-1 a
Median = 37.5455
... Stddev = 154.65
-

1.5
~

o
c(])

:::l

0(])

u:
0.5

OL.-..----L.----L----.L.----..L---~~

-3000

-2500

-2000

-1500
-1000
-500
Difference between premia ($)

o

500

1000

Figure 3: The premia difference distribution is negatively skewed, but has a positive
m~dian for a mean of zero. This implies that the benchmark model (current pricing)
undercharges risky customers, while overcharging typical customers.

Table 2 shows a comparison of this model against the rule-based benchmark. The
improvements are shown across the five types of losses. In all cases the mixture
improves, and the improvement is significant in four out of the five as well as across
the sum of the five.
A qualitative analysis of the resulting predicted premia shows that the mixture
model has smoother and more spread-out premia than the benchmark. The analysis (figure 3) also reveals that the difference between the mixture premia and the
benchmark premia is negatively skewed, with a positive median, i.e., the typical customer will pay less under the new mixture model, but the ""bad"" (risky) customers
will pay much more.
To evaluate fairness, as discussed in the previous section, the distribution of premia computed by the best model is analyzed, splitting the contracts in 10 groups
according to their premium level. Figure 4 shows that the premia charged are fair
for each sub-population.

5

Conclusion

This paper illustrates a successful data-mining application in the insurance industry.
It shows that a specialized model (the mixture model), that was designed taking
into consideration the specific problem posed by the data (outliers, asymmetric distribution, positive outputs), performs significantly better than existing and popular
learning algorithms. It also shows that such models can significantly improve over
the current practice, allowing to compute premia that are lower for less risky contracts and higher for more risky contracts, thereby reducing the cost of the median
contract.
Future work should investigate in more detail the role of temporal pon-stationarity,
how to optimize fairness (rather than just test for it afterwards), and how to further
increase the robustness of the model with respect to large claim amounts.

Difference with incurred claims (sum of all KOL-groups)

200

............ 0

?

..

..

.
0

0

.'

..

..

~
C/}

E

~

0

""'C

~

:5
""~ -200
-5

I

:?

:

:1'

:

?1????????? .\.

.

\

. .
.
.

.......:?
?

\

\

""?
CD

~ -400

CD

~

o
-600

-B- Mixture Model (normalized premia)

-* - Rule-Based Model (normalized premia)
2

4

6

8

10

Decile

Figure 4: We ensure fairness by comparing the average incurred amount and premia
within each decile of the premia distribution; both models are generally fair to subpopu1ations. The error bars denote 95% confidence intervals. The comparisqn is for the sum
of claim amounts over all 5 kinds of losses (KOL).

References
Bailey, R. A. and Simon, L. (1960). Two studies in automobile insurance ratemaking. ASTIN Bulletin, 1(4):192-217.
Biggs, D., Ville, B., and Suen, E. (1991). A method of choosing multiway partitions
for classification and decision trees. Journal of Applied Statistics, 18(1):49-62.
Dugas, C., Bengio, Y., Belisle, F., and Nadeau, C. (2001). Incorporating second
order functional? knowledge into learning algorithms. In Leen, T., Dietterich,
T., and Tresp, V., editors, Advances in Neural Information Processing Systems,
volume 13, pages 472-478.
F.R.Hampel, E.M.Ronchetti, P.J.Rousseeuw, and W.A.Stahel (1986). Robust
Statistics, The Approach based on Influence Functions. John Wiley & Sons.
Huber, P. (1982). Robust Statistics. John Wiley & Sons Inc.
Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. (1991). Adaptive
mixture of local experts. Neural Computation, 3:79-87.
Kass, G. (1980). An exploratory technique for investigating large quantities of
categorical data. Applied Statistics, 29(2):119-127.
McCullagh, P. and NeIder, J. (1989). Generalized Linear Models. Chapman and
Hall, London.
'
Rousseeuw, P. and Leroy, A. (1987). Robust Regression and Outlier Detection. John
Wiley & Sons Inc.
Vapnik, V. (1998). Statistical Learning Theory. Wiley, Lecture Notes in Economics
and Mathematical Systems, volume 454.

"
952,2000,Factored Semi-Tied Covariance Matrices,Abstract Missing,"Factored Semi-Tied Covariance Matrices

M.J.F. Gales
Cambridge University Engineering Department
Trumpington Street, Cambridge. CB2 IPZ
United Kingdom
mjfg@eng.cam.ac.uk

Abstract
A new form of covariance modelling for Gaussian mixture models and
hidden Markov models is presented. This is an extension to an efficient
form of covariance modelling used in speech recognition, semi-tied covariance matrices. In the standard form of semi-tied covariance matrices
the covariance matrix is decomposed into a highly shared decorrelating
transform and a component-specific diagonal covariance matrix. The use
of a factored decorrelating transform is presented in this paper. This factoring effectively increases the number of possible transforms without increasing the number of free parameters. Maximum likelihood estimation
schemes for all the model parameters are presented including the component/transform assignment, transform and component parameters. This
new model form is evaluated on a large vocabulary speech recognition
task. It is shown that using this factored form of covariance modelling
reduces the word error rate.

1 Introduction
A standard problem in machine learning is to how to efficiently model correlations in multidimensional data. Solutions should be efficient both in terms of number of model parameters and cost of the likelihood calculation. For speech recognition this is particularly
important due to the large number of Gaussian components used, typically in the tens of
thousands, and the relatively large dimensionality of the data, typically 30-60.
The following generative model has been used in speech recognition 1

X(T)
O(T)

W

=

F [

(1)

X~T)

]

(2)

where X(T) is the underlying speech signal, F is the observation transformation matrix, W
is generated by a hidden Markov model (HMM) with diagonal covariance matrix Gaussian
IThis describes the static version of the generative model. The more general version is described
by replacing equation 1 by x( T) = Cx( T - 1) + w.

mixture model (GMM) to model each state 2 and v is usually assumed to be generated by a
GMM, which is common to all HMMs. This differs from the static linear Gaussian models
presented in [7] in two important ways. First w is generated by either an HMM or GMM,
rather than a simple Gaussian distribution. The second difference is that the ""noise"" is
now restricted to the null space of the signal x (7). This type of system can be considered
to have two streams. The first stream, the n1 dimensions associated with X(7), is the set
of discriminating, useful, dimensions. The second stream, the n2 dimensions associated
with v, is the set of non-discriminating, nuisance, dimensions. Linear discriminant analysis (LDA) and heteroscedastic LDA (HLDA) [5] are both based on this form of generative
model. When the dimensionality of the nuisance dimensions is reduced to zero this generative model becomes equivalent to a semi-tied covariance matrix system [3] with a single,
global, semi-tied class.
This generative model has a clear advantage during recognition compared to the standard
linear Gaussian models [2] in the reduction in the computational cost of the likelihood
calculation. The likelihood for component m may be computed as 3
(

po

( ).
7

,IL

(m)

,

~(m) F) _
diag'

l(7) N((F-1)

- Idet(F)I

().

[1]07 , IL

(m)

,

~(m))
diag

(3)

where lL(m) is the n1 -dimensional mean and ~~~lg the diagonal covariance matrix of Gaussian component m. l (7) is the nuisance dimension likelihood which is independent of the
component being considered and only needs to be computed once for each time instance.
The initial normalisation term is only required during recognition when multiple transforms are used. The dominant cost is a diagonal Gaussian computation for each component, O(n1) per component. In contrast a scheme such as factor analysis (a covariance
modelling scheme from the linear Gaussian model in [7]) has a cost of O(ni) per component (assuming there are n1 factors). The disadvantage of this form of generative model is
that there is no simple expectation-maximisation (EM) [1] scheme for estimating the model
parameters. However, a simple iterative scheme is available [3].
For some tasks, such as speech recognition where there are many different ""sounds"" to be
recognised, it is unlikely that a single transform is sufficient to well model the data. To
reflect this there has been some work on using multiple feature-spaces [3, 2]. The standard approach for using multiple transforms is to assign each component, m, to a particular
transform, F( Tm). To simplify the description of the new scheme only modifications to the
semi-tied covariance matrix scheme, where the nuisance dimension is zero, are considered.
The generative model is modified to be 0(7) = F(T m )X(7), where Tm is the transform
class associated with the generating component, m, at time instance 7. The assignment
variable, T m , may either be determined by an ""expert"", for example using phonetic context
information, or it may be assigned in a maximum likelihood (ML) fashion [3]. Simply
2 Although it is not strictly necessary to use diagonal covariance matrices, tllese currently dominate
applications in speech recognition. w could also be generated by a simple GMM.
3This paper uses the following convention: capital bold letters refer to matrices e.g. A, bold
letters refer to vectors e.g. b, and scalars are not bold e.g. c. When referring to elements of a matrix
or vector subscripts are used e.g. ai is tlle ith row of matrix A, aij is tlle element of row i column
j of matrix A and bi is element i of vector b. Diagonal matrices are indicated by A diag . Where
multiple streams are used tllis is indicated, for example, by A[s], this is a n. x n matrix (n is tlle
dimensionality of tlle feature vector and n. is tlle size of stream 8). Where subsets of tlle diagonal
matrices are specified tlle matrices are square, e.g. Adiag[s] is ns x ns square diagonal matrix. AT
is tlle transpose of tlle matrix and det( A) is tlle determinant of the matrix.

increasing the number of transforms increases the number of model parameters to be estimated, hence reducing the robustness of the estimates. There is a corresponding increase in
the computational cost during recognition. In the limit there is a single transform per component, the standard full-covariance matrix case. The approach adopted in this paper is to
factor the transform into multiple streams. Each component can then use a different transform for each stream. Hence instead of using an assignment variable an assignment vector
is used. In order to maintain the efficient likelihood computation of equation 3, F(r)-l,
rather than F(r), must be factored into rows. This is a partitioning of the feature space into
a set of observation streams. In common with other factoring schemes this dramatically increases the effective number of transforms from which each component may select without
increasing the number of transform parameters. Though this paper only considers factoring
semi-tied covariance matrices the extension to the ""projection"" schemes presented in [2] is
straightforward.
This paper describes how to estimate the set of transforms and determine which subspaces
a particular component should use. The next section describes how to assign components
to transforms and, given this assignment, how to estimate the appropriate transforms . Some
initial experiments on a large vocabulary speech recognition task are presented in the following section.

2 Factored Semi-Tied Covariance Matrices
In order to factor semi-tied covariance matrices the inverse of the observation transformation for a component is broken into multiple streams. The feature space of each stream is
then determined by selecting from an inventory of possible transforms. Consider the case
where there are S streams. The effective full covariance matrix of component m, ~(m),
may be written as ~(m) = F(z(~)) ~(':') F(Z(~))T where the form of F(z(~)) is restricted
dlag

,

so that 4

(4)

and z(m) is the S-dimensional assignment vector for component m. The complete set of
model parameters, M, consists of the standard model parameters, the component means,

'... ,

Af~')} for each
variances, weights and, additionally, the set of transforms { Af~l
stream s (Rs is the number of transforms associated with stream s) and the assignment
vector z(m) for each component. Note that the semi-tied covariance matrix scheme is the
case when S = 1. The likelihood is efficiently estimated by storing transformed observations for each stream transform, i.e.
O(T).

Af;!

The model parameters are estimated using ML training on a labelled set of training data

o = {0(1), . .. , o(T)}. The likelihood of the training data may be written as
p(OIM) =

LIT (P(q(T)lq(T -1)) L w(m)p(O(T);IL(m),~g;lg'A(Z(~))))
E>

r

mE(}(r)

4A similar factorisation has also been proposed in [4].

(5)

where e is the set of all valid state sequences according to the transcription for the data,
q(T) is the state at time T of the current path, O(T) is the set of Gaussian components belonging to state q(T), and w(m) is the prior of componentm. Directly optimising equation 5
is a very large optimisation task, as there are typically millions of model parameters. Alternatively, as is common with standard HMM training, an EM-based approach is used. The
posterior probability of a particular component, m, generating the observation at a given
time instance is denoted as 'Ym (T). This may be simply found using the forward backward
algorithm [6] and the old set of model parameters M. The new set of model parameters
will be denoted as M. The estimation of the component priors and HMM transition matrices are estimated in the standard fashion [6]. Directly optimising the auxiliary function
for the model parameters is computationally expensive [3] and does not allow the embedding of the assignment process. Instead a simple iterative optimisation scheme is used as
follows :

1. Estimate the within class covariance matrix for each Gaussian component in the
system, W(m), using the values of 'Ym (T). Initialise the set of assignment vectors,

{z} = {Z(1), ... , Z(M)} and the set of transforms for each stream {A} =
A(Rt)
A(1)
A(RS)}
{A (1)
[1)""'""
[1) , ... , [8)""'""
[8)
.

2. Using the current estimates of the transforms and assignment vectors obtain the
ML estimate of the set of component specific diagonal covariance matrices incorporating the appropriate parameter tying as required. This set of parameters will
be denoted as

{t} = {~~~g""'""

~~~}.

3. Estimate the new set of transforms, { A }, using the current set of component covariance matrices { t

} and assignment vectors { Z }. The new auxiliary function

at this stage will be written as Q(M, M;

{t } , { z} ).

4. Update the set of assignment variables for each component { Z }, given the current
set of model transforms, { A }

.

5. Goto (2) until convergence, or an appropriate stopping criterion is satisfied. Oth-

{t}

erwise update
and the component means using the latest transforms and
assignment variables.

There are three distinct optimisation problems within this task. First the ML estimate of
the set of component specific diagonal covariance matrices is required. Second, the new
set of transforms must be estimated. Finally the new set of assignment vectors is required.
The ML estimates of the component specific variances (and means) under a transformation
is a standard problem, e.g. for the semi-tied case see [3] and is not described further. The
ML estimation of the transforms and assignment variables are described below.
The transforms are estimated in an iterative fashion. The proposed scheme is derived by
modifying the standard semi-tied covariance optimisation equation in [3]. A row by row

optimisation is used. Consider row i of stream p of transform r, a[;fi' the auxiliary function
may be written as (ignoring constant scalings and elements independent of a[;fi)

Q(M

M' {t}

"",

{z}) = """"
(3(m) log ((c(z(m?a(Z~~?T)2) _
L...J
[pj.
[pj.
m

(z(m?

L...J

[sj}

[sj}

8,r,j

L

w(m)
(m)2

m:{z~m)=r}

U diag[sjj

K(sr j ) =

and c[pji

"" "" a(r) .K(srj)a(r)T

L 'Ym(r)

(6)

T

is the cofactor of row i of stream p of transform A

(z(m?

(r)

. The gradient j [pji'

differentiating the auxiliary function with respect to a[;fi' is given by5
j(r). =

[pj.

(m)c(z~m?}

""""
L...J

m:{z~m)=r}

[pj.
{ 2 (3
(z(m? (r)T
C[pji

_ 2a(r).K(pri)
[pj.

(8)

a[pji

The main cost for computing the gradient is calculating the cofactors for each component.
Having computed the gradient the Hessian may also be simply calculated as
H(r) .

[pj.

=

(m) (z(m?T (z(m?}

""""
L...J

{ _2(3

m:{z~m)=r}

c [pji

c[pji

( (z(m?
c[pji

_ 2K(pri)

(r)T)2

(9)

a[pji

The Hessian is guaranteed to be negative definite so the Newton direction must head towards a maximum. At the t + 1th iteration
(r) (

a[pji

t+

1) _

-

(r) ()

t -

a[pji

where the gradient and Hessian are based on the
estimation scheme was highly stable.

j(r) H(r)-l

[pji

tth

(10)

[Pji

parameter estimates. In practice this

The assignment for stream s of component m is found using a greedy search technique
based on ML estimation. Stream s of component m is assigned using

(A (u(,rm?) 12
)
Idet ( diag (A[;i
A[;t) ) I
Idet

z(m) s

-

arg max { (

}
(11)

W(m)

rER,

where the hypothesised assignment of factor stream s, u(srm), is given by
(srm) _ {

uj

-

r,
z~m),

j =s
(otherwise)

-------------------------

5When the standard semi-tied system is used (i.e. S
form solution
(r) _
(r) K(lri)-l
a[l]i - C[l ]i

(12)

= 1) the estimation of row, i has the closed

(Lm:{zim)=r} f3(m))
(r) K(lri)-l (r)T
C[l]i
C[l]i

(7)

As the assignment is dependent on the cofactors, which themselves are dependent on the
other stream assignments for that component, an iterative scheme is required. In practice
this was found to converge rapidly.

3 Results and Discussion
An initial investigation of the use of factored semi-tied covariance matrices was carried
out on a large-vocabulary speaker-independent continuous-speech recognition task. The
recognition experiments were performed on the 1994 ARPA Hub 1 data (the HI task), an
unlimited vocabulary task. The results were averaged over the development and evaluation
data. Note that no tuning on the ""development"" data was performed. The baseline system used for the recognition task was a gender-independent cross-word-triphone mixtureGaussian tied-state HMM system. For details of the system see [8]. The total number of
phones (counting silence as a separate phone) was 46, from which 6399 distinct context
states were formed. The speech was parameterised into a 39-dimensional feature vector.
The set of baseline experiments with semi-tied covariance matrices (8 = 1) used ""expert""
knowledge to determine the transform classes. Two sets were used. The first was based
on phone level transforms where all components of all states from the same phone shared
the same class (phone classes). The second used an individual transform per state (state
classes). In addition a global transform (global class) and a full-covariance matrix system
(comp class) were tested. Two systems were examined, a four Gaussian components per
state system and a twelve Gaussian component system. The twelve component system is
the standard system described in [8]. In both cases a diagonal covariance matrix system (labelled none) was generated in the standard HTK fashion [9]. These systems were then used
to generate the initial alignments to build the semi-tied systems. An additional iteration of
Baum-We1ch estimation was then performed.
Three forms of assignment training were compared. The previously described expert system and two ML-based schemes, standard andfactored. The standard scheme used a single
stream (8 = 1) which is similar to the scheme described in [3]. The factored scheme used
the new approach described in this paper with a separate stream for each of the elements of
the feature vector (8 = 39).
Table 1: System performance on the 1994 ARPA HI task
Assignment
Scheme
none
global
phone
state
comp
phone
phone

-

expert
expert
-

standard
factored

10.34
10.04
9.20
9.22
9.73
9.48

8.87
8.86
8.84
9.98
8.62
8.42

The results of the baseline semi-tied covariance matrix systems are shown in table 1. For the
four component system the full covariance matrix system achieved approximately the same
performance as that of the expert state semi-tied system. Both systems significantly (at the

95% level) outperformed the standard 12-component system (9.71 %). The expert phone
system shows around an 9% degradation in performance compared to the state system,
but used less than a hundredth of the number of transforms (46 versus 6399). Using the
standard ML assignment scheme with initial phone classes, S = 1, reduced the error rate
of the phone system by around 3% over the expert system. The factored scheme, S = 39,
achieved further reductions in error rate. A 5% reduction in word error rate was achieved
over the expert system, which is significant at the 95% level.
Table 1 also shows the performance of the twelve component system. The use of a global
semi-tied transform significantly reduced the error rate by around 9% relative. Increasing
the number of transforms using the expert assignment showed no reduction in error rate.
Again using the phone level system and training the component transform assignments,
either the standard or the factored schemes, reduced the word error rate. Using the factored
semi-tied transforms (S = 39) significantly reduced the error rate, by around 5%, compared
to the expert systems.

4 Conclusions
This paper has presented a new form of semi-tied covariance, the factored semi-tied covariance matrix. The theory for estimating these transforms has been developed and implemented on a large vocabulary speech recognition task. On this task the use of these
factored transforms was found to decrease the word error rate by around 5% over using a
single transform, or multiple transforms, where the assignments are expertly determined.
The improvement was significant at the 95% level. In future work the problems of determining the required number of transforms for each of the streams and how to determine the
appropriate dimensions will be investigated.

References
[1] A P Dempster, N M Laird, and D B Rubin. Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Society, 39:1-38, 1977.
[2] M J F Gales. Maximum likelihood multiple projection schemes for hidden Markov models. Technical Report CUEDIF-INFENGffR365, Cambridge University, 1999. Available via anonymous
ftp from: svr-ftp.eng.cam.ac.uk.
[3] M J F Gales. Semi-tied covariance matrices for hidden Markov models. IEEE Transactions
Speech and Audio Processing, 7:272-281, 1999.
[4] N K Goel and R Gopinath. Multiple linear transforms. In Proceedings ICASSP, 2001. To appear.
[5] N Kumar. Investigation of Silicon-Auditory Models and Generalization of Linear Discriminant
Analysisfor Improved Speech Recognition. PhD thesis, John Hopkins University, 1997.
[6] L R Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77, February 1989.
[7] S Roweiss and Z Ghahramani. A unifying review of linear Gaussian models. Neural Computation, 11:305-345, 1999.
[8] PC Woodland, J J Odell, V Valtchev, and S J Young. The development of the 1994 HTK large
vocabulary speech recognition system. In Proceedings ARPA Workshop on Spoken Language
Systems Technology, pages 104-109, 1995.
[9] S J Young, J Jansen, J Odell, D Ollason, and P Woodland. The HTK Book (for HTK Version 2.0).
Cambridge University, 1996.

"
6937,2017,When Cyclic Coordinate Descent Outperforms Randomized Coordinate Descent,"The coordinate descent (CD) method is a classical optimization algorithm that has seen a revival of interest because of its competitive performance in machine learning applications. A number of recent papers provided convergence rate estimates for their deterministic (cyclic) and randomized variants that differ in the selection of update coordinates. These estimates suggest randomized coordinate descent (RCD) performs better than cyclic coordinate descent (CCD), although numerical experiments do not provide clear justification for this comparison. In this paper, we provide examples and more generally problem classes for which CCD (or CD with any deterministic order) is faster than RCD in terms of asymptotic worst-case convergence. Furthermore, we provide lower and upper bounds on the amount of improvement on the rate of CCD relative to RCD, which depends on the deterministic order used. We also provide a characterization of the best deterministic order (that leads to the maximum improvement in convergence rate) in terms of the combinatorial properties of the Hessian matrix of the objective function.","When Cyclic Coordinate Descent Outperforms
Randomized Coordinate Descent
Mert G?rb?zbalaban?, Asuman Ozdaglar?, Pablo A. Parrilo?, N. Denizcan Vanli?
?
Rutgers University, mg1366@rutgers.edu
?
Massachusetts Institute of Technology, {asuman,parrilo,denizcan}@mit.edu

Abstract
The coordinate descent (CD) method is a classical optimization algorithm that
has seen a revival of interest because of its competitive performance in machine
learning applications. A number of recent papers provided convergence rate
estimates for their deterministic (cyclic) and randomized variants that differ in the
selection of update coordinates. These estimates suggest randomized coordinate
descent (RCD) performs better than cyclic coordinate descent (CCD), although
numerical experiments do not provide clear justification for this comparison. In this
paper, we provide examples and more generally problem classes for which CCD
(or CD with any deterministic order) is faster than RCD in terms of asymptotic
worst-case convergence. Furthermore, we provide lower and upper bounds on
the amount of improvement on the rate of CCD relative to RCD, which depends
on the deterministic order used. We also provide a characterization of the best
deterministic order (that leads to the maximum improvement in convergence rate)
in terms of the combinatorial properties of the Hessian matrix of the objective
function.

1

Introduction

We consider solving smooth convex optimization problems using the coordinate descent (CD) method.
The CD method is an iterative algorithm that performs (approximate) global minimizations with
respect to a single coordinate (or several coordinates in the case of block CD) in a sequential manner.
More specifically, at each iteration k, an index ik 2 {1, 2, . . . , n} is selected and the decision vector
is updated to approximately minimize the objective function in the ik -th coordinate [3, 4]. The CD
method can be deterministic or randomized depending on the choice of the update coordinates. If
the coordinate indices ik are chosen in a cyclic manner from the set {1, 2, . . . , n}, then the method
is called the cyclic coordinate descent (CCD) method. When ik is sampled uniformly from the set
{1, 2, . . . , n}, the resulting method is called the randomized coordinate descent (RCD) method.1
The CD method has a long history in optimization and its convergence has been studied extensively
in 80s and 90s (cf. [5, 12, 13, 18]). It has seen a resurgence of recent interest because of its
applicability and superior empirical performance in machine learning and large-scale data analysis
[7, 8]. Several recent influential papers established non-asymptotic convergence rate estimates under
various assumptions. Among these are Nesterov [15], which provided the first global non-asymptotic
convergence rates of RCD for convex and smooth problems (see also [11, 21, 22] for problems with
non-smooth terms), and Beck and Tetruashvili [1], which provided rate estimates for block coordinate
gradient descent method that yields rate results for CCD with exact minimization for quadratic
problems. Tighter rate estimates (with respect to [1]) for CCD are then presented in [23]. These rate
estimates suggest that CCD can be slower than RCD (precisely O(n2 ) times slower for quadratic

1
Note that there are other coordinate selection rules as well (such as the Gauss-Southwell rule [17]). However,
in this paper, we focus on cyclic and randomized rules.

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

problems, where n is the dimension of the problem), which is puzzling in view of the faster empirical
performance of CCD over RCD for various problems (e.g., see numerical examples in [1, 24]). This
gap was investigated in [24], which provided a quadratic problem that attains this performance gap.
In this paper, we investigate performance comparison of deterministic and randomized coordinate
descent and provide examples and more generally problem classes for which CCD (or CD with any
deterministic order) is faster than RCD in terms of asymptotic worst-case convergence. Furthermore,
we provide lower and upper bounds on the amount of improvement on the rate of deterministic CD
relative to RCD. The amount of improvement depends on the deterministic order used. We also
provide a characterization of the best deterministic order (that leads to the maximum improvement
in convergence rate) in terms of the combinatorial properties of the Hessian matrix of the objective
function.
In order to clarify the rate comparison between CCD and RCD, we focus on quadratic optimization
problems. In particular, we consider the problem2
min

x2Rn

1 T
x Ax,
2

(1)

where A is a positive definite matrix. We consider two problem classes: i) A is a 2-cyclic matrix,
whose formal definition is given in Definition 4.4, but an equivalent and insightful definition is the
bipartiteness of the graph induced by the matrix A D, where D is the diagonal part of A; ii) A
is an M-matrix, i.e., the off-diagonal entries of A are nonpositive. These matrices arise in a large
number of applications such as in inference in attractive Gaussian-Markov random fields [14] and in
minimization of quadratic forms of graph Laplacians (for which A = D W , where W
Pdenotes the
weighted adjacency matrix of the graph and D is the diagonal matrix given by Di,i = j Wi,j ), for
example in spectral partitioning [6] and semisupervised learning [2]. We build on the seminal work
of Young [27] and Varga [25] on the analysis of Gauss-Seidel method for solving linear systems of
equations (with matrices satisfying certain properties) and provide a novel analysis that allows us to
compare the asymptotic worst-case convergence rate of CCD and RCD for the aforementioned class
of problems and establish the faster performance of CCD with any deterministic order.
Outline: In the next section, we formally introduce the CCD and RCD methods. In Section 3, we
present the notion of asymptotic convergence rate to compare the CCD and RCD methods and provide
a motivating example for which CCD converges faster than RCD. In Section 4, we present classes of
problems for which the asymptotic convergence rate of CCD is faster than that of RCD. We provide
numerical experiments in Section 5 and concluding remarks in Section 6.
Notation: For a matrix H, we let Hi denote its ith row and Hi,j denote its entry at the ith row
and jth column. For a vector x, we let xi denote its ith entry. Throughout the paper, we reserve
superscripts for iteration counters of iterative algorithms and use x? to denote the optimal solution
of problem (1). For a vector x, kxk denotes its Euclidean norm and for a matrix H, ||H|| denotes
its operator norm. For matrices, and ? are entry-wise operators. The matrices I and 0 denote the
identity matrix and the zero matrix respectively and their dimensions can be understood from the
context.

2

Coordinate Descent Method

Starting from an initial point x0 2 Rn , the coordinate descent (CD) method, at each iteration k, picks
a coordinate of x, say ik , and updates the decision vector by performing exact minimization along
the ik th coordinate, which for problem (1) yields
xk+1 = xk

1
Aik ,ik

Ai k x k e i k ,

k = 0, 1, 2, . . . ,

(2)

where eik is the unit vector, whose ik th entry is 1 and the rest of its entries are 0. Note that this is a
special case of the coordinate gradient projection method (see [1]), which at each iteration updates a
single coordinate, say coordinate ik , along the gradient component direction (with the particular step
size of Ai 1,i ). The coordinate index ik can be selected according to a deterministic or randomized
k k
rule:
2

For ease of presentation, we consider minimization of 12 xT Ax, yet our results directly extend for problems
of the type 12 xT Ax bT x for any b 6= 0.

2

? When ik is chosen using the cyclic rule with order 1, . . . , n (i.e., ik = k (mod n) + 1), the
resulting algorithm is called the cyclic coordinate descent (CCD) method. In order to write
the CCD iterations in a matrix form, we introduce the following decomposition
A=D

L

LT ,

where D is the diagonal part of A and L is the strictly lower triangular part of A. Then,
over each epoch ` 0 (where an epoch is defined to be consecutive n iterations), the CCD
iterations given in (2) can be written as
(`+1)n

xCCD

= C x`n
CCD ,

where

C = (D

L)

1

LT .

(3)

Note that the epoch in (3) is equivalent to one iteration of the Gauss-Seidel (GS) method
applied to the first-order optimality condition of (1), i.e., applied to the linear system Ax = 0
[26].
? When ik is chosen at random among {1, . . . , n} with probabilities {p1 , . . . , pn } independently at each iteration k, the resulting algorithm is called the randomized coordinate descent
(RCD) method3 . Given the kth iterate generated by the RCD algorithm, i.e., xkRCD , we have
?
?
k
Ek xk+1
SD 1 A xkRCD ,
(4)
RCD | xRCD = I

where S = diag(p1 , . . . , pn ) contains the coordinate sampling probabilities and the conditional expectation Ek is taken over the random variable ik given xkRCD . Using the nested
property of the expectations, the RCD iterations in expectation over each epoch ` 0 satisfy
(`+1)n

ExRCD

3

= R Ex`n
RCD

with R := I

SD

1

A

n

.

(5)

Comparison of the Convergence Rates of CCD and RCD Methods

In the following subsection, we define our basis of comparison for rates of CCD and RCD methods.
To measure the performance of these methods, we use the notion of the average worst-case asymptotic
rate that has been studied extensively in the literature for characterizing the rate of iterative algorithms
[25]. In Section 3.2, we construct an example, for which the rate of CCD is more than twice the rate
of RCD. This raises the question whether the best known convergence rates of CCD in the literature
are tight or whether there exist a class of problems for which CCD provably attains better convergence
rates than the best known rates for RCD, a question which we will answer in Section 4.
3.1

Asymptotic Rate of Converge for Iterative Algorithms

Consider an iterative algorithm with update rule x(`+1)n = Cx`n (e.g., the CCD algorithm). The
reduction in the distance to the optimal solution of the iterates generated by this algorithm after `
epochs is given by
x`n x?
C ` (x0 x? )
=
.
(6)
0
?
||x
x ||
||x0 x? ||

Note that the right hand side of (6) can be as large as C ` , hence in the worst-case, the average
decay of distance at each epoch of this algorithm is C `
1/`

1/`

. Over any finite epochs `

1, we

1/`

have C
?(C) and C
! ?(C) as ` ! 1 by Gelfand?s formula. Thus, we define the
asymptotic worst-case convergence rate of an iterative algorithm (with iteration matrix C) as follows
!
x`n x?
1
Rate(CCD) := lim sup
log
= log (?(C)) .
(7)
`!1 x0 2Rn
`
||x0 x? ||
`

`

We emphasize that this notion has been used extensively for studying the performance of iterative
methods such as GS and Jacobi methods [5, 18, 25, 27]. Note that according to our definition in (7),
larger rate means faster algorithm and we will use these terms interchangably in throughout the paper.
3

sd

3

Analogously, for a randomized algorithm with expected update rule Ex(`+1)n = R Ex`n (e.g.,
the RCD algorithm), we consider the asymptotic convergence of the expected iterate error
E(x`n ) x? and define the asymptotic worst-case convergence rate as
!
E(x`n ) x?
1
Rate(RCD) := lim sup
log
= log (?(R)) ,
(8)
`!1 x0 2Rn
`
||x0 x? ||
Note that in (8), we use the distance of the expected iterates Ex`n x? as our convergence criterion. One can also use the expected distance (or the squared distance) of the iterates E x`n x?
as the convergence criterion, which is a stronger convergence criterion than the one in (8). This
follows since E x`n x?
Ex`n x? by Jensen?s inequality and any convergence rate on
`n
?
E x
x immediately implies at least the same convergence rate on Ex`n x? as well.
Since we consider the reciprocal case, i.e., obtain a convergence rate on Ex`n x? and show that
it is slower than that of CCD, our results naturally imply that the convergence rate on E x`n x?
is also slower than that of CCD.
3.2

A Motivating Example

In this section, we provide an example for which the (asymptotic worst-case convergence) rate of
CCD is better than the one of RCD and building on this example, in Section 4, we construct a class
of problems for which CCD attains a better rate than RCD. For some positive integer n 1, consider
the 2n ? 2n symmetric matrix
?
1 0
0n?n
A = I L LT , where L = 2 n?n
,
(9)
n 1n?n 0n?n

and 1n?n is the n ? n matrix with all entries equal to 1 and 0n?n is the n ? n zero matrix. Noting
that A has a special structure (A is equal to the sum of the identity matrix and the rank-two matrix
L LT ), it is easy to check that 1 1/n and 1 + 1/n are eigenvalues of A with the corresponding
T
T
11?n ] . The remaining 2n 2 eigenvalues of A are
eigenvectors [11?n 11?n ] and [11?n
equal to 1.
The iteration matrix of the CCD algorithm when applied to the problem in (1) with the matrix (9) can
be found as
?
1
0
1
C = n?n n12 n?n .
0n?n n3 1n?n

The eigenvalues of C are all zero except the eigenvalue of 1/n2 with the corresponding eigenvector
[n11?n , 11?n ]T . Therefore, ?(C) = 1/n2 and Rate(CCD) = log(?(C)) = 2 log n. On the other
hand, the spectral radius of the expected iteration matrix of RCD can be found as
?
?n
1
min (A)
?(R) = 1
1
,
min (A) =
n
n

which yields Rate(RCD) =

log(?(R)) ? log n. Thus, we conclude
Rate(CCD)
Rate(RCD)

2,

for all n

1.

That is, CCD is at least twice as fast as RCD in terms of the the asymptotic rate. This motivates us
to investigate if there exists a more general class of problems for which the asymptotic worst-case
rate of CCD is larger than that of RCD. The answer to this question turns out to be positive as we
describe in the following section.

4

When Deterministic Orders Outperform Randomized Sampling

In this section, we present special classes of problems (of the form (1)) for which the asymptotical
worst-case rate of CCD is larger than that of RCD. We begin our discussion by highlighting the main
assumption we will use in this section.
Assumption 4.1. A is a symmetric positive definite matrix whose smallest eigenvalue is ? and the
diagonal entries of A are 1.
4

If A is a positive semidefinite matrix, then our results will still hold, where ? is the smallest non-zero
eigenvalue of A and x? is the projection of x0 onto the null space of A. Moreover, given any
positive definite matrix A with diagonals D 6= I, the diagonal entries of the preconditioned matrix
D 1/2 AD 1/2 are 1. Therefore, Assumption 4.1 is mild. The relationship between the smallest
eigenvalue of the original matrix and the preconditioned matrix are as follows. Let > 0 and Lmax
denote the smallest eigenvalue and the largest diagonal entry of the original matrix, respectively.
Then, the smallest eigenvalue of the preconditioned matrix satisfies ?
/Lmax .
Remark 4.2. For the RCD algorithm, the coordinate index ik 2 {1, . . . , n} (at iteration k) can be
chosen using different probability distributions {p1 , . . . , pn }. Two common choices of distributions
A
are pi = n1 , for all i 2 {1, . . . , n} and pi = PN i,iA [15]. Since by Assumption 4.1, the diagonal
J=1

j,j

entries of A are 1, both of these distributions reduces to pi = n1 , for all i 2 {1, . . . , n}. Therefore,
in the rest of the paper, we consider the RCD algorithm with uniform and independent coordinate
selection at each iteration.
In the following lemma, we characterize the spectral radius of the RCD method. This worst-case
rate has been presented in many works in the literature for strongly convex optimization problems
[15, 26]. The proof is deferred to Appendix.
Lemma 4.3. Suppose Assumption 4.1 holds. Then, the spectral radius of the expected iteration
matrix R of the RCD algorithm (defined in (5)) is given by
?
? ?n
?(R) = 1
.
(10)
n
4.1

Convergence Rate of CCD for 2-Cyclic Matrices

In this section, we introduce the class of 2-cyclic matrices and show that the asymptotic worst-case
rate of CCD is more than two times faster than that of RCD.
Definition 4.4 (2-Cyclic Matrix). A matrix H is 2-cyclic if there exists a permutation matrix P such
that
?
0 B1
T
P HP = D +
,
(11)
B2 0
where the diagonal null submatrices are square and D is a diagonal matrix.
This definition can be interpreted as follows. Let H be a 2-cyclic matrix, i.e., H satisfies (11). Then,
the graph induced by the matrix H D is bipartite. The definition in (11) is first introduced in [27],
where it had an alternative name: Property A. A generalization of this property is later introduced by
Varga to the class of p-cyclic matrices [25] where p 2 can be arbitrary.
We next introduce the following definition that will be useful in Theorem 4.12 and explicitly identify
the class of matrices that satisfy this definition in Lemma 4.6.
Definition 4.5 (Consistently Ordered Matrix). For a matrix H, let H = HD HL HU be its
decomposition such that HD is a diagonal matrix, HL (and HU ) is a strictly lower (and upper)
triangular matrix. If the eigenvalues of the matrix ?HL + ?HU
HD are independent of ? for
any 2 R and ? 6= 0, then H is said to be consistently ordered.
Lemma 4.6. [27, Theorem 4.5] A matrix H is 2-cyclic if and only if there exists a permutation matrix
P such that P HP T is consistently ordered.
In the next theorem, we characterize the convergence rate of CCD algorithm applied to a 2-cyclic
matrix. Since ?(R) 1 ? by Lemma 4.3, the following theorem indicates that the spectral radius
of the CCD iteration matrix is smaller than ?2 (R).
Theorem 4.7. Suppose Assumption 4.1 holds and A is a consistently ordered 2-cyclic matrix. Then,
the spectral radius of the CCD algorithm is given by
?(C) = (1

2

?) .

Remark 4.8. Note that our motivating example in (9) is an example of a consistently ordered 2-cyclic
matrix, for which Theorem 4.7 is applicable. In particular, for the example in (9), we can apply
Theorem 4.7 with ? = 1 1/n leading to ?(C) = 1/n2 , which exactly coincides with our previous
computations of ?(C) in Section 3.2. We also give an example in Appendix F, for which CCD is twice
faster than RCD for any arbitrary initialization with probability one.
5

The following corollary states that the asymptotic worst-case rate of CCD is more than twice larger
than that of RCD for quadratic problems whose Hessian is a 2-cyclic matrix. This corollary directly
follows by Theorem 4.7 and definitions (7)-(8).
Corollary 4.9. Suppose Assumption 4.1 holds and A is a consistently ordered 2-cyclic matrix. Then,
the asymptotic worst-case rates of CCD and RCD satisfy
Rate(CCD)
= 2?n ,
Rate(RCD)

where ?n :=

log(1 ?)
.
n log 1 n?

(12)

In the following remark, we highlight several properties of the constant ?n .
Remark 4.10. ?n is a monotonically increasing function of n over the interval [1, 1), where ?1 = 1
?)
and limn!1 ?n = log(1
> 1. Furthermore, lim?!0+ ?n = 1.
?
We emphasize that the CCD method applied to 1 is equivalent to the Gauss-Seidel (GS) algorithm
applied to the linear system Ax = 0 and when A is a 2-cyclic matrix, the GS algorithm is twice as
fast as the Jacobi algorithm [25, 27]. Hence, when A is a 2-cyclic matrix and ? is sufficiently small,
the RCD method is approximately as fast as the Jacobi algorithm.
4.2

Convergence Rate of CCD for Irreducible M-Matrices

In this section, we first define the class of M-matrices and then present the convergence rate of the
CCD algorithm applied to quadratic problems whose Hessian is an M-matrix.
Definition 4.11 (M-matrix). A real matrix A with Ai,j ? 0 for all i 6= j is an M-matrix if A has the
decomposition A = sI B such that B 0 and s ?(B).
We emphasize that M-matrices arise in a variety of applications such as in belief propagation over
Gaussian graphical models [14] and in distributed control of positive systems [20]. Furthermore,
graph Laplacians are M-matrices, therefore solving linear systems with M-matrices (or equivalently
solving (1) for an M-matrix A) arise in a variety of applications for analyzing random walks over
graphs as well as distributed optimization and consensus problems over graphs (cf. [10] for a survey).
For quadratic problems, the Hessian is an M-matrix if and only if the gradient descent mapping is an
isotone operator [5, 22] and in Gaussian graphical models, M-matrices are often referred as attractive
models [14].
In the following theorem, we provide lower and upper bounds on the spectral radius of the iteration
matrix of CCD for quadratic problems, whose Hessian matrix is an irreducible M-matrix. In particular,
we show that the spectral radius of the iteration matrix of CCD is strictly smaller than that of RCD
for irreducible M-matrices.
Theorem 4.12. Suppose Assumption 4.1 holds, A is an irreducible M-matrix and n 2. Then, the
iteration matrix of the CCD algorithm C = (I L) 1 LT satisfies the following inequality
(1

?)2 ? ?(C) ?

1 ?
,
1+?

(13)

where the inequality on the left holds with equality if and only if A is a consistently ordered matrix.
An immediate consequence of Theorem 4.12 is that for quadratic problems whose Hessian is an
irreducible M-matrix, the best cyclic order that should be used in CCD can be characterized as
follows.
Remark 4.13. The standard CCD method follows the standard cyclic order (1, 2, . . . , n) as described
in Section 2. However, we can construct a CCD method that follows an alternative deterministic
order by considering a permutation ? of {1, 2, . . . , n}, and choosing the coordinates according to
the order (?(1), ?(2), . . . , ?(n)) instead. For any given order ?, (1) can be reformulated as follows
min

x? 2Rn

1 T
x A? x ? ,
2 ?

where

A? := P? AP?T

and x? = P? x,

where P? is the corresponding permutation matrix of ?. Supposing that Assumption 4.1 holds, the
corresponding CCD iterations for this problem can be written as follows
x(`+1)n
= C? x`n
?
? ,

where C? = (I
6

L? )

1

LT?

and L? = P? LP? .

If A is an irreducible M-matrix and satisfies Assumptions 4.1, then so does A? . Consequently,
Theorem 4.12 yields the same upper and lower bounds (in (13)) on ?(C? ) as well, i.e., the spectral
radius of the iteration matrix of CCD with any cyclic order ? satisfies
(1

?)2 ? ?(C? ) ?

1 ?
,
1+?

(14)

where the inequality on the left holds with equality if and only if A? is a consistently ordered matrix.
Therefore, if a consistent order ? ? exists, then the CCD method with the consistent order ? ? attains
the smallest spectral radius (or equivalently, the fastest asymptotic worst-case convergence rate)
among the CCD methods with any cyclic order.
Remark 4.14. The irreducibility of A is essential to derive the lower bound in (13) of Theorem 4.12.
However, the upper bound in (13) holds even when A is a reducible matrix.
We next compare the spectral radii bounds for CCD (given in Theorem 4.12) and RCD (given in
Lemma 4.3). Since ? > 0, the right-hand side of (13) can be relaxed to (1 ?)2 ? ?(C) < 1 ?.
A direct consequence of this inequality is the following corollary, which states that the asymptotic
worst-case rate of CCD is strictly better than that of RCD at least by a factor that is strictly greater
than 1.
Corollary 4.15. Suppose Assumption 4.1 holds, A is an irreducible M-matrix and n 2. Then, the
asymptotic worst-case rates of CCD and RCD satisfy
1 < ?n <

Rate(CCD)
? 2?n ,
Rate(RCD)

where

?n :=

log(1 ?)
,
n log 1 n?

(15)

and the inequality on the right holds with equality if and only if A is a consistently ordered matrix.
In the following corollary, we highlight that as the smallest eigenvalue of A goes to zero, the
asymptotic worst-case rate of the CCD algorithm becomes twice the asymptotic worst-case rate of
the RCD algorithm.
Corollary 4.16. Suppose Assumption 4.1 holds, A is an irreducible M-matrix and n 2. Then, we
have
Rate(CCD)
lim
= 2.
?!0+ Rate(RCD)

5

Numerical Experiments

In this section, we compare the performance of CCD and RCD through numerical examples. First,
we consider the quadratic optimization problem in (1), where A is an n ? n matrix defined as follows
?
1
0
0
T
A = I L L , where L =
(16)
n
n
1
0 ,
?
n
2
2
and 1 n2 ? n2 is the n2 ? n2 matrix with all entries equal to 1. Here, it can be easily checked that A is a
consistently ordered, 2-cyclic matrix. By Theorem 4.7 and Corolloary 4.9, the asymptotic worst-case
convergence rate of CCD on this example is
2?n = 2

log(1 ?)
log(0.5)
? 2.77
? =
1
n log 1 n
50 log 1 200

(17)

times faster than that of RCD. This is illustrated in Figure 1 (left), where the distance to the optimal
solution is plotted in a logarithmic scale over epochs. Note that even if our results our asymptotic,
we see the same difference in performances on the early epochs (for small `). On the other hand,
when the matrix A is not consistently ordered, according to Theorem 4.12, CCD is still faster but
the difference in the convergence rates decreases with respect to the consistent ordering case. To
illustrate this, we need to generate an inconsistent ordering of the matrix A. For this goal, we generate
a permutation matrix P and replace A with AP := P AP T in the optimization problem (1) (This is
equivalent to solving the system AP x = 0) so that AP is not consistently ordered (We generate P
randomly and compute AP ). Figure 1 (right) shows that for this inconsistent ordering CCD is still
faster compared to RCD, but not as fast (the slope of the decay of error line in blue marker is less
steep) predicted by our theory.
7

Consistent Ordering, Worst-Case Initialization

Inconsistent Ordering, Worst-Case Initialization

?4

?6

x? ||

?2

?4

?6

?8

log ||x`

?2

x? ||

0

log ||x`

0

?8

?10

?10

CCD
RCD
Expected RCD

?12

?14

1

2

3

4

CCD
RCD
Expected RCD

?12

5
6
7
Number of Epochs `

8

9

?14

10

1

2

3

4

5
6
7
Number of Epochs `

8

9

10

Figure 1: Distance to the optimal solution of the iterates of CCD and RCD for the cyclic matrix in
(16) (left figure) and a randomly permuted version of the same matrix (right figure) where the y-axis
is on a logarithmic scale. The left (right) figure corresponds to the consistent (inconsistent) ordering
for the same quadratic optimization problem.
M-Matrix, Worst-Case Initialization

M-Matrix, Random Initialization

?4

?4
?

?2

||x` x? ||
||x0 x? ||

?2

?

0

||x` x? ||
||x0 x? ||

0

?

?
log

?6

log

?6

?8

?8

?10

?12

?10

CCD
RCD
Expected RCD
0

20

40
60
Number of Epochs `

80

?12

100

CCD
RCD
Expected RCD
0

20

40
60
Number of Epochs `

80

100

Figure 2: Distance to the optimal solution of the iterates of CCD and RCD for the M-matrix matrix
in (18) for the worst-case initialization (left figure) and a random initialization (right figure).
We next consider the case, where A is an irreducible positive definite M-matrix. In particular, we
consider the matrix
A = (1 + )I
1n?n ,
(18)
1
where 1n?n is the n ? n matrix with all entries equal to 1 as before and = n+5
. We set n = 100
and plot the performance of CCD and RCD methods for the quadratic problem defined by this matrix.
In Figure 2, we compare the convergence rate of CCD and RCD for an initial point that corresponds
to a worst-case (left figure) and for a random choice of an initial point (right figure). We conclude
that the asymptotic rate of CCD is faster than that of RCD demonstrating our results in Theorem 4.12
and Corolloary 4.15.

6

Conclusion

In this paper, we compare the CCD and RCD methods for quadratic problems, whose Hessian is
a 2-cyclic matrix or an M-matrix. We show by a novel analysis that for these classes of quadratic
problems, CCD is always faster than RCD in terms of the asymptotic worst-case rate. We also give a
characterization of the best cyclic order to use in the CCD algorithm for these classes of problems and
show that with the best cyclic order, CCD enjoys more than a twice faster asymptotic worst-case rate
with respect to RCD. We also provide numerical experiments that show the tightness of our results.

References
[1] A. Beck and L. Tetruashvili. On the convergence of block coordinate descent type methods. SIAM Journal
on Optimization, 23(4):2037?2060, 2013.
[2] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for learning
from labeled and unlabeled examples. Journal of Machine Learning Research, 7:2399?2434, 2006.

8

[3] D. P. Bertsekas. Nonlinear programming. Athena Scientific, 1999.
[4] D. P. Bertsekas. Convex Optimization Algorithms. Athena Scientific, 2015.
[5] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods. PrenticeHall, Inc., 1989.
[6] F. R. K. Chung. Spectral Graph Theory. American Mathematical Society, 1997.
[7] J. Friedman, T. Hastie, H. H?fling, and R. Tibshirani. Pathwise coordinate optimization. The Annals of
Applied Statistics, 1(2):302?332, 2007.
[8] J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models via coordinate
descent. Journal of Statistical Software, 33(1):1?22, 2010.
[9] J. F. C. Kingman. A convexity property of positive matrices. The Quarterly Journal of Mathematics,
12(1):283?284, 1961.
[10] S. J. Kirkland and M. Neumann. Group inverses of M-matrices and their applications. CRC Press, 2012.
[11] Z. Lu and L. Xiao. On the complexity analysis of randomized block-coordinate descent methods. Mathematical Programming, 152(1):615?642, 2015.
[12] Z.-Q. Luo and P. Tseng. On the convergence of the coordinate descent method for convex differentiable
minimization. Journal of Optimization Theory and Applications, 72(1):7?35, 1992.
[13] Z.-Q. Luo and P. Tseng. Error bounds and convergence analysis of feasible descent methods: a general
approach. Annals of Operations Research, 46(1):157?178, 1993.
[14] D. M. Malioutov, J. K. Johnson, and A. S. Willsky. Walk-sums and belief propagation in gaussian graphical
models. Journal of Machine Learning Research, 7:2031?2064, 2006.
[15] Y. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal
on Optimization, 22(2):341?362, 2012.
[16] Roger D. Nussbaum. Convexity and log convexity for the spectral radius. Linear Algebra and its
Applications, 73(Supplement C):59 ? 122, 1986.
[17] J. Nutini, M. Schmidt, I. H. Laradji, M. Friedlander, and H. Koepke. Coordinate descent converges faster
with the gauss-southwell rule than random selection. In Proceedings of the 32nd International Conference
on International Conference on Machine Learning, pages 1632?1641, 2015.
[18] J. Ortega and W. Rheinboldt. Iterative Solution of Nonlinear Equations in Several Variables. Society for
Industrial and Applied Mathematics, 2000.
[19] R. J. Plemmons. M-matrix characterizations.I?nonsingular m-matrices. Linear Algebra and its Applications, 18(2):175 ? 188, 1977.
[20] A. Rantzer. Distributed control of positive systems. ArXiv:1203.0047, 2014.
[21] P. Richt?rik and M. Tak??c. Parallel coordinate descent methods for big data optimization. Mathematical
Programming, 156(1):433?484, 2016.
[22] A. Saha and A. Tewari. On the nonasymptotic convergence of cyclic coordinate descent methods. SIAM
Journal on Optimization, 23(1):576?601, 2013.
[23] R. Sun and M. Hong. Improved iteration complexity bounds of cyclic block coordinate descent for convex
problems. In Advances in Neural Information Processing Systems 28, pages 1306?1314. 2015.
[24] R. Sun and Y. Ye. Worst-case Complexity of Cyclic Coordinate Descent: O(n2 ) Gap with Randomized
Version. ArXiv:1604.07130, 2016.
[25] R. S. Varga. Matrix iterative analysis. Springer Science & Business Media, 2009.
[26] S. J. Wright. Coordinate descent algorithms. Mathematical Programming, 151(1):3?34, 2015.
[27] D. M. Young. Iterative solution of large linear systems. Academic Press, 1971.

9

"
5598,2016,f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization,"Generative neural networks are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any $f$-divergence can be used for training generative neural networks. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.","f -GAN: Training Generative Neural Samplers using
Variational Divergence Minimization
Sebastian Nowozin, Botond Cseke, Ryota Tomioka
Machine Intelligence and Perception Group
Microsoft Research
{Sebastian.Nowozin, Botond.Cseke, ryoto}@microsoft.com

Abstract
Generative neural samplers are probabilistic models that implement sampling using
feedforward neural networks: they take a random input vector and produce a sample
from a probability distribution defined by the network weights. These models
are expressive and allow efficient computation of samples and derivatives, but
cannot be used for computing likelihoods or for marginalization. The generativeadversarial training method allows to train such models through the use of an
auxiliary discriminative neural network. We show that the generative-adversarial
approach is a special case of an existing more general variational divergence
estimation approach. We show that any f -divergence can be used for training
generative neural samplers. We discuss the benefits of various choices of divergence
functions on training complexity and the quality of the obtained generative models.

1

Introduction

Probabilistic generative models describe a probability distribution over a given domain X , for example
a distribution over natural language sentences, natural images, or recorded waveforms.
Given a generative model Q from a class Q of possible models we are generally interested in
performing one or multiple of the following operations:
? Sampling. Produce a sample from Q. By inspecting samples or calculating a function on
a set of samples we can obtain important insight into the distribution or solve decision
problems.
? Estimation. Given a set of iid samples {x1 , x2 , . . . , xn } from an unknown true distribution
P , find Q ? Q that best describes the true distribution.
? Point-wise likelihood evaluation. Given a sample x, evaluate the likelihood Q(x).
Generative-adversarial networks (GAN) in the form proposed by [10] are an expressive class of
generative models that allow exact sampling and approximate estimation. The model used in GAN is
simply a feedforward neural network which receives as input a vector of random numbers, sampled,
for example, from a uniform distribution. This random input is passed through each layer in the
network and the final layer produces the desired output, for example, an image. Clearly, sampling
from a GAN model is efficient because only one forward pass through the network is needed to
produce one exact sample.
Such probabilistic feedforward neural network models were first considered in [22] and [3], here we
call these models generative neural samplers. GAN is also of this type, as is the decoder model of
a variational autoencoder [18].
In the original GAN paper the authors show that it is possible to estimate neural samplers by
approximate minimization of the symmetric Jensen-Shannon divergence,
DJS (P kQ) = 12 DKL (P k 21 (P + Q)) + 12 DKL (Qk 12 (P + Q)),

(1)

where DKL denotes the Kullback-Leibler divergence. The key technique used in the GAN training
is that of introducing a second ?discriminator? neural networks which is optimized simultaneously.
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

Because DJS (P kQ) is a proper divergence measure between distributions this implies that the true
distribution P can be approximated well in case there are sufficient training samples and the model
class Q is rich enough to represent P .
In this work we show that the principle of GANs is more general and we can extend the variational
divergence estimation framework proposed by Nguyen et al. [25] to recover the GAN training
objective and generalize it to arbitrary f -divergences.
More concretely, we make the following contributions over the state-of-the-art:
? We derive the GAN training objectives for all f -divergences and provide as example
additional divergence functions, including the Kullback-Leibler and Pearson divergences.
? We simplify the saddle-point optimization procedure of Goodfellow et al. [10] and provide
a theoretical justification.
? We provide experimental insight into which divergence function is suitable for estimating
generative neural samplers for natural images.

2

Method

We first review the divergence estimation framework of Nguyen et al. [25] which is based on
f -divergences. We then extend this framework from divergence estimation to model estimation.
2.1

The f-divergence Family

Statistical divergences such as the well-known Kullback-Leibler divergence measure the difference
between two given probability distributions. A large class of different divergences are the so called
f -divergences [5, 21], also known as the Ali-Silvey distances [1]. Given two distributions P and Q
that possess, respectively, an absolutely continuous density function p and q with respect to a base
measure dx defined on the domain X , we define the f -divergence,


Z
p(x)
dx,
(2)
Df (P kQ) =
q(x)f
q(x)
X
where the generator function f : R+ ? R is a convex, lower-semicontinuous function satisfying
f (1) = 0. Different choices of f recover popular divergences as special cases in (2). We illustrate
common choices in Table 1. See supplementary material for more divergences and plots.
2.2

Variational Estimation of f -divergences

Nguyen et al. [25] derive a general variational method to estimate f -divergences given only samples
from P and Q. An equivalent result has also been derived by Reid and Williamson [28]. We will
extend these results from merely estimating a divergence for a fixed model to estimating model
parameters. We call this new method variational divergence minimization (VDM) and show that
generative-adversarial training is a special case of our VDM framework.
For completeness, we first provide a self-contained derivation of Nguyen et al?s divergence estimation
procedure. Every convex, lower-semicontinuous function f has a convex conjugate function f ? , also
known as Fenchel conjugate [15]. This function is defined as
f ? (t) = sup {ut ? f (u)} .

(3)

u?domf

The function f ? is again convex and lower-semicontinuous and the pair (f, f ? ) is dual to another
in the sense that f ?? = f . Therefore, we can also represent f as f (u) = supt?domf ? {tu ? f ? (t)}.
Nguyen et al. leverage the above variational representation of f in the definition of the f -divergence
to obtain a lower bound on the divergence,
R
n
o
?
Df (P kQ) = X q(x) sup
t p(x)
?
f
(t)
dx
q(x)
t?domf ?

? supT ?T

R
X

p(x) T (x) dx ?

R
X
?

q(x) f ? (T (x)) dx

= sup (Ex?P [T (x)] ? Ex?Q [f (T (x))]) ,
T ?T

2


(4)

Jensen-Shannon

Df (P kQ)
R
p(x) log p(x)
q(x) dx
R
q(x)
q(x) log p(x)
dx
R (q(x)?p(x))2
dx
p(x)
2
p
R p
p(x) ? q(x) dx
R
2p(x)
1
p(x) log p(x)+q(x)
+ q(x) log
2

GAN

R

Name
Kullback-Leibler
Reverse KL
Pearson ?2
Squared Hellinger

p(x) log

2p(x)
p(x)+q(x)

+ q(x) log

Generator f (u)

T ? (x)

u log u

p(x)
q(x)
q(x)
? p(x)
2( p(x)
? 1)
qq(x)
p(x)
( q(x) ? 1)
2p(x)
log p(x)+q(x)

1 + log

? log u
(u ? 1)2
?
2
( u ? 1)
2q(x)
p(x)+q(x)

2q(x)
p(x)+q(x)

dx

dx ? log(4)

?(u + 1) log

1+u
2

+ u log u

u log u ? (u + 1) log(u + 1)

log

?

q

q(x)
p(x)

p(x)
p(x)+q(x)

Table 1: List of f -divergences Df (P kQ) together with generator functions. Part of the list of divergences and
their generators is based on [26]. For all divergences we have f : domf ? R ? {+?}, where f is convex and
lower-semicontinuous. Also we have f (1) = 0 which ensures that Df (P kP ) = 0 for any distribution P . As
shown by [10] GAN is related to the Jensen-Shannon divergence through DGAN = 2DJS ? log(4).

where T is an arbitrary class of functions T : X ? R. The above derivation yields a lower bound
because the class of functions T may contain only a subset of all possible functions. By taking the
variation of the lower bound in (4) w.r.t. T , we find that under mild conditions on f [25], the bound
is tight for


p(x)
?
0
T (x) = f
,
(5)
q(x)
where f 0 denotes the first order derivative of f . This condition can serve as a guiding principle for
choosing f and designing the class of functions T . For example, the popular reverse Kullback-Leibler
divergence corresponds to f (u) = ? log(u) resulting in T ? (x) = ?q(x)/p(x), see Table 1.
We list common f -divergences in Table 1 and provide their Fenchel conjugates f ? and the domains domf ? in Table 2. We provide plots of the generator functions and their conjugates in the
supplementary materials.
2.3

Variational Divergence Minimization (VDM)

We now use the variational lower bound (4) on the f -divergence Df (P kQ) in order to estimate a
generative model Q given a true distribution P .
To this end, we follow the generative-adversarial approach [10] and use two neural networks, Q and
T . Q is our generative model, taking as input a random vector and outputting a sample of interest.
We parametrize Q through a vector ? and write Q? . T is our variational function, taking as input a
sample and returning a scalar. We parametrize T using a vector ? and write T? .
We can train a generative model Q? by finding a saddle-point of the following f -GAN objective
function, where we minimize with respect to ? and maximize with respect to ?,
F (?, ?) = Ex?P [T? (x)] ? Ex?Q? [f ? (T? (x))] .

(6)

To optimize (6) on a given finite training data set, we approximate the expectations using minibatch
samples. To approximate Ex?P [?] we sample B instances without replacement from the training set.
To approximate Ex?Q? [?] we sample B instances from the current generative model Q? .
2.4

Representation for the Variational Function

To apply the variational objective (6) for different f -divergences, we need to respect the domain
domf ? of the conjugate functions f ? . To this end, we assume that variational function T? is
represented in the form T? (x) = gf (V? (x)) and rewrite the saddle objective (6) as follows:
F (?, ?) = Ex?P [gf (V? (x))] + Ex?Q? [?f ? (gf (V? (x)))] ,

(7)

where V? : X ? R without any range constraints on the output, and gf : R ? domf ? is an output
activation function specific to the f -divergence used. In Table 2 we propose suitable output activation
functions for the various conjugate functions f ? and their domains.1 Although the choice of gf is
somewhat arbitrary, we choose all of them to be monotone increasing functions so that a large output
Note that for numerical implementation we recommend directly implementing the scalar function f ? (gf (?))
robustly instead of evaluating the two functions in sequence; see Figure 1.
1

3

Name

Output activation gf

domf ?

Conjugate f ? (t)

f 0 (1)

Kullback-Leibler (KL)
Reverse KL
Pearson ?2
Squared Hellinger
Jensen-Shannon
GAN

v
? exp(?v)
v
1 ? exp(?v)
log(2) ? log(1 + exp(?v))
? log(1 + exp(?v))

R
R?
R
t<1
t < log(2)
R?

exp(t ? 1)
?1 ? log(?t)
1 2
4t + t

1
?1
0
0
0
? log(2)

t
1?t

? log(2 ? exp(t))
? log(1 ? exp(t))

Table 2: Recommended final layer activation functions and critical variational function level defined by f 0 (1).
The critical value f 0 (1) can be interpreted as a classification threshold applied to T (x) to distinguish between
true and generated samples.
gf (v)

10

?f ? (gf (v))

10

KL
Reverse KL
Pearson ? 2

5

0

?5

?5

?4

?2

Squared Hellinger
Jensen-Shannon
GAN

5

0

?10
?6

KL
Reverse KL
Pearson ? 2

Squared Hellinger
Jensen-Shannon
GAN

0

2

4

?10
?6

6

?4

?2

0

2

4

6

Figure 1: The two terms in the saddle objective (7) are plotted as a function of the variational function V? (x).

V? (x) corresponds to the belief of the variational function that the sample x comes from the data
distribution P as in the GAN case; see Figure 1. It is also instructive to look at the second term
?f ? (gf (v)) in the saddle objective (7). This term is typically (except for the Pearson ?2 divergence)
a decreasing function of the output V? (x) favoring variational functions that output negative numbers
for samples from the generator.
We can see the GAN objective,
F (?, ?) = Ex?P [log D? (x)] + Ex?Q? [log(1 ? D? (x))] ,

(8)

as a special instance of (7) by identifying each terms in the expectations of (7) and (8). In particular,
choosing the last nonlinearity in the discriminator as the sigmoid D? (x) = 1/(1 + e?V? (x) ),
corresponds to output activation function is gf (v) = ? log(1 + e?v ); see Table 2.
2.5

Example: Univariate Mixture of Gaussians

To demonstrate the properties of the different f -divergences and to validate the variational divergence
estimation framework we perform an experiment similar to the one of [24].
Setup. We approximate a mixture of Gaussians by learning a Gaussian distribution. We represent our
model Q? using a linear function which receives a random z ? N (0, 1) and outputs G? (z) = ? + ?z,
where ? = (?, ?) are the two scalar parameters to be learned. For the variational function T? we use
a neural network with two hidden layers having 64 units each and tanh activations. We optimize the
objective F (?, ?) by using the single-step gradient method presented in Section 3. In each step we
sample batches of size 1024 from p(x) and p(z) and we use a step-size of ? = 0.01 for updating
both ? and ?. We compare the results to the best fit provided by the exact optimization of Df (P kQ? )
w.r.t. ?, which is feasible in this case by solving the required integrals in (2) numerically. We use
? (learned) and ?? (best fit) to distinguish the parameters sets used in these two approaches.
(?
? , ?)
Results. The left side of Table 3 shows the optimal divergence and objective values Df (P ||Q?? )
? as well as the corresponding (optimal) means and standard deviations. Note that the
and F (?
? , ?)
? There is a good
results are in line with the lower bound property, having Df (P ||Q?? ) ? F (?
? , ?).
correspondence between the gap in objectives and the difference between the fitted means and
standard deviations. The right side of Table 3 shows the results of the following experiment: (1) we
train T? and Q? using a particular divergence, then (2) we estimate the divergence and re-train T?
while keeping Q? fixed. As expected, Q? performs best on the divergence it was trained with. We
present further details and plots of the fitted Gaussians and variational functions in the supplementary
materials.
4

KL

KL-rev

JS

Jeffrey

Pearson

Df (P ||Q?? )
?
F (?,
? ?)

0.2831
0.2801

0.2480
0.2415

0.1280
0.1226

0.5705
0.5151

0.6457
0.6379

??
?
?

1.0100
1.0335

1.5782
1.5624

1.3070
1.2854

1.3218
1.2295

0.5737
0.6157

??
?
?

1.8308
1.8236

1.6319
1.6403

1.7542
1.7659

1.7034
1.8087

1.9274
1.9031

train \ test
KL
KL-rev
JS
Jeffrey
Pearson

KL

KL-rev

JS

Jeffrey

Pearson

0.2808
0.3518
0.2871
0.2869
0.2970

0.3423
0.2414
0.2760
0.2975
0.5466

0.1314
0.1228
0.1210
0.1247
0.1665

0.5447
0.5794
0.5260
0.5236
0.7085

0.7345
1.3974
0.92160
0.8849
0.648

Table 3: Gaussian approximation of a mixture of Gaussians. Left: optimal objectives, and the learned mean
?, ?
? ) (learned) and ?? = (?? , ? ? ) (best fit). Right: objective values to the true
and the standard deviation: ?? = (?
distribution for each trained model. For each divergence, the lowest objective function value is achieved by the
model that was trained for this divergence.

In summary, our results demonstrate that when the generative model is misspecified, the divergence
function used for estimation has a strong influence on which model is learned.

3

Algorithms for Variational Divergence Minimization (VDM)

We now discuss numerical methods to find saddle points of the objective (6). To this end, we
distinguish two methods; first, the alternating method originally proposed by Goodfellow et al. [10],
and second, a more direct single-step optimization procedure.
In our variational framework, the alternating gradient method can be described as a double-loop
method; the internal loop tightens the lower bound on the divergence, whereas the outer loop improves
the generator model. While the motivation for this method is plausible, in practice a popular choice is
taking a single step in the inner loop, requiring two backpropagation passes for one outer iteration.
Goodfellow et al. [10] provide a local convergence guarantee.
3.1

Single-Step Gradient Method

Motivated by the success of the alternating gradient method with a single inner step, we propose an
even simpler algorithm shown in Algorithm 1. The algorithm differs from the original one in that there
is no inner loop and the gradients with respect to ? and ? are computed in a single back-propagation.
Algorithm 1 Single-Step Gradient Method
1: function S INGLE S TEP G RADIENT I TERATION(P, ?t , ? t , B, ?)
2:
Sample XP = {x1 , . . . , xB } and XQ = {x01 , . . . , x0B }, from P and Q?t , respectively.
3:
Update: ? t+1 = ? t + ? ?? F (?t , ? t ).
4:
Update: ?t+1 = ?t ? ? ?? F (?t , ? t ).
5: end function

Analysis. Here we show that Algorithm 1 geometrically converges to a saddle point (?? , ? ? ) if
there is a neighborhood around the saddle point in which F is strongly convex in ? and strongly
concave in ?. These assumptions are similar to those made in [10]. Formally, we assume:
?? F (?? , ? ? ) = 0,

?? F (?? , ? ? ) = 0,
?

?2? F (?, ?)  ?I,

?2? F (?, ?)  ??I,

(9)

?

for (?, ?) in the neighborhood of (? , ? ). Note that although there could be many saddle points that
arise from the structure of deep networks [6], they would not qualify as the solution of our variational
framework under these assumptions.
For convenience, let?s define ? t = (?t , ? t ). Now the convergence of Algorithm 1 can be stated as
follows (the proof is given in the supplementary material):
Theorem 1. Suppose that there is a saddle point ? ? = (?? , ? ? ) with a neighborhood that satisfies
conditions (9). Moreover, we define J(?) = 12 k?F (?)k22 and assume that in the above neighborhood,
F is sufficiently smooth so that there is a constant L > 0 such that k?J(? 0 )??J(?)k2 ? Lk? 0 ??k2
for any ?, ? 0 in the neighborhood of ? ? . Then using the step-size ? = ?/L in Algorithm 1, we have

t
?2
J(? t ) ? 1 ?
J(? 0 ).
L
5

That is, the squared norm of the gradient ?F (?) decreases geometrically.
3.2

Practical Considerations

Here we discuss principled extensions of the heuristic proposed in [10] and real/fake statistics
discussed by Larsen and S?nderby2 . Furthermore we discuss practical advice that slightly deviate
from the principled viewpoint.
Goodfellow et al. [10] noticed that training GAN can be significantly sped up by maximizing
Ex?Q? [log D? (x)] instead of minimizing Ex?Q? [log (1 ? D? (x))] for updating the generator. In
the more general f -GAN Algorithm (1) this means that we replace line 4 with the update
?t+1 = ?t + ? ?? Ex?Q?t [gf (V?t (x))],

(10)

thereby maximizing the variational function output on the generated samples. We can show that this
transformation preserves the stationary point as follows (which is a generalization of the argument in
[10]): note that the only difference between the original direction (line 4) and (10) is the scalar factor
f ?0 (T? (x)), which is the derivative of the conjugate function f ? . Since f ?0 is the inverse of f 0 (see
Cor. 1.4.4, Chapter E, [15]), if T = T ? , using (5), we can see that this factor would be the density
ratio p(x)/q(x), which would be one at the stationary point. We found this transformation useful
also for other divergences. We found Adam [17] and gradient clipping to be useful especially in the
large scale experiment on the LSUN dataset.
The original implementation [10] of GANs3 and also Larsen and S?nderby monitor certain real and
fake statistics, which are defined as the true positive and true negative rates of the variational function
viewing it as a binary classifier. Since our output activation gf are all monotone, we can derive similar
statistics for any f -divergence by only changing the decision threshold. Due to the link between the
density ratio and the variational function (5), the threshold lies at f 0 (1) (see Table 2). That is, we
can interpret the output of the variational function as classifying the input x as a true sample if the
variational function T? (x) is larger than f 0 (1), and classifying it as a generator sample otherwise.

4

Experiments

We now train generative neural samplers based on VDM on the MNIST and LSUN datasets.
MNIST Digits. We use the MNIST training data set (60,000 samples, 28-by-28 pixel images) to
train the generator and variational function model proposed in [10] for various f -divergences. With
z ? Uniform100 (?1, 1) as input, the generator model has two linear layers each followed by batch
normalization and ReLU activation and a final linear layer followed by the sigmoid function. The
variational function V? (x) has three linear layers with exponential linear unit [4] in between. The
final activation is specific to each divergence and listed in Table 2. As in [27] we use Adam with a
learning rate of ? = 0.0002 and update weight ? = 0.5. We use a batchsize of 4096, sampled from
the training set without replacement, and train each model for one hour. We also compare against
variational autoencoders [18] with 20 latent dimensions.
Results and Discussion. We evaluate the performance using the kernel density estimation (Parzen
window) approach used in [10]. To this end, we sample 16k images from the model and estimate
a Parzen window estimator using an isotropic Gaussian kernel bandwidth using three fold cross
validation. The final density model is used to evaluate the average log-likelihood on the MNIST test
set (10k samples). We show the results in Table 4, and some samples from our models in Figure 2.
The use of the KDE approach to log-likelihood estimation has known deficiencies [33]. In particular,
for the dimensionality used in MNIST (d = 784) the number of model samples required to obtain
accurate log-likelihood estimates is infeasibly large. We found a large variability (up to 50 nats)
between multiple repetitions. As such the results are not entirely conclusive. We also trained the
same KDE estimator on the MNIST training set, achieving a significantly higher holdout likelihood.
However, it is reassuring to see that the model trained for the Kullback-Leibler divergence indeed
achieves a high holdout likelihood compared to the GAN model.
2
3

http://torch.ch/blog/2015/11/13/gan.html
Available at https://github.com/goodfeli/adversarial

6

Training divergence
Kullback-Leibler
Reverse Kullback-Leibler
Pearson ?2
Neyman ?2
Squared Hellinger
Jeffrey
Jensen-Shannon
GAN

KDE hLLi (nats)

? SEM

416
319
429
300
-708
-2101
367
305

5.62
8.36
5.53
8.33
18.1
29.9
8.19
8.97

445
502

5.36
5.99

Variational Autoencoder [18]
KDE MNIST train (60k)

Table 4: Kernel Density Estimation evaluation on the MNIST test data set. Each
KDE model is build from 16,384 samples from the learned generative model.
We report the mean log-likelihood on the MNIST test set (n = 10, 000) and the
standard error of the mean. The KDE MNIST result is using 60,000 MNIST
training images to fit a single KDE model.

Figure 2: MNIST model
samples trained using KL,
reverse KL, Hellinger,
Jensen from top to bottom.

LSUN Natural Images. Through the DCGAN work [27] the generative-adversarial approach has
shown real promise in generating natural looking images. Here we use the same architecture as as
in [27] and replace the GAN objective with our more general f -GAN objective.
We use the large scale LSUN database [35] of natural images of different categories. To illustrate
the different behaviors of different divergences we train the same model on the classroom category
of images, containing 168,103 images of classroom environments, rescaled and center-cropped to
96-by-96 pixels.
Setup. We use the generator architecture and training settings proposed in DCGAN [27]. The model
receives z ? Uniformdrand (?1, 1) and feeds it through one linear layer and three deconvolution
layers with batch normalization and ReLU activation in between. The variational function is the same
as the discriminator architecture in [27] and follows the structure of a convolutional neural network
with batch normalization, exponential linear units [4] and one final linear layer.
Results. Figure 3 shows 16 random samples from neural samplers trained using GAN, KL, and
squared Hellinger divergences. All three divergences produce equally realistic samples. Note that the
difference in the learned distribution Q? arise only when the generator model is not rich enough.

(a) GAN

(b) KL

(c) Squared Hellinger

Figure 3: Samples from three different divergences.

5

Related Work

We now discuss how our approach relates to existing work. Building generative models of real world
distributions is a fundamental goal of machine learning and much related work exists. We only
discuss work that applies to neural network models.
7

Mixture density networks [2] are neural networks which directly regress the parameters of a finite
parametric mixture model. When combined with a recurrent neural network this yields impressive
generative models of handwritten text [12].
NADE [19] and RNADE [34] perform a factorization of the output using a predefined and somewhat
arbitrary ordering of output dimensions. The resulting model samples one variable at a time conditioning on the entire history of past variables. These models provide tractable likelihood evaluations
and compelling results but it is unclear how to select the factorization order in many applications .
Diffusion probabilistic models [31] define a target distribution as a result of a learned diffusion
process which starts at a trivial known distribution. The learned model provides exact samples and
approximate log-likelihood evaluations.
Noise contrastive estimation (NCE) [14] is a method that estimates the parameters of unnormalized
probabilistic models by performing non-linear logistic regression to discriminate the data from
artificially generated noise. NCE can be viewed as a special case of GAN where the discriminator
is constrained to a specific form that depends on the model (logistic regression classifier) and the
generator (kept fixed) is providing the artificially generated noise (see supplementary material).
The generative neural sampler models of [22] and [3] did not provide satisfactory learning methods;
[22] used importance sampling and [3] expectation maximization. The main difference to GAN and
to our work really is in the learning objective, which is effective and computationally inexpensive.
Variational auto-encoders (VAE) [18, 29] are pairs of probabilistic encoder and decoder models
which map a sample to a latent representation and back, trained using a variational Bayesian learning
objective. The advantage of VAEs is in the encoder model which allows efficient inference from
observation to latent representation and overall they are a compelling alternative to f -GANs and
recent work has studied combinations of the two approaches [23]
As an alternative to the GAN training objective the work [20] and independently [7] considered the
use of the kernel maximum mean discrepancy (MMD) [13, 9] as a training objective for probabilistic
models. This objective is simpler to train compared to GAN models because there is no explicitly
represented variational function. However, it requires the choice of a kernel function and the reported
results so far seem slightly inferior compared to GAN. MMD is a particular instance of a larger class of
probability metrics [32] which all take the form D(P, Q) = supT ?T |Ex?P [T (x)] ? Ex?Q [T (x)]|,
where the function class T is chosen in a manner specific to the divergence. Beyond MMD other
popular metrics of this form are the total variation metric (also an f -divergence), the Wasserstein
distance, and the Kolmogorov distance.
A previous attempt to enable minimization of the KL-divergence in deep generative models is due to
Goodfellow et al. [11], where an approximation to the gradient of the KL divergence is derived.
In [16] another generalization of the GAN objective is proposed by using an alternative JensenShannon divergence that interpolates between the KL and the reverse KL divergence and has JensenShannon as its mid-point. We discuss this work in more detail in the supplementary materials.

6

Discussion

Generative neural samplers offer a powerful way to represent complex distributions without limiting
factorizing assumptions. However, while the purely generative neural samplers as used in this paper
are interesting their use is limited because after training they cannot be conditioned on observed data
and thus are unable to provide inferences.
We believe that in the future the true benefits of neural samplers for representing uncertainty will be
found in discriminative models and our presented methods extend readily to this case by providing
additional inputs to both the generator and variational function as in the conditional GAN model [8].
We hope that the practical difficulties of training with saddle point objectives are not an underlying
feature of the model but instead can be overcome with novel optimization algorithms. Further
investigations, such as [30], are needed to investigate and hopefully overcome these difficulties.
Acknowledgements. We thank Ferenc Husz?ar for discussions on the generative-adversarial approach.

8

References
[1] S. M. Ali and S. D. Silvey. A general class of coefficients of divergence of one distribution from another.
JRSS (B), pages 131?142, 1966.
[2] C. M. Bishop. Mixture density networks. Technical report, Aston University, 1994.
[3] C. M. Bishop, M. Svens?en, and C. K. I. Williams. GTM: The generative topographic mapping. Neural
Computation, 10(1):215?234, 1998.
[4] D. A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by exponential
linear units (ELUs). arXiv:1511.07289, 2015.
[5] I. Csisz?ar and P. C. Shields. Information theory and statistics: A tutorial. Foundations and Trends in
Communications and Information Theory, 1:417?528, 2004.
[6] Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the
saddle point problem in high-dimensional non-convex optimization. In NIPS, pages 2933?2941, 2014.
[7] G. K. Dziugaite, D. M. Roy, and Z. Ghahramani. Training generative neural networks via maximum mean
discrepancy optimization. In UAI, pages 258?267, 2015.
[8] J. Gauthier. Conditional generative adversarial nets for convolutional face generation. Class Project for
Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester 2014, 2014.
[9] T. Gneiting and A. E. Raftery. Strictly proper scoring rules, prediction, and estimation. JASA, 102(477):
359?378, 2007.
[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.
Generative adversarial nets. In NIPS, pages 2672?2680, 2014.
[11] I. J. Goodfellow. On distinguishability criteria for estimating generative models. In International Conference on Learning Representations (ICLR2015), 2015. arXiv:1412.6515.
[12] A. Graves. Generating sequences with recurrent neural networks. arXiv:1308.0850, 2013.
[13] A. Gretton, K. Fukumizu, C. H. Teo, L. Song, B. Sch?olkopf, and A. J. Smola. A kernel statistical test of
independence. In NIPS, pages 585?592, 2007.
[14] M. Gutmann and A. Hyv?arinen. Noise-contrastive estimation: A new estimation principle for unnormalized
statistical models. In AISTATS, pages 297?304, 2010.
[15] J. B. Hiriart-Urruty and C. Lemar?echal. Fundamentals of convex analysis. Springer, 2012.
[16] F. Husz?ar. How (not) to train your generative model: scheduled sampling, likelihood, adversary?
arXiv:1511.05101, 2015.
[17] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.
[18] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. arXiv:1402.0030, 2013.
[19] H. Larochelle and I. Murray. The neural autoregressive distribution estimator. In AISTATS, 2011.
[20] Y. Li, K. Swersky, and R. Zemel. Generative moment matching networks. In ICML, 2015.
[21] F. Liese and I. Vajda. On divergences and informations in statistics and information theory. Information
Theory, IEEE, 52(10):4394?4412, 2006.
[22] D. J. C. MacKay. Bayesian neural networks and density networks. Nucl. Instrum. Meth. A, 354(1):73?80,
1995.
[23] A. Makhzani, J. Shlens, N. Jaitly, and I. Goodfellow. Adversarial autoencoders. arXiv:1511.05644, 2015.
[24] T. Minka. Divergence measures and message passing. Technical report, Microsoft Research, 2005.
[25] X. Nguyen, M. J. Wainwright, and M. I. Jordan. Estimating divergence functionals and the likelihood ratio
by convex risk minimization. Information Theory, IEEE, 56(11):5847?5861, 2010.
[26] F. Nielsen and R. Nock. On the chi-square and higher-order chi distances for approximating f-divergences.
Signal Processing Letters, IEEE, 21(1):10?13, 2014.
[27] A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional
generative adversarial networks. arXiv:1511.06434, 2015.
[28] M. D. Reid and R. C. Williamson. Information, divergence and risk for binary experiments. Journal of
Machine Learning Research, 12(Mar):731?817, 2011.
[29] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in
deep generative models. In ICML, pages 1278?1286, 2014.
[30] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for
training GANs. In NIPS, 2016.
[31] J. Sohl-Dickstein, E. A. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using
non-equilibrium thermodynamics. ICML, pages 2256?2265, 2015.
[32] B. K. Sriperumbudur, A. Gretton, K. Fukumizu, B. Sch?olkopf, and G. Lanckriet. Hilbert space embeddings
and metrics on probability measures. JMLR, 11:1517?1561, 2010.
[33] L. Theis, A. v.d. Oord, and M. Bethge. A note on the evaluation of generative models. arXiv:1511.01844,
2015.
[34] B. Uria, I. Murray, and H. Larochelle. RNADE: The real-valued neural autoregressive density-estimator.
In NIPS, pages 2175?2183, 2013.
[35] F. Yu, Y. Zhang, S. Song, A. Seff, and J. Xiao. LSUN: Construction of a large-scale image dataset using
deep learning with humans in the loop. arXiv:1506.03365, 2015.

9

"
4025,2012,Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL,"Since its inception, the modus operandi of multi-task learning (MTL) has been to minimize the task-wise mean of the empirical risks.   We introduce a generalized loss-compositional paradigm for MTL that includes a spectrum of formulations as a subfamily. One endpoint of this spectrum is minimax MTL: a new MTL formulation that minimizes the maximum of the tasks' empirical risks. Via a certain relaxation of minimax MTL, we obtain a continuum of MTL formulations spanning minimax MTL and classical MTL. The full paradigm itself is loss-compositional, operating on the vector of empirical risks. It incorporates minimax MTL, its relaxations, and many new MTL formulations as special cases. We show theoretically that minimax MTL tends to avoid worst case outcomes on newly drawn test tasks in the learning to learn (LTL) test setting. The results of several MTL formulations on synthetic and real problems in the MTL and LTL test settings are encouraging.","Minimax Multi-Task Learning and a Generalized
Loss-Compositional Paradigm for MTL

Nishant A. Mehta? , Dongryeol Lee?, Alexander G. Gray?
niche@cc.gatech.edu, drselee@gmail.com, agray@cc.gatech.edu
?
College of Computing, Georgia Institute of Technology, Atlanta, GA 30332, USA
?
GE Global Research, Niskayuna, NY 12309, USA

Abstract
Since its inception, the modus operandi of multi-task learning (MTL) has been to
minimize the task-wise mean of the empirical risks. We introduce a generalized
loss-compositional paradigm for MTL that includes a spectrum of formulations as
a subfamily. One endpoint of this spectrum is minimax MTL: a new MTL formulation that minimizes the maximum of the tasks? empirical risks. Via a certain relaxation of minimax MTL, we obtain a continuum of MTL formulations spanning
minimax MTL and classical MTL. The full paradigm itself is loss-compositional,
operating on the vector of empirical risks. It incorporates minimax MTL, its relaxations, and many new MTL formulations as special cases. We show theoretically
that minimax MTL tends to avoid worst case outcomes on newly drawn test tasks
in the learning to learn (LTL) test setting. The results of several MTL formulations
on synthetic and real problems in the MTL and LTL test settings are encouraging.

1

Introduction

The essence of machine learning is to exploit what we observe in order to form accurate predictors
of what we cannot. A multi-task learning (MTL) algorithm learns an inductive bias to learn several
tasks together. MTL is incredibly pervasive in machine learning: it has natural connections to random effects models [15]; user preference prediction (including collaborative filtering) can be framed
as MTL [16]; multi-class classification admits the popular one-vs-all and all-pairs MTL reductions;
and MTL admits provably good learning in settings where single-task learning is hopeless [4, 12].
But if we see examples from a random set of tasks today, which of these tasks will matter tomorrow?
Not knowing in the present what challenges nature has in store for the future, a sensible strategy is
to mitigate the worst case by ensuring some minimum proficiency on each task.
Consider a simple learning scenario: A music preference prediction company is in the business of
predicting what 5-star ratings different users would assign to songs. At training time, the company learns a shared representation for predicting the users? song ratings by pooling together the
company?s limited data on each user?s preferences. Given this learned representation, a separate
predictor for each user can be trained very quickly. At test time, the environment draws a user
according to some (possibly randomized) rule and solicits from the company a prediction of that
user?s preference for a particular song. The environment may also ask for predictions about new
users, described by a few ratings each, and so the company must leverage its existing representation
to rapidly learn new predictors and produce ratings for these new users.
Classically, multi-task learning has sought to minimize the (regularized) sum of the empirical risks
over a set of tasks. In this way, classical MTL implicitly assumes that once the learner has been
trained, it will be tested on test tasks drawn uniformly at random from the empirical task distribution
of the training tasks. Notably, there are several reasons why classical MTL may not be ideal:
?

Work completed while at Georgia Institute of Technology

1

? While at training time the usual flavor of MTL commits to a fixed distribution over users (typically either uniform or proportional to the number of ratings available for each user), at test time
there is no guarantee what user distribution we will encounter. In fact, there may not exist any
fixed user distribution: the sequence of users for which ratings are elicited could be adversarial.
? Even in the case when the distribution over tasks is not adversarial, it may be in the interest of
the music preference prediction company to guarantee some minimum level of accuracy per user
in order to minimize negative feedback and a potential loss of business, rather than maximizing
the mean level of accuracy over all users.
? Whereas minimizing the average prediction error is very much a teleological endeavor, typically
at the expense of some locally egregious outcomes, minimizing the worst-case prediction error
respects a notion of fairness to all tasks (or people).
This work introduces minimax multi-task learning as a response to the above scenario.1 In addition, we cast a spectrum of multi-task learning. At one end of the spectrum lies minimax MTL,
and departing from this point progressively relaxes the ?hardness? of the maximum until full relaxation reaches the second endpoint and recovers classical MTL. We further sculpt a generalized
loss-compositional paradigm for MTL which includes this spectrum and several other new MTL
formulations. This paradigm equally applies to the problem of learning to learn (LTL), in which the
goal is to learn a hypothesis space from a set of training tasks such that this representation admits
good hypotheses on future tasks. In truth, MTL and LTL typically are handled equivalently at training time ? this work will be no exception ? and they diverge only in their test settings and hence
the learning theoretic inquiries they inspire.
Contributions. The first contribution of this work is to introduce minimax MTL and a continuum
of relaxations. Second, we introduce a generalized loss-compositional paradigm for MTL which
admits a number of new MTL formulations and also includes classical MTL as a special case.
Third, we empirically evaluate the performance of several MTL formulations from this paradigm
in the multi-task learning and learning to learn settings, under the task-wise maximum test risk and
task-wise mean test risk criteria, on four datasets (one synthetic, three real). Finally, Theorem 1
is the core theoretical contribution of this work and shows the following: If it is possible to obtain
maximum empirical risk across a set of training tasks below some level ?, then it is likely that the
maximum true risk obtained by the learner on a new task is bounded by roughly ?. Hence, if the goal
is to minimize the worst case outcome over new tasks, the theory suggests minimizing the maximum
of the empirical risks across the training tasks rather than their mean.
In the next section, we recall the settings of multi-task learning and learning to learn, formally
introduce minimax MTL, and motivate it theoretically. In Section 3, we introduce a continuously parametrized family of minimax MTL relaxations and the new generalized loss-compositional
paradigm. Section 4 presents an empirical evaluation of various MTL/LTL formulations with different models on four datasets. Finally, we close with a discussion.

2

Minimax multi-task learning

We begin with a promenade through the basic MTL and LTL setups, with an effort to abide by the
notation introduced by Baxter [4]. Throughout the rest of the paper, each labeled example (x, y)
will live in X ? Y for input instance x and label y. Typical choices of X include Rn or a compact
subset thereof, while Y typically is a compact subset of R or the binary {?1, 1}. In addition,
define a loss function ` : R ? Y ? R+ . For simplicity, this work considers `2 loss (squared loss)
`(y 0 , y) = (y 0 ? y)2 for regression and hinge loss `(y 0 , y) = max{0, 1 ? y 0 y} for classification.
MTL and LTL often are framed as applying an inductive bias to learn a common hypothesis space,
selected from a fixed family of hypothesis spaces, and thereafter learning from this hypothesis space
a hypothesis for each task observed at training time. It will be useful to formalize the various sets
and elements present in the preceding statement. Let H be a family of hypothesis spaces. Any
hypothesis space H ? H itself is a set of hypotheses; each hypothesis h ? H is a map h : X ? R.
1

Note that minimax MTL does not refer to the minimax estimators of statistical decision theory.

2

Learning to learn. In learning to learn, the goal is to achieve inductive transfer to learn the best
H from H. Unlike in MTL, there is a notion of an environment of tasks: an unknown probability
measure Q over a space of task probability measures P. The goal is to find the optimal representation
via the objective
inf EP ?Q inf E(x,y)?P `(y, h(x)).

H?H

h?H

(1)

In practice, T (unobservable) training task probability measures P1 , . . . , PT ? P are drawn iid from
Q, and from each task t a set of m examples are drawn iid from Pt .
Multi-task learning. Whereas in learning to learn there is a distribution over tasks, in multi-task
learning there is a fixed, finite set of tasks indexed by [T ] := {1, . . . , T }. Each task t ? [T ]
is coupled with a fixed but unknown probability measure Pt . Classically, the goal of MTL is to
minimize the expected loss at test time under the uniform distribution on [T ]:
1 X
inf
(2)
inf E(x,y)?Pt `(y, h(x)).
H?H T
h?H
t?[T ]

Notably, this objective is equivalent to (1) when Q is the uniform distribution on {P1 , . . . , PT }. In
terms of the data generation model, MTL differs from LTL since the tasks are fixed; however, just
as in LTL, from each task t a set of m examples are drawn iid from Pt .
2.1

Minimax MTL

A natural generalization of classical MTL results by introducing a prior distribution ? over the index
set of tasks [T ]. Given ?, the (idealized) objective of this generalized MTL is
inf Et?? inf E(x,y)?Pt `(y, h(x)),

H?H

h?H

(3)

given only the training data {(xt,1 , yt,1 ), . . . , (xt,m , yt,m )}t?[T ] . The classical MTL objective (2)
equals (3) when ? is taken to be the uniform prior over [T ]. We argue that in many instances, that
which is most relevant to minimize is not the expected error under a uniform distribution over tasks,
or even any pre-specified ?, but rather the expected error for the worst ?. We propose to minimize
the maximum error over tasks under an adversarial choice of ?, yielding the objective:
inf sup Et?? inf E(x,y)?Pt `(y, h(x)),

H?H ?

h?H

where the supremum is taken over the T -dimensional simplex. As the supremum (assuming it is
attained) is attained at an extreme point of the simplex, this objective is equivalent to
inf max inf E(x,y)?Pt `(y, h(x)).

H?H t?[T ] h?H

In practice, we approximate the true objective via a regularized form of the empirical objective
inf max inf

H?H t?[T ] h?H

m
X

`(yt,i , h(xt,i )).

i=1

In the next section, we motivate minimax MTL theoretically by showing that the worst-case performance on future tasks likely will not be much higher than the maximum of the empirical risks for
the training tasks. In this short paper, we restrict attention to the case of finite H.
2.2

A learning to learn bound for the maximum risk

In this subsection, we use the following notation. Let P (1) , . . . , P (T ) be probability measures drawn
iid from Q, and for t ? [T ] let z(t) be an m-sample (a sample of m points) from P (t) with corre(t)
sponding empirical measure Pm . Also, if P is a probability P
measure then P ` ? h := E`(y, h(x));
m
1
similarly, if Pm is an empirical measure, then Pm ` ? h := m
i=1 `(yi , h(xi )).
Our focus is the learning to learn setting with a minimax lens: when one learns a representation
H ? H from multiple training tasks and observes maximum empirical risk ?, we would like to
3

guarantee that H?s true risk on a newly drawn test task will be bounded by roughly ?. Such a goal is
in striking contrast to the classical emphasis of learning to learn, where the goal is to obtain bounds
on H?s expected true risk. Using H?s expected true risk and Markov?s inequality, Baxter [4, the
display prior to (25) ] showed that the probability that H?s true risk on a newly drawn test task is
above some level ? decays as the expected true risk over ?:
P


(t)
1
t?[T ] Pm ` ? ht + ?
T
Pr inf P ` ? h ? ? ?
(4)
h?H
?
where the size of ? is controlled by T , m, and the complexities of certain spaces.
The expected true risk is not of primary interest for controlling the tail of the (random) true risk,
and a more direct approach yields a much better bound. In this short paper we restrict the space of
representations H to be finite with cardinality C; in this case, the analysis is particularly simple and
illuminates the idea for proving the general case. The next theorem is the main result of this section:
Theorem 1. Let |H| = C, and let the loss ` be L-Lipschitz in its second argument and bounded by
B. Suppose T tasks P (1) , . . . , P (T ) are drawn iid from Q and from each task P (t) an iid m-sample
(t)
z(t) is drawn. Suppose there exists H ? H such that all t ? [T ] satisfy minh?H Pm ` ? h ? ?. Let
? be the empirical risk minimizer over the test
P be newly drawn probability measure from Q. Let h
m-sample. With probability at least 1 ? ? with respect to the random draw of the T tasks and their
T corresponding m-samples:
?
?
s
?
4?
2C
? > ? + 1 + 2L max Rm (H) + 8 log ? ? log ? + logdBe + log(T + 1) . (5)
Pr P ` ? h
H?H
?
T
m ?
T
In the above, Rm (H) is the Rademacher complexity of H (cf. [3]). Critically, in (5) the probability
of observing a task with high true risk decays with T , whereas in (4) the decay is independent of T .
Hence, when the goal is to minimize the probability of bad performance on future tasks uniformly,
this theorem motivates minimizing the maximum of the empirical risks as opposed to their mean.
For the proof of Theorem 1, first consider the singleton case H = {H1 }. Suppose that for ? fixed a
(t)
priori, the maximum of the empirical risks is bounded by ?, i.e. maxt?[T ] minh?H1 Pm ` ? h ? ?.
Let a new probability measure P drawn from Q correspond to a new test task. Suppose the probability of the event [minh?H1 Pm ` ? h > ?] is at least ?. Then the probability that ? bounds all T
empirical risks is at most (1 ? ?)T ? e?T ? . Hence, with probability at least 1 ? e?T ? :
Pr {minh?H1 Pm ` ? h > ?} ? ?.
A simple application of the union bound extends this result for finite H:
Lemma 1. Under the same conditions as Theorem 1, with probability at least 1 ? ?/2 with respect
to the random draw of the T tasks and their T corresponding m-samples:


log 2C
?
Pr min Pm ` ? h > ? ?
.
h?H
T
The bound in the lemma states a 1/T rate of decay for the probability that the empirical risk obtained
by H on a new task exceeds ?. Next, we relate this empirical risk to the true risk obtained by the
empirical risk minimizer. Note that at test time H is fixed and hence independent of any test msample. Then, from by now standard learning theory results of Bartlett and Mendelson [3]:
Lemma 2. Take loss ` as in Theorem 1. With probability at least 1 ? ?/2, for all h ? H uniformly:
p
P ` ? h ? Pm ` ? h + 2LRm (H) + (8 log(4/?))/m.
In particular, with high probability the true risk of the empirical risk minimizer is not much larger
than its empirical risk. Theorem 1 now follows from Lemmas 1 and 2 and a union bound over
? ? ? := {0, 1/T, 2/T, . . . , dBe}; note that mapping the observed maximum empirical risk ? to
min{? 0 ? ? | ? ? ? 0 } picks up the additional T1 term in (5).
In the next section, we introduce a loss-compositional paradigm for multi-task learning which includes as special cases minimax MTL and classical MTL.
4

3

A generalized loss-compositional paradigm for MTL

The paradigm can benefit from a bit of notation. Given a set of T tasks, we represent the empirical
Pm
risk for hypothesis ht ? H (? H) on task t ? [T ] as `?t (ht ) := i=1 `(yt,i , ht (xt,i )). Additionally
define a set of hypotheses for multiple tasks h := (h1 , . . . , hT ) ? HT and the vector of empirical
?
risks `(h)
:= (`?1 (h1 ), . . . , `?T (hT )).
With this notation set, the proposed loss-compositional paradigm encompasses any regularized minimization of a (typically convex) function ? : RT+ ? R+ of the empirical risks:


?
inf inf ? `(h)
+ ? (H, h) ,
(6)
H?H h?HT

where ?(?) : H ? ?H?H HT ? R+ is a regularizer.
`p MTL. One notable specialization that is still quite general is the case when ? is an `p -norm,
yielding `p MTL. This subfamily encompasses classical MTL and many new MTL formulations:
? Classical MTL as `1 MTL:

1 X ?
inf inf
`(ht ) + ? (H, h)
H?H h?HT T
t?[T ]

? Minimax MTL as `? MTL:

? t ) + ? (H, h)
inf inf max `(h
H?H h?HT t?[T ]


1 ?
k`(h)k1 + ? (H, h) .
T

? inf

inf

? inf


?
inf k`(h)k
? + ? (H, h) .

H?H h?HT

H?H h?HT

? A new formulation, `2 MTL:
1 X
 1/2


1 ?
? t) 2
`(h
+ ? (H, h) ? inf inf ? k`(h)k
inf inf
2 + ? (H, h) .
H?H h?HT
H?H h?HT T
T
t?[T ]
A natural question is why one might consider minimizing `p -norms of the empirical risks vector for
1 < p < ?, as in `2 MTL. The contour of the `1 -norm of the empirical risks evenly trades off
empirical risks between different tasks; however, it has been observed that overfitting often happens
near the end of learning, rather than the beginning [14]. More precisely, when the empirical risk is
high, the gradient of the empirical risk (taken with respect to the parameter (H, h)) is likely to have
positive inner product with the gradient of the true risk. Therefore, given a candidate solution with a
corresponding vector of empirical risks, a sensible strategy is to take a step in solution space which
places more emphasis on tasks with higher empirical risk. This strategy is particularly appropriate
when the class of learners has high capacity relative to the amount of available data. This observation
sets the foundation for an approach that minimizes norms of the empirical risks.
In this work, we also discuss an interesting subset of the loss-compositional paradigm which does
not fit into `p MTL; this subfamily embodies a continuum of relaxations of minimax MTL.
?-minimax MTL. In some cases, minimizing the maximum loss can exhibit certain disadvantages because the maximum loss is not robust to situations when a small fraction of the tasks are
fundamentally harder than the remaining tasks. Consider the case when the empirical risk for each
task in this small fraction can not be reduced below a level u. Rather than rigidly minimizing the
maximum loss, a more robust alternative is to minimize the maximize loss in a soft way. Intuitively, the idea is to ensure that most tasks have low empirical risk, but a small fraction of tasks are
permitted to have higher loss. We formalize this as ?-minimax MTL, via the relaxed objective:
n
o

1 X
minimize min b +
max{0, `?t (ht ) ? b} + ? (H, h) .
b?0
?
H?H,h?HT
t?[T ]

In the above, ? from the loss-compositional paradigm (6) is a variational function of the empirical
risks vector. The above optimization problem is equivalent to the perhaps more intuitive problem:

1 X
minimize
b+
?t + ? (H, h)
subject to `?t (ht ) ? b + ?t , t ? [T ].
T
?
H?H,h?H ,b?0,??0
t?[T ]

5

Here, b plays the role of the relaxed maximum, and each ?t ?s deviation from zero indicates the
deviation from the (loosely enforced) maximum. We expect ? to be sparse.
To help understand how ? affects the learning problem, let us consider a few cases:
(1) When ? > T , the optimal value of b is zero, and the problem is equivalent to classical MTL. To
see this, note that for a given candidate solution with b > 0 the objective always can be reduced
by reducing b by some ? and increasing each ?t by the same ?.
(2) Suppose one task is much harder than all the other tasks (e.g. an outlier task), and its empirical
risk is separated from the maximum empirical risk of the other tasks by ?. Let 1 < ? < 2; now,
at the optimal hard maximum solution (where ? = 0), the objective can be reduced by increasing
one of the ?t ?s by ? and decreasing b by ?. Thus, the objective can focus on minimizing the
maximum risk of the set of T ? 1 easier tasks. In this special setting, this argument can be
extended to the more general case k < ? < k + 1 and k outlier tasks, for k ? [T ].
(3) As ? approaches 0, we recover the hard maximum case of minimax MTL.
This work focuses on ?-minimax MTL with ? = 2/(d0.1T + 0.5e?1 + d0.1T + 1.5e?1 ) i.e. the
harmonic mean of d0.1T + 0.5e and d0.1T + 1.5e. The reason for this choice is that in the idealized
case (2) above, for large T this setting of ? makes the relaxed maximum consider all but the hardest
10% of the tasks. We also try the 20% level (i.e. 0.2T replacing 0.1T in the above).
Models. We now provide examples of how specific models fit into this framework. We consider
two convex multi-task learning formulations: Evgeniou and Pontil?s regularized multi-task learning
(the EP model) [5] and Argyriou, Evgeniou, and Pontil?s convex multi-task feature learning (the
AEP model) [1]. The EP model is a linear model with a shared parameter v0 ? Rd and task-specific
parameters vt ? Rd (for t ? [T ]). Evgeniou and Pontil presented this model as
P
Pm
P
minv0 ,{vt }t?[T ] t?[T ] i=1 `(yt,i , hv0 + vt , xt,i i) + ?0 kv0 k2 + ?T1 t?[T ] kvt k2 ,
for ` the hinge loss or squared loss. This can be set in the new paradigm via H = {Hv0 | v0 ? Rd },
Pm
1
Hv0 = {h : x 7? hv0 + vt , xi | vt ? Rd }, and `?t (ht ) = m
i=1 ` yt,i , hv0 + vt , xt,i i .
The AEP model minimizes the task-wise average loss with the trace norm (nuclear norm) penalty:
P Pm
minW t i=1 `(yt,i , hWt , xt,i i) + ?kW ktr ,
P
where k?ktr : W 7? i ?i (W ) is the trace norm. In the new paradigm, H is a set where each element
is a k-dimensional subspace of linear estimators (for k  d). Each ht = Wt in some H ? H lives
Pm
1
in H?s corresponding low-dimensional subspace. Also, `?t (ht ) = m
i=1 ` yt,i , hht , xt,i i .
For easy empirical comparison between the various MTL formulations from the paradigm, at times
it will be convenient to use constrained formulations of the EP and AEP model. If the regularized
forms are used, a fair comparison of the methods warrants plotting results according to the size of
the optimal parameter found (i.e. kW ktr for AEP). For EP, the constrained form is:
P
Pm
minv0 ,{vt }t?[T ] t?[T ] i=1 `(yt,i , hv0 + vt , xt,i i) subject to kv0 k ? ?0 , kvt k ? ?1 for t ? [T ].
P Pm
For AEP, the constrained form is: minW t i=1 `(yt,i , hWt , xt,i i) subject to kW ktr ? r.

4

Empirical evaluation

We consider four learning problems; the first three involve regression (MTL model in parentheses):
? A synthetic dataset composed from two modes of tasks (EP model),
? The school dataset from the Inner London Education Authority (EP model),
? The conjoint analysis personal computer ratings dataset 2 [11] (AEP model).
The fourth problem is multi-class classification from the MNIST digits dataset [10] with a reduction
to multi-task learning using a tournament of pairwise (binary) classifiers. We use the AEP model.
Given data, each problem involved a choice of MTL formulation (e.g. minimax MTL), model (EP
or AEP), and choice of regularized versus constrained. All the problems were solved with just a few
lines of code using CVX [9, 8]. In this work, we considered convex multi-task learning formulations
in order to make clear statements about the optimal solutions attained for various learning problems.
2

This data, collected at the University of Michigan MBA program, generously was provided by Peter Lenk.

6

250

250

200

200

400
350

100

squared?loss risk

squared?loss risk

squared?loss risk

300

150

150

100

250
200
150
100

50

50
50

0
?1

0

1

?noise

2

3

0
?1

4

0

1

?noise

2

3

0
?0.5

4

0

0.5

1
?task

1.5

2

2.5

Figure 1: Max `2 -risk (Top two lines) and mean `2 -risk (Bottom two lines). At Left and Center: `2 -risk vs
noise level, for ?task = 0.1 and ?task = 0.5 respectively. At Right: `2 -risk vs task variation, for ?noise = 0.1.
Dashed red is `1 , dashed blue is minimax. Error bars indicate one standard deviation. MTL results (not shown)
were similar to LTL results (shown), with MTL-LTL relative difference below 6.8% for all points plotted.

Two modes. The two modes regression problem consists of 50 linear prediction tasks for the first
type of task and 5 linear prediction tasks for the second task type. The true parameter for the first
task type is a vector ? drawn uniformly from the sphere of radius 5; the true parameter for the
second task type is ?2?. Each task is drawn from an isotropic Gaussian with mean taken from the
task type and the standard deviation of all dimensions set to ?task . Each data point for each task is
drawn from a product of 10 standard normals (so xt,i ? R10 ). The targets are generated according
to hWt , xt,i i + ?t , where the ?t ?s are iid univariate centered normals with standard deviation ?noise .
We fixed ?0 to a large value (in this case, ?0 = 10 is sufficient since the mean for the largest task
fits into a ball of radius 10) and ?1 to a small value (?1 = 2). We compute the average mean and
maximum test error over 100 instances of the 55-task multi-task problem. Each task?s training set
and test set are 5 and 15 points respectively. The average maximum (mean) test error is the 100experiment-average of the task-wise maximum (mean) of the `2 risks. For each LTL experiment, 55
new test tasks were drawn using the same ? as from the training tasks.
Figure 1 shows a tradeoff: when each task group is fairly homogeneous (left and center plots),
minimax is better at minimizing the maximum of the test risks while `1 is better at minimizing the
mean of the test risks. As task homogeneity decreases (right plot), the gap in performance closes
with respect to the maximum of the test risks and remains roughly the same with respect to the mean.
0.9

1.5

0.88

1.45

0.86

1.4

0.84

1.35

0.82
0.8

1.3

0.78

0

0.2

0.4

0.6

?1

0

0.2

0.4

0.6

0

0.2

0.4

0.6

?

0

0.2

0.4

0.6

1

Figure 2: Maximum RMSE (Left) and normalized mean RMSE (Right) versus task-specific parameter bound
?1 , for shared parameter bound ?0 fixed. In each figure, Left section is ?0 is 0.2 and Right section is ?0 = 0.6.
Solid red  is `1 , solid blue ? is minimax, dashed green N is (0.1T )-minimax, dashed black H is (0.2T )minimax. The results for `2 MTL were visually identical to `1 MTL and hence were not plotted.

School. The school dataset has appeared in many previous works [7, 2, 6]. For brevity we just say
the goal is to predict student test scores using certain student-level features. Each school is treated as
a separate task. We report both the task-wise maximum of the root mean square error (RMSE) and
the taskwise-mean of the RMSE (normalized by number of points per task, as in previous works).
The results (see Figure 2) demonstrate that when the learner has moderate shared capacity ?0 and
high task-specific capacity ?1 , minimax MTL outperforms `1 MTL for the max objective; additionally, for the max objective in almost all parameter settings (0.1T )-minimax and (0.2T )-minimax
MTL outperform `1 MTL, and they also outperform minimax MTL when the task-specific capacity
?1 is not too large. We hypothesize that minimax MTL performs the best in the high??1 regime because stopping learning once the maximum of the empirical risks cannot be improved invokes early
stopping and its built-in regularization properties (see e.g. [13]). Interestingly, for the normalized
mean RMSE objective, both minimax relaxations are competitive with `1 MTL; however, when the
7

shared capacity ?0 is high (right section, right plot), `1 MTL performs the best. For high task-specific
capacity ?1 , minimax MTL and its relaxations again seem to resist overfitting compared to `1 MTL.

CV?mean Mean squared loss

CV?mean Maximum squared loss

0.24

0.85

0.8

0.75

0.7

0.65
0

5

10
15
bound on trace norm of W

0.5

0.2
0.18
0.16
0.14

5

10
15
bound on trace norm of W

20

0.155

CV?mean Mean squared loss

0.48
0.47
0.46
0.45

0.15

0.145

0.14

0.135

0.44
0.43
0

5

10
15
bound on trace norm of W

20

0.13
0

5

10
15
bound on trace norm of W

20

Figure 3: MTL (Top) and LTL (Bottom). Maximum `2 risk (Left)
and Mean `2 risk (Right) vs bound on kW ktr . LTL used 10-fold
cross-validation (10% of tasks left out in each fold). Solid red  is
`1 , solid blue ? is minimax, dashed green N is (0.1T )-minimax,
dashed black H is (0.2T )-minimax, solid gold  is `2 .

MNIST. The MNIST task is a 10-class problem; we approach it via a reduction to a tournament of 45 binary classifiers trained via the AEP model. The dimensionality was
reduced to 50 using principal component analysis (computed on the full training set), and only the first 2% of
each class?s training points were used for training.
Intuitively, the performance of the tournament tree of binary classifiers can only be as accurate as its paths, and
the accuracy of each path depends on the accuracy of
the nodes. Hence, our hypothesis is that minimax MTL
should outperform `1 MTL. The results in Figure 4 confirm our hypothesis. Minimax MTL outperforms `1 MTL
when the capacity kW ktr is somewhat limited, with the
gap widening as the capacity decreases. Furthermore, at
every capacity minimax MTL is competitive with `1 MTL.

5

0.22

0.12
0

20

0.49

0.5
0.45
Test multiclass zero?one loss

The results are shown in Figure 3. In
the MTL setting, for both the maximum RMSE objective and the mean
RMSE objective, `1 MTL appears to
perform the best. When the trace
norm of W is high, minimax MTL
displays resistance to overfitting and
obtains the lowest mean RMSE. In
the LTL setting for the maximum
RMSE objective, `2 , minimax, and
(0.1T )-minimax MTL all outperform
`1 MTL. For the mean RMSE, `1
MTL obtains the lowest risk for almost all parameter settings.

0.9

CV?mean Maximum squared loss

Personal computer. The personal
computer dataset is composed of 189
human subjects each of which rated
on a 0-10 scale the same 20 computers (16 training, 4 test). Each computer has 13 binary features (amount
of memory, screen size, price, etc.).

0.4
0.35
0.3
0.25
0.2
0.15
0.1

40

60

80
100
trace norm of W

120

140

Figure 4: Test multiclass 0-1 loss vs
kW ktr . Solid red is `1 MTL, solid blue is
minimax, dashed green is (0.1T )-minimax,
dashed black is (0.2T )-minimax. Regularized AEP used for speed and trace norm of
W ?s computed, so samples differ per curve.

Discussion

We have established a continuum of formulations for MTL which recovers as special cases classical
MTL and the newly formulated minimax MTL. In between these extreme points lies a continuum of
relaxed minimax MTL formulations. More generally, we introduced a loss-compositional paradigm
that operates on the vector of empirical risks, inducing the additional `p MTL paradigms. The empirical evaluations indicate that ?-minimax MTL at either the 10% or 20% level often outperforms `1
MTL in terms of the maximum test risk objective and sometimes even in the mean test risk objective.
All the minimax or ?-minimax MTL formulations exhibit a built-in safeguard against overfitting in
the case of learning with a model that is very complex relative to the available data.
Although efficient algorithms may make the various new MTL learning formulations practical for
large problems, a proper effort to develop fast algorithms in this setting would have detracted from
the main point of this first study. A good direction for the future is to obtain efficient algorithms
for minimax and ?-minimax MTL. In fact, such algorithms might have applications beyond MTL
and even machine learning. Another area ripe for exploration is to establish more general learning
bounds for minimax MTL and to extend these bounds to ?-minimax MTL.
8

References
[1] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Convex multi-task feature
learning. Machine Learning, 73(3):243?272, 2008.
[2] Bart Bakker and Tom Heskes. Task clustering and gating for bayesian multitask learning.
Journal of Machine Learning Research, 4:83?99, 2003.
[3] Peter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds
and structural results. Journal of Machine Learning Research, 3:463?482, 2002.
[4] Jonathan Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research, 12(1):149?198, 2000.
[5] Theodoros Evgeniou and Massimiliano Pontil. Regularized multi-task learning. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data
mining, pages 109?117. ACM, 2004.
[6] Theodoros Evgeniou, Massimiliano Pontil, and Olivier Toubia. A convex optimization approach to modeling consumer heterogeneity in conjoint estimation. Marketing Science,
26(6):805?818, 2007.
[7] Harvey Goldstein. Multilevel modelling of survey data. Journal of the Royal Statistical Society.
Series D (The Statistician), 40(2):235?244, 1991.
[8] Michael C. Grant and Stephen P. Boyd. Graph implementations for nonsmooth convex programs. In V. Blondel, S. Boyd, and H. Kimura, editors, Recent Advances in Learning and
Control, Lecture Notes in Control and Information Sciences, pages 95?110. Springer-Verlag
Limited, 2008.
[9] Michael C. Grant and Stephen P. Boyd. CVX: Matlab software for disciplined convex programming, version 1.21, April 2011.
[10] Yann LeCun, L?eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE, 86(11):2278?2324, 1998.
[11] Peter J. Lenk, Wayne S. DeSarbo, Paul E. Green, and Martin R. Young. Hierarchical bayes
conjoint analysis: Recovery of partworth heterogeneity from reduced experimental designs.
Marketing Science, pages 173?191, 1996.
[12] Andreas Maurer. Transfer bounds for linear feature learning. Machine learning, 75(3):327?
350, 2009.
[13] Noboru Murata and Shun-ichi Amari. Statistical analysis of learning dynamics. Signal Processing, 74(1):3?28, 1999.
[14] Nicolas Le Roux, Pierre-Antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural
gradient algorithm. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in
Neural Information Processing Systems 20, pages 849?856. MIT Press, Cambridge, MA, 2008.
[15] Kai Yu, John Lafferty, Shenghuo Zhu, and Yihong Gong. Large-scale collaborative prediction
using a nonparametric random effects model. In Proceedings of the 26th Annual International
Conference on Machine Learning, pages 1185?1192. ACM, 2009.
[16] Liang Zhang, Deepak Agarwal, and Bee-Chung Chen. Generalizing matrix factorization
through flexible regression priors. In Proceedings of the fifth ACM conference on Recommender systems, pages 13?20. ACM, 2011.

9

"
2062,2005,Modeling Memory Transfer and Saving in Cerebellar Motor Learning,Abstract Missing,"Modeling Memory Transfer and Savings in
Cerebellar Motor Learning

Naoki Masuda
RIKEN Brain Science Institute
Wako, Saitama 351-0198, Japan
masuda@brain.riken.jp

Shun-ichi Amari
RIKEN Brain Science Institute
Wako, Saitama 351-0198, Japan
amari@brain.riken.jp

Abstract
There is a long-standing controversy on the site of the cerebellar motor
learning. Different theories and experimental results suggest that either
the cerebellar flocculus or the brainstem learns the task and stores the
memory. With a dynamical system approach, we clarify the mechanism
of transferring the memory generated in the flocculus to the brainstem
and that of so-called savings phenomena. The brainstem learning must
comply with a sort of Hebbian rule depending on Purkinje-cell activities.
In contrast to earlier numerical models, our model is simple but it accommodates explanations and predictions of experimental situations as
qualitative features of trajectories in the phase space of synaptic weights,
without fine parameter tuning.

1

Introduction

The cerebellum is involved in various types of motor learning. As schematically shown in
Fig. 1, the cerebellum is composed of the cerebellar cortex and the cerebellar nuclei (we
depict the vestibular nucleus V N in Fig. 1). There are two main pathways linking external
input from the mossy fibers (mf ) to motor outputs, which originate from the cerebellar
nuclei. The pathway that relays the mossy fibers directly to the cerebellar nuclei is called
the direct pathway. Each nucleus cell receives about 104 mossy fiber synapses.
The pathway involving the mossy fibers, the granule cells (gr), the parallel fibers (pl), and
the Purkinje cells (P r) in the flocculo-nodular lobes of the cerebellar cortex, is called the
indirect pathway. Because the Purkinje cells, which are the sole source of output from
the cerebellar cortex, are GABAergic, firing rates of the nuclei are suppressed when this
pathway is active. The indirect pathway also includes recurrent collaterals terminating
on various types of inhibitory cells. Another anatomical feature of the indirect pathway
is that climbing fibers (Cm in Fig. 1) from the inferior olive (IO) innervate on Purkinje
cells. Taking into account the huge mass of intermediate computational units in the indirect
pathway, or the granule cells, Marr conjectured that the cerebellum operates as a perceptron
with high computational power [8]. The climbing fibers were thought to induce long-term
potentiation (LTP) of pl-P r synapses to reinforce the signal transduction. Albus claimed
that long-term depression (LTD) rather than LTP should occur so that the Purkinje cells
inhibit the nuclei [2]. The climbing fibers were thought to serve as teaching lines that
convey error-correcting signals.

u

e=ru-z
gr x=Au Pr
IO
pl *w
Cm
y=wx=wAu
mf
*v
z=vu-y
VN

Figure 1: Architecture of the VOR model.

The vestibulo-ocular reflex (VOR) is a standard benchmark for exploring synaptic substrates of cerebellar motor learning. The VOR is a short-latency reflex eye movement that
stabilizes images on the retina during head movement. Motion of the head drives eye movements in the opposite direction. When a subject wears a prism, adaptation of the VOR gain
occurs for image stabilization. In this context, in vivo experiments confirmed that the LTD
hypothesis is correct (reviewed in [6]). However, the cerebellum is not the only site of
convergence of visual and vestibular signals. The learning scheme depending only on the
indirect pathway is called the flocculus hypothesis. An alternative is the brainstem hypothesis in which synaptic plasticity is assumed to occur in the direct pathway (mf ? V N )
[12]. This idea is supported by experimental evidence that flocculus shutdown after 3 days
of VOR adaptation does not impair the motor memory [7]. Moreover, in other experiments,
plasticity of the Purkinje cells in response to vestibular inputs, as required in the flocculus
hypothesis, really occurs but in the direction opposite to that predicted by the flocculus
hypothesis [5, 12]. Also, LTP of the mf -V N synapses, which is necessary to implement
the brainstem hypothesis [3], has been suggested in experiments [14].
Relative contributions of the flocculus mechanism and the brainstem mechanism to motor
learning remain illusive [3, 5, 9]. The same controversy exists regarding the mechanism of
associative eyelid conditioning [9, 10, 11]. Related is the distinction between short-term
and long-term plasticities. Many of the experiments in favor of the flocculus hypothesis are
concerned with short-term learning, whereas plasticity involving the vestibular nuclei is
suggested to be functional in the long term. Short-term motor memory in the flocculus may
eventually be transferred to the brainstem. This is termed the memory transfer hypothesis
[9]. Medina and Mauk proposed a numerical model and examined what types of brainstem
learning rules are compatible with memory transfer [10]. They concluded that the brainstem plasticity should be driven by coincident activities of the Purkinje cells and the mossy
fibers. The necessity of Hebbian type of learning in the direct pathway is also supported
by another numerical model [13]. We propose a much simpler model to understand the
essential mechanism of memory transfer without fine parameter manipulations.
Another goal of this work is to explain savings of learning. Savings are observed in natural
learning tasks. Because animals can be trained just for a limited amount of time per day,
the task period and the rest period, of e.g. 1 day, alternate. Performance is improved during
the task period, and it degrades during the rest period (in the dark). However, when the
alternation is repeated, the performance is enhanced more rapidly and progressively in later
sessions [7] (also, S. Nagao, private communication). The flocculus may be responsible
for daily rapid learning and forgetting, and the brainstem may underlie gradual memory
consolidation [11]. While our target phenomenon of interest is the VOR, the proposed
model is fairly general.

2

Model

Looking at Fig. 1, let us denote by u ? Rm the external input to the mossy fibers. It is
propagated to the granule cells via synaptic connectivity represented by an n by m matrix
A, where presumably n  m. The output of the granule cells, or x ? Au ? Rn ,
is received by the Purkinje-cell layer. For simplicity, we assume just one Purkinje cell
whose output is written as y ? wx, where w ? R ? Rm . Since pl-P r synapses are
excitatory, the elements of w are positive. The direct pathway (mf ? V N ) is defined by
a plastic connection matrix v ? R ? Rm . The output to the VOR actuator is given by
z = vu ? y = vu ? wAu, which is the output of the sole neuron of the cerebellar nuclei.
This form of z takes into account that the contribution of the indirect pathway is inhibitory
and that of the direct pathway is excitatory.
The animal learns to adapt z as close as possible to the desirable motor output ru. For
a large (resp. small) desirable gain r, the correct direction of synaptic changes is the decrease (resp. increase) in w and the increase (resp. decrease) in v [5]. The learning error
e ? ru ? z is carried by the climbing fibers and projects onto the Purkinje cell, which
enables supervised learning [6]. The LTD of w occurs when the parallel-fiber input and the
climbing-fiber input are simultaneously large [6, 9]. Since we can write
1 ?e2
? = ??1 ex = ? ?1
w
,
(1)
2 ?w
where ?1 is the learning rate, w evolves to minimize e2 . Equation (1) is a type of WidrowHoff rule [4, p. 320]. With spontaneous inputs only, or in the presence of x and the absence
of e, w experiences LTP [6, 9]. We model this effect by adding ?2 x to Eq. (1). This term
provides subtractive normalization that counteracts the use-dependent LTD [4, p. 290].
However, subtractive normalization cannot prohibit w from running away when the error
signal is turned off. Therefore, we additionally assume multiplicative normalization term
?3 w to limit the magnitude of w [4, p. 290, 314]. In the end, Eq. (1) is modified to
? = ??1 (ru ? vu + wAu)Au + ?2 Au ? ?3 w,
w

(2)

where ?2 and ?3 are rates of memory decay satisfying ?2 , ?3  ?1 .
In the dark, the VOR gain, which might have changed via adaptation, tends back to a value
close to unity [5]. Let us represent this reference gain by r = r0 . With the synaptic
strengths in this null condition denoted by (w, v) = (w0 , v0 ), we obtain r0 u = v0 u ?
? = 0 in Eq. (2), we derive
w0 Au. By setting w
?2 Au = ?1 (r0 u ? v0 u + w0 Au)Au + ?3 w0 = ?3 w0 .

(3)

Substituting Eq. (3) into Eq. (2) results in
? = ??1 (ru ? vu + wAu) Au ? ?3 (w ? w0 ) .
w

(4)

Experiments show that v can be potentiated [14]. Enhancement of the excitability of the
nucleus output (z) in response to tetannic stimulation, or sustained u, is also in line with the
LTP of v [1]. In contrast, LTD of v is biologically unknown. Numerical models suggest
that LTP in the nuclei should be driven by y [10, 11]. However, the mechanism and the
specificity underlying plasticity of v are not well understood [9]. Therefore, we assume that
both LTP and LTD of v occur in an associative manner, and we represent the LTP effect
by a general function F . In parallel to the learning rule of w, we assume a subtractive
normalization term ??5 u [10]. We also add a multiplicative normalization term ?6 v to
constrain v. Finally, we obtain
v? = ?4 F(u, y, z, e) ? ?5 u ? ?6 v.

(5)

Presumably, v changes much more slowly (on a time scale of 8?12 hr) than w changes (0.5
hr) [10, 13]. Therefore, we assume ?1  ?4  ?5 , ?6 .

3

Analysis of Memory Transfer

Let us examine a couple of learning rules in the direct pathway to identify robust learning
mechanisms.
3.1

Supervised learning

Although the climbing fibers carrying e send excitatory collaterals to the cerebellar nuclei, supervised learning there has very little experimental support [5]. Here we show that
supervised learning in the direct pathway is theoretically unlikely. Let us assume that modification of v decreases |e|. Accordingly, we set F = ??e2 /?v = eu. Then, Eq. (5)
becomes
v? = ?4 (ru ? vu + wAu)u ? ?5 u ? ?6 v.
(6)
In the natural situation, r = r0 . Hence,
?5 u = ?4 (r0 u ? v0 u + w0 Au)u ? ?6 v0 = ??6 v0 .

(7)

Inserting Eq. (7) into Eq. (6) yields
v? = ?4 (ru ? vu + wAu) u ? ?6 (v ? v0 ).

(8)

For further analysis, let us assume m = n = 1 (for which we quit bold notations) and
perform the slow-fast analysis based on ?1  ?3 , ?4  ?6 . Equations (4) and (8) define
the nullclines w? = 0 and v? = 0, which are represented respectively by
v
v

?1 A2 u2 + ? 3
(w ? w0 ), and
?1 Au2
?4 u2
?4 Au2
= v0 +
(r
?
r
)
+
(w ? w0 ).
0
?4 u2 + ? 6
?4 u2 + ? 6
= v0 + r ? r 0 +

(9)
(10)

Since w? = O(?1 )  O(?4 ) = v? in an early stage, a trajectory in the w-v plane initially
approaches the fast manifold (Eq. (9)) and moves along it toward the equilibrium given by
w ? = w0 ?

?1 ?6 Au2 (r ? r0 )
,
?1 ?6 A2 u2 + ? 3 ?4 u2 + ? 3 ?6

v ? = v0 +

?3 ?4 u2 (r ? r0 )
. (11)
?1 ?6 A2 u2 + ? 3 ?4 u2 + ? 3 ?6

LTD of w and LTP of v are expected for adaptation to a larger gain (r > r0 ), and LTP of
w and LTD of v are expected for r < r0 . The results are consistent with both the flocculus
hypothesis and the brainstem hypothesis as far as the direction of learning is concerned [5].
When r > r0 (resp. r < r0 ), LTD (resp. LTP) of w first occurs to decrease the learning
error. Then, the motor memory stored in w is gradually transferred by LTP (resp. LTD) of
v replacing LTD (resp. LTP) of w. In the long run, the memory is stored mainly in v, not
in w.
However, the memory transfer based on supervised learning has fundamental deficiencies.
First, since ?1  ?3 and ?4  ?6 , both nullclines Eqs. (9) and (10) have a slope close
to A in the w-v plane. This means that the relative position of the equilibrium depends
heavily on the parameter values, especially on the learning rates, the choice of which is
rather arbitrary. Then, (w ? , v ? ) may be located so that, for example, the LTP of w or LTD
of v results from r > r0 . Also, the degree of transfer, or |w ? ? w0 | / |v ? ? v0 |, is not robust
against parameter changes. This may underlie the fact that LTD of w was not followed by
partial LTP in the numerical simulations in [10]. Even if the position of (w ? , v ? ) happens
to support LTD of w and LTP of v, memory transfer takes a long time. This is because
Eqs. (9) and (10) are fairly close, which means that v? is small on the fast manifold (w? = 0).
We can also imagine a type of Hebbian rule with F = ?z 2 /?v = zu. Similar calculations
show that this rule also realizes memory transfer only in an unreliable manner.

A

B
v
e=0

.
w=0

w*,v*

v
w0,v0

w0,v0
.
w=0

.
v=0

w*,v*
e=0

w

.
v=0
w

Figure 2: Dynamics of the synaptic weights in the Purkinje cell-dependent learning. (A)
r > r0 and (B) r < r0 .

3.2

Purkinje cell-dependent learning

Results of numerical studies support that v should be subject to a type of Hebbian learning
depending on two afferents to the vestibular nuclei, namely, u and y [10, 11, 13]. Changes
in the VOR gain are signaled by y. Since LTP should logically occur when y is small and u
is large, we set F = (ymax ? y)u, where ymax is the maximum firing rate of the Purkinje
cell. Then, we obtain
v? = ?4 (ymax ? wAu)u ? ?5 u ? ?6 v.

(12)

The subtraction normalization is determined from the equilibrum condition:
?5 u = ?4 (ymax ? w0 Au) ? ?6 v0 .

(13)

Substituting Eq. (13) into Eq. (12) yields
v? = ?4 (w0 ? w)Au2 + ?6 (v0 ? v).

(14)

When m = n = 1, the nullclines are given by Eq. (9) and
v = v0 ?

?4 Au2
(w ? w0 ),
?6

(15)

which are depicted in Fig. 2(A) and (B) for r > r0 and r < r0 , respectively. As shown by
arrows in Fig. 2, trajectories in the w-v space first approach the fast manifold Eq. (9) and
then move along it toward the equilibrium given by
w ? = w0 ?

?1 ?6 Au2 (r ? r0 )
,
?1 ?4 A2 u4 + ? 1 ?6 A2 u2 + ? 3 ?6

v ? = v0 +

?1 ?4 A2 u4 (r ? r0 )
.
?1 ?4 A2 u4 + ? 1 ?6 A2 u2 + ? 3 ?6
(16)

Equation (15) has a large negative slope because ?4  ?6 . Consequently, setting r > r0
(resp. r < r0 ) duly results in LTD (resp. LTP) of w and LTP (resp. LTD) of v. At the
same time, LTD (resp. LTP) of w in an early stage of learning is partially compensated
by subsequent LTP (resp. LTD) of w, which agrees with previously reported numerical
results [10]. In contrast to the supervised and Hebbian learning rules, this learning is robust
against parameter changes since the positions and the slopes of the two nullclines are apart
from each other. Owing to this property, in the long term, the memory is transferred more
rapidly along the w-nullcline than for the other two learning rules. Another benefit of the
large negative slope of Eq. (15) is that |v ? ? v0 |  |w? ? w0 | holds, which means efficient
memory transfer from w to v.

The error at the equilibrum state is
e?

=

?3 ?6 (r ? r0 )u
2
?1 ?4 A u4 + ? 1 ?6 A2 u2

+ ? 3 ?6

.

(17)

Equation (17) guarantees that the e = 0 line is located as shown in Fig. 2, and the learning
proceeds so as to decrease |e|. The performance overshoot, which is unrealistic, does not
occur.

4

Numerical Simulations of Savings

The learning rule proposed in Sec. 3.2 explains savings as well. To show this, we mimic a
situation of savings by periodically alternating the task period and the rest period. Specifically, we start with r = r0 = 1, w = w0 , v = v0 , and the learning condition (r = 2 or
r = 0.5) is applied for 4 hours a day. During the rest of the day (20 hours), the dark condition is simulated by giving no teaching signal to the model. Changes in the VOR gains
for 8 consecutive days are shown in Fig. 3(A) and (C) for r = 2 and r = 0.5, respectively.
The numerical results are consistent with the savings found in other reported experiments
[7] and models [11]; the animal forgets much of the acquired gain in the dark, while a
small fraction is transferred each day to the cerebellar nuclei. The time-dependent synaptic
weights are shown in Fig. 3(B) (r = 2) and (D) (r = 0.5) and suggest that v is really
responsible for savings and that its plasticity needs guidance under the short-term learning
of w. The memory transfer occurs even in the dark condition, as indicated by the increase
(resp. decrease) of v in the dark shown in Fig. 3(B) (resp. (D)). This happens because ruin
of the short-term memory of w drives the learning of v for some time even after the daily
training has finished. For the indirect pathway, a dark condition defines an off-task period
during which w gradually loses its associations.
For comparison, let us deal with the case in which v is fixed. Then, the learning rule Eq. (4)
is reduced to
w? = ??1 [(r ? r0 ) u + (w ? w0 ) Au] Au ? ?3 (w ? w0 ) .

(18)

The VOR adaptation with this rule is shown in Fig. 4(A) (r = 2) and (B) (r = 0.5). Longterm retention of the acquired gain is now impossible, whereas the short-term learning, or
the adaptation within a day, deteriorates little. Since savings do not occur, the ultimate
learning error is larger than when v is plastic.
However, if w is fixed and v is plastic, the VOR gain is not adaptive, since y does not
carry teaching signals any longer. In this case, we must implement supervised learning of
v for learning to occur. Then, r adapts only gradually on the slow time scale of ? 4 , and the
short-term learning is lost.

5

Discussion

Our model explains how the flocculus and the brainstem cooperate in motor learning. Presumably, the indirect pathway involving the flocculus is computationally powerful because
of a huge number of intermediate granule cells, but its memory is of short-term nature.
The direct pathway bypassing the mossy fibers to the cerebellar nuclei is likely to have
less computational power but stores motor memory for a long period. A part of the motor
memory is expected to be passed from the flocculus to the nuclei. This happens in a robust
manner if the direct pathway is equipped with the learning rule dependent on correlation
between the Purkinje-cell firing and the mossy-fiber firing. To explore whether associative
LTP/LTD in the cerebellar nuclei really exists will be a subject of future experimental work.
Our model is also applicable to savings.

A

B
2

3
2.5

1.5

v

r

1.75

2

1.25
1

1.5
0

50

100
time [hr]

150

0

2
w

B

D
1

2

0.75

1.5

v

r

1

0.5

1
0

50

100
time [hr]

150

2

2.5

3

w

Figure 3: Numerical simulations of savings with the Purkinje cell-dependent learning rule.
We set A = 0.4, u = 1, w0 = 2, r0 = 1, v0 = r0 + Aw0 , ?1 = 7, ?3 = 0.3, ?4 = 0.05,
?6 = 0.002. The target gains are (A, B) r = 2 and (C, D) r = 0.5. (A) and (C) show VOR
gains. (B) and (D) show trajectories in the w-v space (thin solid lines) together with the
nullclines (thick solid lines) and e = 0 (thick dotted lines).

A

B
2

1

1.5

r

r

1.75
0.75

1.25
1

0.5
0

50

100
time [hr]

150

0

50

100
time [hr]

150

Figure 4: Numerical simulations of savings with fixed v. The parameter values are the
same as those used in Fig. 3. The target gains are (A) r = 2 and (B) r = 0.5.

In the earlier models [10, 11], quantitative meanings were given to the equilibrium synaptic
weights. Actually, they are solely determined from non-experimentally determined parameters, namely, the balance between the learning rates (in our terminology, ? 1 , ?2 , ?4 and
?5 ). Also, the balance seems to play a role in preventing runaway of synaptic weights. In
contrast, our model uses the ratio of learning rates (and values of other parameters) just for
qualitative purposes and is capable of explaning and predicting experimental settings without parameter tuning. For example, the earlier arguments negating the flocculus hypothesis
are based on the fact that the plasticity of the flocculus (w) responding to vestibular inputs
occurs but in the direction opposite to the expectation of the flocculus hypothesis [5, 12].
However, this experimental observation is not necessarily contradictory to either the flocculus hypothesis or the two-site hypothesis. As shown in Fig. 2(A), when adapting to a
large VOR gain, w experiences LTD in the initial stage [6]. Then, partial LTP ensues as
the motor memory is transferred to the nuclei. Another prediction is about adaptation to a
small gain. Figure 2(B) predicts that, in this case, LTP in the indirect pathway is gradually
transferred to LTD in the direct pathway. Partial LTD following LTP is anticipated in the
flocculus. This implies savings in unlearning.

Acknowledgments
We thank S. Nagao for helpful discussions. This work was supported by the Special Postdoctoral Researchers Program of RIKEN.

References
[1] C. D. Aizenman, D. J. Linden. Rapid, synaptically driven increases in the intrinsic excitability
of cerebellar deep nuclear neurons. Nat. Neurosci., 3, 109?111 (2000).
[2] J. S. Albus. A theory of cerebellar function. Math. Biosci., 10, 25?61 (1971).
[3] E. S. Boyden, A. Katoh, J. L. Raymond. Cerebellum-dependent learning: the role of multiple
plasticity mechanisms. Annu. Rev. Neurosci., 27, 581?609 (2004).
[4] P. Dayan, L. F. Abbott. Theoretial Neuroscience ? Computational and Mathematical Modeling
of Neural Systems. MIT (2001).
[5] S. du Lac, J. L. Raymond, T. J. Sejnowski, S. G. Lisberger. Learning and memory in the
vestibulo-ocular reflex. Annu. Rev. Neurosci., 18, 409?441 (1995).
[6] M. Ito. Long-term depression. Ann. Rev. Neurosci., 12, 85?102 (1989).
[7] A. E. Luebke, D. A. Robinson. Gain changes of the cat?s vestibulo-ocular reflex after flocculus
deactivation. Exp. Brain Res., 98, 379?390 (1994).
[8] D. Marr. A theory of cerebellar cortex. J. Physiol., 202, 437?470 (1969).
[9] M. D. Mauk. Roles of cerebellar cortex and nuclei in motor learning: contradictions or clues?
Neuron, 18, 343?346 (1997).
[10] J. F. Medina, M. D. Mauk. Simulations of cerebellar motor learning: computational analysis of
plasticity at the mossy fiber to deep nucleus synapse. J. Neurosci., 19, 7140?7151 (1999).
[11] J. F. Medina, K. S. Garcia, M. D. Mauk. A mechanism for savings in the cerebellum. J. Neurosci., 21, 4081?4089 (2001).
[12] F. A. Miles, D. J. Braitman, B. M. Dow. Long-term adaptive changes in primate vestibuloocular
reflex. IV. Electrophysiological observations in flocculus of adapted monkeys. J. Neurophysiol.,
43, 1477?1493 (1980).
[13] B. W. Peterson, J. F. Baker, J. C. Houk. A model of adaptive control of vestibuloocular reflex
based on properties of cross-axis adaptation. Ann. New York Acad. Sci. 627, 319?337 (1991).
[14] R. J. Racine, D. A. Wilson, R. Gingell, D. Sunderland. Long-term potentiation in the interpositus and vestibular nuclei in the rat. Exp. Brain Res., 63, 158?162 (1986).

"
437,1997,Task and Spatial Frequency Effects on Face Specialization,Abstract Missing,"Task and Spatial Frequency Effects on Face
Specialization

Matthew N. Dailey

Garrison W. Cottrell

Department of Computer Science and Engineering
U.C. San Diego
La Jolla, CA 92093-0114

{mdailey,gary}@cs.ucsd.edu

Abstract
There is strong evidence that face processing is localized in the brain.
The double dissociation between prosopagnosia, a face recognition
deficit occurring after brain damage, and visual object agnosia, difficulty
recognizing otber kinds of complex objects, indicates tbat face and nonface object recognition may be served by partially independent mechanisms in the brain. Is neural specialization innate or learned? We suggest that this specialization could be tbe result of a competitive learning mechanism that, during development, devotes neural resources to the
tasks they are best at performing. Furtber, we suggest that the specialization arises as an interaction between task requirements and developmental constraints. In this paper, we present a feed-forward computational
model of visual processing, in which two modules compete to classify
input stimuli. When one module receives low spatial frequency information and the other receives high spatial frequency information, and
the task is to identify the faces while simply classifying the objects, the
low frequency network shows a strong specialization for faces. No otber
combination of tasks and inputs shows this strong specialization. We
take these results as support for the idea that an innately-specified face
processing module is unnecessary.

1 Background
Studies of the preserved and impaired abilities in brain damaged patients provide important
clues on how the brain is organized. Cases of prosop agnosia, a face recognition deficit often
sparing recognition of non-face objects, and visual object agnosia, an object recognition
deficit that can occur witbout appreciable impairment of face recognition, provide evidence
that face recognition is served by a ""special"" mechanism. (For a recent review of this

18

M. N. Dailey and G. W. Cottrell

evidence, see Moscovitch, Winocur, and Behrmann (1997?. In this study, we begin to
provide a computational account of the double dissociation.
Evidence indicates that face recognition is based primarily on holistic, configural information, whereas non-face object recognition relies more heavily on local features and analysis
of the parts of an object (Farah, 1991; Tanaka and Sengco, 1997). For instance, the distance
between the tip of the nose and an eye in a face is an important factor in face recognition,
but such subtle measurements are rarely as critical for distinguishing, say, two buildings.
There is also evidence that con figural information is highly relevant when a human becomes an ""expert"" at identifying individuals within other visually homogeneous object
classes (Gauthier and Tarr, 1997).
What role might configural information play in the development of a specialization for face
recognition? de Schonen and Mancini (1995) have proposed that several factors, including
different rates of maturation in different areas of cortex, an infant's tendency to track the
faces in its environment, and the gradual increase in visual acuity as an infant develops,
all combine to force an early specialization for face recognition. If this scenario is correct,
the infant begins to form configural face representations very soon after birth, based primarily on the low spatial frequency information present in face stimuli. Indeed, Costen,
Parker, and Craw (1996) showed that although both high-pass and low-pass image filtering decrease face recognition accuracy, high-pass filtering degrades identification accuracy
more quickly than low-pass filtering. Furthermore, Schyns and Oliva (1997) have shown
that when asked to recognize the identity of the ""face"" in a briefly-presented hybrid image
containing a low-pass filtered image of one individual's face and a high-pass filtered image
of another individual's face, subjects consistently use the lOW-frequency compone.nt of the
image for the task. This work indicates that low spatial frequency information may be more
important for face identification than high spatial frequency information.
Jacobs and Kosslyn (1994) showed how differential availability of large and small receptive
field sizes in a mixture of experts network (Jacobs, Jordan, Nowlan, and Hinton, 1991)
can lead to experts that specialize for ""what"" and ""where"" tasks. In previous work, we
proposed that a neural mechanism allocating resources according to their ability to perform
a given task could explain the apparent specialization for face recognition evidenced by
prosopagnosia (Dailey, Cottrell, and Padgett, 1997). We showed that a model based on
the mixture of experts architecture, in which a gating network implements competitive
learning between two simple homogeneous modules, could develop a specialization such
that damage to one module disproportionately impaired face recognition compared to nonface object recognition.
In the current study, we consider how the availability of spatial frequency information affects face recognition specialization given this hypothesis of neural resource allocation by
competitive learning. We find that when high and low frequency information is ""split""
between the two modules in our system, and the task is to identify the faces while simply
classifying the objects, the low-frequency module consistently specializes for face recognition. After describing the study, we discuss its results and their implications.

2 Experimental Methods
We presented a modular feed-forward neural network preprocessed images of 12 different faces, 12 different books, 12 different cups, and 12 different soda cans. We gave the
network two types of tasks:
1. Learning to recognize the superordinate classes of all four object types (hereafter
referred to as classification).
2. Learning to distinguish the individual members of one class (hereafter referred to

Task and Spatial Frequency Effects on Face Specialization

19

as identification) while simply classifying objects of the other three types.
For each task, we investigated the effects of high and low spatial frequency information on
identification and classification in a visual processing system with two competing modules.
We observed how splitting the range of spatial frequency information between the two
modules affected the specializations developed by the network.

2.1

Image Data

We acquired face images from the Cottrell and Metcalfe facial expression database (1991 )
and captured multiple images of several books, cups, and soda cans with a CCD camera
and video frame grabber. For the face images, we chose five grayscale images of each of 12
individuals. The images were photographed under controlled lighting and pose conditions;
the subjects portrayed a different facial expression in each image. For each of the non-face
object classes, we captured five different grayscale images of each of 12 books, 12 cups,
and 12 cans. These images were also captured under controlled lighting conditions, with
small variations in position and orientation between photos. The entire image set contained
240 images, each of which we cropped and scaled to a size of 64x64 pixels.

2.2 Image Preprocessing
To convert the raw grayscale images to a biologically plausible representation more suitable
for network learning and generalization, and to experiment with the effect of high and
low spatial frequency information available in a stimulus, we extracted Gabor jet features
from the images at multiple spatial frequency scales then performed a separate principal
components analysis on the data from each filter scale separately to reduce input pattern
dimensionality.

2.2.1

Gabor jet features

The basic two-dimensional Gabor wavelet resembles a sinusoid grating restricted by a twodimensional Gaussian, and may be tuned to a particular orientation and sinusoidal frequency scale. The wavelet can be used to model simple cell receptive fields in cat primary
visual cortex (Jones and Palmer, 1987). Buhmann, Lades, and von der Malsburg (1990)
describe the Gabor ""jet,"" a vector consisting of filter responses at multiple orientations and
scales.
We convolved each of the 240 images in the input data set with two-dimensional Gabor
? elg
? h t onentatlons
.
.
(0 '8'
11' 11' 311' 11' 511' 311' 711')
filters at fi ve scaIes m
4' 8' 2' T' 4' 8 an d sub sampled an
8x8 grid of the responses to each filter. The process resulted in 2560 complex numbers
describing each image.

2.2.2 Principal components analysis
To reduce the dimensionality of the Gabor jet representation while maintaining a segregation of the responses from each filter scale, we performed a separate PCA on each spatial
frequency component of the pattern vector described above. For each of the 5 filter scales
in the jet, we extracted the subvectors corresponding to that scale from each pattern in the
training set, computed the eigenvectors of their covariance matrix, projected the sub vectors
from each of the patterns onto these eigenvectors, and retained the eight most significant
coefficients. Reassembling the pattern set resulted in 240 40-dimensional vectors.

M. N. Dailey and G. W. Cottrell

20
Module 1

Inputs

Figure 1: Modular network architecture. The gating network units mix the outputs of the
hidden layers multiplicatively.

2.3 The Model
The model is a simple modular feed-forward network inspired by the mixture of experts
architecture (Jordan and Jacobs, 1995); however, it contains hidden layers and is trained by
backpropagation of error rather than maximum likelihood estimation or expectation maximization. The connections to the output units come from two separate inputlhidden layer
pairs; these connections are gated multiplicatively by a simple linear network with softmax outputs. Figure 1 illustrates the model's architecture. During training, the network's
weights are adjusted by backpropagation of error. The connections from the softmax units
in the gating network to the connections between the hidden layers and output layer can
be thought of as multiplicative connections with a constant weight of 1. The resulting
learning rules gate the amount of error feedback received by a module according to the
gating network's current estimate of its ability to process the current training pattern. Thus
the model implements a form of competitive learning in which the gating network learns
which module is better able to process a given pattern and rewards the ""winner"" with more
error feedback.

2.4 Training Procedure
Preprocessing the images resulted in 240 40-dimensional vectors; four examples of each
face and object composed a I92-element training set, and one example of each face and
object composed a 48-element test set. We held out one example of each individual in
the training set for use in determining when to stop network training. We set the learning
rate for all network weights to 0.1 and their momentum to 0.5. Both of the hidden layers
contained 15 units in all experiments. For the identification tasks, we determined that a
mean squared error (MSE) threshold of 0.02 provided adequate classification performance
on the hold out set without overtraining and allowed the gate network to settle to stable
values. For the four-way classification task, we found that an MSE threshold of 0.002 was
necessary to give the gate network time to stabilize and did not result in overtraining. On
all runs reported in the results section, we simply trained the network until it reached the
relevant MSE threshold.
For each of the tasks reported in the results section (four-way classification, book identification, and face identification), we performed two experiments. In the first, as a control,
both modules and the gating network were trained and tested with the fu1l40-dimensional
pattern vector. In the second, the gating network received the full 40-dimensional vector,

Task and Spatial Frequency Effects on Face Specialization

21

but module 1 received a vector in which the elements corresponding to the largest two Gabor filter scales were set to 0, and the elements corresponding to the middle filter scale were
reduced by 0.5. Module 2, on the other hand, received a vector in which the elements corresponding to the smallest two filter scales were set to 0 and the elements corresponding to
the middle filter were reduced by 0.5. Thus module 1 received mostly high-frequency information, whereas module 2 received mostly low-frequency information, with deemphasized
overlap in the middle range.
For each of these six experiments, we trained the network using 20 different initial random
weight sets and recorded the softmax outputs learned by the gating network on each training
pattern.

3 Results
Figure 2 displays the resulting degree of specialization of each module on each stimulus
class. Each chart plots the average weight the gating network assigns to each module for
the training patterns from each stimulus class, averaged over 20 training runs with different
initial random weights. The error bars denote standard error. For each of the three reported
tasks (four-way claSSification, book identification, and face identification), one chart shows
division of labor between the two modules in the control situation, in which both modules
receive the same patterns, and the other chart shows division of labor between the two
modules when one module receives low-frequency information and the other receives highfrequency information.
When required to identify faces on the basis of high- or lOW-frequency information, compared with the four-way-classification and same-pattern controls, the lOW-frequency module wins the competition for face patterns extremely consistently (lower right graph). Book
identification specialization, however, shows considerably less sensitivity to spatial frequency.
We have also performed the equivalent experiments with a cup discrimination and a can
discrimination task. Both of these tasks show a low-frequency sensitivity lower than that
for face identification but higher than that for book identification. Due to space limitations,
these results are not presented here.
The specialized face identification networks also provide good models of prosopagnosia
and visual object agnosia: when the face-specialized module's output is ""damaged"" by removing connections from its hidden layer to the output layer, the overall network's generalization performance on face identification drops dramatically, while its generalization performance on object recognition drops much more slowly. When the non-face-specialized
(high frequency) module'S outputs are damaged, the opposite effect occurs: the overall network's performance on each of the object recognition tasks drops, whereas its performance
on face identification remains high.

4 Discussion
The results in Figure 2 show a strong preference for low-frequency information in the face
identification task, empirically demonstrating that, given a choice, a competitive mechanism will choose a module receiving low-frequency, large receptive field information for
this task. This result concurs with the psychological evidence for configural face representations based upon low spatial frequency information, and suggests how the developing
brain could be biased toward a specialization for face recognition by the infant's initially
low visual acuity.
On the basis of these results, we predict that human subjects performing face and object

22

M. N Dailey and G. W. Cottrell
Classification (split frequencies)

Classification (control)

i

'0;

~

i

:

1.0

\.0

0.8

0.8

0.6
1m Module 1

a

Module 2

0.4

~

i""11

r

~

CModule 1
(highfreq)
? Module 2
(low freq)

0.6

0.4

~

0.2

0.2

0.0

0.0
Faces Books Cups Cans

Faces Books Cups Cans

SdmulusType

Stimulus Type

Bookid task (control)

Book id task (split frequencies)

\.0

1.0

0.8

0.8

j

j
-;

f

0.6

r:::::J Module 1

. . Module 2
0.4

-;

0.6

r

0.4

:

0.2

0.2

0.0

0.0
Faces Books Cups Cans

Faces Books Cups Cans

Sdmul.. Type

Stimulus Type

Face id task (control)

r
:

(highfreq)
. . Module 2
(low freq)

~

~

i-;

c:I Module 1

Face id task (split frequencies)

1.0

1.0

0.8

0.8

i

0.6

C!I Module 1
. . Module 2

0.4

~

-;

r

Cl Module 1

0.6

(high freq)
IilII Module 2

0.4

(low freq)

~

0.2

0.2

0.0

0.0

Faces Books Cup. Cans
Stimulus Type

Faces Books Cups Cans

Stimulus Type

Figure 2: Average weight assigned to each module broken down by stimulus class. For each
task, in the control experiment, each module receives the same pattern; the split-frequency
charts summarize the specialization resulting when module 1 receives high-frequency Gabor filter information and module 2 receives low-frequency Gabor filter information.

Task and Spatial Frequency Effects on Face Specialization

23

identification tasks will show more degradation of performance in high-pass filtered images
of faces than in high-pass filtered images of other objects. To our knowledge, this has not
been empirically tested, although Costen et al. (1996) have investigated the effect of highpass and low-pass filtering on face images in isolation, and Parker, Lishman, and Hughes
(1996) have investigated the effect of high-pass and low-pass filtering of face and object
images used as 100 ms cues for a same/different task. Their results indicate that relevant
high-pass filtered images cue object processing better than low-pass filtered images, but the
two types of filtering cue face processing equally well. Similarly, Schyns & Oliva's (1997)
results described earlier suggest that the human face identification network preferentially
responds to low spatial frequency inputs.
Our results suggest that simple data-driven competitive learning combined with constraints
and biases known or thought to exist during visual system development can account for
some of the effects observed in normal and brain-damaged humans. The study lends support to the claim that there is no need for an innately-specified face processing module face recognition is only ""special"" insofar as faces form a remarkably homogeneous category of stimuli for which Within-category discrimination is ecologically beneficial.
References

Buhmann, J., Lades, M., and von der Malsburg, C. (1990). Size and distortion invariant object recognition by hierarchical graph matching. In Proceedings of the IJCNN
International Joint Conference on Neural Networks, volume II, pages 411-416.
Costen, N., Parker, D., and Craw, I. (1996). Effects of high-pass and low-pass spatial
filtering on face identification. Perception & Psychophysics, 38(4):602-612.
Cottrell, G. and Metcalfe, J. (1991). Empath: Face, gender and emotion recognition using
holons. In Lippman, R., Moody, J., and Touretzky, D., editors, Advances in Neural
Information ProceSSing Systems 3, pages 564-571.
Dailey, M., Cottrell, G., and Padgett, C. (1997). A mixture of experts model exhibiting
pro sop agnosia. In Proceedings of the Nineteenth Annual Conference of the Cognitive
Science Society, pp. 155-160. Stanford, CA, Mahwah: Lawrence Erlbaum.
de Schonen, S. and Mancini, J. (1995). About functional brain specialization: The development of face recognition. TR 95.1, MRC Cognitive Development Unit, London,
UK.
Farah, M. (1991). Patterns of co-occurrence among the associative agnosias: Implications
for visual object representation. Cognitive Neuropsychology, 8:1-19.
Gauthier, I. and Tarr, M. (1997). Becoming a ""greeble"" expert: Exploring mechanisms for
face recognition. Vision Research. In press.
Jacobs, R. and Kosslyn, S. (1994). Encoding shape and spatial relations - The role of
receptive field size in coordinating complementary representations. Cognitive Science,
18(3):361-386.
Jacobs, R, Jordan, M., Nowlan, S., and Hinton, G. (1991). Adaptive mixtures of local
experts. Neural Computation, 3:79-87.
Jones, J. and Palmer, L. (1987). An evaluation of the two-dimensional Gabor filter model
of simple receptive fields in cat striate cortex. 1 Neurophys., 58(6):1233-1258.
Moscovitch, M., Winocur, G., and Behrmann, M. (1997). What is special about face
recognition? Nineteen experiments on a person with visual object agnosia and dyslexia
but normal face recognition. Journal of Cognitive Neuroscience, 9(5):555-604.
Parker, D., Lishman, J., and Hughes, J. (1996). Role of coarse and fine spatial information
in face and object processing. Journal of Experimental Psychology: Human Perception
and Performance, 22(6):1445-1466.
Schyns, P. and Oliva, A. (1997). Dr. Angry and Mr. Smile: The multiple faces of perceptual
categorizations. Submitted for publication.
Tanaka, J. and Sengco, 1. (1997). Features and their configuration in face recognition.
Memory and Cognition. In press.

"
6505,2017,Beyond Parity: Fairness Objectives for Collaborative Filtering,"We study fairness in collaborative-filtering recommender systems, which are sensitive to discrimination that exists in historical data. Biased data can lead collaborative-filtering methods to make unfair predictions for users from minority groups. We identify the insufficiency of existing fairness metrics and propose four new metrics that address different forms of unfairness. These fairness metrics can be optimized by adding fairness terms to the learning objective. Experiments on synthetic and real data show that our new metrics can better measure fairness than the baseline, and that the fairness objectives effectively help reduce unfairness.","Beyond Parity:
Fairness Objectives for Collaborative Filtering

Sirui Yao
Department of Computer Science
Virginia Tech
Blacksburg, VA 24061
ysirui@vt.edu

Bert Huang
Department of Computer Science
Virginia Tech
Blacksburg, VA 24061
bhuang@vt.edu

Abstract
We study fairness in collaborative-filtering recommender systems, which are
sensitive to discrimination that exists in historical data. Biased data can lead
collaborative-filtering methods to make unfair predictions for users from minority
groups. We identify the insufficiency of existing fairness metrics and propose four
new metrics that address different forms of unfairness. These fairness metrics can
be optimized by adding fairness terms to the learning objective. Experiments on
synthetic and real data show that our new metrics can better measure fairness than
the baseline, and that the fairness objectives effectively help reduce unfairness.

1

Introduction

This paper introduces new measures of unfairness in algorithmic recommendation and demonstrates
how to optimize these metrics to reduce different forms of unfairness. Recommender systems study
user behavior and make recommendations to support decision making. They have been widely applied
in various fields to recommend items such as movies, products, jobs, and courses. However, since
recommender systems make predictions based on observed data, they can easily inherit bias that may
already exist. To address this issue, we first formalize the problem of unfairness in recommender
systems and identify the insufficiency of demographic parity for this setting. We then propose four
new unfairness metrics that address different forms of unfairness. We compare our fairness measures
with non-parity on biased, synthetic training data and prove that our metrics can better measure
unfairness. To improve model fairness, we provide five fairness objectives that can be optimized, each
adding unfairness penalties as regularizers. Experimenting on real and synthetic data, we demonstrate
that each fairness metric can be optimized without much degradation in prediction accuracy, but that
trade-offs exist among the different forms of unfairness.
We focus on a frequently practiced approach for recommendation called collaborative filtering,
which makes recommendations based on the ratings or behavior of other users in the system. The
fundamental assumption behind collaborative filtering is that other users? opinions can be selected
and aggregated in such a way as to provide a reasonable prediction of the active user?s preference [7].
For example, if a user likes item A, and many other users who like item A also like item B, then it is
reasonable to expect that the user will also like item B. Collaborative filtering methods would predict
that the user will give item B a high rating.
With this approach, predictions are made based on co-occurrence statistics, and most methods assume
that the missing ratings are missing at random. Unfortunately, researchers have shown that sampled
ratings have markedly different properties from the users? true preferences [21, 22]. Sampling is
heavily influenced by social bias, which results in more missing ratings in some cases than others.
This non-random pattern of missing and observed rating data is a potential source of unfairness.
For the purpose of improving recommendation accuracy, there are collaborative filtering models
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

[2, 21, 25] that use side information to address the problem of imbalanced data, but in this work,
to test the properties and effectiveness of our metrics, we focus on the basic matrix-factorization
algorithm first. Investigating how these other models could reduce unfairness is one direction for
future research.
Throughout the paper, we consider a running example of unfair recommendation. We consider
recommendation in education, and unfairness that may occur in areas with current gender imbalance,
such as science, technology, engineering, and mathematics (STEM) topics. Due to societal and
cultural influences, fewer female students currently choose careers in STEM. For example, in
2010, women accounted for only 18% of the bachelor?s degrees awarded in computer science [3].
The underrepresentation of women causes historical rating data of computer-science courses to be
dominated by men. Consequently, the learned model may underestimate women?s preferences and
be biased toward men. We consider the setting in which, even if the ratings provided by students
accurately reflect their true preferences, the bias in which ratings are reported leads to unfairness.
The remainder of the paper is organized as follows. First, we review previous relevant work in
Section 2. In Section 3, we formalize the recommendation problem, and we introduce four new
unfairness metrics and give justifications and examples. In Section 4, we show that unfairness
occurs as data gets more imbalanced, and we present results that successfully minimize each form of
unfairness. Finally, Section 5 concludes the paper and proposes possible future work.

2

Related Work

As machine learning is being more widely applied in modern society, researchers have begun
identifying the criticality of algorithmic fairness. Various studies have considered algorithmic fairness
in problems such as supervised classification [20, 23, 28]. When aiming to protect algorithms from
treating people differently for prejudicial reasons, removing sensitive features (e.g., gender, race, or
age) can help alleviate unfairness but is often insufficient. Features are often correlated, so other
unprotected attributes can be related to the sensitive features and therefore still cause the model to
be biased [17, 29]. Moreover, in problems such as collaborative filtering, algorithms do not directly
consider measured features and instead infer latent user attributes from their behavior.
Another frequently practiced strategy for encouraging fairness is to enforce demographic parity,
which is to achieve statistical parity among groups. The goal is to ensure that the overall proportion
of members in the protected group receiving positive (or negative) classifications is identical to the
proportion of the population as a whole [29]. For example, in the case of a binary decision Y? ? {0, 1}
and a binary protected attribute A ? {0, 1}, this constraint can be formalized as [9]
Pr{Y? = 1|A = 0} = Pr{Y? = 1|A = 1} .

(1)

Kamishima et al. [13?17] evaluate model fairness based on this non-parity unfairness concept, or try
to solve the unfairness issue in recommender systems by adding a regularization term that enforces
demographic parity. The objective penalizes the differences among the average predicted ratings of
user groups. However, demographic parity is only appropriate when preferences are unrelated to
the sensitive features. In tasks such as recommendation, user preferences are indeed influenced by
sensitive features such as gender, race, and age [4, 6]. Therefore, enforcing demographic parity may
significantly damage the quality of recommendations.
To address the issue of demographic parity, Hardt et al. [9] propose to measure unfairness with the
true positive rate and true negative rate. This idea encourages what they refer to as equal opportunity
and no longer relies on the implicit assumption of demographic parity that the target variable is
independent of sensitive features. They propose that, in a binary setting, given a decision Y? ? {0, 1},
a protected attribute A ? {0, 1}, and the true label Y ? {0, 1}, the constraints are equivalent to [9]
Pr{Y? = 1|A = 0, Y = y} = Pr{Y? = 1|A = 1, Y = y}, y ? {0, 1} .

(2)

This constraint upholds fairness and simultaneously respects group differences. It penalizes models
that only perform well on the majority groups. This idea is also the basis of the unfairness metrics we
propose for recommendation.
Our running example of recommendation in education is inspired by the recent interest in using
algorithms in this domain [5, 24, 27]. Student decisions about which courses to study can have
2

significant impacts on their lives, so the usage of algorithmic recommendation in this setting has
consequences that will affect society for generations. Coupling the importance of this application with
the issue of gender imbalance in STEM [1] and challenges in retention of students with backgrounds
underrepresented in STEM [8, 26], we find this setting a serious motivation to advance scientific
understanding of unfairness?and methods to reduce unfairness?in recommendation.

3

Fairness Objectives for Collaborative Filtering

This section introduces fairness objectives for collaborative filtering. We begin by reviewing the
matrix factorization method. We then describe the various fairness objectives we consider, providing
formal definitions and discussion of their motivations.
3.1

Matrix Factorization for Recommendation

We consider the task of collaborative filtering using matrix factorization [19]. We have a set of users
indexed from 1 to m and a set of items indexed from 1 to n. For the ith user, let gi be a variable
indicating which group the ith user belongs to. For example, it may indicate whether user i identifies
as a woman, a man, or with a non-binary gender identity. For the jth item, let hj indicate the item
group that it belongs to. For example, hj may represent a genre of a movie or topic of a course. Let
rij be the preference score of the ith user for the jth item. The ratings can be viewed as entries in a
rating matrix R.
The matrix-factorization formulation builds on the assumption that each rating can be represented as
the product of vectors representing the user and item. With additional bias terms for users and items,
this assumption can be summarized as follows:
rij ? p>
i q j + ui + vj ,

(3)

where pi is a d-dimensional vector representing the ith user, q j is a d-dimensional vector representing
the jth item, and ui and vj are scalar bias terms for the user and item, respectively. The matrixfactorization learning algorithm seeks to learn these parameters from observed ratings X, typically
by minimizing a regularized, squared reconstruction error:

1 X
?
2
||P ||2F + ||Q||2F +
(yij ? rij ) ,
(4)
J(P , Q, u, v) =
2
|X|
(i,j)?X

where u and v are the vectors of bias terms, || ? ||F represents the Frobenius norm, and
yij = p>
i q j + ui + vj .

(5)

Strategies for minimizing this non-convex objective are well studied, and a general approach is to
compute the gradient and use a gradient-based optimizer. In our experiments, we use the Adam
algorithm [18], which combines adaptive learning rates with momentum.
3.2

Unfair Recommendations from Underrepresentation

In this section, we describe a process through which matrix factorization leads to unfair recommendations, even when rating data accurately reflects users? true preferences. Such unfairness can
occur with imbalanced data. We identify two forms of underrepresentation: population imbalance
and observation bias. We later demonstrate that either leads to unfair recommendation, and both
forms together lead to worse unfairness. In our discussion, we use a running example of course
recommendation, highlighting effects of underrepresentation in STEM education.
Population imbalance occurs when different types of users occur in the dataset with varied frequencies.
For example, we consider four types of users defined by two aspects. First, each individual identifies
with a gender. For simplicity, we only consider binary gender identities, though in this example, it
would also be appropriate to consider men as one gender group and women and all non-binary gender
identities as the second group. Second, each individual is either someone who enjoys and would
excel in STEM topics or someone who does and would not. Population imbalance occurs in STEM
education when, because of systemic bias or other societal problems, there may be significantly fewer
women who succeed in STEM (WS) than those who do not (W), and because of converse societal
3

unfairness, there may be more men who succeed in STEM (MS) than those who do not (M). This
four-way separation of user groups is not available to the recommender system, which instead may
only know the gender group of each user, but not their proclivity for STEM.
Observation bias is a related but distinct form of data imbalance, in which certain types of users
may have different tendencies to rate different types of items. This bias is often part of a feedback
loop involving existing methods of recommendation, whether by algorithms or by humans. If an
individual is never recommended a particular item, they will likely never provide rating data for that
item. Therefore, algorithms will never be able to directly learn about this preference relationship.
In the education example, if women are rarely recommended to take STEM courses, there may be
significantly less training data about women in STEM courses.
We simulate these two types of data bias with two stochastic block models [11]. We create one block
model that determines the probability that an individual in a particular user group likes an item in a
particular item group. The group ratios may be non-uniform, leading to population imbalance. We
then use a second block model to determine the probability that an individual in a user group rates an
item in an item group. Non-uniformity in the second block model will lead to observation bias.
Formally, let matrix L ? [0, 1]|g|?|h| be the block-model parameters for rating probability. For the
ith user and the jth item, the probability of rij = +1 is L(gi ,hj ) , and otherwise rij = ?1. Morever,
let O ? [0, 1]|g|?|h| be such that the probability of observing rij is O(gi ,hj ) .
3.3

Fairness Metrics

In this section, we present four new unfairness metrics for preference prediction, all measuring a
discrepancy between the prediction behavior for disadvantaged users and advantaged users. Each
metric captures a different type of unfairness that may have different consequences. We describe the
mathematical formulation of each metric, its justification, and examples of consequences the metric
may indicate. We consider a binary group feature and refer to disadvantaged and advantaged groups,
which may represent women and men in our education example.
The first metric is value unfairness, which measures inconsistency in signed estimation error across
the user types, computed as
n

 
1 X 

(6)
Uval =
 Eg [y]j ? Eg [r]j ? E?g [y]j ? E?g [r]j  ,
n j=1
where Eg [y]j is the average predicted score for the jth item from disadvantaged users, E?g [y]j is
the average predicted score for advantaged users, and Eg [r]j and E?g [r]j are the average ratings for
the disadvantaged and advantaged users, respectively. Precisely, the quantity Eg [y]j is computed as
X
1
Eg [y]j :=
yij ,
(7)
|{i : ((i, j) ? X) ? gi }|
i:((i,j)?X)?gi

and the other averages are computed analogously.
Value unfairness occurs when one class of user is consistently given higher or lower predictions
than their true preferences. If the errors in prediction are evenly balanced between overestimation
and underestimation or if both classes of users have the same direction and magnitude of error, the
value unfairness becomes small. Value unfairness becomes large when predictions for one class
are consistently overestimated and predictions for the other class are consistently underestimated.
For example, in a course recommender, value unfairness may manifest in male students being
recommended STEM courses even when they are not interested in STEM topics and female students
not being recommended STEM courses even if they are interested in STEM topics.
The second metric is absolute unfairness, which measures inconsistency in absolute estimation error
across user types, computed as
n
 

1 X 
 

Uabs =
(8)
Eg [y]j ? Eg [r]j  ? E?g [y]j ? E?g [r]j  .
n j=1
Absolute unfairness is unsigned, so it captures a single statistic representing the quality of prediction
for each user type. If one user type has small reconstruction error and the other user type has large
4

reconstruction error, one type of user has the unfair advantage of good recommendation, while the
other user type has poor recommendation. In contrast to value unfairness, absolute unfairness does
not consider the direction of error. For example, if female students are given predictions 0.5 points
below their true preferences and male students are given predictions 0.5 points above their true
preferences, there is no absolute unfairness. Conversely, if female students are given ratings that are
off by 2 points in either direction while male students are rated within 1 point of their true preferences,
absolute unfairness is high, while value unfairness may be low.
The third metric is underestimation unfairness, which measures inconsistency in how much the
predictions underestimate the true ratings:
Uunder =

n

1 X 

max{0, Eg [r]j ? Eg [y]j } ? max{0, E?g [r]j ? E?g [y]j } .
n j=1

(9)

Underestimation unfairness is important in settings where missing recommendations are more critical
than extra recommendations. For example, underestimation could lead to a top student not being
recommended to explore a topic they would excel in.
Conversely, the fourth new metric is overestimation unfairness, which measures inconsistency in how
much the predictions overestimate the true ratings:
Uover =

n

1 X 

max{0, Eg [y]j ? Eg [r]j } ? max{0, E?g [y]j ? E?g [r]j } .
n j=1

(10)

Overestimation unfairness may be important in settings where users may be overwhelmed by recommendations, so providing too many recommendations would be especially detrimental. For example,
if users must invest large amounts of time to evaluate each recommended item, overestimating
essentially costs the user time. Thus, uneven amounts of overestimation could cost one type of user
more time than the other.
Finally, a non-parity unfairness measure based on the regularization term introduced by Kamishima
et al. [17] can be computed as the absolute difference between the overall average ratings of disadvantaged users and those of advantaged users:
Upar = |Eg [y] ? E?g [y]| .
Each of these metrics has a straightforward subgradient and can be optimized by various subgradient
optimization techniques. We augment the learning objective by adding a smoothed variation of a
fairness metric based on the Huber loss [12], where the outer absolute value is replaced with the
squared difference if it is less than 1. We solve for a local minimum, i.e,
min

P ,Q,u,v

J(P , Q, u, v) + U .

(11)

The smoothed penalty helps reduce discontinuities in the objective, making optimization more
efficient. It is also straightforward to add a scalar trade-off term to weight the fairness against the
loss. In our experiments, we use equal weighting, so we omit the term from Eq. (11).

4

Experiments

We run experiments on synthetic data based on the simulated course-recommendation scenario and
real movie rating data [10]. For each experiment, we investigate whether the learning objectives
augmented with unfairness penalties successfully reduce unfairness.
4.1

Synthetic Data

In our synthetic experiments, we generate simulated course-recommendation data from a block model
as described in Section 3.2. We consider four user groups g ? {W, WS, M, MS} and three item
groups h ? {Fem, STEM, Masc}. The user groups can be thought of as women who do not enjoy
STEM topics (W), women who do enjoy STEM topics (WS), men who do not enjoy STEM topics
(M), and men who do (MS). The item groups can be thought of as courses that tend to appeal to most
5

0.12

Error

0.08
0.06
0.04
0.02
U

O

P

O+P

0.02
0.01

U

O

P

O+P

0.00

U

O

P

O+P

U

O

P

O+P

0.25
0.020

0.20

0.015

0.15

Over

Under

0.0200
0.0175
0.0150
0.0125
0.0100
0.0075
0.0050
0.0025
0.0000

0.03

Parity

0.00

0.04

Value

0.10

0.040
0.035
0.030
0.025
0.020
0.015
0.010
0.005
0.000

Absolute

0.14

0.010

0.10

0.005
U

O

P

O+P

0.000

0.05
U

O

P

O+P

0.00

Figure 1: Average unfairness scores for standard matrix factorization on synthetic data generated from different
underrepresentation schemes. For each metric, the four sampling schemes are uniform (U), biased observations
(O), biased populations (P), and both biases (O+P). The reconstruction error and the first four unfairness metrics
follow the same trend, while non-parity exhibits different behavior.

women (Fem), STEM courses, and courses that tend to appeal to most men (Masc). Based on these
groups, we consider the rating block model
?
?
Fem STEM Masc
?
?
0.8
0.2
0.2 ?
? W
?
L = ? WS 0.8
(12)
0.8
0.2 ?
?.
? MS 0.2
0.8
0.8 ?
M
0.2
0.2
0.8
We also consider two observation block models: one with uniform observation probability across all
groups O uni = [0.4]4?3 and one with unbalanced observation probability inspired by how students
are often encouraged to take certain courses
?
?
Fem STEM Masc
?
?
0.6
0.2
0.1 ?
? W
bias
?
(13)
O = ? WS 0.3
0.4
0.2 ?
? .
? MS 0.1
0.3
0.5 ?
0.05
0.5
0.35
M
We define two different user group distributions: one in which each of the four groups is exactly a
quarter of the population, and an imbalanced setting where 0.4 of the population is in W, 0.1 in WS,
0.4 in MS, and 0.1 in M. This heavy imbalance is inspired by some of the severe gender imbalances
in certain STEM areas today.
For each experiment, we select an observation matrix and user group distribution, generate 400 users
and 300 items, and sample preferences and observations of those preferences from the block models.
Training on these ratings, we evaluate on the remaining entries of the rating matrix, comparing the
predicted rating against the true expected rating, 2L(gi ,hj ) ? 1.
4.1.1

Unfairness from different types of underrepresentation

Using standard matrix factorization, we measure the various unfairness metrics under the different
sampling conditions. We average over five random trials and plot the average score in Fig. 1.
We label the settings as follows: uniform user groups and uniform observation probabilities (U),
uniform groups and biased observation probabilities (O), biased user group populations and uniform
observations (P), and biased populations and biased observations (P+O).
The statistics demonstrate that each type of underrepresentation contributes to various forms of
unfairness. For all metrics except parity, there is a strict order of unfairness: uniform data is the most
6

Table 1: Average error and unfairness metrics for synthetic data using different fairness objectives. The best
scores and those that are statistically indistinguishable from the best are printed in bold. Each row represents a
different unfairness penalty, and each column is the measured metric on the expected value of unseen ratings.
Unfairness

Error

Value

Absolute

Underestimation

Overestimation

Non-Parity

None
Value
Absolute
Under
Over
Non-Parity

0.317 ? 1.3e-02
0.130 ? 1.0e-02
0.205 ? 8.8e-03
0.269 ? 1.6e-02
0.130 ? 6.5e-03
0.324 ? 1.3e-02

0.649 ? 1.8e-02
0.245 ? 1.4e-02
0.535 ? 1.6e-02
0.512 ? 2.3e-02
0.296 ? 1.2e-02
0.697 ? 1.8e-02

0.443 ? 2.2e-02
0.177 ? 1.5e-02
0.267 ? 1.3e-02
0.401 ? 2.4e-02
0.172 ? 1.3e-02
0.453 ? 2.2e-02

0.107 ? 6.5e-03
0.063 ? 4.1e-03
0.135 ? 6.2e-03
0.060 ? 3.5e-03
0.074 ? 6.0e-03
0.124 ? 6.9e-03

0.544 ? 2.0e-02
0.199 ? 1.5e-02
0.400 ? 1.4e-02
0.456 ? 2.3e-02
0.228 ? 1.1e-02
0.573 ? 1.9e-02

0.362 ? 1.6e-02
0.324 ? 1.2e-02
0.294 ? 1.0e-02
0.357 ? 1.6e-02
0.321 ? 1.2e-02
0.251 ? 1.0e-02

fair; biased observations is the next most fair; biased populations is worse; and biasing the populations
and observations causes the most unfairness. The squared rating error also follows this same trend.
In contrast, non-parity behaves differently, in that it is heavily amplified by biased observations but
seems unaffected by biased populations. Note that though non-parity is high when the observations
are imbalanced, because of the imbalance in the observations, one should actually expect non-parity
in the labeled ratings, so it a high non-parity score does not necessarily indicate an unfair situation.
The other unfairness metrics, on the other hand, describe examples of unfair behavior by the rating
predictor. These tests verify that unfairness can occur with imbalanced populations or observations,
even when the measured ratings accurately represent user preferences.
4.1.2

Optimization of unfairness metrics

As before, we generate rating data using the block model under the most imbalanced setting: The
user populations are imbalanced, and the sampling rate is skewed. We provide the sampled ratings
to the matrix factorization algorithms and evaluate on the remaining entries of the expected rating
matrix. We again use two-dimensional vectors to represent the users and items, a regularization term
of ? = 10?3 , and optimize for 250 iterations using the full gradient. We generate three datasets each
and measure squared reconstruction error and the six unfairness metrics.
The results are listed in Table 1. For each metric, we print in bold the best average score and any
scores that are not statistically significantly distinct according to paired t-tests with threshold 0.05.
The results indicate that the learning algorithm successfully minimizes the unfairness penalties,
generalizing to unseen, held-out user-item pairs. And reducing any unfairness metric does not lead to
a significant increase in reconstruction error.
The complexity of computing the unfairness metrics is similar to that of the error computation, which
is linear in the number of ratings, so adding the fairness term approximately doubles the training time.
In our implementation, learning with fairness terms takes longer because loops and backpropagation
introduce extra overhead. For example, with synthetic data of 400 users and 300 items, it takes 13.46
seconds to train a matrix factorization model without any unfairness term and 43.71 seconds for one
with value unfairness.
While optimizing each metric leads to improved performance on itself (see the highlighted entries
in Table 1), a few trends are worth noting. Optimizing any of our new unfairness metrics almost
always reduces the other forms of unfairness. An exception is that optimizing absolute unfairness
leads to an increase in underestimation. Value unfairness is closely related to underestimation and
overestimation, since optimizing value unfairness is even more effective at reducing underestimation
and overestimation than directly optimizing them. Also, optimizing value and overestimation are
more effective in reducing absolute unfairness than directly optimizing it. Finally, optimizing parity
unfairness leads to increases in all unfairness metrics except absolute unfairness and parity itself.
These relationships among the metrics suggest a need for practitioners to decide which types of
fairness are most important for their applications.
4.2

Real Data

We use the Movielens Million Dataset [10], which contains ratings (from 1 to 5) by 6,040 users of
3,883 movies. The users are annotated with demographic variables including gender, and the movies
are each annotated with a set of genres. We manually selected genres that feature different forms of
7

Table 2: Gender-based statistics of movie genres in Movielens data.
Count
Ratings per female user
Ratings per male user
Average rating by women
Average rating by men

Romance

Action

Sci-Fi

Musical

Crime

325
54.79
36.97
3.64
3.55

425
52.00
82.97
3.45
3.45

237
31.19
50.46
3.42
3.44

93
15.04
10.83
3.79
3.58

142
17.45
23.90
3.65
3.68

Table 3: Average error and unfairness metrics for movie-rating data using different fairness objectives.
Unfairness

Error

Value

Absolute

Underestimation

Overestimation

Non-Parity

None
Value
Absolute
Under
Over
Non-Parity

0.887 ? 1.9e-03
0.886 ? 2.2e-03
0.887 ? 2.0e-03
0.888 ? 2.2e-03
0.885 ? 1.9e-03
0.887 ? 1.9e-03

0.234 ? 6.3e-03
0.223 ? 6.9e-03
0.235 ? 6.2e-03
0.233 ? 6.8e-03
0.234 ? 5.8e-03
0.236 ? 6.0e-03

0.126 ? 1.7e-03
0.128 ? 2.2e-03
0.124 ? 1.7e-03
0.128 ? 1.8e-03
0.125 ? 1.6e-03
0.126 ? 1.6e-03

0.107 ? 1.6e-03
0.102 ? 1.9e-03
0.110 ? 1.8e-03
0.102 ? 1.7e-03
0.112 ? 1.9e-03
0.110 ? 1.7e-03

0.153 ? 3.9e-03
0.148 ? 4.9e-03
0.151 ? 4.2e-03
0.156 ? 4.2e-03
0.148 ? 4.1e-03
0.152 ? 3.9e-03

0.036 ? 1.3e-03
0.041 ? 1.6e-03
0.023 ? 2.7e-03
0.058 ? 9.3e-04
0.015 ? 2.0e-03
0.010 ? 1.5e-03

gender imbalance and only consider movies that list these genres. Then we filter the users to only
consider those who rated at least 50 of the selected movies.
The genres we selected are action, crime, musical, romance, and sci-fi. We selected these genres
because they each have a noticeable gender effect in the data. Women rate musical and romance films
higher and more frequently than men. Women and men both score action, crime, and sci-fi films
about equally, but men rate these film much more frequently. Table 2 lists these statistics in detail.
After filtering by genre and rating frequency, we have 2,953 users and 1,006 movies in the dataset.
We run five trials in which we randomly split the ratings into training and testing sets, train each
objective function on the training set, and evaluate each metric on the testing set. The average scores
are listed in Table 3, where bold scores again indicate being statistically indistinguishable from the
best average score. On real data, the results show that optimizing each unfairness metric leads to the
best performance on that metric without a significant change in the reconstruction error. As in the
synthetic data, optimizing value unfairness leads to the most decrease on under- and overestimation.
Optimizing non-parity again causes an increase or no change in almost all the other unfairness
metrics.

5

Conclusion

In this paper, we discussed various types of unfairness that can occur in collaborative filtering. We
demonstrate that these forms of unfairness can occur even when the observed rating data is correct, in
the sense that it accurately reflects the preferences of the users. We identify two forms of data bias
that can lead to such unfairness. We then demonstrate that augmenting matrix-factorization objectives
with these unfairness metrics as penalty functions enables a learning algorithm to minimize each of
them. Our experiments on synthetic and real data show that minimization of these forms of unfairness
is possible with no significant increase in reconstruction error.
We also demonstrate a combined objective that penalizes both overestimation and underestimation.
Minimizing this objective leads to small unfairness penalties for the other forms of unfairness. Using
this combined objective may be a good approach for practitioners. However, no single objective
was the best for all unfairness metrics, so it remains necessary for practitioners to consider precisely
which form of fairness is most important in their application and optimize that specific objective.
Future Work While our work in this paper focused on improving fairness among users so that the
model treats different groups of users fairly, we did not address fair treatment of different item groups.
The model could be biased toward certain items, e.g., performing better at prediction for some items
than others in terms of accuracy or over- and underestimation. Achieving fairness for both users and
items may be important when considering that the items may also suffer from discrimination or bias,
for example, when courses are taught by instructors with different demographics.
Our experiments demonstrate that minimizing empirical unfairness generalizes, but this generalization
is dependent on data density. When ratings are especially sparse, the empirical fairness does not
8

always generalize well to held-out predictions. We are investigating methods that are more robust to
data sparsity in future work.
Moreover, our fairness metrics assume that users rate items according to their true preferences.
This assumption is likely to be violated in real data, since ratings can also be influenced by various
environmental factors. E.g., in education, a student?s rating for a course also depends on whether the
course has an inclusive and welcoming learning environment. However, addressing this type of bias
may require additional information or external interventions beyond the provided rating data.
Finally, we are investigating methods to reduce unfairness by directly modeling the two-stage
sampling process we used to generate synthetic, biased data. We hypothesize that by explicitly
modeling the rating and observation probabilities as separate variables, we may be able to derive a
principled, probabilistic approach to address these forms of data imbalance.

References
[1] D. N. Beede, T. A. Julian, D. Langdon, G. McKittrick, B. Khan, and M. E. Doms. Women in
STEM: A gender gap to innovation. U.S. Department of Commerce, Economics and Statistics
Administration, 2011.
[2] A. Beutel, E. H. Chi, Z. Cheng, H. Pham, and J. Anderson. Beyond globally optimal: Focused
learning for improved recommendations. In Proceedings of the 26th International Conference
on World Wide Web, pages 203?212. International World Wide Web Conferences Steering
Committee, 2017.
[3] S. Broad and M. McGee. Recruiting women into computer science and information systems.
Proceedings of the Association Supporting Computer Users in Education Annual Conference,
pages 29?40, 2014.
[4] O. Chausson. Who watches what? Assessing the impact of gender and personality on film
preferences. http://mypersonality.org/wiki/doku.php?id=movie_tastes_and_personality, 2010.
[5] M.-I. Dascalu, C.-N. Bodea, M. N. Mihailescu, E. A. Tanase, and P. Ordo?ez de Pablos.
Educational recommender systems and their application in lifelong learning. Behaviour &
Information Technology, 35(4):290?297, 2016.
[6] T. N. Daymont and P. J. Andrisani. Job preferences, college major, and the gender gap in
earnings. Journal of Human Resources, pages 408?428, 1984.
[7] M. D. Ekstrand, J. T. Riedl, J. A. Konstan, et al. Collaborative filtering recommender systems.
Foundations and Trends in Human-Computer Interaction, 4(2):81?173, 2011.
[8] A. L. Griffith. Persistence of women and minorities in STEM field majors: Is it the school that
matters? Economics of Education Review, 29(6):911?922, 2010.
[9] M. Hardt, E. Price, N. Srebro, et al. Equality of opportunity in supervised learning. In Advances
in Neural Information Processing Systems, pages 3315?3323, 2016.
[10] F. M. Harper and J. A. Konstan. The Movielens datasets: History and context. ACM Transactions
on Interactive Intelligent Systems (TiiS), 5(4):19, 2016.
[11] P. W. Holland and S. Leinhardt. Local structure in social networks. Sociological Methodology,
7:1?45, 1976.
[12] P. J. Huber. Robust estimation of a location parameter. The Annals of Mathematical Statistics,
pages 73?101, 1964.
[13] T. Kamishima, S. Akaho, H. Asoh, and J. Sakuma. Enhancement of the neutrality in recommendation. In Decisions@ RecSys, pages 8?14, 2012.
[14] T. Kamishima, S. Akaho, H. Asoh, and J. Sakuma. Efficiency improvement of neutralityenhanced recommendation. In Decisions@ RecSys, pages 1?8, 2013.
[15] T. Kamishima, S. Akaho, H. Asoh, and J. Sakuma. Correcting popularity bias by enhancing
recommendation neutrality. In RecSys Posters, 2014.
9

[16] T. Kamishima, S. Akaho, H. Asoh, and I. Sato. Model-based approaches for independenceenhanced recommendation. In Data Mining Workshops (ICDMW), 2016 IEEE 16th International
Conference on, pages 860?867. IEEE, 2016.
[17] T. Kamishima, S. Akaho, and J. Sakuma. Fairness-aware learning through regularization
approach. In 11th International Conference on Data Mining Workshops (ICDMW), pages
643?650. IEEE, 2011.
[18] D. Kingma and J. Ba.
arXiv:1412.6980, 2014.

Adam: A method for stochastic optimization.

arXiv preprint

[19] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems.
Computer, 42(8), 2009.
[20] K. Lum and J. Johndrow. A statistical framework for fair predictive algorithms. arXiv preprint
arXiv:1610.08077, 2016.
[21] B. Marlin, R. S. Zemel, S. Roweis, and M. Slaney. Collaborative filtering and the missing at
random assumption. arXiv preprint arXiv:1206.5267, 2012.
[22] B. M. Marlin and R. S. Zemel. Collaborative prediction and ranking with non-random missing
data. In Proceedings of the third ACM conference on Recommender systems, pages 5?12. ACM,
2009.
[23] D. Pedreshi, S. Ruggieri, and F. Turini. Discrimination-aware data mining. In Proceedings of
the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
pages 560?568. ACM, 2008.
[24] C. V. Sacin, J. B. Agapito, L. Shafti, and A. Ortigosa. Recommendation in higher education
using data mining techniques. In Educational Data Mining, 2009.
[25] S. Sahebi and P. Brusilovsky. It takes two to tango: An exploration of domain pairs for crossdomain collaborative filtering. In Proceedings of the 9th ACM Conference on Recommender
Systems, pages 131?138. ACM, 2015.
[26] E. Smith. Women into science and engineering? Gendered participation in higher education
STEM subjects. British Educational Research Journal, 37(6):993?1014, 2011.
[27] N. Thai-Nghe, L. Drumond, A. Krohn-Grimberghe, and L. Schmidt-Thieme. Recommender
system for predicting student performance. Procedia Computer Science, 1(2):2811?2819, 2010.
[28] M. B. Zafar, I. Valera, M. Gomez Rodriguez, and K. P. Gummadi. Fairness constraints:
Mechanisms for fair classification. arXiv preprint arXiv:1507.05259, 2017.
[29] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork. Learning fair representations. In
Proceedings of the 30th International Conference on Machine Learning, pages 325?333, 2013.

10

"
1046,2001,The Infinite Hidden Markov Model,Abstract Missing,"The Infinite Hidden Markov Model
Matthew J. Beal

Zoubin Ghahramani

Carl Edward Rasmussen

Gatsby Computational Neuroscience Unit
University College London
17 Queen Square, London WC1N 3AR, England
http://www.gatsby.ucl.ac.uk
m.beal,zoubin,edward @gatsby.ucl.ac.uk



Abstract
We show that it is possible to extend hidden Markov models to have
a countably infinite number of hidden states. By using the theory of
Dirichlet processes we can implicitly integrate out the infinitely many
transition parameters, leaving only three hyperparameters which can be
learned from data. These three hyperparameters define a hierarchical
Dirichlet process capable of capturing a rich set of transition dynamics.
The three hyperparameters control the time scale of the dynamics, the
sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a finite sequence. In this framework it
is also natural to allow the alphabet of emitted symbols to be infinite?
consider, for example, symbols being possible words appearing in English text.

1 Introduction
Hidden Markov models (HMMs) are one of the most popular methods in machine
learning and statistics for modelling sequences such as speech and proteins. An
HMM defines a probability distribution over sequences of observations (symbols)
by invoking another sequence of unobserved, or hidden, discrete
state variables
. The basic idea in an HMM is that the seis independent of
quence of hidden states has Markov dynamics?i.e. given ,
for all
?and that the observations
are independent of all other variables
given . The model is defined in terms of two sets of parameters, the transition matrix
whose
element is
and the emission matrix whose
element
is
. The usual procedure for estimating the parameters of an HMM is
the Baum-Welch algorithm, a special case of EM, which estimates expected values of two
matrices and corresponding to counts of transitions and emissions respectively, where
the expectation is taken over the posterior probability of hidden state sequences [6].


	
			    		  		 


'

! #""$&%  *12  
.0/  (*);+-, 9<4  (=7 .0/ 3)54 6(87
> ?




  

(:9+-,

Both the standard estimation procedure and the model definition for HMMs suffer from
important limitations. First, maximum likelihood estimation procedures do not consider
the complexity of the model, making it hard to avoid over or underfitting. Second, the
model structure has to be specified in advance. Motivated in part by these problems there
have been attempts to approximate a full Bayesian analysis of HMMs which integrates over,
rather than optimises, the parameters. It has been proposed to approximate such Bayesian
integration both using variational methods [3] and by conditioning on a single most likely
hidden state sequence [8].

In this paper we start from the point of view that the basic modelling assumption of
HMMs?that the data was generated by some discrete state variable which can take on
one of several values?is unreasonable for most real-world problems. Instead we formulate the idea of HMMs with a countably infinite number of hidden states. In principle,
such models have infinitely many parameters in the state transition matrix. Obviously it
would not be sensible to optimise these parameters; instead we use the theory of Dirichlet
processes (DPs) [2, 1] to implicitly integrate them out, leaving just three hyperparameters
defining the prior over transition dynamics.
The idea of using DPs to define mixture models with infinite number of components has
been previously explored in [5] and [7]. This simple form of the DP turns out to be inadequate for HMMs.1 Because of this we have extended the notion of a DP to a two-stage hierarchical process which couples transitions between different states. It should be stressed
that Dirichlet distributions have been used extensively both as priors for mixing proportions and to smooth n-gram models over finite alphabets [4], which differs considerably
from the model presented here. To our knowledge no one has studied inference in discrete
infinite-state HMMs.
We begin with a review of Dirichlet processes in section 2 which we will use as the basis
for the notion of a hierarchical Dirichlet process (HDP) described in section 3. We explore
properties of the HDP prior, showing that it can generate interesting hidden state sequences
and that it can also be used as an emission model for an infinite alphabet of symbols. This
infinite emission model is controlled by two additional hyperparameters. In section 4 we
describe the procedures for inference (Gibbs sampling the hidden states), learning (optimising the hyperparameters), and likelihood evaluation (infinite-state particle filtering).
We present experimental results in section 5 and conclude in section 6.

  (

2 Properties of the Dirichlet Process

 *12
*12

Let us examine in detail the statistics of hidden state transitions
 from a particular state
to
, with the number of hidden states finite and equal to . The transition probabilities
row of the transition
matrix can be interpreted as mixing proportions for
given in the

that we call 
.

( +-,  		 

	 	  from a discrete indicator variable which can take
Imagine drawing
samples
>


	






	
on values
 with proportions given by . The joint distribution of these indicators
is multinomial
.0/  		 4 7   	 with >   / 8	 )7
(1)
	 7 iff  , and otherwise)
where we have used the Kronecker-delta
function ( /

*

2
1

0) has been drawn. Let us see what happens to
to count the number of times > that
the distribution of these indicators when we integrate out the mixing proportions under a



	












 
















 
 

   






 















conjugate prior. We give the mixing proportions a symmetric Dirichlet prior with positive
concentration hyperparameter 


.0/ 4 27





/ 	 	 7$ / / 7 7 

""!$#&%'#)(*+&,.- 0/



0/



1

2

1
0/

 


43
5


 	

76

(2)

where  is restricted to be on the simplex of mixing proportions that sum to 1. We can
analytically integrate out  under this prior to yield:
1
That is, if we only applied the mechanism described in section 2, then state trajectories under the
prior would never visit the same state twice; since each new state will have no previous transitions
from it, the DP would choose randomly between all infinitely many states, therefore transitioning to
another new state with probability 1.



0. /  		 4 7  .0/  		 4 78.0/ 4 27  /->/ 27 27  /*> /  7 7  (3)
Thus
of a particular sequence of indicators is only a function of the counts
	the 	 probability
>indicators
> (denoted
 . The conditional
probability of an indicator given the setting of all other
 	 ) is given by
	
.0/   )54  
 	 27  > > 
   	
(4)

 is the counts as in (1) with the '+-, indicator removed. Note the self-reinforcing
where >

property of (4): is more likely to choose an already popular state. A key property of DPs,


' 








' 




2



1



1









1

1

0/

0/






6



6





6

6





0/









1 

which is at the very heart of the model in this paper, is the expression for (4) when we take
tends to infinity:
the limit as the number of hidden 
 states


) '	 	  i.e. represented

3

6
 	
0. /  )54  27  
 63 1 3 for all unrepresented ) , combined
(5)
 is the number of represented states (i.e. for which > 6	   ), which cannot
where

 6



	 	 > 



be infinite
of
  since.  is finite.  can be interpreted as the number of pseudo-observations

/
/
, i.e. the strength of belief in the symmetric prior.2 In the infinite limit
 acts as an ?innovation? parameter, controlling the tendency for the model to populate a
previously unrepresented state.

3 Hierarchical Dirichlet Process (HDP)
We now consider modelling each row of the transition and emission matrices of an HMM as
a DP. Two key results from the previous section form the basis of the HDP model for infinite
HMMs. The first is that we can integrate out the infinite number of transition parameters,
and represent the process with a finite number of indicator variables. The second is that
under a DP there is a natural tendency to use existing transitions in proportion to their
previous usage, which gives rise to typical trajectories. In sections 3.1 and 3.2 we describe
in detail the HDP model for transitions and emissions for an infinite-state HMM.
3.1 Hidden state transition mechanism

"" >! 
""      8	  >   1  	
(
)
  (
 *12
/ 8( 7 / ) 7
( +-,
>
(6)
.0/ *12  )54   ( 	 > 	 27  $"" #  > %>    ) 	 	  
Note
do not sum to 1?under the DP there is a finite probability
</ "" that> the above7 ofprobabilities
not selecting
transitions. In this case, the model defaults
 *12one
 withof these
to a second different DP (5) on
parameter & whose counts are given by a vector
counts as the oracle. Given that we have
> ' . We refer to the default DP and its associated
defaulted to the oracle DP, the probabilities
now become
*) 	1 of1 transitioning
'

	





	

  i.e. ) represented 	
  +-, /.0 1 ) ) 


	
	
*

2
1


.0/  )54  ( > ' &
7 ( +-, /.0 ) 	1 1 3) 2 '	 	  i.e. ) is a new state  (7)

Imagine we have generated a hidden state sequence up to and including time , building
6
a table
of counts  for transitions that have occured so far from state to , i.e. 



. Given that we are in state
, we impose on state
a DP


(5) with parameter  whose counts are those entries in the
row of , i.e. we prefer to
reuse transitions we have used before and follow typical trajectories (see Figure 1):




0/

 

 

 



 











 


 




 



4

2
Under the infinite model, at any time, there are an infinite number of (indistinguishable) unrepresented states available, each of which have infinitesimal mass proportional to .

nii + ?

? nij + ? + ?
j

self
transition

nij

a)

b)

c)

d)

?

? nij + ? + ? ?nij + ? + ?
j

j

existing
transition

oracle

j=i

njo

?

? n jo + ?

? n jo + ?

existing
state

new
state

j

j

 (time along horizontal axis) from the HDP: we give examples of four modes of
	

 4

 , explores many states with a sparse transition matrix. (b)
 4

 , retraces multiple interacting trajectory segments. (c)  ! 4""# ,
switches between a few different states. (d) $
 4%&
#'&
 , has strict left-to-right transition
Figure 1:

(left) State transition generative mechanism. (right a-d) Sampled state trajectories

of length
behaviour. (a)

dynamics with long linger time.

&

Under the oracle, with probability proportional to an entirely new state is transitioned
to. This is the only mechanism for visiting new
 states from the infinitely many available to

us. After each transition we set 
and, if we transitioned
to the state via the


. If we transitioned to a new
oracle DP just described then in addition we set 
state then the size of and
will increase.

>

>'

>  )( >% 

)

>!' ( > ' 

Self-transitions are special because their probability defines a time scale over which the
dynamics of the hidden state evolves. We assign a finite prior mass to self transitions for
each state; this is the third hyperparameter in our model. Therefore, when first visited (via
in the HDP), its self-transition count is initialised to .

&

*

*

The full hidden state transition mechanism is a two-level DP hierarchy shown in decision
tree form in Figure 1. Alongside are shown typical state trajectories under the prior with
different hyperparameters. We can see that, with just three hyperparameters, there are a
wealth of types of possible trajectories. Note that controls the expected number of represented hidden states, and  influences the tendency to explore new transitions, corresponding to the size and density respectively of the resulting transition count matrix. Finally
controls the prior tendency to linger in a state.

&

*

The role of the oracle is two-fold. First it serves to couple the transition DPs from different
hidden states. Since a newly visited state has no previous transitions to existing states,
without an oracle (which necessarily has knowledge of all represented states as it created
them) it would transition to itself or yet another new state with probability 1. By consulting
the oracle, new states can have finite probability of transitioning to represented states. The
second role of the oracle is to allow some states to be more influential (more commonly
transitioned to) than others.

 ,+  

3.2 Emission mechanism

 -+  *12

The emission process
is identical to the transition process
in every
respect except that there is no concept analogous to a self-transition. Therefore we need
6
only introduce two further hyperparameters  and
for the emission HDP. Like for state
2
2

which is the number
transitions we keep a table of counts


of times before that state has emitted symbol , and
is the number of times symbol

""

(

/.   &).
? 10  ""  9  / ?  8	' (=7 /   8	 9 7
0

miq

?e

?miq + ?e

? miq + ?e

2

10

2500

2000

q

q

existing
emission

oracle

1500
1

10
1000

?e

mq

?q mqo + ?e
existing
symbol

? mqo + ?e
q

new
symbol

500

0
0

0.5

1

1.5

2

0

2.5
4

x 10

10

0

20

40

60

80

100

Figure 2:

(left) State emission generative mechanism. (middle) Word occurence for entire Alice
novel: each word is assigned a unique integer identity as it appears. Word identity (vertical) is plotted
against the word position (horizontal) in the text. (right) (Exp 1) Evolution of number of represented
(vertical), plotted against iterations of Gibbs sweeps (horizontal) during learning of the
states
ascending-descending sequence which requires exactly 10 states to model the data perfectly. Each
line represents initialising the hidden state to a random sequence containing
distinct represented states. (Hyperparameters are not optimised.)

""
 !


9

has been emitted using the emission oracle.

For some applications the training sequence is not expected to contain all possible observation symbols. Consider the occurence of words in natural text e.g. as shown in Figure 2
(middle) for the Alice novel. The upper envelope demonstrates that new words continue to
appear in the novel. A property of the DP is that the expected number of distinct symbols
(i.e. words here) increases as the logarithm of the sequence length. The combination of
an HDP for both hidden states and emissions may well be able to capture the somewhat
super-logarithmic word generation found in Alice.

4 Inference, learning and likelihoods

 	 	
 

	 	& 	 .	& .

Given a sequence of observations, there are two sets of unknowns in the infinite HMM:
, and the five hyperparameters
the hidden state sequence


defining the transition and emission HDPs. Note that by using HDPs for both states and
observations, we have implicitly integrated out the infinitely many transition and emission
parameters. Making an analogy with non-parametric models such as Gaussian Processes,
we define a learned model as a set of counts
and optimised hyperparameters


.

*

> 	 >!' 	 ? 	 ?3'

* 	 	 & 	 . 	 & .

We first describe an approximate Gibbs sampling procedure for inferring the posterior over
the hidden state sequence. We then describe hyperparameter optimisation. Lastly, for calculating the likelihood we introduce an infinite-state particle filter. The following algorithm
summarises the learning procedure:
1. Instantiate a random hidden state sequence
2. For

  
#




	
 .

- Gibbs sample given hyperparameter settings, count matrices, and observations.
- Update count matrices to reflect new ; this may change , the number of represented hidden states.
3. End
4. Update hyperparameters
given hidden state statistics.
5. Goto step 2.






  4 #  4 #


4.1 Gibbs sampling the hidden state sequence

>

?  

> ' > ?  ' ?

Define and as the results of removing from and the transition and emission counts
contributed by . Define similar items
and
related to the transition and emission

/ 7
 ""  	 	

 		*12  / 7
	>  ?  	 >  ' 	 ?3 '  
In order to facilitate hyperparameter learning and improve the mixing
time
of the Gibbs



	

*

2
1


	

sampler,
we
also
sample
a
set
of
auxiliary
indicator
variables
.  alongside
 ; 	each
is a binary variable denoting whether the oracle was used to generate
 *12 	of  these
 respectively.






oracle vectors. An exact Gibbs sweep of the hidden state from
takes 
operations, since under the HDP generative process changing affects the probability of
all subsequent hidden state transitions and emissions.3 However this computation can be
reasonably approximated
in 
, by basing the Gibbs update for only on the state of
6
its neigbours
and the total counts
.4




	 	 & 	 . 	 )& .

4.2 Hyperparameter optimisation

*





We place vague Gamma priors5 on the hyperparameters
. We derive an
approximate form for the hyperparameter posteriors from (3) by treating each level of the
HDPs separately. The following expressions for the posterior for ,  , and  are accurate
for large , while the expressions for and
are exact:

&

*

&

'.

& . 6
# #
.0/ * 	  4 7 / 	' 7 / 3 	 3 7     1 / * / *7  27 / "" 1  /*>%>      ** 7  27 	
# 1  . # 1/2 .7 	

	

	
.0/2 . 4  27 / 3 3 7    / "" 0 ?  1 0   . 7
1
# / &
7 	
#
' 1
	
1
&
.0/ & 4 7 / 7 / ' 1  &7 .0/ & . 4  	 27 / 1 	' 1 7 & / ' . 1  / &)&. 7 . 7
1
  is the number of1 represented states that are transitioned to from
( (includwhere
  is the number of possible emissions from state (state
. ' and ' .
ing itself); similarly .


	





 


 












	





 

 

	 . 



are the number of times the oracle has been
for the transition and emission processes,
 used

calculated from the indicator variables
. We solve for the maximum a posteriori
(MAP) setting for each hyperparameter; for example  MAP is obtained as the solution to
following equation using gradient following techniques such as Newton-Raphson:

/.

"" #    .  /	 .  / . 7!
 / "" 0 ?3 0   . 7 


 

MAP



!

MAP

MAP #""



3 

 / 3 
  '7 /	 .   


MAP

4.3 Infinite-state particle filter
The likelihood for a particular observable sequence of symbols involves intractable sums
over the possible hidden state trajectories. Integrating out the parameters in any HMM
induces long range dependencies between states. In particular, in the DP, making the transition
makes that transition more likely later on in the sequence, so we cannot use
standard tricks like dynamic programming. Furthermore, the number of distinct states can
grow with the sequence length as new states are generated. If the chain starts with distinct states, at time there could be
possible distinct states making the total number
%$ /
$.
of trajectories over the entire length of the sequence

(+ )

  ""

""

/  7 



3
Although the hidden states in an HMM satisfy the Markov condition, integrating out the parameters induces these long-range dependencies.
4
This approximation can be motivated in the following way. Consider sampling parameters &
from the posterior distribution '()&+* , .-%/ of parameter matrices, which will depend on the count
matrices. By the Markov property, for a given & , the probability of only depends on 10 , 2 and
43 , and can therefore be computed without considering its effect on future states.
0 .E 0GF1H
5 567
(48 :9;/ <9=?>A@B(48C/+D 5 =
, with 8 and 9 the shape and inverse-scale parameters.

 	



 )

	



 	 

We propose estimating the likelihood of a test sequence given a learned model using particle
filtering. The idea is to start with some number of particles distributed on the represented
hidden states according to the final state marginal from the training sequence
(some of the

may fall onto new states).6 Starting from the set of particles
, the tables from
 the recursive procedure is as specified
the training sequences
,
and

6
 
below, where
:


  		  

> 	 	>  '  
	 ?  	 ?3'
  "" ""    	 



	






/ 7
.0/ 4
7

     
  #
 	   
 	

  #

 
 	
  	       
  

Compute 	 <'(42 *

 / for each particle  .
  '(42 * 2
( >
 /

:2 10 / .
Calculate 

Resample  particles 
 6 ( %>
	 /

  (
 / .


Update transition and emission tables   ,   for each particle.
6 '( 43 *
For each  sample forward dynamics: 
 3
   / ; this may
cause particles to land on novel states. Update  and  .
6. If !
, Goto 1 with
"" .

1.
2.
3.
4.
5.



"" 



The log likelihood of the test sequence is computed as
+#%$& . Since it is a discrete
state space, with much of the probability mass concentrated on the represented states, it is
feasible to use '
particles.

/ 7

5 Synthetic experiments
Exp 1: Discovering the number of hidden states We applied the infinite HMM inference algorithm to the ascending-descending observation sequence consisting of 30 concatenated copies of (*),+.-0/213/4-5+.) . The most parsimonious HMM which models this
data perfectly has exactly 10 hidden states. The infinite HMM was initialised with a random hidden state sequence, containing distinct represented states. In Figure 2 (right) we
show how the number of represented states evolves with successive Gibbs sweeps, starting
from a variety of initial . In all cases converges to 10, while occasionally exploring 9
and 11.









Exp 2: Expansive A sequence of length 76   was generated from a 4-state 8-symbol
HMM with the transition and emission probabilities as shown in Figure 3 (top left).



Exp 3: Compressive A sequence of length 86   was generated from a 4-state 3-symbol
HMM with the transition and emission probabilities as shown in Figure 3 (bottom left).

 

In both Exp 2 and Exp 3 the infinite HMM was initialised with a hidden state sequence
with :9 distinct states. Figure 3 shows that, over successive Gibbs sweeps and hyperparameter learning, the count matrices for the infinite HMM converge to resemble the true
probability matrices as shown on the far left.

6 Discussion
We have shown how a two-level Hierarchical Dirichlet Process can be used to define a nonparametric Bayesian HMM. The HDP implicity integrates out the transition and emission
parameters of the HMM. An advantage of this is that it is no longer necessary to constrain
the HMM to have finitely many states and observation symbols. The prior over hidden state
transitions defined by the HDP is capable of producing a wealth of interesting trajectories
by varying the three hyperparameters that control it.
We have presented the necessary tools for using the infinite HMM, namely a linear-time
approximate Gibbs sampler for inference, equations for hyperparameter learning, and a
particle filter for likelihood evaluation.
6
Different particle initialisations apply if we do not assume that the test sequence immediately
follows the training sequence.

True transition and
emission probability
matrices used for Exp 2

0

0

 
True transition and
emission probability
matrices used for Exp 3

0

 

 

0



 







0













0







00



0



 





00






0







0



 



0







	
  	
 

Figure 3:

The far left pair of Hinton diagrams represent the true transition and emission probabilities used to generate the data for each experiment 2 and 3 (up to a permutation of the hidden
states; lighter boxes correspond to higher values). (top row) Exp 2: Expansive HMM. Count matrix
are displayed after
sweeps of Gibbs sampling. (bottom
row) Exp 3:
pairs  



sweeps of
Compressive HMM. Similar to top row displaying count matrices after
Gibbs sampling. In both rows the display after a single Gibbs sweep has been reduced in size for
clarity.

 ) 

 
 !

!
   

""

!
 !   

On synthetic data we have shown that the infinite HMM discovers both the appropriate
number of states required to model the data and the structure of the emission and transition
matrices. It is important to emphasise that although the count matrices found by the infinite
HMM resemble point estimates of HMM parameters (e.g. Figure 3), they are better thought
of as the sufficient statistics for the HDP posterior distribution over parameters.
We believe that for many problems the infinite HMM?s flexibile nature and its ability to
automatically determine the required number of hidden states make it superior to the conventional treatment of HMMs with its associated difficult model selection problem. While
the results in this paper are promising, they are limited to synthetic data; in future we hope
to explore the potential of this model on real-world problems.
Acknowledgements
The authors would like to thank David Mackay for suggesting the use of an oracle, and
Quaid Morris for his Perl expertise.

References
[1] C. E. Antoniak. Mixtures of Dirichlet processes with applications to Bayesian nonparametric
problems. Annals of Statistics, 2(6):1152?1174, 1974.
[2] T. S. Ferguson. A Bayesian analysis of some nonparametric problems. Annals of Statistics,
1(2):209?230, March 1973.
[3] D. J. C. MacKay. Ensemble learning for hidden Markov models. Technical report, Cavendish
Laboratory, University of Cambridge, 1997.
[4] D. J. C. MacKay and L. C. Peto. A hierarchical Dirichlet language model. Natural Language
Engineering, 1(3):1?19, 1995.
[5] R. M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Technical
Report 9815, Dept. of Statistics, University of Toronto, 1998.
[6] L. R. Rabiner and B. H. Juang. An introduction to hidden Markov models. IEEE Acoustics,
Speech & Signal Processing Magazine, 3:4?16, 1986.
[7] C. E. Rasmussen. The infinite Gaussian mixture model. In Advances in Neural Information
Processing Systems 12, Cambridge, MA, 2000. MIT Press.
[8] A. Stolcke and S. Omohundro. Hidden Markov model induction by Bayesian model merging. In
S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, Advances in Neural Information Processing
Systems 5, pages 11?18, San Francisco, CA, 1993. Morgan Kaufmann.

"
586,1998,"Divisive Normalization, Line Attractor Networks and Ideal Observers",Abstract Missing,"Divisive Normalization, Line Attractor
Networks and Ideal Observers
Sophie Deneve l Alexandre Pougetl, and P.E. Latham 2
Institute for Computational and Cognitive Sciences,
Georgetown University, Washington, DC 20007-2197
2Dpt of Neurobiology, UCLA, Los Angeles, CA 90095-1763, U.S.A.
1 Georgetown

Abstract
Gain control by divisive inhibition, a.k.a. divisive normalization,
has been proposed to be a general mechanism throughout the visual cortex. We explore in this study the statistical properties
of this normalization in the presence of noise. Using simulations,
we show that divisive normalization is a close approximation to a
maximum likelihood estimator, which, in the context of population
coding, is the same as an ideal observer. We also demonstrate analytically that this is a general property of a large class of nonlinear
recurrent networks with line attractors. Our work suggests that
divisive normalization plays a critical role in noise filtering, and
that every cortical layer may be an ideal observer of the activity in
the preceding layer.
Information processing in the cortex is often formalized as a sequence of a linear
stages followed by a nonlinearity. In the visual cortex, the nonlinearity is best described by squaring combined with a divisive pooling of local activities. The divisive
part of the nonlinearity has been extensively studied by Heeger and colleagues [1],
and several authors have explored the role of this normalization in the computation
of high order visual features such as orientation of edges or first and second order
motion[ 4]. We show in this paper that divisive normalization can also playa role in
noise filtering. More specifically, we demonstrate through simulations that networks
implementing this normalization come close to performing maximum likelihood estimation. We then demonstrate analytically that the ability to perform maximum
likelihood estimation, and thus efficiently extract information from a population of
noisy neurons, is a property exhibited by a large class of networks.
Maximum likelihood estimation is a framework commonly used in the theory of
ideal observers. A recent example comes from the work of Itti et al., 1998, who have
shown that it is possible to account for the behavior of human subjects in simple
discrimination tasks. Their model comprised two distinct stages: 1) a network

105

Divisive Normalization. Line Attractor Networks and Ideal Observers

which models the noisy response of neurons with tuning curves to orientation and
spatial frequency combined with divisive normalization, and 2) an ideal observer (a
maximum likelihood estimator) to read out the population activity of the network.
Our work suggests that there is no need to distinguish between these two stages,
since, as we will show, divisive normalization comes close to providing a maximum
likelihood estimation. More generally, we propose that there may not be any part
of the cortex that acts as an ideal observer for patterns of activity in sensory areas
but, instead , that each cortical layer acts as an ideal observer of the activity in the
preceding layer.

1

The network

Our network is a simplified model of a cortical hypercolumn for spatial frequency
and orientation. It consists of a two dimensional array of units in which each unit
is indexed by its preferred orientation, 8i , and spatial frequency, >'j.
1.1

LGN model

Units in the cortical layer are assumed to receive direct inputs from the lateral
geniculate nucleus (LG N). Here we do not model explicitly the LG N, but focus
instead on the pooled LGN input onto each cortical unit . The input to each unit
is denoted aij' We distinguish between the mean pooled LGN input, fij(8, >'), as
a function of orientation, 8, and spatial frequency, >., and the noise distribution
around this mean, P(aijI8, >.).
In response to a stimulus of orientation, 8, spatial frequency, >., and contrast, G,
the mean LGN input onto unit ij is a circular Gaussian with a small amount of
spontaneous activity, 1/:

J'J,,(8 ,/\')

- KG
-

exp

(COS(>. - >'j) 2

~A

1+

cos(8 - 8i )
2

~o

-

1) +

1/,

(1)

where K is a constant. Note that spatial frequency is treated as a periodic variable;
this was done for convenience only and should have negligible effects on our results
as long as we keep>. far from 27m, n an integer.
On any given trial the LGN input to cortical unit ij, aij, is sampled from a Gaussian
noise distribution with variance ~;j:

(2)

In our simulations, the variance of the noise was either kept fixed (~'fj = ~2) or set
to the mean activity (~t = Jij(8, >.)). The latter is more consistent with the noise
that has been measured experimentally in the cortex. We show in figure I-A an
example of a noisy LGN pattern of activity.
1.2

Cortical Model: Divisive Normalization

Activities in the cortical layer are updated over time according to:

S. Deneve, A. Pouget and P. E. Latham

106
A.

CORTEX

:::::::: r:::L:: t ::;-:::;:::::
- r- , _ :-

:::~
'~

DO

0.1

0.2

0.3

0.4

0.5

0.8

0.7

0.1

D.'

1

Contrast

Figure 1: A- LGN input (bottom) and stable hill in the cortical network after
relaxation (top). The position of the stable hill can be used to estimate orientation
(0) and spatial frequency (5.). B- Inverse of the variance of the network estimate for
orientation using Gaussian noise with variance equal to the mean as a function of
contrast and number of iterations (0, dashed; 1, diamond; 2, circle; and 3, square).
The continuous curve corresponds to the theoretical upper bound on the inverse
of the variance (i.e. an ideal observer). C- Gain curve for contrast for the cortical
units after 1, 2 and 3 iterations.

(3)

where {Wij,kt} are the filtering weights, Oij(t) is the activity of unit ij at time t,
S is a constant, and J1. is what we call the divisive inhibition weight. The filtering
weights implement a two dimensional Gaussian filter:

Wij,kl

=

Wi-k,j - l

= Kwexp (COS[27!'(i

-2 k )/P]
~w~

where Kw is a constant,
there are p 2 units.

~w~

and

~WA

-1

+ cos[27!'(j ~ l)/Pl-1)

(4)

~WA

control the width of the filtering weights, and

On each iteration the activity is filtered by the weights, squared, and then normalized by the total local activity. Divisive normalization per se only involves the
squaring and division by local activity. We have added the filtering weights to obtain a local pooling of activity between cells with similar preferred orientations and
spatial frequencies. This pooling can easily be implemented with cortical lateral
connections and it is reasonable to think that such a pooling takes place in the
cortex.

Divisive Normalization, Line Attractor Networks and Ideal Observers

2

107

Simulation Results

Our simulations consist of iterating equation 3 with initial conditions determined by
the presentation orientation and spatial frequency. The initial conditions are chosen
as follows: For a given presentation angle, (}o, and spatial frequency, Ao, determine
the mean cortical activity, /ij((}o, AO), via equation 1. Then generate the actual
cortical activity, {aij}, by sampling from the distribution given in equation 2. This
serves as our set of initial conditions: Oij (t = 0) = aij'
Iterating equation 3 with the above initial conditions, we found that for very low
contrast the activity of all cortical units decayed to zero. Above some contrast
threshold, however, the activities converged to a smooth stable hill (see figure I-A
for an example with parameters (Jw(} = (Jw).. = (J(} = (J).. = I/V8, K = 74, C = 1,
J.L = 0.01). The width of the hill is controlled by the width of the filtering weights.
Its peak, on the other hand, depends on the orientation and spatial frequency of the
LGN input, (}o and Ao. The peak can thus be used to estimate these quantities (see
figure I-A). To compute the position of the final hill, we used a population vector
estimator [3] although any unbiased estimator would work as well. In all cases we
looked at, the network produced an unbiased estimate of (}o and Ao.
In our simulations we adjusted (Jw(} and (Jw).. so that the stable hill had the same
profile as the mean LGN input (equation 1). As a result, the tuning curves of the
cortical units match the tuning curves specified by the pooled LGN input. For this
case, we found that the estimate obtained from the network has a variance close
to the theoretical minimum, known as the Cramer-Rao bound [3]. For Gaussian
noise of fixed variance, the variance of the estimate was 16.6% above this bound,
compared to 3833% for the population vector applied directly to the LGN input.
In a ID network (orientation alone), these numbers go to 12.9% for the network
versus 613% for population vector. For Gaussian noise with variance proportional
to the mean, the network was 8.8% above the bound, compared to 722% for the
population vector applied directly to the input. These numbers are respectively 9%
and 108% for the I-D network. The network is therefore a close approximation to
a maximum likelihood estimator, i.e., it is close to being an ideal observer of the
LGN activity with respect to orientation and spatial frequency.
As long as the contrast, C, was superthreshold, large variations in contrast did not
affect our results (figure I-B). However, the tuning of the network units to contrast
after reaching the stable state was found to follow a step function whereas, for real
neurons, the curves are better described by a sigmoid [2]. Improved agreement
with experiment was achieved by taking only 2-3 iterations, at which point the
performance of the network is close to optimal (figure I-B) and the tuning curves to
contrast are more realistic and closer to sigmoids (figure I-C). Therefore, reaching
a stable state is not required for optimal performance, and in fact leads to contrast
tuning curves that are inconsistent with experiment.

3

Mathematical Analysis

We first prove that line attractor networks with sufficiently small noise are close
approximations to a maximum likelihood estimator. We then show how this result
applies to our simulations with divisive normalization.

S. Deneve, A. Pouget and P. E. Latham

J08

3.1

General Case: Line Attractor Networks

Let On be the activity vector (denoted by bold type) at discrete time, n, for a set
of P interconnected units. We consider a one dimensional network, i.e., only one
feature is encoded; generalization to multidimensional networks is straightforward.
A generic mapping for this network may be written

(5)

where H is a nonlinear function. We assume that this mapping admits a line
attractor, which we denote G(O), for which G(O) = H(G(O)) where 0 is a continuous
variable. 1 Let the initial state of the network be a function of the presentation
parameter, 00 , plus noise,

00

= F(Oo)

+N

(6)

where F(Oo) is the function used to generate the data (in our simulations this
would correspond to the mean LGN input, equation 1). Iterating the mapping,
equation 5, leads eventually to a point on the line attractor. Consequently, as
n -+ 00 , On -+ G(O) . The parameter 0 provides an estimate of 00 .
To determine how well the network does we need to find fJO :::: 0 - 00 as a function
of the noise, N, then average over the noise to compute the mean and variance of
fJO. Because the mapping, equation 5, is nonlinear, this cannot be done exactly. For
small noise, however, we can take a perturbative approach and expand around a
point on the attractor. For line at tractors there is no general method for choosing
which point on the attractor to expand around. Our approach will be to expand
around an arbitrary point, G( 0), and choose 0 by requiring that the quadratic terms
be finite. Keeping terms up to quadratic order, equation 6 may be written

G(O)
In .

+ fJo n .

fJo o +

(7)
n-l

I.: (Jm . fJo

~

o) .

H"" . (J m . fJo o )

,

(8)

m=O

where J(O) == [8G (o)H(G(0))f is the Jacobian (the subscript T means transpose),
H"" is the Hessian of H evaluated at G(O) and a ""."" represents the standard dot
product.
Because the mapping, equation 5, admits a line attractor , J has one eigenvalue
equal to 1 and all others less than 1. Denote the eigenvector with eigenvalue 1 as
y and its adjoint v t : J . v = v and JT . v t = yt. It is not hard to show that y =
8oG(0), up to a multiplicative constant. Since J has an eigenvalue equal to 1, to
avoid the quadratic term in Eq. 8 approaching infinity as n -+ 00 we require that

lim I

n-too

n .

fJo o = O.

(9)

IThe line attractor is, in fact , an idealization ; for P units the attractors associated with
equation 5 consists of P isolated points. However, for P large, the attractors are spaced
closely enough that they may be considered a line.

Divisive Normalization. Line Attractor Networks and Ideal Observers

109

This equations has an important consequence: it implies that, to linear order,
limn-too 60 n = 0 (see equation 8), which in turn implies that 0 00 = G(O) which,
~nally, implies that 0 = O. Consequently we can find the network estimator of 00 ,
0, by computing O. We now turn to that task.
It is straightforward to show that JOO = vv t . Combining this expression for J with
equation 9, using equation 7 to express 600 in terms of 00 and G(O), and, finally
using equation 6 to express 00 in terms of the initial mean activity, F(Oo), and the

noise, N, we find that
v t (0) . [F(Oo) - G(O)
Using 00

=0-

+ N] = O.

(10)

60 and expanding F(Oo) to first order in 60 then yields
60

= vt(O) . [N + F(O) -

G(O)]

vt(O) . F'(O)

.

(11)

As long as v t is orthogonal to F(O) - G(O), (60) = 0 and the estimator is unbiased.
This must be checked on a case by case basis, but for the circularly symmetric
networks we considered orthogonality is satisfied.
We can now calculate the variance of the network estimate, (60)2. Assuming v t .
[F(O) - G(O)] = 0, equation 11 implies that
2
vt.R?v t
(60) = [v t . F'F

'

(12)

where a prime denotes a derivative with respect to 0 and R is the covariance matrix
of the noise, R = (NN). The network is equivalent to maximum likelihood when
this variance is equal to the Cramer-Rao bound [3], (60)bR. If the noise, N, is
Gaussian with a covariance matrix independent of 0, this bound is equal to:
2

(60)CR

=

1

F'. R - l . F'

(13)

For independent Gaussian noise of fixed variance, (T2, and zero covariance, the
variance of the network estimate, equation 12, becomes (T2 1(IF'1 2 cos2 f-L) where f-L
is the angle between v t and F'. The Cramer-Rao bound, on the other hand, is
equal to (T2 IIF'1 2 . These expressions differ only by cos 2 J1., which is 1 if F ex v t . In
addition, it is close to 1 for networks that have identical input and output tunin
curves, F(O) = G(O), and the Jacobian, J, is nearly symmetric, so that v ::::: v
(recall that v = G'). If these last two conditions are satisfied, the network comes
close to being a maximum likelihood estimator.

1

3.2

Application to Divisive Normalization

Divisive normalization is a particular example of the general case considered above.
For simplicity, in our simulations we chose the input and output tuning curves to
be equal (F = G in the above notation), which lead to a value of 0.87 for cos2 f-L
(evaluated numerically). This predicted a variance 15% above the Cramer-Rao

S. Deneve, A. Pouget and P E. Latham

110

bound for independent Gaussian noise with fixed variance, consistent with the 16%
we obtained in our simulations. The network also handles fairly well other noise
distributions, such as Gaussian noise with variance proportional to the mean, as
illustrated by our simulations.

4

Conclusions

We have recently shown that a subclass of line attractor networks can be used as
maximum likelihood estimators[3]. This paper extend this conclusion to a much
wider class of networks, namely, any network that admits a line (or, by straightforward extension of the above analysis, a higher dimensional) attractor. This is true
in particular for networks using divisive normalization, a normalization which is
thought to match quite closely the nonlinearity found in the primary visual cortex
and MT.
Although our analysis relies on the existence of an attractor, this is not a requirement for obtaining near optimal noise filtering. As we have seen, 2-3 iterations
are enough to achieve asymptotic performance (except at contrasts barely above
threshold). What matters most is that our network implement a sequence of low
pass filtering to filter out the noise, followed by a square nonlinearity to compensate
for the widening of the tuning curve due to the low pass filter, and a normalization
to weaken contrast dependence. It is likely that this process would still clean up
noise efficiently in the first 2-3 iterations even if activity decayed to zero eventually,
that is to say, even if the hills of activity were not stable states. This would allow us
to apply our approach to other types of networks, including those lacking circular
symmetry and networks with continuously clamped inputs.
To conclude, we propose that each cortical layer may read out the activity in the
preceding layer in an optimal way thanks to the nonlinear pooling properties of
divisive normalization, and, as a result, may behave like an ideal observer. It is
therefore possible that the ability to read out neuronal codes in the sensory cortices
in an optimal way may not be confined to a few areas like the parietal or frontal
cortex, but may instead be a general property of every cortical layer.

References
[1] D. Heeger. Normalization of cell responses in cat striate cortex. Visual Neuroscience, 9:181- 197,1992.
[2] L. Itti, C. Koch, and J. Braun. A quantitative model for human spatial vision threshold on the basis of non-linear interactions among spatial filters. In
R. Lippman, J. Moody, and D. Touretzky, editors, Advances in Neural Information Processing Systems, volume 11. Morgan-Kaufmann, San Mateo, 1998.
[3] A. Pouget, K. Zhang, S. Deneve, and P. Latham. Statistically efficient estimation
using population coding. Neural Computation, 10:373- 401, 1998.
[4] E. Simoncelli and D. Heeger. A model of neuronal responses in visual area MT.
Vision Research, 38(5):743- 761 , 1998.

"
4472,2013,Stochastic blockmodel approximation of a graphon: Theory and consistent estimation,"Given a convergent sequence of graphs, there exists a limit object called the graphon from which random graphs are generated. This nonparametric perspective of random graphs opens the door to study graphs beyond the traditional parametric models, but at the same time also poses the challenging question of how to estimate the graphon underlying observed graphs. In this paper, we propose a computationally efficient algorithm to estimate a graphon from a set of observed graphs generated from it. We show that, by approximating the graphon with stochastic block models, the graphon can be consistently estimated, that is, the estimation error vanishes as the size of the graph approaches infinity.","Stochastic blockmodel approximation of a graphon:
Theory and consistent estimation

Edoardo M. Airoldi
Dept. Statistics
Harvard University

Thiago B. Costa
SEAS, and Dept. Statistics
Harvard University

Stanley H. Chan
SEAS, and Dept. Statistics
Harvard University

Abstract
Non-parametric approaches for analyzing network data based on exchangeable
graph models (ExGM) have recently gained interest. The key object that defines
an ExGM is often referred to as a graphon. This non-parametric perspective on
network modeling poses challenging questions on how to make inference on the
graphon underlying observed network data. In this paper, we propose a computationally efficient procedure to estimate a graphon from a set of observed networks
generated from it. This procedure is based on a stochastic blockmodel approximation (SBA) of the graphon. We show that, by approximating the graphon with
a stochastic block model, the graphon can be consistently estimated, that is, the
estimation error vanishes as the size of the graph approaches infinity.

1 Introduction
Revealing hidden structures of a graph is the heart of many data analysis problems. From the wellknown small-world network to the recent large-scale data collected from online service providers
such as Wikipedia, Twitter and Facebook, there is always a momentum in seeking better and more
informative representations of the graphs [1, 14, 29, 3, 26, 12]. In this paper, we develop a new computational tool to study one type of non-parametric representations which recently draws significant
attentions from the community [4, 19, 5, 30, 23].
The root of the non-parametric model discussed in this paper is in the theory of exchangeable random arrays [2, 15, 16], and it is presented in [11] as a link connecting de Finetti?s work on partial
exchangeability and graph limits [20, 6]. In a nutshell, the theory predicts that every convergent
sequence of graphs (Gn ) has a limit object that preserves many local and global properties of the
graphs in the sequence. This limit object, which is called a graphon, can be represented by measurable functions w : [0, 1]2 ? [0, 1], in a way that any w? obtained from measure preserving
transformations of w describes the same graphon.
Graphons are usually seen as kernel functions for random network models [18]. To construct an
n-vertex random graph G(n, w) for a given w, we first assign a random label ui ? Uniform[0, 1] to
each vertex i ? {1, . . . , n}, and connect any two vertices i and j with probability w(ui , uj ), i.e.,
Pr (G[i, j] = 1 | ui , uj ) = w(ui , uj ),

i, j = 1, . . . , n,

(1)

where G[i, j] denotes the (i, j)th entry of the adjacency matrix representing a particular realization
of G(n, w) (See Figure 1). As an example, we note that the stochastic block-model is the case where
w(x, y) is a piecewise constant function.
The problem of interest is defined as follows: Given a sequence of 2T observed directed graphs
G1 , . . . , G2T , can we make an estimate w
b of w, such that w
b ? w with high probability as n ? ??
This question has been loosely attempted in the literature, but none of which has a complete solution.
For example, Lloyd et al. [19] proposed a Bayesian estimator without a consistency proof; Choi and
1

w

G2T

w(ui , uj )

?
ui

uj

(ui , uj )

G1

Figure 1: [Left] Given a graphon w : [0, 1]2 ? [0, 1], we draw i.i.d. samples ui , uj from
Uniform[0,1] and assign Gt [i, j] = 1 with probability w(ui , uj ), for t = 1, . . . , 2T . [Middle]
Heat map of a graphon w. [Right] A random graph generated by the graphon shown in the middle.
Rows and columns of the graph are ordered by increasing ui , instead of i for better visualization.
Wolfe [9] studied the consistency properties, but did not provide algorithms to estimate the graphon.
To the best of our knowledge, the only method that estimates graphons consistently, besides ours, is
USVT [8]. However, our algorithm has better complexity and outperforms USVT in our simulations.
More recently, other groups have begun exploring approaches related to ours [28, 24].
The proposed approximation procedure requires w to be piecewise Lipschitz. The basic idea is to
approximate w by a two-dimensional step function w
b with diminishing intervals as n increases.The
proposed method is called the stochastic blockmodel approximation (SBA) algorithm, as the idea of
using a two-dimensional step function for approximation is equivalent to using the stochastic block
models [10, 22, 13, 7, 25]. The SBA algorithm is defined up to permutations of the nodes, so the
estimated graphon is not canonical. However, this does not affect the consistency properties of the
SBA algorithm, as the consistency is measured w.r.t. the graphon that generates the graphs.

2 Stochastic blockmodel approximation: Procedure
In this section we present the proposed SBA algorithm and discuss its basic properties.
2.1 Assumptions on graphons
We assume that w is piecewise Lipschitz, i.e., there exists a sequence of non-overlaping intervals
Ik = [?k?1 , ?k ] defined by 0 = ?0 < . . . < ?K = 1, and a constant L > 0 such that, for any
(x1 , y1 ) and (x2 , y2 ) ? Iij = Ii ? Ij ,
|w(x1 , y1 ) ? w(x2 , y2 )| ? L (|x1 ? x2 | + |y1 ? y2 |) .
For generality we assume w to be asymmetric i.e., w(u, v) 6= w(v, u), so that symmetric graphons
can be considered as a special case. Consequently, a random graph G(n, w) generated by w is
directed, i.e., G[i, j] 6= G[j, i].
2.2 Similarity of graphon slices
The intuition of the proposed SBA algorithm is that if the graphon is smooth, neighboring crosssections of the graphon should be similar. In other words, if two labels ui and uj are close i.e.,
|ui ? uj | ? 0, then the difference between the row slices |w(ui , ?) ? w(uj , ?)| and the column slices
|w(?, ui ) ? w(?, uj )| should also be small. To measure the similarity between two labels using the
graphon slices, we define the following distance
Z 1

Z 1
1
2
2
[w(x, ui ) ? w(x, uj )] dx +
[w(ui , y) ? w(uj , y)] dy .
(2)
dij =
2
0
0
2

Thus, dij is small only if both row and column slices of the graphon are similar.
The usage of dij for graphon estimation will be discussed in the next subsection. But before
we proceed, it should be noted that in practice dij has to be estimated from the observed graphs
G1 , . . . , G2T . To derive an estimator dbij of dij , it is helpful to express dij in a way that the estimators can be easily obtained. To this end, we let
Z 1
Z 1
w(x, ui )w(x, uj )dx
and
rij =
w(ui , y)w(uj , y)dy,
cij =
0

0

h
i
and express dij as dij = (cii ?cij ?cji +cjj )+(rii ?rij ?rji +rjj ) . Inspecting this expression,
we consider the following estimators for cij and rij :
?
??
?
X
X
1
Gt1 [k, i]? ?
Gt2 [k, j]? ,
(3)
ckij = 2 ?
b
T
1?t1 ?T
T <t2 ?2T
?
??
?
X
X
1
k
rbij
= 2?
Gt1 [i, k]? ?
Gt2 [j, k]? .
(4)
T
1
2

1?t1 ?T

T <t2 ?2T

Here, the superscript k can be interpreted as the dummy variables x and y in defining cij and rij ,
respectively. Summing all possible k?s yields an estimator dbij that looks similar to dij :
""
#
X

	
1
1
k
k
k
k
dbij =
rbii
? rbij
? rbji
+ rbjj
+ b
ckii ? b
ckij ? b
ckji + b
ckjj
,
(5)
2 S
k?S

where S = {1, . . . , n}\{i, j} is the set of summation indices.

The motivation of defining the estimators in (3) and (4) is that a row of the adjacency matrix G[i, ?]
is fully
the graphon w(ui , ?). Thus the expected value of
P characterized by
 the corresponding row ofP
1
1
k
bij
is an estimator for rij . To theoretically
1?t1 ?T Gt1 [i, ?] is w(ui , ?), and hence S
k?S r
T
justify this intuition, we will show in Section 3 that dbij is indeed a good estimator: it is not only
unbiased, but is also concentrated round dij for large n. Furthermore, we will show that it is possible
to use a random subset of S instead of {1, . . . , n}\{i, j} to achieve the same asymptotic behavior.
As a result, the estimation of dij can be performed locally in a neighborhood of i and j, instead of
all n vertices.
2.3 Blocking the vertices
The similarity metric dbij discussed above suggests one simple method to approximate w by a piecewise constant function w
b (i.e., a stochastic block-model). Given G1 , . . . , G2T , we can cluster the
b1 , . . . , B
bK using a procedure described below. Once
(unknown) labels {u1 , . . . , un } into K blocks B
b1 , . . . , B
bK are defined, we can then determine w(u
the blocks B
b i , uj ) by computing the empirical
b
b
frequency of edges that are present across blocks Bi and Bj :
X X 1
1
w(u
b i , uj ) =
(G1 [ix , jy ] + G2 [ix , jy ] + . . . + G2T [ix , jy ]) ,
(6)
bi | |B
bj |
2T
|B
b
b
ix ?Bi jy ?Bj

bi is the block containing ui so that summing Gt [x, y] over x ? B
bi and y ? B
bj yields an
where B
b
b
estimate of the expected number of edges linking block Bi and Bj .

To cluster the unknown labels {u1 , . . . , un } we propose a greedy approach as shown in Algorithm
1. Starting with ? = {u1 , . . . , un }, we randomly pick a node ip and call it the pivot. Then for all
other vertices iv ? ?\{ip }, we compute the distance dbip ,iv and check whether dbip ,iv < ?2 for some
precision parameter ? > 0. If dbip ,iv < ?2 , then we assign iv to the same block as ip . Therefore,
b1 = {ip , iv1 , iv2 , . . .} will be defined. By updating ? as
after scanning through ? once, a block B
b
? ? ?\B1 , the process repeats until ? = ?.
3

The proposed greedy algorithm is only a local solution in a sense that it does not return the globally
optimal clusters. However, as will be shown in Section 3, although the clustering algorithm is not
globally optimal, the estimated graphon w
b is still guaranteed to be a consistent estimate of the true
graphon w as n ? ?. Since the greedy algorithm is numerically efficient, it serves as a practical
computational tool to estimate w.
2.4 Main algorithm
Algorithm 1 Stochastic blockmodel approximation
Input: A set of observed graphs G1 , . . . , G2T and the precision parameter ?.
b1 , . . . , B
bK .
Output: Estimated stochastic blocks B
Initialize: ? = {1, . . . , n}, and k = 1.
while ? 6= ? do
bk : B
bk ? ip .
Randomly choose a vertex ip from ? and assign it as the pivot for B
for Every other vertices iv ? ?\{ip } do
Compute the distance estimate dbip ,iv .
bk : B
bk ? iv .
If dbip ,iv ? ?2 , then assign iv as a member of B
end for
bk .
Update ?: ? ? ?\B
Update counter: k ? k + 1.
end while
Algorithm 1 illustrates the pseudo-code for the proposed stochastic block-model approximation.
The complexity of this algorithm is O(T SKn), where T is half the number of observations, S is
the size of the neighborhood, K is the number of blocks and n is number of vertices of the graph.

3 Stochastic blockmodel approximation: Theory of estimation
In this section we present the theoretical aspects of the proposed SBA algorithm. We will first
discuss the properties of the estimator dbij , and then show the consistency of the estimated graphon
w.
b Details of the proofs can be found in the supplementary material.
3.1 Concentration analysis of dbij

Our first theorem below shows that the proposed estimator dbij is both unbiased, and is concentrated
around its expected value dij .

Theorem 1. The estimator dbij for dij is unbiased, i.e., E[dbij ] = dij . Further, for any ? > 0,

h
i
S?2


Pr dbij ? dij  > ? ? 8e? 32/T +8?/3 ,

(7)

where S is the size of the neighborhood S, and 2T is the number of observations.

Proof. Here we only highlight the important steps to present the intuition. The basic idea of the
k
proof is to zoom-in a microscopic term of rbij
and show that it is unbiased. To this end, we use the
fact that Gt1 [i, k] and Gt2 [j, k] are conditionally independent on uk to show
E[Gt1 [i, k]Gt2 [j, k] | uk ] = Pr[Gt1 [i, k] = 1, Gt2 [j, k] = 1 | uk ]
(a)

= Pr[Gt1 [i, k] = 1 | uk ] Pr[Gt2 [j, k] = 1 | uk ]
= w(ui , uk )w(uj , uk ),
k
k
which then implies E[b
rij
| uk ] = w(ui , uk )w(uj , uk ), and by iterated expectation we have E[b
rij
]=
k
E[E[b
rij | uk ]] = rij . The concentration inequality follows from a similar idea to bound the variance
k
of rbij
and apply Bernstein?s inequality.

4

That Gt1 [i, k] and Gt2 [j, k] are conditionally independent on uk is a critical fact for the success of
the proposed algorithm. It also explains why at least 2 independently observed graphs are necessary,
for otherwise we cannot separate the probability in the second equality above marked with (a).
3.2 Choosing the number of blocks
The performance of the Algorithm 1 is sensitive to the number of blocks it defines. On the one hand,
it is desirable to have more blocks so that the graphon can be finely approximated. But on the other
hand, if the number of blocks is too large then each block will contain only few vertices. This is bad
because in order to estimate the value on each block, a sufficient number of vertices in each block is
required. The trade-off between these two cases is controlled by the precision parameter ?: a large
? generates few large clusters, while small ? generates many small clusters. A precise relationship
between the ? and K, the number of blocks generated the algorithm, is given in Theorem 2.
Theorem 2. Let ? be the accuracy parameter and K be the number of blocks estimated by Algorithm 1, then
""
? #
S?4
QL 2
?
Pr K >
? 8n2 e 128/T +16?2 /3 ,
(8)
?
where L is the Lipschitz constant and Q is the number of Lipschitz blocks in w.
In practice, we estimate ? using a cross-validation scheme to find the optimal 2D histogram bin
width [27]. The idea is to test a sequence of potential values of ? and seek the one that minimizes
the cross validation risk, defined as
K

b
J(?)
=

2
n+1 X 2
?
pb ,
h(n ? 1) h(n ? 1) j=1 j

(9)

bj |/n and h = 1/K. Algorithm 2 details the proposed cross-validation scheme.
where pbj = |B
Algorithm 2 Cross Validation

Input: Graphs G1 , . . . , G2T .
b1 , . . . , B
bK , and optimal ?.
Output: Blocks B
for a sequence of ??s do
b1 , . . . , B
bK from G1 , . . . , G2T . [Algorithm 1]
Estimate blocks B
bj |/n, for j = 1, . . . , K.
Compute pbj = |B
n+1 PK
2
b
? h(n?1)
b2j , with h = 1/K.
Compute J(?)
= h(n?1)
j=1 p
end for
b
b1 , . . . , B
bK .
Pick the ? with minimum J(?),
and the corresponding B
3.3 Consistency of w
b

The goal of our next theorem is to show that w
b is a consistent estimate of w, i.e., w
b ? w as n ? ?.
To begin with, let us first recall two commonly used metric:
Definition 1. The mean squared error (MSE) and mean absolute error (MAE) are defined as
MSE(w)
b =

n
n
1 X X
2
(w(uiv , ujv ) ? w(u
b iv , ujv ))
n2 i =1 j =1
v

v

n
n
1 X X
MAE(w)
b = 2
|w(uiv , ujv ) ? w(u
b iv , ujv )| .
n i =1 j =1

Theorem 3. If S ? ?(n) and ? ? ?

v

v



log(n)
n

lim E[MAE(w)]
b =0

n??

 14 
and

5

? o(1), then

lim E[MSE(w)]
b = 0.

n??

Proof. The details of the proof can be found in the supplementary material . Here we only outline
the key steps to present the intuition of the theorem. The goal of Theorem 3 is to show convergence
of |w(u
b i , uj ) ? w(ui , uj )|. The idea is to consider the following two quantities:
X X
1
w(ui , uj ) =
w(uix , ujx ),
bi | |B
bj |
|B
w(u
b i , uj ) =

1

bi | |B
bj |
|B

bi jx ?B
bj
ix ?B

X X

bi jy ?B
bj
ix ?B

1
(G1 [ix , jy ] + G2 [ix , jy ] + . . . + G2T [ix , jy ]) ,
2T

so that if we can bound |w(ui , uj ) ? w(ui , uj )| and |w(ui , uj ) ? w(u
b i , uj )|, then consequently
|w(u
b i , uj ) ? w(ui , uj )| can also be bounded.

The bound for the first term |w(ui , uj ) ? w(ui , uj )| is shown in Lemma 1: By Algorithm 1, any
bi is guaranteed to be within a distance ? from the pivot of B
bi . Since w(ui , uj ) is an
vertex iv ? B
b
b
average over Bi and Bj , by Theorem 1 a probability bound involving ? can be obtained.

b i , uj )| is shown in Lemma 2. Different from Lemma
The bound for the second term |w(ui , uj )? w(u
1, here we need to consider two possible situations: either the intermediate estimate w(ui , uj ) is
close to the ground truth w(ui , uj ), or w(ui , uj ) is far from the ground truth w(ui , uj ). This accounts for the sum in Lemma 2. Individual bounds are derived based on Lemma 1 and Theorem 1.
Combining Lemma 1 and Lemma 2, we can then bound the error and show convergence.
bi and jv ? B
bj ,
Lemma 1. For any iv ? B
i
h
S?4
b i | |B
bj |e? 32/T +8?2 /3 .
Pr |w(ui , uj ) ? w(uiv , ujv )| > 8?1/2 L1/4 ? 32|B

bi and jv ? B
bj ,
Lemma 2. For any iv ? B
h
i
?
S?4
b
b
bi |2 |B
bj |2 e? 32/T +8?2 /3) .
Pr |w
bij ? w ij | > 8?1/2 L1/4 ? 2e?256(T |Bi | |Bj | L?) + 32|B

(10)

(11)

The condition S ? ?(n) is necessary to make Theorem 3 valid, because if S is independent of n,
the right hand sides of (10) and (11) cannot approach 0 even if n ? ?. The condition on ? is also
important as it forces the numerators and denominators in the exponentials of (10) and (11) to be
well behaved.

4 Experiments
In this section we evaluate the proposed SBA algorithm by showing some empirical results. For the
purpose of comparison, we consider (i) the universal singular value thresholding (USVT) [8]; (ii)
the largest-gap algorithm (LG) [7]; (iii) matrix completion from few entries (OptSpace) [17].
4.1 Estimating stochastic blockmodels
Accuracy as a function of growing graph size. Our first experiment is to evaluate the proposed
SBA algorithm for estimating stochastic blockmodels. For this purpose, we generate (arbitrarily) a
graphon
?
?
0.8 0.9 0.4 0.5
?0.1 0.6 0.3 0.2?
w=?
,
(12)
0.3 0.2 0.8 0.3?
0.4 0.1 0.2 0.9
which represents a piecewise constant function with 4 ? 4 equi-space blocks.

Since USVT and LG use only one observed graph whereas the proposed SBA require at least 2
observations, in order to make the comparison fair, we use half of the nodes for SBA by generating
two independent n2 ? n2 observed graphs. For USVT and LG, we use one n ? n observed graph.
Figure 2(a) shows the asymptotic behavior of the algorithms when n grows. Figure 2(b) shows the
estimation error of SBA algorithm as T grows for graphs of size 200 vertices.
6

?0.5

?2
Proposed
?2.1

?1

?2.2

log 10 (MAE)

log 10 (MAE)

?2.3
?1.5

?2

?2.4
?2.5
?2.6
?2.7

?2.5

?3
0

Proposed
Largest Gap
OptSpace
USVT
200

?2.8
?2.9
400

n

600

800

?3
0

1000

(a) Growing graph size, n

5

10

15

20
2T

25

30

35

40

(b) Growing no. observations, 2T

Figure 2: (a) MAE reduces as graph size grows. For the fairness of the amount of data that can be
used, we use n2 ? n2 ? 2 observations for SBA, and n ? n ? 1 observation for USVT [8] and LG
[7]. (b) MAE of the proposed SBA algorithm reduces when more observations T is available. Both
plots are averaged over 100 independent trials.
Accuracy as a function of growing number of blocks. Our second experiment is to evaluate the
performance of the algorithms as K, the number of blocks, increases. To this end, we consider a
sequence of K, and for each K we generate a graphon w of K ? K blocks. Each entry of the
block is a random number generated from Uniform[0, 1]. Same as the previous experiment, we fix
n = 200 and T = 1. The experiment is repeated over 100 trials so that in every trial a different
graphon is generated. The result shown in Figure 3(a) indicates that while estimation error increases
as K grows, the proposed SBA algorithm still attains the lowest MAE for all K.
?0.7

?0.6
?0.7

?0.8
?0.8
?0.9
log 10 (MAE)

log 10 (MAE)

?0.9
?1
?1.1

?1.2
?1.3

?1.2

5

10
K

15

Proposed
Largest Gap
OptSpace
USVT

?1.4

Proposed
Largest Gap
USVT

?1.3
?1.4
0

?1
?1.1

?1.5
?1.6
0

20

(a) Growing no. blocks, K

5

10
% missing links

15

20

(b) Missing links

Figure 3: (a) As K increases, MAE of all three algorithm increases but SBA still attains the lowest
MAE. Here, we use n2 ? n2 ? 2 observations for SBA, and n ? n ? 1 observation for USVT [8] and
LG [7]. (b) Estimation of graphon in the presence of missing links: As the amount of missing links
increases, estimation error also increases.
4.2 Estimation with missing edges
Our next experiment is to evaluate the performance of proposed SBA algorithm when there are
missing edges in the observed graph. To model missing edges, we construct an n ? n binary matrix
M with probability Pr[M [i, j] = 0] = ?, where 0 ? ? ? 1 defines the percentage of missing
edges. Given ?, 2T matrices are generated with missing edges, and the observed graphs are defined
as M1 ? G1 , . . . , M2T ? G2T , where ? denotes the element-wise multiplication. The goal is to
study how well SBA can reconstruct the graphon w
b in the presence of missing links.
7

The modification of the proposed SBA algorithm for the case missing links is minimal: when combi and jy ? B
bj , we only average ix ? B
bi and jy ? B
bj
puting (6), instead of averaging over all ix ? B
that are not masked out by all M ? s. Figure 3(b) shows the result of average over 100 independent
trials. Here, we consider the graphon given in (12), with n = 200 and T = 1. It is evident that SBA
outperforms its counterparts at a lower rate of missing links.
4.3 Estimating continuous graphons
Our final experiment is to evaluate the proposed SBA algorithm in estimating continuous graphons.
Here, we consider two of the graphons reported in [8]:
1
w1 (u, v) =
, and w2 (u, v) = uv,
1 + exp{?50(u2 + v 2 )}
where u, v ? [0, 1]. Here, w2 can be considered as a special case of the Eigenmodel [13] or latent
feature relational model [21].
The results in Figure 4 shows that while both algorithms have improved estimates when n grows, the
performance depends on which of w1 and w2 that we are studying. This suggests that in practice the
choice of the algorithm should depend on the expected structure of the graphon to be estimated: If the
graph generated by the graphon demonstrates some low-rank properties, then USVT is likely to be
a better option. For more structured or complex graphons the proposed procedure is recommended.
?2.9

?0.6
Proposed
USVT

Proposed
USVT

?0.8

?2.95

?1
log 10 (MAE)

log 10 (MAE)

?3

?3.05

?1.2
?1.4

?3.1
?1.6
?3.15

?3.2
0

?1.8

200

400

n

600

800

?2
0

1000

(a) graphon w1

200

400

n

600

800

1000

(b) graphon w2

Figure 4: Comparison between SBA and USVT in estimating two continuous graphons w1 and w2 .
Evidently, SBA performs better for w1 (high-rank) and worse for w2 (low-rank).

5 Concluding remarks
We presented a new computational tool for estimating graphons. The proposed algorithm approximates the continuous graphon by a stochastic block-model, in which the first step is to cluster
the unknown vertex labels into blocks by using an empirical estimate of the distance between two
graphon slices, and the second step is to build an empirical histogram to estimate the graphon. Complete consistency analysis of the algorithm is derived. The algorithm was evaluated experimentally,
and we found that the algorithm is effective in estimating block structured graphons.
Implementation of the SBA algorithm is available online at https://github.com/airoldilab/SBA.
Acknowledgments. EMA is partially supported by NSF CAREER award IIS-1149662, ARO MURI
award W911NF-11-1-0036, and an Alfred P. Sloan Research Fellowship. SHC is partially supported
by a Croucher Foundation Post-Doctoral Research Fellowship.

References
[1] E.M. Airoldi, D.M. Blei, S.E. Fienberg, and E.P. Xing. Mixed-membership stochastic blockmodels.
Journal of Machine Learning Research, 9:1981?2014, 2008.

8

[2] D.J. Aldous. Representations for partially exchangeable arrays of random variables. Journal of Multivariate Analysis, 11:581?598, 1981.
[3] H. Azari and E. M. Airoldi. Graphlet decomposition of a weighted network. Journal of Machine Learning
Research, W&CP, 22:54?63, 2012.
[4] P.J. Bickel and A. Chen. A nonparametric view of network models and Newman-Girvan and other modularities. Proc. Natl. Acad. Sci. USA, 106:21068?21073, 2009.
[5] P.J. Bickel, A. Chen, and E. Levina. The method of moments and degree distributions for network models.
Annals of Statistics, 39(5):2280?2301, 2011.
[6] C. Borgs, J. Chayes, L. Lov?asz, V. T. S?os, B. Szegedy, and K. Vesztergombi. Graph limits and parameter
testing. In Proc. ACM Symposium on Theory of Computing, pages 261?270, 2006.
[7] A. Channarond, J. Daudin, and S. Robin. Classification and estimation in the Stochastic Blockmodel
based on the empirical degrees. Electronic Journal of Statistics, 6:2574?2601, 2012.
[8] S. Chatterjee. Matrix estimation by universal singular value thresholding. ArXiv:1212.1247. 2012.
[9] D.S. Choi and P.J. Wolfe. Co-clustering separately exchangeable network data. ArXiv:1212.4093. 2012.
[10] D.S. Choi, P.J. Wolfe, and E.M. Airoldi. Stochastic blockmodels with a growing number of classes.
Biometrika, 99:273?284, 2012.
[11] P. Diaconis and S. Janson. Graph limits and exchangeable random graphs. Rendiconti di Matematica e
delle sue Applicazioni, Series VII, pages 33?61, 2008.
[12] A. Goldenberg, A.X. Zheng, S.E. Fienberg, and E.M. Airoldi. A survey of statistical network models.
Foundations and Trends in Machine Learning, 2:129?233, 2009.
[13] P.D. Hoff. Modeling homophily and stochastic equivalence in symmetric relational data. In Neural
Information Processing Systems (NIPS), volume 20, pages 657?664, 2008.
[14] P.D. Hoff, A.E. Raftery, and M.S. Handcock. Latent space approaches to social network analysis. Journal
of the American Statistical Association, 97(460):1090?1098, 2002.
[15] D.N. Hoover. Relations on probability spaces and arrays of random variables. Preprint, Institute for
Advanced Study, Princeton, NJ, 1979.
[16] O. Kallenberg. On the representation theorem for exchangeable arrays. Journal of Multivariate Analysis,
30(1):137?154, 1989.
[17] R.H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries. IEEE Trans. Information
Theory, 56:2980?2998, Jun. 2010.
[18] N.D. Lawrence. Probabilistic non-linear principal component analysis with Gaussian process latent variable models. Journal of Machine Learning Research, 6:1783?1816, 2005.
[19] J.R. Lloyd, P. Orbanz, Z. Ghahramani, and D.M. Roy. Random function priors for exchangeable arrays
with applications to graphs and relational data. In Neural Information Processing Systems (NIPS), 2012.
[20] L. Lov?asz and B. Szegedy. Limits of dense graph sequences. Journal of Combinatorial Theory, Series B,
96:933?957, 2006.
[21] K.T. Miller, T.L. Griffiths, and M.I. Jordan. Nonparametric latent fature models for link prediction. In
Neural Information Processing Systems (NIPS), 2009.
[22] K. Nowicki and T.A. Snijders. Estimation and prediction of stochastic block structures. Journal of
American Statistical Association, 96:1077?1087, 2001.
[23] P. Orbanz and D.M. Roy. Bayesian models of graphs, arrays and other exchangeable random structures,
2013. Unpublished manuscript.
[24] P.Latouche and S. Robin. Bayesian model averaging of stochastic block models to estimate the graphon
function and motif frequencies in a w-graph model. ArXiv:1310.6150, October 2013. Unpublished
manuscript.
[25] K. Rohe, S. Chatterjee, and B. Yu. Spectral clustering and the high-dimensional stochastic blockmodel.
Annals of Statistics, 39(4):1878?1915, 2011.
[26] M. Tang, D.L. Sussman, and C.E. Priebe. Universally consistent vertex classification for latent positions
graphs. Annals of Statistics, 2013. In press.
[27] L. Wasserman. All of Nonparametric Statistics. Springer, 2005.
[28] P.J. Wolfe and S.C. Olhede. Nonparametric graphon estimation. ArXiv:1309.5936, September 2013.
Unpublished manuscript.
[29] Z. Xu, F. Yan, and Y. Qi. Infinite Tucker decomposition: nonparametric Bayesian models for multiway
data analysis. In Proc. Intl. Conf. Machine Learning (ICML), 2012.
[30] Y. Zhao, E. Levina, and J. Zhu. Community extraction for social networks. In Proc. Natl. Acad. Sci. USA,
volume 108, pages 7321?7326, 2011.

9

"
2166,2006,Large Margin Multi-channel Analog-to-Digital Conversion with Applications to Neural Prosthesis,Abstract Missing,"Large Margin Multi-channel Analog-to-Digital
Conversion with Applications to Neural Prosthesis

Amit Gore and Shantanu Chakrabartty
Department of Electrical and Computer Engineering
Michigan State University
East Lansing, MI 48823
{goreamit,shantanu}@egr.msu.edu

Abstract
A key challenge in designing analog-to-digital converters for cortically implanted
prosthesis is to sense and process high-dimensional neural signals recorded by
the micro-electrode arrays. In this paper, we describe a novel architecture for
analog-to-digital (A/D) conversion that combines ?? conversion with spatial
de-correlation within a single module. The architecture called multiple-input
multiple-output (MIMO) ?? is based on a min-max gradient descent optimization of a regularized linear cost function that naturally lends to an A/D formulation. Using an online formulation, the architecture can adapt to slow variations in cross-channel correlations, observed due to relative motion of the microelectrodes with respect to the signal sources. Experimental results with real
recorded multi-channel neural data demonstrate the effectiveness of the proposed
algorithm in alleviating cross-channel redundancy across electrodes and performing data-compression directly at the A/D converter.

1

Introduction

Design of cortically implanted neural prosthetic sensors (CINPS)is an active area of research in
the rapidly emerging field of brain machine interfaces (BMI) [1, 2]. The core technology used in
these sensors are micro-electrode arrays (MEAs) that facilitate real-time recording from thousands
of neurons simultaneously. These recordings are then actively processed at the sensor (shown in Figure 1) and transmitted to an off-scalp neural processor which controls the movement of a prosthetic
limb [1]. A key challenge in designing implanted integrated circuits (IC) for CINPS is to efficiently
process high-dimensional signals generated at the interface of micro-electrode arrays [3, 4]. Sensor arrays consisting of more than 1000 recording elements are common [5, 6] which significantly
increase the transmission rate at the sensor. A simple strategy of recording, parallel data conversion and transmitting the recorded neural signals ( at a sampling rate of 10 KHz) can easily exceed
the power dissipation limit of 80mW/cm2 determined by local heating of biological tissue [7]. In
addition to increased power dissipation, high-transmission rate also adversely affects the real-time
control of neural prosthesis [3].
One of the solutions that have been proposed by several researchers is to perform compression of
the neural signals directly at the sensor, to reduce its wireless transmission rate and hence its power
dissipation [8, 4]. In this paper we present an approach where de-correlation or redundancy elimination is performed directly at analog-to-digital converter. It has been shown that neural cross-talk and
common-mode effects introduces unwanted redundancy at the output of the electrode array [4]. As
a result, neural signals typically occupy only a small sub-space within the high-dimensional space
spanned by the micro-electrode signals. An optimal strategy for designing a multi-channel analogto-digital converter is to identify and operate within the sub-space spanned by the neural signals
and in the process eliminate cross-channel redundancy. To achieve this goal, in this paper we pro-

Figure 1: Functional architecture of a cortically implanted neural prosthesis illustrating the interface
of the data converter to micro-electrode arrays and signal processing modules

pose to use large margin principles [10], which have been highly successful in high-dimensional
information processing [11, 10]. Our approach will be to formalize a cost function consisting of
L1 norm of the internal state vector whose gradient updates naturally lends to a digital time-series
expansion. Within this framework the correlation distance between the channels will be minimized
which amounts to searching for signal spaces that are maximally separated from each other.
The architecture called multiple-input multiple-output (MIMO) ?? converter is the first reported
data conversion technique to embed large margin principles. The approach, however, is generic and
can be extended to designing higher order ADC. To illustrate the concept of MIMO A/D conversion,
the paper is organized as follows: section 2 introduces a regularization framework for the proposed
MIMO data converter and introduces the min-max gradient descent approach. Section 3 applies the
technique to simulated and recorded neural data. Section 4 concludes with final remarks and future
directions.

2

Regularization Framework and Generalized ?? Converters

In this section we introduce an optimization framework for deriving MIMO ?? converters. For the
sake of simplicity we will first assume that the input to converter is a M dimensional vector x ? RM
where each dimension represents a single channel in the multi-electrode array. It is also assumed
that the vector x is stationary with respect to discrete time instances n. The validity and limitation
of this assumption is explained briefly at the end of this section. Also denote a linear transformation
matrix A ? RM ?M and an regression weight vector w ? RM . Consider the following optimization
problem
min f (w, A)
(1)
w

where
f (w, A) = |w|T 1 ? wT Ax

(2)

and 1 represents a column vector whose elements are unity. The cost function in equation 2 consists
of two factors: the first factor is an L1 regularizer which constrains the norm of the vector w and the
second factor that maximizes the correlation between vector w and an input vector x transformed
using a linear projection denoted by matrix A. The choice of L1 norm and the form of cost function
in equation (2) will become clear when we present its corresponding gradient update rule. To ensure
that the optimization problem in equation 1 is well defined, the norm of the input vector ||x||? ? 1
will be assumed to be bounded.
Under bounded condition, the closed form solution to optimization problem in equation 1 can be
found to be w? = 0. From the perspective of A/D conversion we will show that the iterative steps
leading towards solution to the optimization problem in equation 1 are more important than the final
solution itself. Given an initial estimate of the state vector w[0] the online gradient descent step for

Figure 2: Architecture of the proposed first-order MIMO ?? converter.
minimizing 1 at iteration n is given by
?f
(3)
?w
where ? > 0 is defined as the learning rate. The choice of L1 norm in optimization function in
equation 1 ensures that for ? > 0 the iteration 3 exhibits oscillatory behavior around the solution
w? . Combining equation (3) with equation (2) the following recursion is obtained:
w[n] = w[n ? 1] ? ?

w[n] = w[n ? 1] + ?(Ax ? d[n])

(4)

d[n] = sgn(w[n ? 1])

(5)

where
and sgn(u) denotes an element-wise signum operation such that d[n] ? {+1, ?1}M represents a
digital time-series. The iterations in 3 represents the recursion step for M first-order ?? converters [9] coupled together by the linear transform A. If we assume that the norm of matrix ||A||? ? 1
is bounded, it can be shown that ||w? || < 1 + ?. Following N update steps the recursion given by
equation 4 yields

Ax ?

N
1 X
1
d[n] =
(w[N ] ? w[0])
N n=1
?N

(6)

which using the bounded property of w asymptotically leads to
N
1 X
d[n] ?? Ax
N n=1

(7)

as N ? ?.
Therefore consistent with the theory of ?? conversion [9] the moving average of vector digital
sequence d[n] converges to the transformed input vector Ax as the number of update steps N
increases. It can also be shown that N update steps yields a digital representation which is log2 (N )
bits accurate.
2.1

Online adaptation and compression

The next step is to determine the form of the matrix A which parameterize the family of linear
transformations spanning the signal space. The aim of optimizing for A is to find multi-channel
signal configuration that is maximally separated from each other. For this purposes we denote one
channel as a reference relative to which all distances/correlations will be measured. This is unlike
independent component analysis (ICA) based approaches [12], where the objective is to search for
maximally independent signal space including the reference channel. Even though several forms of
the matrix A = [aij ] can be chosen, for reasons which will discussed later in this paper the matrix
A is chosen to be a lower triangular matrix such that aij = 0; i < j and aij = 1; i = j. The
choice of a lower triangular matrix ensures that the matrix A is always invertible. It also implies

that the first channel is unaffected by the proposed transform A and will be the reference channel.
The problem of compression or redundancy elimination is therefore to optimize the cross-elements
aij , i 6= j such that the cross-correlation terms in optimization function given by equation 1 are
minimized. This can be written as a min-max optimization criterion where an inner optimization
performs analog-to-digital conversion, where as the outer loop adapts the linear transform matrix
A such as to maximize the margin of separation between the respective signal spaces. This can be
denoted by the following equation:
max (min f (w, A))

aij i6=j

(8)

w

In conjunction with the gradient descent steps in equation 4 the update rule for elements of A follows
a gradient ascent step given by
aij [n] = aij [n ? 1] ? ?ui [n]xj ; ?i > j

(9)

where ? is a learning rate parameter. The update rule in equation 9 can be made amenable to
hardware implementation by considering only the sign of the regression vector w[n] and the input
vector x as
aij [n] = aij [n ? 1] ? ?di [n] sign(xj ); ?i > j.

(10)

The update rule in equation 10 bears strong resemblance to online update rules used in independent
component analysis (ICA) [12, 13]. The difference with the proposed technique however is the
integrated data conversion coupled with spatial decorrelation/compression. The output of the MIMO
?? converter is a digital stream whose pulse density is proportional to the transformed input data
vector as
N
1 X
d[n] ?? A[n]x
N n=1

(11)

By construction the MIMO converter produces a digital stream whose pulse-density contains only
non-redundant information. To achieve compression some of the digital channels can be discarded
(based on their relative energy criterion ) and can also be shut down to conserve power. The original
signal can be reconstructed from the compressed digital stream by applying an inverse transformation A?1 as

x
b=

N
X
1
A[n]?1 (
d[n]).
N
n=1

(12)

An advantage of using a lower triangular form for the linear transformation matrix A with its diagonal elements as unity, is that its inverse always well-defined. Thus signal reconstruction using the
output of the analog-to-digital converter is also always well defined. Since the transformation matrix A is continually being updated, the information related to the linear transform also needs to be
periodically transmitted to ensure faithful reconstruction at the external prosthetic controller. However, analogous to many naturally occurring signal the underlying statistics of multi-dimensional
signal changes slowly as the signal itself. Therefore the transmission of the matrix A needs to be
performed at a relatively slower rate than the transmission of the compressed neural signals.
Similar to conventional ?? conversion [9], the framework for MIMO ?? can be extended to timevarying input vector under the assumption of high oversampling criterion [9]. For a MIMO A/D
converter oversampling ratio (OSR) is defined by the ratio of the update frequency fs and the maximum Nyquist rate amongst all elements of the input vector x[n]. The resolution of the MIMO
?? is also determined by the OSR as log2 (OSR) and during the oversampling period the input signal vector can be assumed to be approximately stationary. For time-varying input vector

(a)

(b)

Figure 3: Functional verification of MIMO ?? converter on artificially generated multi-channel
data (a) Data presented to the MIMO ?? converter (b) Analog representation of digital output
produced by MIMO converter

x[n] = {xj [n]}, j = 1, .., M the matrix update equation in equation 10 can be generalized after N
steps as
N
1 X
1
aij [N ] = ?
di [n]sgn(xj [n]); ?i > j.
N
N n=1

(13)

Thus if the norm of the matrix A is bounded, then asymptotically N ? ? the equation 13 imply
that the cross-channel correlation between the digital output and the sign of the input signal approaches zero. This is similar to formulations in ICA where higher-order de-correlation is achieved
using non-linear functions of random variables [12].
The architecture for the MIMO ?? converter illustrating recursions (4) and (11) is shown in Figure
2. As shown in the Figure 2 the regression vectors w[n] within the framework of MIMO ??
represents the output of the ?? integrator. All the adaptation and linear transformation steps can
be implemented using analog VLSI with adaptation steps implemented either using multiplying
digital-to-analog converters or floating gates synapses. Even though any channel can be chosen as
a reference channel, our experiments indicate that the channel with maximum cross-correlation and
maximum signal power serves as the best choice.

Figure 4: Reconstruction performance in terms of mean square error computed using artificial data
for different OSR

3

Results

The functionality of the proposed MIMO sigma-delta converter was verified using artificially generated data and with real multi-channel recorded neural data. The first set of experiments simulated
an artificially generated 8 channel data. Figure 3(a) illustrates the multi-channel data where each
channel was obtained by random linear mixing of two sinusoids with frequency 20Hz and 40Hz.
The multi-channel data was presented to a MIMO sigma delta converter implemented in software.
The equivalent analog representation of the pulse density encoded digital stream was obtained using
a moving window averaging technique with window size equal to the oversampling ratio (OSR).
The resultant analog representation of the ADC output is shown in 3(b). It can be seen in the
figure that after initial adaptation steps the output corresponding to first two channels converges to
the fundamental sinusoids, where as the rest of the digital streams converged to an equivalent zero
output. This simple experiment demonstrates the functionality of MIMO sigma-delta in eliminating cross-channel redundancy. The first two digital streams were used to reconstruct the original
recording using equation 12. Figure 4 shows the reconstruction error averaged over a time window
of 2048 samples showing that the error indeed converges to zero, as the MIMO converter adapts.
The Figure 4 also shows the error curves for different OSR. It can be seen that even though better
reconstruction error can be achieved by using higher OSR, the adaptation procedure compensates
for errors introduced due to low resolution. In fact the reconstruction performance is optimal for
intermediate OSR.

Figure 5: Functional verification of the MIMO sigma-delta converter for multi-channel neural data:
(a) Original multichannel data (b) analog representation of digital output produced by the converter
The multi-channel experiments were repeated with an eight channel neural data recorded from dorsal cochlear nucleus of adult guinea pigs. The data was recorded at a sampling rate of 20KHz and
at a resolution of 16 bits. Figure 5(a) shows a clip of multi-channel recording for duration of 0.5
seconds. It can be seen from highlighted portion of Figure 5(a) that the data exhibits high degree of
cross-channel correlation. Similar to the first set of experiments the MIMO converter eliminates spatial redundancy between channels as shown by the analog representation of the reconstructed output

Figure 6: Reconstruction performance in terms of mean square error computed using neural data for
different OSR

in Figure 5(b). An interesting observation in this experiment is that even though the statistics of the
input signals varies in time as shown in Figure 5 (a) and (b), the transformation matrix A remains
relatively stationary during the duration of the conversion, which is illustrated through the reconstruction error graph in Figure 6. This validates the principle of operation of the MIMO conversion
where the multi-channel neural recording lie on a low-dimensional manifold whose parameters are
relatively stationary with respect to the signal statistics.

Figure 7: Demonstration of common-mode rejection performed by MIMO ??: (a) Original multichannel signal at the input of converter (b) analog representation of the converter output (c) a magnified clip of the output produced by the converter illustrating preservation of neural information.

The last set of experiments demonstrate the ability of the proposed MIMO converter to reject common mode disturbance across all the channels. Rejection of common-mode signal is one of the most
important requirement for processing neural signals whose amplitude range from 50?V - 500?V ,
where as the common-mode interference resulting from EMG or electrical coupling could be as high
as 10mV [14]. Therefore most of the micro-electrode arrays use bio-potential amplifiers for enhancing signal-to-noise ratio and common-mode rejection. For this set of experiments, the recorded neural data obtained from the previous experiment was contaminated by an additive 60Hz sinusoidal
interference of amplitude 1mV . The results are shown in Figure 7 illustrating that the reference
channel absorbs all the common-mode disturbance where as the neural information is preserved in
other channels. In fact theoretically it can be shown that the common-mode rejection ratio for the
proposed MIMO ADC is dependent only on the OSR and is given by 20 log10 OSR.

4

Conclusion

In this paper we presented a novel MIMO analog-to-digital conversion algorithm with application to
multi-channel neural prosthesis. The roots of the algorithm lie within the framework of large margin
principles, where the data converter maximizes the relative distance between signal space corresponding to different channels. Experimental results with real multi-channel neural data demonstrate the effectiveness of the proposed method in eliminating cross-channel redundancy and hence
reducing data throughput and power dissipation requirements of a multi-channel biotelemetry sensor. There are several open questions that needs to be addressed as a continuation of this research
which includes extension of the algorithm second-order ?? architectures, embedding of kernels
into the ADC formulation and reformulation of the update rule to perform ICA directly on the ADC.
Acknowledgments
This work is supported by grant from National Institute of Health (R21NS047516-01A2). The
authors would also like to thank Prof. Karim Oweiss for providing multi-channel neural data for the
MIMO ADC experiments.

References
[1] Kennedy, P. R., R. A. Bakay, M. M. Moore, K. Adams, and J. Goldwaithe. Direct control of a computer
from the human central nervous system. IEEE Trans Rehabil Eng 8:198-202, 2000.
[2] J. Carmena, M. Lebedev, R. E. Crist, J. E. ODoherty, D. M. Santucci, D. Dimitrov, P. Patil, C. S. Henriquez, and M. A. Nicolelis, Learning to control a brain-machine interface for reaching and grasping by
primates, PLoS Biol., vol. 1, no. 2, pp. 193208, Nov. 2003.
[3] G. Santhanam, S. I. Ryu, B. M. Yu, and K. V. Shenoy, High information transmission rates in a neural
prosthetic system, in Soc. Neurosci., 2004, Program 263.2.
[4] K. Oweiss, D. Anderson, M. Papaefthymiou, Optimizing Signal Coding in Neural Interface System-ona-Chip Modules, IEEE Conf. on EMBS, pp. 2016-2019, Sept. 2003.
[5] K. Wise et. al., Wireless Implantable Microsystems: High-Density Electronic Interfaces to the Nervous
System, Proc. of the IEEE, Vol.: 92-1, pp: 7697, Jan. 2004.
[6] Maynard EM, Nordhausen CT, Normann RA, The Utah intracortical electrode array: a recording structure
for potential brain computer interfaces. Electroencephalogr Clin Neurophysiol 102: 228239, 1997.
[7] T. M. Seese, H. Harasaki, G. M. Saidel, and C. R. Davies, Characterization of tissue morphology, angiogenesis, and temperature in adaptive response of muscle tissue to chronic heating, Lab Investigation, vol.
78, no. 12, pp. 15531562, Dec. 1998.
[8] R. R. Harrison, A low-power integrated cicuit for adaptive detection of action potentials in noisy signals,
in Proc. 25th Ann. Conf. IEEE EMBS, Cancun, Mexico, Sep. 2003, pp. 33253328.
[9] J. C. Candy and G. C. Temes, Oversampled methods for A/D and D/A conversion, in Oversampled DeltaSigma Data Converters. Piscataway, NJ: IEEE Press, 1992, pp. 1- 29.
[10] Vapnik, V. The Nature of Statistical Learning Theory, New York: Springer-Verlag, 1995.
[11] Girosi, F., Jones, M. and Poggio, T. Regularization Theory and Neural Networks Architectures, Neural
Computation, vol. 7, pp 219-269, 1996.
[12] Hyvrinen, A. Survey on independent component, analysis. Neural Computing Surveys, 2:94128, 1999.
[13] A. Celik, M. Stanacevic and G. Cauwenberghs, Gradient Flow Independent Component Analysis in Micropower VLSI, Adv. Neural Information Processing Systems (NIPS?2005), Cambridge: MIT Press, 18,
2006
[14] Pedram Mohseni and Khalil Najafi. A fully integrated neural recording amplifier with DC input stabilization. Biomedical Engineering, IEEE Transactions on Volume 51, Issue 5, May 2004.

"
6308,2017,Revisiting Perceptron: Efficient and Label-Optimal Learning of Halfspaces,"It has been a long-standing problem to efficiently learn a halfspace using as few labels as possible in the presence of noise. In this work, we propose an efficient Perceptron-based algorithm for actively learning homogeneous halfspaces under the uniform distribution over the unit sphere. Under the bounded noise condition~\cite{MN06}, where each label is flipped with probability at most $\eta < \frac 1 2$, our algorithm achieves a near-optimal label complexity of $\tilde{O}\left(\frac{d}{(1-2\eta)^2}\ln\frac{1}{\epsilon}\right)$ in time $\tilde{O}\left(\frac{d^2}{\epsilon(1-2\eta)^3}\right)$. Under the adversarial noise condition~\cite{ABL14, KLS09, KKMS08}, where at most a $\tilde \Omega(\epsilon)$ fraction of labels can be flipped, our algorithm achieves a near-optimal label complexity of $\tilde{O}\left(d\ln\frac{1}{\epsilon}\right)$ in time $\tilde{O}\left(\frac{d^2}{\epsilon}\right)$. Furthermore, we show that our active learning algorithm can be converted to an efficient passive learning algorithm that has near-optimal sample complexities with respect to $\epsilon$ and $d$.","Revisiting Perceptron:
Ef?cient and Label-Optimal Learning of Halfspaces
Songbai Yan
UC San Diego
La Jolla, CA
yansongbai@ucsd.edu

Chicheng Zhang?
Microsoft Research
New York, NY
chicheng.zhang@microsoft.com

Abstract
It has been a long-standing problem to ef?ciently learn a halfspace using as few
labels as possible in the presence of noise. In this work, we propose an ef?cient
Perceptron-based algorithm for actively learning homogeneous halfspaces under the
uniform distribution over the unit sphere. Under the bounded noise condition [49],
where each label is ?ipped with probability
at most?? < 12 , our algorithm
achieves
a
?
?
?
d
1 2
d2
?
?
near-optimal label complexity of O
in time O
. Under
2 ln
3
(1?2?)

?

?(1?2?)

?
the adversarial noise condition [6, 45, 42], where at most a ?(?)
fraction of
? labels?
? d ln 1
O
can be ?ipped,
our
algorithm
achieves
a
near-optimal
label
complexity
of
?
? ?
? d2 . Furthermore, we show that our active learning algorithm can be
in time O
?

converted to an ef?cient passive learning algorithm that has near-optimal sample
complexities with respect to ? and d.

1

Introduction

We study the problem of designing ef?cient noise-tolerant algorithms for actively learning homogeneous halfspaces in the streaming setting. We are given access to a data distribution from which we
can draw unlabeled examples, and a noisy labeling oracle O that we can query for labels. The goal is
to ?nd a computationally ef?cient algorithm to learn a halfspace that best classi?es the data while
making as few queries to the labeling oracle as possible.
Active learning arises naturally in many machine learning applications where unlabeled examples are
abundant and cheap, but labeling requires human effort and is expensive. For those applications, one
natural question is whether we can learn an accurate classi?er using as few labels as possible. Active
learning addresses this question by allowing the learning algorithm to sequentially select examples
to query for labels, and avoid requesting labels which are less informative, or can be inferred from
previously-observed examples.
There has been a large body of work on the theory of active learning, showing sharp distributiondependent label complexity bounds [21, 11, 34, 27, 35, 46, 60, 41]. However, most of these general
active learning algorithms rely on solving empirical risk minimization problems, which are computationally hard in the presence of noise [5].
On the other hand, existing computationally ef?cient algorithms for learning halfspaces [17, 29, 42,
45, 6, 23, 7, 8] are not optimal in terms of label requirements. These algorithms have different degrees
of noise tolerance (e.g. adversarial noise [6], malicious noise [43], random classi?cation noise [3],
?

Work done while at UC San Diego.
?
? (?)) := O(f (?) ln f (?)), and ?(f
? (?)) := ?(f (?)/ ln f (?)). We say f (?) = ?(g(?))
We use O(f
if f (?) =
?
?
?
?
O(g(?)) and f (?) = ? g(?)
2

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

bounded noise [49], etc), and run in time polynomial in 1? and d. Some of them naturally exploit the
utility of active learning [6, 7, 8], but they do not achieve the sharpest label complexity bounds in
contrast to those computationally-inef?cient active learning algorithms [10, 9, 60].
Therefore, a natural question is: is there any active learning halfspace algorithm that is computationally
ef?cient, and has a minimum label requirement? This has been posed as an open problem in [50].
In the realizable setting, [26, 10, 9, 56] give ef?cient algorithms that have optimal label complexity
? ln 1 ) under some distributional assumptions. However, the challenge still remains open in
of O(d
?
the nonrealizable setting. It has been shown that learning halfspaces with agnostic noise even under
Gaussian unlabeled distribution is hard [44]. Nonetheless, we give an af?rmative answer to this
question under two moderate noise settings: bounded noise and adversarial noise.
1.1

Our Results

We propose a Perceptron-based algorithm, ACTIVE -P ERCEPTRON, for actively learning homogeneous halfspaces under the uniform distribution over the unit sphere. It works under two noise
settings: bounded noise and adversarial noise. Our work answers an open question by [26] on
whether Perceptron-based active learning algorithms can be modi?ed to tolerate label noise.
In the ?-bounded noise setting (also known as the Massart noise model [49]), the label of an example
x ? Rd is generated by sign(u ? x) for some underlying
halfspace
u, and ?ipped
?
? with probabil?
?
d2
d
1
?
?
O
ity ?(x) ? ? < 1 . Our algorithm runs in time O
,
and
requires
3
2 ? ln
2

(1?2?) ?

(1?2?)

?

labels. We show that this label complexity
is nearly ?optimal by providing an almost matching
?
d
information-theoretic lower bound of ? (1?2?)2 ? ln 1? . Our time and label complexities substan?
tially improve over the state of the art result of [8], which runs in time O(d
1
O(
)
1
4
?
(1?2?)
O(d
ln ) labels.

1
O( (1?2?)
4)1
?)

and requires

?

Our main theorem on learning under bounded noise is as follows:
Theorem 2 (Informal). Suppose the labeling oracle O satis?es the ?-bounded noise condition with
respect to u, then for ACTIVE -P ERCEPTRON, with probability at least 1??: (1) The output halfspace
v is such? that P[sign(v?? X) ?= sign(u ? X)] ? ?; (2) The number of label queries to? oracle O?is at
d
1
d
?
?
most O
(1?2?)2 ? ln ? ; (3) The number of unlabeled examples drawn is at most O (1?2?)3 ? ; (4)
?
?
d2
?
The algorithm runs in time O
(1?2?)3 ? .
In addition, we show that our algorithm also works in a more challenging setting, the ?-adversarial
noise setting [6, 42, 45].3 In this setting, the examples still come iid from a distribution, but the
assumption on the labels is just that P[sign(u ? X) ?= Y ] ? ? for some halfspace u. Under this
assumption, the Bayes classi?er may not be a halfspace.
We?show that our algorithm achieves an
?
? ?
?
? d2 , and requires
error of ? while tolerating a noise level of ? = ? ln d +ln ln 1 . It runs in time O
?
?
?
?
?
1
? d ? ln
only O
? labels which is near-optimal. ACTIVE -P ERCEPTRON has a label complexity bound
that matches the state of the art result of [39]4 , while having a lower running time.
Our main theorem on learning under adversarial noise is as follows:

Theorem 3 (Informal). Suppose the labeling oracle O satis?es the ?-adversarial noise condition
?
with respect to u, where ? < ?( ln d +ln
). Then for ACTIVE -P ERCEPTRON, with probability at
ln 1
?

?

least 1 ? ?: (1) The output halfspace v is such
? that?P[sign(v ? X) ?= sign(u ? X)] ? ?; (2) The number
? d ? ln 1 ; (3) The number of unlabeled examples drawn is
O
of label queries
to
oracle
O
is
at
most
? ? ?
? ?
? d ; (4) The algorithm runs in time O
? d2 .
at most O
?

?

3
Note that the adversarial noise model is not the same as that in online learning [18], where each example
can be chosen adversarially.
4
The label complexity bound is implicit in [39] by a re?ned analysis of the algorithm of [6] (See their Lemma
8 for details).

2

Table 1: A comparison of algorithms for active learning of halfspaces under the uniform distribution,
in the ?-bounded noise model.
Algorithm
[10, 9, 60]

Label Complexity
d
1
?
O(
)
2 ln

[8]
Our Work

?
O(d
?
O(

Time Complexity

(1?2?)
?
1
O( (1?2?)
4)
d
(1?2?)2

? ln 1? )
ln 1? )

superpoly(d, 1? ) 5
1
? O( (1?2?)4 ) ? 1 )
O(d
? ?
?
d2
1
?
O
3
(1?2?) ?

Table 2: A comparison of algorithms for active learning of halfspaces under the uniform distribution,
in the ?-adversarial noise model.
Algorithm

Noise Tolerance

[60]
[39]
Our Work

? = ?(?)
? = ?(?)
?
)
? = ?( ln d+ln
ln 1

Label Complexity
? ln 1 )
O(d
?
? ln 1 )
O(d
?
? ln 1 )
O(d
?

?

Time Complexity
superpoly(d, 1? )
1
poly(d,
? 2 1? ?)
?
O d ??

Throughout the paper, ACTIVE -P ERCEPTRON is shown to work if the unlabeled examples are drawn
uniformly from the unit sphere. The algorithm and analysis can be easily generalized to any spherical
symmetrical distributions, for example, isotropic Gaussian distributions. They can also be generalized
to distributions whose densities with respect to uniform distribution are bounded away from 0.
In addition, we show in Section 6 that ACTIVE -P ERCEPTRON can be converted to a passive learning
algorithm, PASSIVE -P ERCEPTRON, that has near optimal sample complexities with respect to ? and
d under the two noise settings. We defer the discussion to the end of the paper.

2

Related Work

Active Learning. The recent decades have seen much success in both theory and practice of active
learning; see the excellent surveys by [54, 37, 25]. On the theory side, many label-ef?cient active
learning algorithms have been proposed and analyzed. An incomplete list includes [21, 11, 34, 27,
35, 46, 60, 41]. Most algorithms relies on solving empirical risk minimization problems, which are
computationally hard in the presence of noise [5].
Computational Hardness of Learning Halfspaces. Ef?cient learning of halfspaces is one of
the central problems in machine learning [22]. In the realizable case, it is well known that linear
programming will ?nd a consistent hypothesis over data ef?ciently. In the nonrealizable setting,
however, the problem is much more challenging.
A series of papers have shown the hardness of learning halfspaces with agnostic noise [5, 30, 33, 44,
23]. The state of the art result [23] shows that under standard complexity-theoretic assumptions, there
exists a data distribution, such that the best linear classi?er has error o(1), but no polynomial time
algorithms can achieve an error at most 12 ? d1c for every c > 0, even with improper learning. [44]
shows that under standard assumptions, even if the unlabeled distribution is Gaussian, any agnostic
halfspace learning algorithm must run in time ( 1? )?(ln d) to achieve an excess error of ?. These results
indicate that, to have nontrivial guarantees on learning halfspaces with noise in polynomial time, one
has to make additional assumptions on the data distribution over instances and labels.
Ef?cient Active Learning of Halfspaces. Despite considerable efforts, there are only a few halfspace learning algorithms that are both computationally-ef?cient and label-ef?cient even under the
uniform distribution. In the realizable setting, [26, 10, 9] propose computationally ef?cient active
? ln 1 ).
learning algorithms which have an optimal label complexity of O(d
?
Since it is believed to be hard for learning halfspaces in the general agnostic setting, it is natural
to consider algorithms that work under more moderate noise conditions. Under the bounded noise
5

The algorithm needs to minimize 0-1 loss, the best known method for which requires superpolynomial time.

3

setting [49], the only known algorithms that are both label-ef?cient and computationally-ef?cient are
[7, 8]. [7] uses a margin-based framework which queries the labels of examples near the decision
boundary. To achieve computational ef?ciency, it adaptively chooses a sequence of hinge loss
minimization problems to optimize as opposed to directly optimizing the 0-1 loss. It works only
when the label ?ipping probability upper bound ? is small (? ? 1.8 ? 10?6 ). [8] improves over [7]
by adapting a polynomial regression procedure into the margin-based framework. It works for any
1
O(
)
? < 1/2, but its label complexity is O(d (1?2?)4 ln 1? ), which is far worse than the informationd
1
theoretic lower bound ?( (1?2?)
2 ln ? ). Recently [20] gives an ef?cient algorithm with a near-optimal
label complexity under the membership query model where the learner can query on synthesized
points. In contrast, in our stream-based model, the learner can only query on points drawn from the
data distribution. We note that learning in the stream-based model is harder than in the membership
query model, and it is unclear how to transform the DC algorithm in [20] into a computationally
ef?cient stream-based active learning algorithm.
Under the more challenging ?-adversarial noise setting, [6] proposes a margin-based algorithm that
reduces the problem to a sequence of hinge loss minimization problems. Their algorithm achieves an
? 2 ln 1 ) labels. Later, [39] performs a
error of ? in polynomial time when ? = ?(?), but requires O(d
?
? ln 1 ), but the time complexity of
re?ned analysis to achieve a near-optimal label complexity of O(d
?
the algorithm is still an unspeci?ed high order polynomial.
Tables 1 and 2 present comparisons between our results and results most closely related to ours
in the literature. Due to space limitations, discussions of additional related work are deferred to
Appendix A.

3

De?nitions and Settings

We consider learning homogeneous halfspaces under uniform
distribution. ?
The instance space X
?
is the unit sphere in Rd , which we denote by Sd?1 := x ? Rd : ?x? = 1 . We assume d ? 3
throughout this paper. The label space Y = {+1, ?1}. We assume all data points (x, y) are drawn
i.i.d. from an underlying distribution D over X ? Y. We denote by DX the marginal of D over X
(which is uniform over Sd?1 ), and DY |X the conditional distribution of Y given X. Our algorithm is
allowed to draw unlabeled examples x ? X from DX , and to make queries to a labeling oracle O for
labels. Upon query x, O returns a label y ?
drawn from DY |X=x . The hypothesis
? class of interest is
d?1
the set of homogeneous halfspaces H := hw (x) = sign(w ? x) | w ? S
. For any hypothesis
h ? H, we de?ne its error rate err(h) := PD [h(X)
=
?
Y
].
We
will
drop
the
subscript
D in PD when
?
?
it is clear from the context. Given a dataset S = ?(X1 , Y1 ), . .?. , (Xm , Ym ) , we de?ne the empirical
?m
1
error rate of h over S as errS (h) := m
i=1 1 h(xi ) ?= yi .
De?nition 1 (Bounded Noise [49]). We say that the labeling oracle O satis?es the ?-bounded noise
condition for some ? ? [0, 1/2) with respect to u, if for any x, P[Y ?= sign(u ? x) | X = x] ? ?.
It can be seen that under ?-bounded noise condition, hu is the Bayes classi?er.
De?nition 2 (Adversarial Noise [6]). We say that the labeling oracle O satis?es the ?-adversarial
noise condition for some ? ? [0, 1] with respect to u, if P[Y ?= sign(u ? X)] ? ?.
For two unit vectors v1 , v2 , denote by ?(v1 , v2 ) = arccos(v1 ? v2 ) the angle between them. The
following lemma gives relationships between errors and angles (see also Lemma 1 in [8]).
?
?
?
?
Lemma 1. For any v1 , v2 ? Sd?1 , ?err(hv1 ) ? err(hv2 )? ? P hv1 (X) ?= hv2 (X) = ?(v1?,v2 ) .

Additionally, if? the labeling oracle? satis?es the ?-bounded
noise condition
with respect to u, then for
?
?
any vector v, ?err(hv ) ? err(hu )? ? (1 ? 2?)P hv (X) ?= hu (X) = 1?2?
? ?(v, u).

Given access to unlabeled examples drawn from DX and a labeling oracle O, our goal is to ?nd a
polynomial time algorithm A such that with probability at least 1 ? ?, A outputs a halfspace hv ? H
with P[sign(v ? X) ?= sign(u ? X)] ? ? for some target accuracy ? and con?dence ?. (By Lemma 1,
this guarantees that the excess error of hv is at most ?, namely, err(hv ) ? err(hu ) ? ?.) The desired
algorithm should make as few queries to the labeling oracle O as possible.
4

We say an algorithm A achieves a label complexity of ?(?, ?), if for any target halfspace hu ? H,
with probability at least 1 ? ?, A outputs a halfspace hv ? H such that err(hv ) ? err(hu ) + ?, and
requests at most ?(?, ?) labels from oracle O.

4

Main Algorithm

Our main algorithm, ACTIVE -P ERCEPTRON (Algorithm 1), works in epochs. It works under the
bounded and the adversarial noise models, if its sample schedule {mk } and band width {bk } are set
appropriately with respect to each noise model. At the beginning of each epoch k, it assumes an upper
bound of 2?k on ?(vk?1 , u), the angle between current iterate vk?1 and the underlying halfspace u.
As we will see, this can be shown to hold with high probability inductively. Then, it calls procedure
M ODIFIED -P ERCEPTRON (Algorithm 2) to ?nd an new iterate vk , which can be shown to have an
?
angle with u at most 2k+1
with high probability. The algorithm ends when a total of k0 = ?log2 1? ?
epochs have passed.
For simplicity, we assume for the rest of the paper that the angle between the initial halfspace v0 and
the underlying halfspace u is acute, that is, ?(v0 , u) ? ?2 ; Appendix F shows that this assumption
can be removed with a constant overhead in terms of label and time complexities.
Algorithm 1 ACTIVE -P ERCEPTRON
Input: Labeling oracle O, initial halfspace v0 , target error ?, con?dence ?, sample schedule {mk },
band width {bk }.
Output: learned halfspace v.
1: Let k0 = ?log2 1? ?.
2: for k = 1, 2, . . . , k0 do
?
3:
vk ? M ODIFIED -P ERCEPTRON(O, vk?1 , 2?k , k(k+1)
, mk , bk ).
4: end for
5: return vk0 .
Procedure M ODIFIED -P ERCEPTRON (Algorithm 2) is the core component of ACTIVE -P ERCEPTRON.
It sequentially performs a modi?ed Perceptron update rule on the selected new examples (xt , yt ) [51,
17, 26]:
(1)
wt+1 ? wt ? 21 {yt wt ? xt < 0} (wt ? xt ) ? xt
De?ne ?t := ?(wt , u). Update rule (1) implies the following relationship between ?t+1 and ?t (See
Lemma 8 in Appendix E for its proof):
cos ?t+1 ? cos ?t = ?21 {yt wt ? xt < 0} (wt ? xt ) ? (u ? xt )

(2)

This motivates us to take cos ?t as our measure of progress; we would like to drive cos ?t up to 1(so
that ?t goes down to 0) as fast as possible.
To this end, M ODIFIED -P ERCEPTRON samples
new points xt under time-varying
distributions DX |Rt
?
?
d?1 : b
and query for their labels, where Rt = x ? S
2 ? wt ? x ? b is a band inside the unit sphere.
The rationale behind the choice of Rt is twofold:
?
1. We set Rt to have a probability mass of ?(?),
so that the time complexity of rejection
1
?
sampling is at most O( ? ) per example. Moreover, in the adversarial noise setting, we set Rt
?
large enough to dominate the noise of magnitude ? = ?(?).
2. Unlike the active Perceptron algorithm in [26] or other margin-based approaches (for
example [55, 10]) where examples with small margin are queried, we query the label of the
examples with a range of margin [ 2b , b]. From a technical perspective, this ensures that ?t
decreases by a decent amount in expectation (see Lemmas 9 and 10 for details).
Following the insight of [32], we remark that the modi?ed Perceptron update (1) on distribution
DX |Rt can be alternatively viewed as performing stochastic gradient descent on a special non-convex
loss function ?(w, (x, y)) = min(1, max(0, ?1? 2b yw?x)). It is an interesting open question whether
optimizing this new loss function can lead to improved empirical results for learning halfspaces.
5

Algorithm 2 M ODIFIED -P ERCEPTRON
Input: Labeling oracle O, initial halfspace w0 , angle upper bound ?, con?dence ?, number of
iterations m, band width b.
Output: Improved halfspace wm .
1: for t = 0, 1, 2, . . . , m ?
? 1 do
?
2:
De?ne region Rt = x ? Sd?1 : 2b ? wt ? x ? b .
Rejection sample xt ? DX |Rt . In other words, draw xt from DX until xt is in Rt . Query O
for its label yt .
4:
wt+1 ? wt ? 21 {yt wt ? xt < 0} ? (wt ? xt ) ? xt .
5: end for
6: return wm .
3:

5

Performance Guarantees

We show that ACTIVE -P ERCEPTRON works in the bounded and the adversarial noise models, achieving computational ef?ciency and near-optimal label complexities. To this end, we ?rst give a lower
bound on the label complexity under bounded noise, and then give computational and label complexity
upper bounds under the two noise conditions respectively. We defer all proofs to the Appendix.
5.1

A Lower Bound under Bounded Noise

We ?rst present an information-theoretic lower bound on the label complexity in the bounded noise
setting under uniform distribution. This extends the distribution-free lower bounds of [53, 37], and
generalizes the realizable-case lower bound of [47] to the bounded noise setting. Our lower bound
can also be viewed as an extension of [59]?s Theorem 3; speci?cally it addresses the hardness under
the ?-Tsybakov noise condition where ? = 0 (while [59]?s Theorem 3 provides lower boundes when
? ? (0, 1)).
1
Theorem 1. For any d > 4, 0 ? ? < 12 , 0 < ? ? 4?
, 0 < ? ? 14 , for any active learning algorithm
d?1
A, there is a u ? S , and a labeling oracle O that satis?es ?-bounded noise condition with respect
to u, such that if with probability at least 1 ? ?, A makes at most n queries
of labels to O?and outputs
?
d log 1
? log 1
v ? Sd?1 such that P[sign(v ? X) ?= sign(u ? X)] ? ?, then n ? ? (1?2?)? 2 + (1?2?)?2 .

5.2

Bounded Noise

We establish Theorem 2 in the bounded noise setting. The theorem implies that, with appropriate
settings of input parameters, ACTIVE -P ERCEPTRON ef?ciently learns a halfspace of excess error
at most ? with probability at least 1 ? ?, under the assumption that DX is uniform over the unit
d
1
?
sphere and O has bounded noise. In addition, it queries at most O(
(1?2?)2 ln ? ) labels. This matches
the lower bound of Theorem 1, and improves over the state of the art result of [8], where a label
1
? O( (1?2?)4 ) ln 1 ) is shown using a different algorithm.
complexity of O(d
?
The proof and the precise setting of parameters (mk and bk ) are given in Appendix C.
Theorem 2 (ACTIVE -P ERCEPTRON under Bounded Noise). Suppose Algorithm 1 has inputs labeling oracle O that satis?es ?-bounded noise condition with respect to halfspace u, initial halfspace v0 such that ?(v0 , u) ? [0, ?2 ], target error ?, con?dence ?, sample
? schedule {m
? k } where
?
?
?k
2
(1?2?)
d
d
k
?
mk = ? (1?2?)
. Then with
2 (ln (1?2?)2 + ln ? ) , band width {bk } where bk = ?
d ln(km /?)
k

probability at least 1 ? ?:

1. The output halfspace v is such that P[sign(v ? X) ?= sign(u ? X)] ? ?.
?
?
??
d
1
d
1
1
?
ln
?
ln
+
ln
+
ln
ln
2. The number of label queries is O (1?2?)
.
2
?
(1?2?)2
?
?
6

3. The
is
? number of?unlabeled examples drawn ?
2
d
d
1
1
O (1?2?)3 ? ln (1?2?)2 + ln ? + ln ln ? ?
4. The algorithm runs in time O

?

d2
(1?2?)3

1
?

?
ln 1? .

?
?2
d
1
1
? ln (1?2?)
?
2 + ln ? + ln ln ?

1
?

?
ln 1? .

The theorem follows from Lemma 2 below. The key ingredient of the lemma is a delicate analysis
m
of the dynamics of the angles {?t }t=0 , where ?t = ?(wt , u) is the angle between the iterate wt
and the halfspace u. Since xt is randomly sampled and yt is noisy, we are only able to show that
?t decreases by a decent amount in expectation. To remedy the stochastic ?uctuations, we apply
m
martingale concentration inequalities to carefully control the upper envelope of sequence {?t }t=0 .
Lemma 2 (M ODIFIED -P ERCEPTRON under Bounded Noise). Suppose Algorithm 2 has inputs
labeling oracle O that satis?es ?-bounded noise condition with respect to halfspace u, initial
halfspace w0 and angle upper bound ? ? (0, ?2 ] such that ?(w0 , u) ???, con?dence
? ?, number
?(1?2?)
?
d ln(m/?)

d
d
1
of iterations m = ?( (1?2?)
2 (ln (1?2?)2 + ln ? )), band width b = ?

. Then with

probability at least 1 ? ?:

1. The output halfspace wm is such that ?(wm , u) ? ?2 .
?
?
??
d
d
1
2. The number of label queries is O (1?2?)2 ln (1?2?)2 + ln ? .
3. The number of unlabeled examples drawn is O
4. The algorithm runs in time O
5.3

?

d2
(1?2?)3

?

? ln

?

d
(1?2?)3

d
(1?2?)2

?

? ln

+ ln

1
?

?2

d
(1?2?)2

?

1
?

?

+ ln

1
?

?2

?

1
?

?

.

.

Adversarial Noise

We establish Theorem 3 in the adversarial noise setting. The theorem implies that, with appropriate
settings of input parameters, ACTIVE -P ERCEPTRON ef?ciently learns a halfspace of excess error at
most ? with probability at least 1 ? ?, under the assumption that DX is uniform over the unit sphere
?
and O has an adversarial noise of magnitude ? = ?( ln d+ln
). In addition, it queries at most
ln 1?
1
?
O(d ln ? ) labels. Our label complexity bound is information-theoretically optimal [47], and matches
the state of the art result of [39]. The bene?t of our approach is computational: it has a running time
? d2 ), while [39] needs to solve a convex optimization problem whose running time is some
of O(
?
polynomial over d and 1? with an unspeci?ed degree.
The proof and the precise setting of parameters (mk and bk ) are given in Appendix C.
Theorem 3 (ACTIVE -P ERCEPTRON under Adversarial Noise). Suppose Algorithm 1 has inputs
labeling oracle O that satis?es ?-adversarial noise condition with respect to halfspace u, initial
halfspace v0 such that ?(v0 , u) ? ?2 , target error ?, con?dence
{mk } where
?
? ?, sample schedule
mk = ?(d(ln d + ln k? )), band width {bk } where bk = ?

?
?( ln d +ln
ln
?

1
?

?

2?k
d ln(kmk /?)

). Then with probability at least 1 ? ?:

1. The output halfspace v is such that P[sign(v ? X) ?= sign(u ? X)] ? ?.
?
??
?
2. The number of label queries is O d ? ln 1? ? ln d + ln 1? + ln ln 1? .

. Additionally ? ?

? ?
?2
3. The number of unlabeled examples drawn is O d ? ln d + ln 1? + ln ln 1? ?

?
?
?2
4. The algorithm runs in time O d2 ? ln d + ln 1? + ln ln 1? ?
7

1
?

?
ln 1? .

1
?

?
ln 1? .

The theorem follows from Lemma 3 below, whose proof is similar to Lemma 2.
Lemma 3 (M ODIFIED -P ERCEPTRON under Adversarial Noise). Suppose Algorithm 2 has inputs
labeling oracle O that satis?es ?-adversarial noise condition with respect to halfspace u, initial
halfspace w0 and angle upper bound ? ? (0, ?2 ] such?that ?(w0 , ?
u) ? ?, con?dence ?, number of
iterations m = ?(d(ln d + ln 1? )), band width b = ?

?

?
d ln(m/?)

Then with probability at least 1 ? ?:

?
. Additionally ? ? ?( ln(m/?))
).

1. The output halfspace wm is such that ?(wm , u) ? ?2 .
? ?
??
2. The number of label queries is O d ? ln d + ln 1? .

? ?
?2 ?
3. The number of unlabeled examples drawn is O d ? ln d + ln 1? ? ?1

6

?
?
?2 ?
4. The algorithm runs in time O d2 ? ln d + ln 1? ? ?1 .

Implications to Passive Learning

ACTIVE -P ERCEPTRON can be converted to a passive learning algorithm, PASSIVE -P ERCEPTRON,
for learning homogeneous halfspaces under the uniform distribution over the unit sphere.
PASSIVE -P ERCEPTRON has PAC sample complexities close to the lower bounds under the two
noise models. We give a formal description of PASSIVE -P ERCEPTRON in Appendix B. We give its
formal guarantees in the corollaries below, which are immediate consequences of Theorems 2 and 3.
In the ?-bounded noise model, the sample complexity of PASSIVE -P ERCEPTRON improves over the
O(

1

)

? d (1?2?)4 ) is obtained. The bound
state of the art result of [8], where a sample complexity of O(
?
d
?
has the same dependency on ? and d as the minimax upper bound of ?(
?(1?2?) ) by [49], which is
achieved by a computationally inef?cient ERM algorithm.
Corollary 1 (PASSIVE -P ERCEPTRON under Bounded Noise). Suppose PASSIVE -P ERCEPTRON has
inputs distribution D that satis?es ?-bounded noise condition with respect
to u, initial halfspace v?0 ,
?
d
d
k
target error ?, con?dence ?, sample schedule {mk } where mk = ? (1?2?)
2 (ln (1?2?)2 + ln ? ) ,
?
?
?k
(1?2?)
band width {bk } where bk = ? ?2d ln(km
. Then with probability at least 1 ? ?: (1) The
/?)
k

output
? halfspace
? v is such that err(hv ) ? err(hu )?+ ?; 2(2) The
? number of labeled examples drawn is
d
d
?
?
O
. (3) The algorithm runs in time O
.
3
3
(1?2?) ?

(1?2?) ?

In the ?-adversarial noise model, the sample complexity of PASSIVE -P ERCEPTRON matches the
? d ) obtained in [39]. Same as in active
minimax optimal sample complexity upper bound of ?(
?
learning, our algorithm has a faster running time than [39].
Corollary 2 (PASSIVE -P ERCEPTRON under Adversarial Noise). Suppose PASSIVE -P ERCEPTRON
has inputs distribution D that satis?es ?-adversarial noise condition with respect
to u, initial
?
?
halfspace v0 , target error ?, con?dence ?, sample schedule {mk } where mk = ? d(ln d + ln k? ) ,
?
?
2?k
?
band width {bk } where bk = ?
. Furthermore ? = ?( ln ln 1?+ln d ). Then with
d ln(km /?)
k

?

?

probability at least 1 ? ?: (1) The output
v is such that err(hv ) ? err(h
u ) + ?; (2) The
?
? 2?
? halfspace
d
d
?
?
. (3) The algorithm runs in time O
.
number of labeled examples drawn is O
?

?

Tables 3 and 4 present comparisons between our results and results most closely related to ours.

Acknowledgments. The authors thank Kamalika Chaudhuri for help and support, Hongyang Zhang
for thought-provoking initial conversations, Jiapeng Zhang for helpful discussions, and the anonymous
reviewers for their insightful feedback. Much of this work is supported by NSF IIS-1167157 and
1162581.
8

Table 3: A comparison of algorithms for PAC learning halfspaces under the uniform distribution, in
the ?-bounded noise model.
Algorithm
[8]
ERM [49]
Our Work

Sample Complexity
?
O(
?
O(

1
O(
)
d (1?2?)4

?

Time Complexity
O(

)

d
(1?2?)? )
d
?
O(
(1?2?)3 ? )

1

)

? d (1?2?)4 )
O(
?
superpoly(d, 1? )
d2
1
?
O(
)
3 ?
(1?2?)

?

Table 4: A comparison of algorithms for PAC learning halfspaces under the uniform distribution, in
the ?-adversarial noise model where ? = ?( ln ln 1?+ln d ).
?

Algorithm
[39]
ERM [57]
Our Work

Sample Complexity
? d)
O(
?
? d)
O(
?
? d)
O(
?

Time Complexity
poly(d, 1? )
superpoly(d, 1? )
? d2 )
O(
?

References
[1] Alekh Agarwal. Selective sampling algorithms for cost-sensitive multiclass prediction. ICML (3), 28:
1220?1228, 2013.
[2] Nir Ailon, Ron Begleiter, and Esther Ezra. Active learning using smooth relative regret approximations
with applications. Journal of Machine Learning Research, 15(1):885?920, 2014.
[3] Dana Angluin and Philip Laird. Learning from noisy examples. Machine Learning, 2(4):343?370, Apr
1988. ISSN 1573-0565. doi: 10.1023/A:1022873112823. URL https://doi.org/10.1023/A:
1022873112823.
[4] Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. Cambridge
University Press, 2009.
[5] Sanjeev Arora, L?szl? Babai, Jacques Stern, and Z Sweedyk. The hardness of approximate optima in
lattices, codes, and systems of linear equations. In Foundations of Computer Science, 1993. Proceedings.,
34th Annual Symposium on, pages 724?733. IEEE, 1993.
[6] Pranjal Awasthi, Maria Florina Balcan, and Philip M Long. The power of localization for ef?ciently
learning linear separators with noise. In Proceedings of the 46th Annual ACM Symposium on Theory of
Computing, pages 449?458. ACM, 2014.
[7] Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, and Ruth Urner. Ef?cient learning of linear
separators under bounded noise. In COLT, pages 167?190, 2015.
[8] Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, and Hongyang Zhang. Learning and 1-bit
compressed sensing under asymmetric noise. In Proceedings of The 28th Conference on Learning Theory,
COLT 2016, 2016.
[9] M.-F. Balcan and P. M. Long. Active and passive learning of linear separators under log-concave distributions. In COLT, 2013.
[10] M.-F. Balcan, A. Z. Broder, and T. Zhang. Margin based active learning. In COLT, 2007.
[11] M.-F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. J. Comput. Syst. Sci., 75(1):
78?89, 2009.
[12] Maria-Florina Balcan and Vitaly Feldman. Statistical active learning algorithms. In NIPS, pages 1295?1303,
2013.
[13] Maria-Florina Balcan and Hongyang Zhang. S-concave distributions: Towards broader distributions for
noise-tolerant and sample-ef?cient learning algorithms. arXiv preprint arXiv:1703.07758, 2017.
[14] Maria-Florina Balcan, Steve Hanneke, and Jennifer Wortman Vaughan. The true sample complexity of
active learning. Machine learning, 80(2-3):111?139, 2010.

9

[15] A. Beygelzimer, D. Hsu, J. Langford, and T. Zhang. Agnostic active learning without constraints. In NIPS,
2010.
[16] Alina Beygelzimer, Sanjoy Dasgupta, and John Langford. Importance weighted active learning. In
Twenty-Sixth International Conference on Machine Learning, 2009.
[17] Avrim Blum, Alan M. Frieze, Ravi Kannan, and Santosh Vempala. A polynomial-time algorithm for
learning noisy linear threshold functions. Algorithmica, 22(1/2):35?52, 1998.
[18] Nicolo Cesa-Bianchi and G?bor Lugosi. Prediction, learning, and games. Cambridge university press,
2006.
[19] Nicol? Cesa-Bianchi, Claudio Gentile, and erancesco Orabona. Robust bounds for classi?cation via
selective sampling. In Proceedings of the 26th Annual International Conference on Machine Learning,
ICML 2009, Montreal, Quebec, Canada, June 14-18, 2009, pages 121?128, 2009.
[20] Lin Chen, Hamed Hassani, and Amin Karbasi. Near-optimal active learning of halfspaces via query
synthesis in the noisy setting. In Thirty-First AAAI Conference on Arti?cial Intelligence, 2017.
[21] David A. Cohn, Les E. Atlas, and Richard E. Ladner. Improving generalization with active learning.
Machine Learning, 15(2):201?221, 1994.
[22] Nello Cristianini and John Shawe-Taylor. An introduction to support vector machines and other kernelbased learning methods. 2000.
[23] Amit Daniely. Complexity theoretic limitations on learning halfspaces. arXiv preprint arXiv:1505.05800,
2015.
[24] S. Dasgupta. Coarse sample complexity bounds for active learning. In NIPS, 2005.
[25] Sanjoy Dasgupta. Two faces of active learning. Theoretical computer science, 412(19):1767?1781, 2011.
[26] Sanjoy Dasgupta, Adam Tauman Kalai, and Claire Monteleoni. Analysis of perceptron-based active
learning. In Learning Theory, 18th Annual Conference on Learning Theory, COLT 2005, Bertinoro, Italy,
June 27-30, 2005, Proceedings, pages 249?263, 2005.
[27] Sanjoy Dasgupta, Daniel Hsu, and Claire Monteleoni. A general agnostic active learning algorithm. In
Advances in Neural Information Processing Systems 20, 2007.
[28] Ofer Dekel, Claudio Gentile, and Karthik Sridharan. Selective sampling and active learning from single
and multiple teachers. Journal of Machine Learning Research, 13(Sep):2655?2697, 2012.
[29] John Dunagan and Santosh Vempala. A simple polynomial-time rescaling algorithm for solving linear
programs. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages
315?320. ACM, 2004.
[30] Vitaly Feldman, Parikshit Gopalan, Subhash Khot, and Ashok Kumar Ponnuswami. New results for
learning noisy parities and halfspaces. In Foundations of Computer Science, 2006. FOCS?06. 47th Annual
IEEE Symposium on, pages 563?574. IEEE, 2006.
[31] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. Selective sampling using the query by committee
algorithm. Machine Learning, 28(2-3):133?168, 1997.
[32] Andrew Guillory, Erick Chastain, and Jeff Bilmes. Active learning as non-convex optimization. In
International Conference on Arti?cial Intelligence and Statistics, pages 201?208, 2009.
[33] Venkatesan Guruswami and Prasad Raghavendra. Hardness of learning halfspaces with noise. SIAM
Journal on Computing, 39(2):742?765, 2009.
[34] S. Hanneke. A bound on the label complexity of agnostic active learning. In ICML, 2007.
[35] S. Hanneke. Theoretical Foundations of Active Learning. PhD thesis, Carnegie Mellon University, 2009.
[36] Steve Hanneke. Rates of convergence in active learning. The Annals of Statistics, 39(1):333?361, 2011.
R in Machine
[37] Steve Hanneke. Theory of disagreement-based active learning. Foundations and Trends?
Learning, 7(2-3):131?309, 2014.

[38] Steve Hanneke and Liu Yang.
arXiv:1207.3772, 2012.

Surrogate losses in passive and active learning.

10

arXiv preprint

[39] Steve Hanneke, Varun Kanade, and Liu Yang. Learning with a drifting target concept. In International
Conference on Algorithmic Learning Theory, pages 149?164. Springer, 2015.
[40] D. Hsu. Algorithms for Active Learning. PhD thesis, UC San Diego, 2010.
[41] Tzu-Kuo Huang, Alekh Agarwal, Daniel Hsu, John Langford, and Robert E. Schapire. Ef?cient and
parsimonious agnostic active learning. CoRR, abs/1506.08669, 2015.
[42] Adam Tauman Kalai, Adam R Klivans, Yishay Mansour, and Rocco A Servedio. Agnostically learning
halfspaces. SIAM Journal on Computing, 37(6):1777?1805, 2008.
[43] Michael Kearns and Ming Li. Learning in the presence of malicious errors. SIAM Journal on Computing,
22(4):807?837, 1993.
[44] Adam Klivans and Pravesh Kothari. Embedding Hard Learning Problems Into Gaussian Space. In
APPROX/RANDOM 2014, pages 793?809, 2014.
[45] Adam R Klivans, Philip M Long, and Rocco A Servedio. Learning halfspaces with malicious noise.
Journal of Machine Learning Research, 10(Dec):2715?2740, 2009.
[46] V. Koltchinskii. Rademacher complexities and bounding the excess risk in active learning. JMLR, 2010.
[47] Sanjeev R Kulkarni, Sanjoy K Mitter, and John N Tsitsiklis. Active learning using arbitrary binary valued
queries. Machine Learning, 11(1):23?35, 1993.
[48] Philip M Long. On the sample complexity of pac learning half-spaces against the uniform distribution.
IEEE Transactions on Neural Networks, 6(6):1556?1559, 1995.
[49] Pascal Massart and ?lodie N?d?lec. Risk bounds for statistical learning. The Annals of Statistics, pages
2326?2366, 2006.
[50] Claire Monteleoni. Ef?cient algorithms for general active learning. In International Conference on
Computational Learning Theory, pages 650?652. Springer, 2006.
[51] TS Motzkin and IJ Schoenberg. The relaxation method for linear inequalities. Canadian Journal of
Mathematics, 6(3):393?404, 1954.
[52] Francesco Orabona and Nicolo Cesa-Bianchi. Better algorithms for selective sampling. In Proceedings of
the 28th international conference on Machine learning (ICML-11), pages 433?440, 2011.
[53] Maxim Raginsky and Alexander Rakhlin. Lower bounds for passive and active learning. In Advances in
Neural Information Processing Systems, pages 1026?1034, 2011.
[54] Burr Settles. Active learning literature survey. University of Wisconsin, Madison, 52(55-66):11, 2010.
[55] Simon Tong and Daphne Koller. Support vector machine active learning with applications to text classi?cation. Journal of machine learning research, 2(Nov):45?66, 2001.
[56] Christopher Tosh and Sanjoy Dasgupta. Diameter-based active learning. In ICML, pages 3444?3452, 2017.
[57] Vladimir N. Vapnik and Alexey Ya. Chervonenkis. On the uniform convergence of relative frequencies of
events to their probabilities. Theory of Probability and Its Applications, 16(2):264?280, 1971.
[58] Liwei Wang. Smoothness, disagreement coef?cient, and the label complexity of agnostic active learning.
Journal of Machine Learning Research, 12(Jul):2269?2292, 2011.
[59] Yining Wang and Aarti Singh. Noise-adaptive margin-based active learning and lower bounds under
tsybakov noise condition. In AAAI, 2016.
[60] Chicheng Zhang and Kamalika Chaudhuri. Beyond disagreement-based agnostic active learning. In
Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information
Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 442?450, 2014.
[61] Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient langevin
dynamics. In COLT, pages 1980?2022, 2017.

11

"
3087,2009,Evaluating multi-class learning strategies in a generative hierarchical framework for object detection,"Multiple object class learning and detection is a challenging problem due to the large number of object classes and their high visual variability. Specialized detectors usually excel in performance, while joint representations optimize sharing and reduce inference time --- but are complex to train. Conveniently, sequential learning of categories cuts down training time by transferring existing knowledge to novel classes, but cannot fully exploit the richness of shareability and might depend on ordering in learning. In hierarchical frameworks these issues have been little explored. In this paper, we show how different types of multi-class learning can be done within one generative hierarchical framework and provide a rigorous experimental analysis of various object class learning strategies as the number of classes grows. Specifically, we propose, evaluate and compare three important types of multi-class learning: 1.) independent training of individual categories, 2.) joint training of classes, 3.) sequential learning of classes. We explore and compare their computational behavior (space and time) and detection performance as a function of the number of learned classes on several recognition data sets.","Evaluating multi-class learning strategies in a
hierarchical framework for object detection

Sanja Fidler
Marko Boben
Ale?s Leonardis
Faculty of Computer and Information Science
University of Ljubljana, Slovenia
{sanja.fidler, marko.boben, ales.leonardis}@fri.uni-lj.si

Abstract
Multi-class object learning and detection is a challenging problem due to the
large number of object classes and their high visual variability. Specialized detectors usually excel in performance, while joint representations optimize sharing
and reduce inference time ? but are complex to train. Conveniently, sequential
class learning cuts down training time by transferring existing knowledge to novel
classes, but cannot fully exploit the shareability of features among object classes
and might depend on ordering of classes during learning. In hierarchical frameworks these issues have been little explored. In this paper, we provide a rigorous
experimental analysis of various multiple object class learning strategies within a
generative hierarchical framework. Specifically, we propose, evaluate and compare three important types of multi-class learning: 1.) independent training of
individual categories, 2.) joint training of classes, and 3.) sequential learning of
classes. We explore and compare their computational behavior (space and time)
and detection performance as a function of the number of learned object classes
on several recognition datasets. We show that sequential training achieves the best
trade-off between inference and training times at a comparable detection performance and could thus be used to learn the classes on a larger scale.

1

Introduction

Object class detection has been one of the mainstream research areas in computer vision. In recent
years we have seen a significant trend towards larger recognition datasets with an increasing number
of object classes [1]. This necessitates representing, learning and detecting multiple object classes,
which is a challenging problem due to the large number and the high visual variability of objects.
To learn and represent multiple object classes there have mainly been two strategies: the detectors
for each class have either been trained in isolation, or trained on all classes simultaneously. Both
exert certain advantages and disadvantages. Training independently allows us to apply complex
probabilistic models that use a significant amount of class specific features and allows us to tune
the parameters for each class separately. For object class detection, these approaches had notable
success [2]. However, representing multiple classes in this way, means stacking together specific
class representations. This, on the one hand, implies that each novel class can be added in constant
time, however, the representation grows clearly linearly with the number of classes and is thus also
linear in inference. On the other hand, joint representations enlarge sublinearly by virtue of sharing
the features among several object classes [3, 4]. This means sharing common computations and
increasing the speed of the joint detector. Training, however, is usually quadratic in the number of
classes. Furthermore, adding just one more class forces us to re-train the representation altogether.
Receiving somewhat less attention, the strategy to learn the classes sequentially (but not independently) potentially enjoys the traits of both learning types [4, 5, 6]. By learning one class after
1

another, we can transfer the knowledge acquired so far to novel classes and thus likely achieve both,
sublinearity in inference and cut down training time. In order to scale to a higher number of object
classes, learning them sequentially lends itself as the best choice.
In literature, the approaches have mainly used one of these three learning strategies in isolation.
To the best of our knowledge, little research has been done on analyzing and comparing them with
respect to one another. This is important because it allows us to point to losses and gains of each
particular learning setting, which could focus further research and improve the performance. This is
exactly what this paper is set to do ? we present a hierarchical framework within which all of the
aforementioned learning strategies can be unbiasedly evaluated and put into perspective.
Prominent work on these issues has been done in the domain of flat representations [4, 3], where
each class is modeled as an immediate aggregate of local features. However, there is an increasing
literature consensus, that hierarchies provide a more suitable form of multi-class representation [7,
8, 9, 10, 11, 12]. Hierarchies not only share complex object parts among similar classes, but can
re-use features at several levels of granularity also for dissimilar objects.
In this paper, we provide a rigorous experimental evaluation of several important multi-class learning
strategies for object detection within a generative hierarchical framework. We make use of the
hierarchical learning approach by [13]. Here we propose and evaluate three types of multi-class
learning: 1.) independent training of individual categories, 2.) joint training, 3.) sequential training
of classes. Several issues were evaluated on multiple object classes: 1.) growth of representation,
2.) training and 3.) inference time, 4.) degree of feature sharing and re-use at each level of the
hierarchy, 5.) influence of class ordering in sequential learning, and 6.) detection performance, all
as a function of the number of classes learned. We show that sequential training achieves the best
trade-off between inference and training times at a comparable detection performance and could
thus be used to learn the classes on a larger scale.
Related work. Prior work on multi-class learning in generative hierarchies either learns separate
hierarchies for each class [14, 15, 16, 10, 17], trains jointly [7, 18, 9, 19, 20, 11], whereas work
on sequential learning of classes has been particularly scarce [6, 13]. However, to the best of our
knowledge, no work has dealt with, evaluated and compared multiple important learning concepts
under one hierarchical framework.

2

The hierarchical model and inference

The hierarchical model. We use the hierarchical model of [13, 21], which we summarize here. Objects are represented with a recursive compositional shape vocabulary which is learned from images.
The vocabulary contains a set of shape models or compositions at each layer. Each shape model in
the hierarchy is modeled as a conjunction of a small number of parts (shapes from the previous
layer). Each part is spatially constrained on the parent shape model via a spatial relation which is
modeled with a two-dimensional Gaussian distribution. The number and the type of parts can differ
across the shape models and is learned from the data without supervision. At the lowest layer, the
vocabulary consists of a small number of short oriented contour fragments, while the vocabulary at
the top-most layer contains models that code the shapes of the whole objects. For training, we need
a positive and a validation set of class images, while the structure of the representation is learned in
an unsupervised way (no labels on object parts or smaller subparts need to be given).
The hierarchical vocabulary V = (V, E) is represented with a directed graph, where multiple edges
between two vertices are allowed. The vertices V of the graph represent the shape models and
the edges E represent the composition relations between them. The graph V has a hierarchical
structure, where the set of vertices V is partitioned into subsets V 1 , . . . , V O , each containing the
shapes at a particular layer. The vertices {vi1 }6i=1 at the lowest layer V 1 represent 6 oriented contour
fragments. The vertices at the top-most layer V O , referred to as the object layer represent the whole
shapes of the objects. Each object class C is assigned a subset of vertices VCO ? V O that code the
object layer shapes of that particular class. We denote the set of edges between the vertex layers
` `?1
V ` and V `?1 with E ` . Each edge e`Ri = vR
vi in E ` is associated with the Gaussian parameters
`
`
`
`
`
?Ri := ?(eRi ) = (?Ri , ?Ri ) of the spatial relation between the parent shape vR
and its part vi`?1 .
`
`
`
We will use ? R = (?Ri )i to denote the vector of all the parameters of a shape model vR
. The pair
`
`
`
V := (V , E ) will be referred to as the vocabulary at layer `.
2

Inference. We infer object class instances in a query image I in the following way. We follow the
contour extraction of [13], which finds local maxima in oriented Gabor energy. This gives us the
contour fragments F and their positions X. In the process of inference we build a (directed acyclic)
inference graph G = (Z, Q). The vertices Z are partitioned into vertex layers 1 to O (object layer),
Z = Z 1 ?? ? ??Z O , and similarly also the edges, Q = Q1 ?? ? ??QO . Each vertex z ` = (v ` , x` ) ? Z `
represents a hypothesis that a particular shape v ` ? V ` from the vocabulary is present at location x` .
`
The edges in Q` connect each parent hypothesis zR
to all of its part hypotheses zi`?1 . The edges in
1
the bottom layer Q connect the hypotheses in the first layer Z 1 with the observations. With S(z)
we denote the subgraph of G that contains the vertices and edges of all descendants of vertex z.
Since our definition of each vocabulary shape model assumes that its parts are conditionally independent, we can calculate the likelihood of the part hypotheses zi`?1 = (vi`?1 , x`?1
) under a parent
i
`
`
hypothesis zR
= (vR
, x`R ) by taking a product over the individual likelihoods of the parts:
Y
`
`
`
p(v`?1 , x`?1 | vR
, x`R , ? `R ) =
p(x`?1
| x`R , vi`?1 , vR
, ?Ri
)
(1)
i
` v `?1
e`Ri =vR
i

`
`
The term pRi := p(x`?1
| x`R , vi`?1 , vR
, ?Ri
) stands for the spatial constraint imposed by a vocabi
`
`
ulary edge eRi between a parent hypothesis zR
and its part hypothesis zi`?1 . It is modeled by a
`?1
`
`
`
= (?`Ri , ?`Ri ). If the likelihood in (1) is
normal distribution, pRi = N (xi ? xR | ?Ri ), where ?Ri
`
above a threshold, we add edges between zR and its most likely part hypotheses. The log-likelihood
`
`
of the observations under a hypothesis zR
is then calculated recursively over the subgraph S(zR
):
X
X
1:`?1
`
1
log p(F, X, z
| zR ; V) =
log pR0 i0 +
log p(F, X | zi0 ),
(2)
` ))
zR0 zi0 ?E(S(zR

` ))
zi10 ?V (S(zR

`
`
`
), respectively.
)) denote the edges and vertices of the subgraph S(zR
)) and V (S(zR
where E(S(zR
The last term is the likelihood of the Gabor features under a particular contour fragment hypothesis.

3

Multi-class learning strategies

We first define the objective function for multi-class learning and show how different learning strategies can be used with it in the following subsections. Our goal is to find a hierarchical vocabulary
V that well represents the distribution p(I | C) ? p(F, X | C; V) at minimal complexity of the
representation (C denotes the class variable). Specifically, we seek for a vocabulary V = ?` V ` that
optimizes the function f over the data D = {(Fn , Xn , Cn )}N
n=1 (N training images):
?
V = arg max f (V), where f (V) = L(D | V) ? ? ? T (V)
(3)
V

The first term in (3) represents the log-likelihood:
N
N
X
X
X
L(D | V) =
log p(Fn , Xn | C; V) =
log
p(Fn , Xn , z | C; V),
n=1

n=1

(4)

z

while T (V) penalizes the complexity of the model [21] and ? controls the amount of penalization.
Several approximations are made to learn the vocabulary; namely, the vocabulary is learned layer by
layer (in a bottom-up way) by finding frequent spatial layouts of parts from the previous layer [13]
and then using f to select a minimal set of models at each layer that still produce a good wholeobject shape representation at the final, object layer [21]. The top layer models are validated on a
set of validation images and those yielding a high rate of false-positives are removed from V.
We next show how different training strategies are performed to learn a joint multi-class vocabulary.
3.1

Independent training of individual classes

In independent training, a class specific vocabulary Vc is learned using the training images of each
particular class C = c. We learn Vc by maximizing f over the data D = {(Fn , Xn , C = c)}. For
the negative images in the validation step, we randomly sample images from other classes. The joint
multi-class representation V is then obtained by stacking the class specific vocabularies Vc together,
V ` = ?c Vc` (the edges E are added accordingly). Note that Vc1 is the only layer common to all
classes (6 oriented contour fragments), thus V 1 = Vc1 .
3

3.2

Joint training of classes

In joint training, the learning phase has two steps. In the first step, the training data D for all
the classes is presented to the algorithm simultaneously, and is treated as unlabeled. The spatial
parameters ? of the models at each layer are then inferred from images of all classes, and will code
?average? spatial part dispositions. The joint statistics also influences the structure of the models by
preferring those that are most repeatable over the classes. This way, the jointly learned vocabulary
V will be the best trade-off between the likelihood L and the complexity T over all the classes in the
dataset. However, the final, top-level likelihood for each particular class could be low because the
more discriminative class-specific information has been lost. Thus, we employ a second step which
`
revisits each class separately. Here, we use the joint vocabulary V and add new models vR
to each
layer ` if they further increase the score f for each particular class. This procedure is similar to that
used in sequential training and will be explained in more detail in the following subsection. Object
layer V O is consequently learned and added to V for each class. We validate the object models after
all classes have been trained. A similarity measure is used to compare every two classes based on the
degree of feature sharing between them. In validation, we choose the negative images by sampling
the images of the classes according to the distribution defined by the similarity measure. This way,
we discard the models that poorly discriminate between the similar classes.
3.3

Sequential training of classes

When training the classes sequentially, we train on each class separately, however, our aim is to
1.) maximize the re-use of compositions learned for the previous classes, and 2.) add those missing
(class-specific) compositions that are needed to represent class k sufficiently well. Let V1:k?1 denote
the vocabulary learned for classes 1 to k ? 1. To learn a novel class k, for each layer ` we seek a
new set of shape models that maximizes f over the data D = {(Fn , Xn , C = k)} conditionally on
`
the already learned vocabulary V1:k?1
. This is done by treating the hypotheses inferred with respect
`
`
to V1:k?1 as fixed, which gives us a starting value of the score function f . Each new model vR
`
is then evaluated and selected conditionally on this value, i.e such that the difference f (V1:k?1 ?
`
`
vR
) ? f (V1:k?1
) is maximized. Since according to the definition in (4) the likelihood L increases
the most when the hypotheses have largely disjoint supports, we can greatly speed up the learning
process: the models need to be learned only with respect to those (F, X) in an image that have a
`
low likelihood under the vocabulary V1:k?1
, which can be determined prior to training.

4

Experimental results

We have evaluated the hierarchical multi-class learning strategies on several object classes. Specifically, we used: UIUC multi-scale cars [22], GRAZ [4] cows and persons, Weizmann multi-scale
horses (adapted by Shotton et al. [23]), all five classes from the ETH dataset [24], and all ten classes
from TUD shape2 [25]. Basic information is given in Table 1. A 6-layer vocabulary was learned. 1
The bounding box information was used during training.
When evaluating detection performance, a detection will be counted as correct, if the predicted
bounding box coincides with groundtruth more than 50%. On the ETH dataset alone, this threshold
is lowered to 0.3 to enable a fair comparison with the related work [24]. The performance will be
given either with recall at equal error rate (EER), positive detection rate at low FPPI, or as classif.by-detection (on TUD shape2), depending on the type of results reported on that dataset thus-far.
To evaluate the shareability of compositions between the classes, we will use the following measure:
`
1 X (# of classes that use vR
)?1
deg share(`) = `
,
|V | ` `
# of all classes ? 1
vR ?V

`
defined for each layer ` separately. By ?vR
used by class C? it is meant that there is a path of edges
`
connecting any of the class specific shapes VCO and vR
. To give some intuition behind the measure:
1
The number of layers depends on the objects? size in the training images (it is logarithmic with the number
of non-overlapping contour fragments in an image). To enable a consistent evaluation of feature sharing, etc,
we have scaled the training images in a way which produced the whole-shape models at layer 6 for each class.

4

deg share = 0 if no shape from layer ` is shared (each class uses its own set of shapes), and it is 1
if each shape is used by all the classes. Beside the mean (which defines deg share), the plots will
also show the standard deviation. In sequential training, we can additionally evaluate the degree of
re-use when learning each novel class. Higher re-use means lower training time and a more compact
representation. We expect a tendency of higher re-use as the number k of classes grows, thus we
define it with respect to the number of learned classes:
`
`
# of vR
? V1:k?1
used by ck
deg transfer(k, `) =
(5)
` ? V ` used by c
# of all vR
k
1:k
Evaluation was performed by progressively increasing the number of object classes (from 2 to 10).
The individual training will be denoted by I, joint by J, and sequential by S.
Table 2 relates the detection performances of I to those of the related work. On the left side, we
report detection accuracy at low FPPI rate for the ETH dataset, averaged over 5 random splits of
training/test images as in [24]. On the right side, recall at EER is given for a number of classes.
Two classes. We performed evaluation on two visually very similar classes (cow, horse), and two
dissimilar classes (person, car). Table 3 gives information on 1.) size (the number of compositions
at each layer), 2.) training and 3.) inference times, 4.) recall at EER. In sequential training, both
possible orders were used (denoted with S1 and S2) to see whether different learning orders (of
classes) affect the performance. The first two rows show the results for each class individually,
while the last row contains information with respect to the conjoined representations. Already for
two classes, the cumulative training time is slightly lower for S than I, while both being much
smaller than that of J.
Degree of sharing. The hierarchies learned in I, J, and S on cows and horses, and J for car-person
are shown in Fig. 2 in a respective order from left to right. The red nodes depict cow/car and blue
horse/person compositions. The green nodes depict the shared compositions. We can observe a
slightly lower number of shareable nodes for S compared to J, yet still the lower layers for cowhorse are almost completely re-used. Even for the visually dissimilar classes (car-person) sharing is
present at lower layers. Numerically, the degrees of sharing and transfer are plotted in Fig. 1.
Detection rate. The recall values for each class are reported in Table 3. Interestingly, ?knowing?
horses improved the performance for cows. For car-person, individual training produced the best
result, while training person before car turned out to be a better strategy for S. Fig. 1 shows the
detection rates for cows and horses on the joint test set (the strongest class hypothesis is evaluated),
which allows for a much higher false-positive rate. We evaluate it with F-measure (to account for
FP). A higher performance for all joint representations over the independent one can be observed.
This is due to the high degree of sharing in J and S, which puts similar hypotheses in perspective
and thus discriminates between them better.
Five classes. The results for ETH-5 are reported in Table 4. We used half of the images for training,
and the other half for testing. The split was random, but the same for I, J, and S. We also test
whether different orders in S affect performance (we report an average over 3 random S runs).
Ordering does slightly affect performance, which means we may try to find an optimal order of
classes in training. We can also observe that the number of compositions at each layer is higher for
S as for J (both being much smaller than I), but this only slightly showed in inference times.
Ten classes. The results on TUD-10 are presented in Table 5. A few examples of the learned shapes
for S are shown in Fig. 3. Due to the high training complexity of J, we have only ran J for 2, 5 and
10 classes. We report classif.-by-detection (the strongest class hypothesis in an image must overlap
with groundtruth more than 50%). To demonstrate the strength of our representation, we have also
ran (linear) SVM on top of hypotheses from Layers 1 ? 3, and compared the performances. Already
here, Layer 3 + SVM outperforms prior work [25] by 10%. Fig. 4-(11.) shows classification as a
number of learned classes. Our approach consistently outperforms SVM, which is likely due to the
high scale- and rotation- variability of images with which our approach copes well. Fig. 4 shows:
inference time, cumulative training time, degree of sharing (for the final 10-class repr.), transfer, and
classification rates as a function of the number of learned classes.
Vocabulary size. The top row in Fig 4 shows representation size for I, J and S as a function
of learned classes. With respect to worst case (I), both J and S have a highly sublinear growth.
Moreover, in layers 2 and 3, where the burden on inference is the highest (the highest number of
5

inferred hypotheses), an almost constant tendency can be seen. We also compare the curves with
those reported for a flat approach by Opelt et al. [4] in Fig 4-(5). We plot the number of models at
Layer 5 which are approximately of the same granularity as the learned boundary parts in [4]. Both,
J and S hierarchical learning types show a significantly better logarithmic tendency as in [4].
Fig 4-(6) shows the size of the hierarchy file stored on disk. It is worth emphasizing that the hierarchy
subsuming 10 classes uses only 0.5Mb on disk and could fit on an average mobile device.
50 classes. To increase the scale of the experiments we show the performance of sequential training
on 50 classes from LabelMe [1]. The results are presented in Fig. 5. For I in the inference time plot
we used the inference time for the first class linearly extrapolated with the number of classes. We
can observe that S achieves much lower inference times than I, although it is clear that for a higher
number of classes more research is needed to cut down the inference times to a practical value.
Detection rate: JOINT cow+horse dataset

Degree of sharing / re?use: cow?horse

Degree of sharing / re?use: car?person

degree of sharing

F?measure

0.95
0.9
0.85
0.8
0.75
0.7

I

J

S1

learning type

S2

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

degree of sharing

1

Joint training
Sequential 1: cow + horse
Sequential 2: horse + car
1

2

3

4

5

layer

Joint training
Sequential 1: car + person
Sequential 2: person + car

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

1

2

3

4

5

layer

Figure 1: From left to right: 1.) detection rate (F measure) on the joint cow-horse test set. 2.) degree of
sharing for cow-horse, 3.) car-person vocabularies, 4.) an example detection of a person and horse.

`
, the
Figure 2: Learned 2-class vocabularies for different learning types (the nodes depict the compositions vR

links represent the edges e`Ri between them ? the parameters ? ` are not shown). From left to right: cow-horse
hierarchy for 1.) I, 2.) J, 3.) S1, and 4.) car-person J. Green nodes denoted shared compositions.

5

Conclusions and discussion

We evaluated three types of multi-class learning strategies in a hierarchical compositional framework, namely 1.) independent, 2). joint, and 3.) sequential training. A comparison was made
through several important computational aspects as well as by detection performance. We conclude
that: 1.) Both joint and sequential training strategies exert sublinear growth in vocabulary size (more
evidently so in the lower layers) and, consequently, sublinear inference time. This is due to a high
degree of sharing and transfer within the resulting vocabularies. The hierarchy obtained by sequential training grows somewhat faster, but not significantly so. 2.) Training time was expectedly worst
for joint training, while training time even reduced with each additional class during sequential
training. 3.) Different training orders of classes did perform somewhat differently ? this means we
might try to find an ?optimal? order of learning. 4.) Training independently has mostly yielded the
best detection rates, but the discrepancy with the other two strategies was low. For similar classes
(cow-horse), sequential learning even improved the detection performance, and was in most cases
above the joint?s performance. By training sequentially, we can learn class specific features (yet
still have a high degree of sharing) which boost performance. Most importantly, sequential training
has achieved the best trade-off between detection performance, re-usability, inference and training
time. The observed computational properties of all the strategies in general, and sequential learning
in particular, go well beyond the reported behavior of flat approaches [4]. This makes sequential
learning of compositional hierarchies suitable for representing the classes on a larger scale.
Acknowledgments
This research has been supported in part by the following funds: EU FP7-215843 project POETICON, EU
FP7-215181 project CogX, Research program Computer Vision P2-0214 and Project J2-2221 (ARRS).

6

layer 1 layer 2

layer 3

layer 4

layer 5

layer 6
hammer
pliers

saucepan
scissors

Figure 3: A few examples from the learned hierarchical shape vocabulary for S on TUD-10. Each shape in
the hierarchy is a composition of shapes from the layer below. Only the mean of each shape is shown.
method
size on disk
classf. rate
Stark et al.[25]
/
44%
Level 1 + SVM
206 Kb
32%
Level 2 + SVM
3, 913 Kb
44%
size of representation
train. time
infer. time
Level 3 + SVM
34, 508 Kb
54%
L2
L3
L4
L5
Independent
1, 249 Kb
71%
207 min
12.2 sec
74
96
159
181
Joint
408 Kb
69%
752 min
2.0 sec
14
23
39
59
Sequential
490 Kb
71%
151 min
2.4 sec
9
21
49
76
Table 5: Results on the TUD-10. Classification obtained as classification-by-detection.

40
30
20
10
2

3

4

5

6

7

8

9

60
40
20
0
1

10

2

3

number of classes
12

inference time (sec)

size of representation

400
300
200
100
0
1

2

3

4

5

6

7

5

6

7

8

9

0
1

10

2

3

8

4

9

8
6
4
2
0

10

2

number of classes

4

7

8

9

50

0
1

10

2

3

6

8

1200

independent
joint
sequential
Layer 2 + SVM
Layer 3 + SVM

1000
800
600
400
200
0

10

2

4

number of classes

4

5

6

7

8

9

10

number of classes

Cumulative training time

1400

6

8

200

150

100

50

independent
sequential
0

10

2

4

6

8

10

number of classes

number of classes

Degree of transfer per layer

Degree of sharing: TUD

6

independent
joint
sequential

100

Size on disk

independent
joint
sequential

10

5

150

number of classes

Inference time per image

Opelt et al.
independent
joint
sequential

500

4

50

number of classes

Growth of representation

Layer 5

independent
joint
sequential

100

cumulative size on disk (Kb)

0
1

independent
joint
sequential

80

Growth of representation

Layer 4

150

size of representation

independent
joint
sequential

50

Growth of representation

Layer 3
size of representation

size of representation

60

size of representation

Growth of representation
100

Layer 2

cumulative training time (min)

Growth of representation
70

Classification rate

Joint training
Sequential: alphabetical order
1

2

3

4

5

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Layer 1
Layer 2
Layer 3
Layer 4
Layer 5
1

2

3

4

5

6

7

8

9

Classification rate (%)

degree of transfer

degree of sharing

100

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

90
80
70
60
50
40

10

sequential
SVM on Layer 2
SVM on Layer 3
2

number of learnt classes

layer

4

6

8

number of classes

Figure 4: Results on TUD-10. Top: (1-4) repr. size as a function of the number of learned classes. Middle:
5.) repr. size compared to [4], 6.) size of hierarchy on disk, 7.) avg. inference time per image, 8.) cumulative
train. time. Bottom: degree of 9.) sharing and 10.) transfer, 11.) classif. rates, 12.) example detection of cup.
inference time per image (sec)

number of compositions

250

Layer 1
Layer 2
Layer 3
Layer 4
Layer 5

200
150
100
50
0

10

20

30

number of classes

40

Inference time per image (# classes)

50

350
300

0.9

250
200
150
100
50
0

Degree of transfer per layer
1

independent
sequential

degree of transfer

Size of representation (# classes)
300

0.8
0.7
0.6

Layer 1
Layer 2
Layer 3
Layer 4
Layer 5

0.5
0.4
0.3
0.2
0.1

10

20

30

number of classes

40

50

0

5

10

15

20

25

30

35

40

45

50

number of learnt classes

Figure 5: Results on 50 object classes from LabelMe [1]. From left to right: Size of representation (number of
compositions per layer), inference times, and deg transfer, all as a function of the number of learned classes.

7

8

apple
19
21

ETH shape
bottle giraffe mug
21
30
24
27
57
24
swan
15
17

cup
20
10

fork
20
10

TUD shape1 (train) + TUD shape2 (test)
hammer knife mug pan pliers pot saucepan
20
20
20
20
20
20
20
10
10
10
10
10
10
10

Table 1: Dataset information.
scissors
20
10

cow
20
65

Graz
person
19
19

UIUC
car
40
108

Weizm.
horse
20
228

L2
17
12
29
6
9
15

bottle
83.2(7.5)
76.8(6.1)
86.2(2.8)
0.36 FPPI

mug
83.6(8.6)
82.7(5.1)
84.6(2.3)
0.27 FPPI

swan
75.4(13.4)
84.0(8.4)
78.2(5.4)
0.26 FPPI

average
76.8
84.8
83.7

L3 L4 L5
30 27 28
11 22 22
13 22 37
16 25 23
18 29 26
88 125 136

I
L2
14
13
15
16
16
16

ind. train.

Related work

cow
100. [4]
100. [7]
98.5 [23]
98.5

25
20
45
35
17
52

I
/
/
65
/
/
85

J
25
17
42
35
15
50

S1

19
20
39
33
17
50

S2

1.9
2.3
4.3
3.4
2.3
6.3

I

2.0
2.6
2.5
5.2
2.6
4.8

J

S2

I

S
86.4
80.0
84.6
83.3
72.7
81.4

86.4
80.0
81.3
83.3
69.7
80.1

J

FPPI

96.9
93.4
95.6
93.5
56.3
74.9

S1

S

98.5
94.3
96.4
92.4
60.4
76.4

0.34 0.27 0.28
0.4 0.34 0.32
0.19 0.16 0.18
0.31 0.22 0.22
0.28 0.22 0.21
0.30 0.24 0.24

I

96.9
93.4
95.6
91.7
58.3
75.0

J

S2

person
52.6 [4]
52.4 [23]
50.0 [28]
60.4

rec. at EER (%)

car scale
90.6 [27]
93.5 [29]
92.1 [13]
93.5

J

rate (%)

2.0
2.7
2.5
5.4
3.0
5.0

size of representation
trn.time(min)
infr.time(sec)
det.
J
S, mean (std) - over 3 runs
I
J
S
I
J
S
I
L3 L4 L5
L2
L3
L4
L5
15 21 28 10(0.6) 25(1.7) 27(12.7) 23(7.5) 23
/
23 3.6 11.1 12.1 88.6
16 21 22 9(0.6) 22(8.1)
28(2)
22(3.6) 25
/
21 3.4 11.1 12.1 85.5
19 26 26 10(0.6) 28(5.9) 35(1.7) 30(1.2) 31
/
26 3.2 11.1 12.1 82.4
19 25 34 10(0.6) 25(7.9) 30(4.7) 29(4.9) 31
/
18 3.6 11.1 12.1 84.9
20 26 27 10(0) 23(6.4) 30(4.0) 27(1.5) 17
/
12 2.8 11.1 12.1 75.8
22 32 55 11(0.6) 33(2.5) 61(9.5) 79(13.7) 127 235 100 16.6 11.1 12.1 83.4

2.1
2.7
2.6
5.3
2.8
4.9

S1

96.9
94.3
95.6
93.5
60.4
77.0

horse
89.0 [23]
93.0 [28]
/
94.3

infer. time (sec)

Table 4: Results for different learning types on the 5?class ETH shape dataset.

L5
20
24
38
18
21
38

train. time (min)

Table 3: Results for different learning types on the cow-horse, and car-person classes.

giraffe
58.6(14.6)
90.5(5.4)
83.3(4.3)
0.21 FPPI

size of representation (number of compositions per layer)
I
J
S1 (1 + 2)
S2 (2 + 1)
L3 L4 L5 L2 L3 L4 L5 L2 L3 L4 L5 L2 L3 L4
17 23 25 17 25 27 25 17 17 23 25 14 17 20
12 18 24 17 26 26 27 18 18 24 21 12 12 18
29 41 49 17 26 30 36 18 21 33 40 14 19 29
10 13 16 7 16 20 20 6 10 13 16 11 19 18
16 19 21 7 12 14 22 11 12 15 23 9 16 19
26 32 37 7 18 25 42 12 19 27 39 11 25 31

applelogo
83.2(1.7)
89.9(4.5)
87.3(2.6)
0.32 FPPI

L2
applelogo 11
bottle
7
giraffe
5
mug
9
swan
11
all
43

class

cow (1)
horse (2)
cow+hrs.
car (1)
person (2)
car+prsn.

class

[24]
[26]
ind. train.

Table 2: Comparison of detection rates with related work. Left: Average detection-rate (in %) at 0.4 FPPI for the related work, while we also report the actual
FPPI, for the ETH shape database. Right: Recall at EER for various classes. The approaches that use more than just contour information (and are thus not directly
comparable to ours) are shaded gray.

dataset
class
# train
# test

References
[1] Russell, B., Torralba, A., Murphy, K., and Freeman, W. T. (2008) Labelme: a database and web-based
tool for image annotation. IJCV, 77, 157?173.
[2] Leibe, B., Leonardis, A., and Schiele, B. (2008) Robust object detection with interleaved categorization
and segmentation. IJCV, 77, 259?289.
[3] Torralba, A., Murphy, K. P., and Freeman, W. T. (2007) Sharing visual features for multiclass and multiview object detection. IEEE PAMI, 29, 854?869.
[4] Opelt, A., Pinz, A., and Zisserman, A. (2008) Learning an alphabet of shape and appearance for multiclass object detection. IJCV, 80, 16?44.
[5] Fei-Fei, L., Fergus, R., and Perona, P. (2004) Learning generative visual models from few training examples: an incremental bayesian approach tested on 101 object categories. IEEE CVPR?04 Workshop on
Generative-Model Based Vision.
[6] Krempp, S., Geman, D., and Amit, Y. (2002) Sequential learning of reusable parts for object detection.
Tech. rep.
[7] Todorovic, S. and Ahuja, N. (2007) Unsupervised category modeling, recognition, and segmentation in
images. IEEE PAMI.
[8] Zhu, S. and Mumford, D. (2006) A stochastic grammar of images. Found. and Trends in Comp. Graphics
and Vision, 2, 259?362.
[9] Ranzato, M. A., Huang, F.-J., Boureau, Y.-L., and LeCun, Y. (2007) Unsupervised learning of invariant
feature hierarchies with applications to object recognition. CVPR.
[10] Ullman, S. and Epshtein, B. (2006) Visual Classification by a Hierarchy of Extended Features.. Towards
Category-Level Object Recognition, Springer-Verlag.
[11] Sivic, J., Russell, B. C., Zisserman, A., Freeman, W. T., and Efros, A. A. (2008) Unsupervised discovery
of visual object class hierarchies. CVPR.
[12] Bart, I., Porteous, E., Perona, P., and Wellings, M. (2008) Unsupervised learning of visual taxonomies.
CVPR.
[13] Fidler, S. and Leonardis, A. (2007) Towards scalable representations of visual categories: Learning a
hierarchy of parts. CVPR.
[14] Scalzo, F. and Piater, J. H. (2005) Statistical learning of visual feature hierarchies. W. on Learning, CVPR.
[15] Zhu, L., Lin, C., Huang, H., Chen, Y., and Yuille, A. (2008) Unsupervised structure learning: Hierarchical
recursive composition, suspicious coincidence and competitive exclusion. ECCV, vol. 2, pp. 759?773.
[16] Fleuret, F. and Geman, D. (2001) Coarse-to-fine face detection. IJCV, 41, 85?107.
[17] Schwartz, J. and Felzenszwalb, P. (2007) Hierarchical matching of deformable shapes. CVPR.
[18] Ommer, B. and Buhmann, J. M. (2007) Learning the compositional nature of visual objects. CVPR.
[19] Serre, T., Wolf, L., Bileschi, S., Riesenhuber, M., and Poggio, T. (2007) Object recognition with cortexlike mechanisms. IEEE PAMI, 29, 411?426.
[20] Sudderth, E., Torralba, A., Freeman, W. T., and Willsky, A. (2008) Describing visual scenes using transformed objects and parts. IJCV, pp. 291?330.
[21] Fidler, S., Boben, M., and Leonardis, A. (2009) Optimization framework for learning a hierarchical shape
vocabulary for object class detection. BMVC.
[22] Agarwal, S., Awan, A., and Roth, D. (2004) Learning to detect objects in images via a sparse, part-based
representation. IEEE PAMI, 26, 1475?1490.
[23] Shotton, J., Blake, A., and Cipolla, R. (2008) Multi-scale categorical object recognition using contour
fragments. PAMI, 30, 1270?1281.
[24] Ferrari, V., Fevrier, L., Jurie, F., and Schmid, C. (2007) Accurate object detection with deformable shape
models learnt from images. CVPR.
[25] Stark, M. and Schiele, B. (2007) How good are local features for classes of geometric objects? ICCV.
[26] Fritz, M. and Schiele, B. (2008) Decomposition, discovery and detection of visual categories using topic
models. CVPR.
[27] Mutch, J. and Lowe, D. G. (2006) Multiclass object recognition with sparse, localized features. CVPR,
pp. 11?18.
[28] Shotton, J., Blake, A., and Cipolla, R. (2008) Efficiently combining contour and texture cues for object
recognition. BMVC.
[29] Ahuja, N. and Todorovic, S. (2008) Connected segmentation tree ? a joint representation of region layout
and hierarchy. CVPR.

9

"
1954,2005,Data-Driven Online to Batch Conversions,Abstract Missing,"Data-Driven Online to Batch Conversions
Ofer Dekel and Yoram Singer
School of Computer Science and Engineering
The Hebrew University, Jerusalem 91904, Israel
{oferd,singer}@cs.huji.ac.il

Abstract
Online learning algorithms are typically fast, memory efficient, and simple to implement. However, many common learning problems fit more
naturally in the batch learning setting. The power of online learning
algorithms can be exploited in batch settings by using online-to-batch
conversions techniques which build a new batch algorithm from an existing online algorithm. We first give a unified overview of three existing online-to-batch conversion techniques which do not use training data
in the conversion process. We then build upon these data-independent
conversions to derive and analyze data-driven conversions. Our conversions find hypotheses with a small risk by explicitly minimizing datadependent generalization bounds. We experimentally demonstrate the
usefulness of our approach and in particular show that the data-driven
conversions consistently outperform the data-independent conversions.

1 Introduction
Batch learning is probably the most common supervised machine-learning setting. In the
batch setting, instances are drawn from a domain X and are associated with target values
from a target set Y. The learning algorithm is given a training set of examples, where each
example is an instance-target pair, and attempts to identify an underlying rule that can be
used to predict the target values of new unseen examples. In other words, we would like
the algorithm to generalize from the training set to the entire domain of examples. The
target space Y can be either discrete, as in the case of classification, or continuous, as in
the case of regression. Concretely, the learning algorithm is confined to a predetermined
set of candidate hypotheses H, where each hypothesis h ? H is a mapping from X to
Y, and the algorithm must select a ?good? hypothesis from H. The quality of different
hypotheses in H is evaluated with respect to a loss function ?, where ?(y, y ? ) is interpreted
as the penalty for predicting the target value y ? when the correct target is y. Therefore,
?(y, h(x)) indicates how well hypothesis h performs with respect to the example (x, y).
When Y is a discrete set, we often use the 0-1 loss, defined by ?(y, y ? ) = 1y6=y? . We also
assume that there exists a probability distribution D over the product space X ? Y, and
that the training set was sampled i.i.d. from this distribution. Moreover, the existence of D
enables us to reason about the average performance of an hypothesis over its entire domain.
Formally, the risk of an hypothesis h is defined to be,
RiskD (h) = E(x,y)?D [?(y, h(x))] .

(1)

The goal of a batch learning algorithm is to use the training set to find a hypothesis that
does well on average, or more formally, to find h ? H with a small risk.
In contrast to the batch learning setting, online learning takes place in a sequence of rounds.
On any given round, t, the learning algorithm receives a single instance xt ? X and predicts
its target value using an hypothesis ht?1 , which was generated on the previous round. On
the first round, the algorithm uses a default hypothesis h0 . Immediately after the prediction
is made, the correct target value yt is revealed and the algorithm suffers an instantaneous
loss of ?(yt , ht?1 (xt )). Finally, the online algorithm may use the newly obtained example
(xt , yt ) to improve its prediction strategy, namely to replace ht?1 with a new hypothesis
ht . Alternatively, the algorithm may choose to stick with its current hypothesis and sets
ht = ht?1 . An online algorithm is therefore defined by its default hypothesis h0 and the
update rule it uses to define new hypotheses. The cumulative loss suffered on a sequence
of rounds is the sum of instantaneous losses suffered on each one of the rounds in the
sequence. In the online setting there is typically no need for any statistical assumptions
since there is no notion of generalization. The goal of the online algorithm is simply to
suffer a small cumulative loss on the sequence of examples it is given, and examples that
are not in this sequence are entirely irrelevant.
Throughout this paper, we assume that we have access to a good online learning algorithm
A for the task on hand. Moreover, A is computationally efficient and easy to implement.
However, the learning problem we face fits much more naturally within the batch learning
setting. We would like to develop a batch algorithm B that exhibits the desirable characteristics of A but also has good generalization properties. A simple and powerful way to
achieve this is to use an online-to-batch conversion technique. This is a general name for
any technique which uses A as a building block in the construction of B. Several different online-to-batch conversion techniques have been developed over the years. Littlestone
and Warmuth [11] introduced an explicit relation between compression and learnability,
which immediately lent itself to a conversion technique for classification algorithms. Gallant [7] presented the Pocket algorithm, a conversion of Rosenblatt?s online Perceptron to
the batch setting. Littlestone [10] presented the Cross-Validation conversion which was
further developed by Cesa-Bianchi, Conconi and Gentile [2]. All of these techniques begin
by presenting the training set (x1 , y1 ), . . . , (xm , ym ) to A in some arbitrary order. As A
performs the m online rounds, it generates a sequence of online hypotheses which it uses to
make predictions on each round. This sequence includes the default hypothesis h0 and the
m hypotheses h1 , . . . , hm generated by the update rule. The aforementioned techniques all
share a common property: they all choose h, the output of the batch algorithm B, to be one
of the online hypotheses h0 , . . . , hm .
In this paper, we focus on a second family of conversions, which evolved somewhat later
and is due to the work of Helmbold and Warmuth [8], Freund and Schapire [6] and CesaBianchi, Conconi and Gentile [2]. The conversion strategies in this family also begin by
using A to generate the sequence of online hypotheses. However, instead of relying on
a single hypothesis from the sequence, they set h to be some combination of the entire
sequence. Another characteristic shared by these three conversions is that the training data
does not play a part in determining how the online hypotheses are combined. That is, the
training data is not used in any way other than to generate the sequence h0 , . . . , hm . In
this sense, these conversion techniques are data-independent. In this paper, we build on the
foundations of these data-independent conversions, and define conversion techniques that
explicitly use the training data to derive the batch algorithm from the online algorithm. By
doing so, we effectively define the data-driven counterparts of the algorithms in [8, 6, 2].
This paper is organized as follows. In Sec. 2 we review the data-independent conversion
techniques from [8, 6, 2] and give a simple unified analysis for all three conversions. At the
same time, we present a general framework which serves as a building-block for our datadriven conversions. Then, in Sec. 3, we derive three special cases of the general framework

and demonstrate some useful properties of the data-driven conversions. Finally, in Sec. 4,
we compare the different conversion techniques on several benchmark datasets and show
that our data-driven approach outperforms the existing data-independent approach.

2 Voting, Averaging, and Sampling
The first conversion we discuss is the voting conversion [6], which applies to problems
where the target space Y is discrete (and relatively small), such as classification problems.
The conversion presents the training set (x1 , y1 ), . . . , (xm , ym ) to the online algorithm A,
which generates the sequence of online hypotheses, h0 , . . . , hm . The conversion then outputs the hypothesis hV , which is defined as follows: given an input x ? X , each online
hypothesis casts a vote of hi (x) and then hV outputs the target value that receives the highest number of votes. For simplicity, assume that ties are broken arbitrarily. The second
conversion is the averaging conversion [2] which applies to problems where Y is a convex
set. For example, this conversion is applicable to margin-based online classifiers or to regression problems where, in both cases, Y = R. This conversion also begins
by using A to
1 Pm
generate h0 , . . . , hm . Then the batch hypothesis hA is defined to be m+1
i=0 hi (x). The
third and last conversion discussed here is the sampling conversion [8]. This conversion is
the most general and applicable to any learning problem, however this generality comes at
a price. The resulting hypothesis, hS , is a stochastic function and not a deterministic one.
In other words, if applied twice to the same instance, hS may output different target values.
Again, this conversion begins by applying A to the training set and obtaining the sequence
of online hypotheses. Every time hS is evaluated, it randomly selects one of h0 , . . . , hm and
uses it to make the prediction. Since hS is a stochastic function, the definition of RiskD (hS )
changes slightly and expectation in Eq. (1) is taken also over the random function hS .
Simple data-dependent bounds on the risk of hV , hA and hS can be derived, and these
bounds are special cases of the more general analysis given below. We now describe a
simple generalization of these three conversion techniques. It is reasonable to assume that
some of the online hypotheses generated by A are better than others. For instance, the
default hypothesis h0 is determined without observing even a single training example. This
surfaces the question whether it is possible to isolate the ?best? online hypotheses and only
use them to define the batch hypothesis. Formally, let [m] denote the set {0, . . . , m} and
let I be some non-empty subset of [m]. Now define hVI (x) to be the hypothesis which
performs voting as described above, with the single difference that only
P the members of
{hi : i ? I} participate in the vote. Similarly, define hAI (x) = (1/|I|) i?I hi (x), and let
hSI be the stochastic function that randomly chooses a function from the set {hi : i ? I}
every time it is evaluated, and predicts according to it. The data-independent conversions
presented in the beginning of this section are obtained by setting I = [m]. Our idea is to
use the training data to find a set I which induces the batch hypotheses hVI , hAI , and hSI with
the smallest risk.
Since there is an exponential number of potential subsets of [m], we need to restrict ourselves to a smaller set of candidate sets. Formally, let I be a family of subsets of [m], and
we restrict our search for I to the family I. Following in the footsteps of [2], we make the
simplifying assumption that none of the sets in I include the largest index m. This is a
technical assumption which can be relaxed at the price of a slightly less elegant analysis.
We use two intuitive concepts
to guide our search for I. First, for any set J ? [m ? 1],
P
define L(J) = (1/|J|) j?J ?(yj+1 , hj (xj+1 )). L(J) is the empirical evaluation of the
loss of the hypotheses indexed by J. We would like to find a set J for which L(J) is small
since we expect that good empirical loss of the online hypotheses indicates a low risk of
the batch hypothesis. Second, we would like |J| to be large so that the presence of a few
bad online hypotheses in J will not have a devastating effect on the performance of the
batch hypothesis. The trade-off between these two competing concepts can be formalized

as follows. Let C be a non-negative constant and define,
1

?(J) = L(J) + C |J|? 2 .

(2)

The function ? decreases as the average empirical loss L(J) decreases, and also as |J|
increases. It therefore captures the intuition described above. The function ? serves as our
yardstick when evaluating the candidates in I. Specifically, we set I = arg minJ?I ?(J).
Below we formally justify our choice of ?, and specifically show that ?(J) is a rather tight
upper bound on the risk of hAJ , hVJ and hSJ . The first lemma relates the risk of these functions
with the average risk of the hypotheses indexed by J.
Lemma 1. Let (x1 , y1 ), . . . , (xm , ym ) be a sequence of examples which is presented to the
online algorithm A and let h0 , . . . , hm be the resulting sequence of online hypotheses. Let
J be a non-empty subset of [m ? 1] andPlet ? : Y ? Y ? R+ be a loss function. (1) If ? is
the 0-1 loss then RiskD (hVJ ) ? (2/|J|)P i?J RiskD (hi (x)). (2) If ? is convex in its second
argument then RiskD (hAJ ) ? (1/|J|)
i?J RiskD (hi (x)). (3) For any loss function ? it
P
holds that RiskD (hSJ ) = (1/|J|) i?J RiskD (hi (x)).

Proof. Beginning with the voting conversion, recall that the loss function being used is the
0-1 loss, namely there is a single correct prediction which incurs a loss of 0 and every other
prediction incurs a loss of 1. For any example (x, y), if more than half of the hypotheses
in {hi }i?J predict the correct outcome then clearly hVJ also predicts this outcome and
?(y, hVJ (x)) = 0. Therefore, if ?(y, hVJ (x)) = 1 P
then at least half of the hypotheses in
{hi }i?J make incorrect predictions and (|J|/2) ? i?J ?(y, hi (x)). We therefore get,
2 X
?(y, hVJ (x)) ?
?(y, hi (x)) .
|J|
i?J

The above holds for any example (x, y) and therefore also holds after taking expectations
on both sides of the inequality. The bound now follows from the linearity of expectation
and the definition of the risk function in Eq. (1).
Moving on to the second claim of the lemma, we assume that ? is convex in its second
argument. The claim now follows from a direct application of Jensen?s inequality.
Finally, hSJ chooses its outcome by randomly choosing an hypothesis in {hi : i ? J},
where the probability of choosing each hypothesis in this set equals
(1/|J|). Therefore, the
P
expected loss suffered by hSJ on an example (x, y) is (1/|J|) i?J ?(y, hi (x)). The risk of
hSJ is simply the expected value of this term with respect to the random selection of (x, y).
Again using the linearity of expectation, we obtain the third claim of the lemma.
The next lemma relates the average risk of the hypotheses indexed by J with the empirical
performance of these hypotheses, L(J). In the following lemma, we use capital letters to
emphasize that we are dealing with random variables.
Lemma 2. Let (X1 , Y1 ), . . . , (Xm , Ym ) be a sequence of examples independently sampled according to D. Let, H0 , . . . , Hm be the sequence of online hypotheses generated by
A while observing this sequence of examples. Assume that the loss function ? is upperbounded by R. Then for any J ? [m ? 1],
""
#


1 X
C2
RiskD (Hi ) > ?(J) < exp ? 2
,
Pr
|J|
2R
i?J

where C is the constant used in the definition of ? (Eq. (2)).

The proof of this lemma is a direct application of Azuma?s bound on the concentration of
Lipschitz martingales [1], and is identical to that of Proposition 1 in [2]. For concreteness,

we now focus on the averaging conversion and note that the analyses of the other two
conversion strategies are virtually identical. By combining the first claim of Lemma 1 with
A
Lemma 2, we get that for any
 J ? I it holds that RiskD (hJ ) ?A ?(J) with probability at
2
2
least 1 ? exp ?C /(2R ) . Using the union bound, RiskD (hJ ) ? ?(J) for all J ? I
simultaneously with probability at least,


C2
.
1 ? |I| exp ? 2
2R
The greater the value of C, the more ? is influenced by the term |J|. On the other hand,
a large value of C increases the probability that ? indeed upper bounds RiskD (hAJ ) for all
J ? I. In conclusion, we have theoretically justified our choice of ? in Eq. (2).

3 Concrete Data-Driven Conversions
In this section we build on the ideas of the previous section and derive three concrete datadriven conversion techniques.
Suffix Conversion: An intuitive argument against selecting I = [m], as done by the dataindependent conversions, is that many online algorithms tend to generate bad hypotheses
during the first few rounds of learning. As previously noted, the default hypothesis h0 is
determined without observing any training data, and we should expect the first few online
hypotheses to be inferior to those that are generated further along. This argument motivates
us to consider subsets J of the form {a, a + 1, . . . , m ? 1}, where a is a positive integer
less than or equal to m ? 1. Li [9] proposed this idea in the context of the voting conversion
and gave a heuristic criterion for choosing a. Our formal setting gives a different criterion
for choosing a. In this conversion we define I to be the set of all suffixes of [m ? 1]. After
the algorithm generates h0 , . . . , hm , we set I to be I = arg minJ?I ?(J).
Interval
Conversion: Kernel-based hypotheses are functions that take the form, h(x) =
Pn
?
K(zj , x), where K is a Mercer kernel, z1 , . . . , zn are instances, often referred
j
j=1
to as support patterns and ?1 , . . . , ?n are real weights. A variety of different batch algorithms produce kernel-based hypotheses, including the Support Vector Machine [12]. An
important learning problem, which is currently addressed by only a handful of algorithms,
is to learn a kernel-based hypothesis h which is defined by at most B support patterns. The
parameter B is a predefined constant often referred to as the budget of support patterns.
Naturally, kernel-based hypotheses which are represented by a few support patterns are
memory efficient and faster to calculate. A similar problem arises in the online learning
setting where the goal is to construct online algorithms where each online hypothesis hi is
a kernel-based function defined by at most B vectors. Several online algorithms have been
proposed for this problem [4, 13, 5]. First note that the data-independent conversions, with
I = [m], are inadequate for this setting. Although each individual online hypothesis is
defined by at most B vectors, hA is defined by the union of these sets, which can be much
larger than B.
To convert a budget-constrained online algorithm A into a budget-constrained batch algorithm, we make an additional assumption on the update strategy employed by A. We
assume that whenever A updates its online hypothesis, it adds a single new support pattern
into the set used to represent the kernel hypothesis, and possibly removes some other pattern from this set. The algorithms in [4, 13, 5] all fall into this category. Therefore, if we
choose I to be the set {a, a + 1, . . . , b} for some integers 0 ? a < b < m, and A updates
its hypothesis k times during rounds a + 1 through b, then hAI is defined by at most B + k
support patterns. Concretely, define I to be the set of all non-empty intervals in [m ? 1].
With C set properly, ?(J) bounds RiskD (hAJ ) for every J ? I with high probability. Next,

J0,7

z

z

J0,3
J0,1

z }| {
h0 h1

}|

J2,3

{

z }| {
h2 h3

}|

J4,7

z

J4,5

z }| {
h4 h5

}|

J6,7

{
{

z }| {
h6 h7

J8,11

z

J8,9

z }| {
h8 h9

}|

J10,11

{

z }| {
h10 h11

h12 . . .

Figure 1: An illustration of the tree-based conversion.

generate h0 , . . . , hm by running A with a budget parameter of B/2. Finally, choose I to
be the set in I which contains at most B/2 updates and also minimizes the ? function. By
construction, the resulting hypothesis, hAI , is defined using at most B support patterns.
Tree-Based Conversion: A drawback of the suffix conversions is that it must be performed in two consecutive stages. First h0 , . . . , hm are generated and stored in memory.
Only then can we calculate ?(J) for every J ? I and perform the conversion. Therefore,
the memory requirements of this conversions grow linearly with m. We now present a
conversion that can sidestep this problem by interleaving the conversion with the online
hypothesis generation. This conversion slightly deviates from the general framework described in the previous section: instead of predefining a set of candidates I, we construct
the optimal subset I in a recursive manner. As a consequence, the analysis in the previous
section does not directly provide a generalization bound for this conversion. Assume for a
moment that m is a power of 2. For all 0 ? a ? m ? 1 define Ja,a = {a}. Now, assume
that we have already constructed the sets Ja,b and Jc,d , where a, b, c, d are integers such
that a < d, b = (a + d ? 1)/2, and c = b + 1. Given these sets, define Ja,d as follows:
(
Ja,b
if ?(Ja,b ) ? ?(Jc,d ) ? ?(Ja,b ) ? ?(Ja,b ? Jc,d )
Jc,d
if ?(Jc,d ) ? ?(Ja,b ) ? ?(Jc,d ) ? ?(Ja,b ? Jc,d ) . (3)
Ja,d =
Ja,b ? Jc,d otherwise
Finally, define I = J0,m?1 and output the batch hypothesis hAI . An illustration of this
process is given in Fig. 1. Note that the definition of I requires only m ? 1 recursive
evaluations of Eq. (3). When m is not a power of 2, we can pad the sequence of online
hypotheses with virtual hypotheses, each of which attains an infinite loss. This conversion
can be performed in parallel with the online rounds since on round t we already have all of
the information required to calculate Ja,b for all b < t.
In the special case where the instances are vectors in Rn , h0 , . . . , hm are linear hypotheses
and we use the averaging technique, the implementation of the tree-based conversion becomes memory efficient. Specifically, assume that each hi takes the form hi (x) = wi ? x
where wi is a vector of weights in Rn . In this case, storing an onlinePhypothesis hi is
equivalent to storing its weight vector wi . For any J ? [m ? 1], storing j?J hj requires
P
storing the single n-dimensional vector j?J wj . Hence, once we calculate Ja,b we can
discard the original online hypotheses ha , . . . , hb and instead merely keep hAJa,b . Moreover,
in order to calculate ? we do not need to keep the set Ja,b itself but rather the values L(Ja,b )
and |Ja,b |. Overall, storing hAJa,b , L(Ja,b ), and |Ja,b | requires only a constant amount of
memory. It can be verified using an inductive argument that the overall memory utilization
of this conversion is O(log(m)), which is significantly less than the O(m) space required
by the suffix conversion.

4 Experiments
We now turn to an empirical evaluation of the averaging and voting conversions. We
chose multiclass classification as the underlying task and used the multiclass version of

MNIST LETTER

3-fold

4-fold

5-fold

6-fold

7-fold

8-fold

9-fold

10-fold

S

S

S

S

S

2
0
?2
1
0

ISOLET

USPS

?1
1
0
?1
4
0
?4
S

I

T

S

I

T

S

I

T

I

T

I

T

I

T

I

T

I

T

Figure 2: Comparison of the three data-driven averaging conversions with the dataindependent averaging conversion, for different datasets (Y-axis) and different training-set
sizes (X-axis). Each bar shows the difference between the error percentages of a datadriven conversion (suffix (S), interval (I) or tree-based (T)) and of the data-independent
conversion. Error bars show standard deviation over the k folds.

the Passive-Aggressive (PA) algorithm [3] as the online algorithm. The PA algorithm is a
kernel-based large-margin online classifier. To apply the voting conversion, Y should be a
finite set. Indeed, in multiclass categorization problems the set Y consists of all possible
labels. To apply the averaging conversion Y must be a convex set. To achieve this, we use
the fact that PA associates a margin value with each class, and define Y = Rs (where s is
the number of classes).
In our experiments, we used the datasets LETTER, MNIST, USPS (training set only), and
ISOLET. These datasets are of size 20000, 70000, 7291 and 7797 respectively. MNIST and
USPS both contain images of handwritten digits and thus induce 10-class problems. The
other datasets contain images (LETTER) and utterances (ISOLET) of the English alphabet.
We did not use the standard splits into training set and test set and instead performed crossvalidation in all of our experiments. For various values of k, we split each dataset into k
parts, trained each algorithm using each of these parts and tested on the k ? 1 remaining
parts. Specifically, we ran this experiment for k = 3, . . . , 10. The reason for doing this
is that the experiment is most interesting when the training sets are small and the learning
task becomes difficult.
We applied the data-independent averaging and voting conversions, as well as the three
data-driven variants of these conversions (6 data-driven conversions in all). The interval
conversion was set to choose an interval containing 500 updates. The parameter C was
arbitrarily set to 3. Additionally, we evaluated the test error of the last hypothesis generated by the online algorithm, hm . It is common malpractice amongst practitioners to use
hm as if it were a batch hypothesis, instead of using an online-to-batch conversion. As a
byproduct of our experiments, we show that hm performs significantly worse than any of
the conversion techniques discussed in this paper. The kernel used in all of the experiments
is the Gaussian kernel with default kernel parameters. We would like to emphasize that our
goal was not to achieve state-of-the-art results on these datasets but rather to compare the
different conversion strategies on the same sequence of hypotheses. To achieve the best
results, one would have to tune C and the various kernel parameters.
The results for the different variants of the averaging conversion are depicted in Fig. 2.

LETTER 5-fold
LETTER 10-fold
MNIST 5-fold
MNIST 10-fold
USPS 5-fold
USPS 10-fold
ISOLET 5-fold
ISOLET 10-fold

last
29.9 ? 1.8
37.3 ? 2.1
7.2 ? 0.5
13.8 ? 2.3
9.7 ? 1.0
12.7 ? 4.7
20.1 ? 3.8
28.6 ? 3.6

average
21.2 ? 0.5
26.9 ? 0.7
5.9 ? 0.4
9.5 ? 0.8
7.5 ? 0.4
10.1 ? 0.7
17.6 ? 4.1
25.8 ? 2.8

average-sfx
20.5 ? 0.6
26.5 ? 0.6
5.3 ? 0.6
9.1 ? 0.8
7.1 ? 0.4
9.5 ? 0.8
16.7 ? 3.3
22.7 ? 3.3

voting
23.4 ? 0.8
30.2 ? 1.0
7.0 ? 0.5
8.7 ? 0.5
9.4 ? 0.4
12.5 ? 1.0
20.6 ? 3.4
29.3 ? 3.1

voting-sfx
21.5 ? 0.8
27.9 ? 0.6
6.5 ? 0.5
8.0 ? 0.5
8.8 ? 0.3
11.3 ? 0.6
18.3 ? 3.9
26.7 ? 4.0

Table 1: Percent of errors averaged over the k folds with standard deviation. Results are
given for the last online hypothesis (hm ), the data-independent averaging and voting conversions, and their suffix variants. The lowest error on each row is shown in bold.
For each dataset and each training-set size, we present a bar-plot which represents by how
much each of the data-driven averaging conversions improves over the data-independent
averaging conversion. For instance, the left bar in each plot shows the difference between
the test errors of the suffix conversion and the data-independent conversion. A negative
value means that the data-driven technique outperforms the data-independent one. The
results clearly indicate that the suffix and tree-based conversions consistently improve over
the data-independent conversion. The interval conversion does not improve as much and
occasionally even looses to the data-independent conversion. However, this is a small price
to pay in situations where it is important to generate a compact kernel-based hypothesis.
Due to the lack of space, we omit a similar figure for the voting conversion and merely note
that the plots are very similar to the ones in Fig. 2.
In Table 1 we give some concrete values of test error, and compare data-independent and
data-driven versions of averaging and voting, using the suffix conversion. As a reference,
we also give the results obtained by the last hypothesis generated by the online algorithm.
In all of the experiments, the data-driven conversion outperforms the data-independent conversion. In general, averaging exhibits better results than voting, while the last online hypothesis is almost always inferior to all of the online-to-batch conversions.

References
[1] K. Azuma. Weighted sums of certain dependent random variables. Tohoku Mathematical Journal, 68:357?367, 1967.
[2] N. Cesa-Bianchi, A. Conconi, and C.Gentile. On the generalization ability of on-line learning
algorithms. IEEE Transactions on Information Theory, 2004.
[3] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. Online passive aggressive
algorithms. Journal of Machine Learning Research, 2006.
[4] K. Crammer, J. Kandola, and Y. Singer. Online classification on a budget. NIPS 16, 2003.
[5] O. Dekel, S. Shalev-Shwartz, and Y. Singer. The Forgetron: A kernel-based perceptron on a
fixed budget. NIPS 18, 2005.
[6] Y. Freund and R. E. Schapire. Large margin classification using the perceptron algorithm.
Machine Learning, 37(3):277?296, 1999.
[7] S. I. Gallant. Optimal linear discriminants. ICPR 8, pages 849?852. IEEE, 1986.
[8] D. P. Helmbold and M. K. Warmuth. On weak learning. Journal of Computer and System
Sciences, 50:551?573, 1995.
[9] Y. Li. Selective voting for perceptron-like on-line learning. In ICML 17, 2000.
[10] N. Littlestone. From on-line to batch learning. COLT 2, pages 269?284, July 1989.
[11] N. Littlestone and M. Warmuth. Relating data compression and learnability. Unpublished
manuscript, November 1986.
[12] V. N. Vapnik. Statistical Learning Theory. Wiley, 1998.
[13] J. Weston, A. Bordes, and L. Bottou. Online (and offline) on a tighter budget. AISTAT 10, 2005.

"
3914,2012,Optimal Regularized Dual Averaging Methods for Stochastic Optimization,"This paper considers a wide spectrum of regularized stochastic optimization problems where both the loss function and regularizer can be non-smooth.  We develop a novel algorithm based on the regularized dual averaging (RDA) method, that can simultaneously achieve the optimal convergence rates for both convex and strongly convex loss. In particular, for strongly convex loss, it achieves the optimal  rate of $O(\frac{1}{N}+\frac{1}{N^2})$ for $N$  iterations, which improves the best known rate $O(\frac{\log N }{N})$ of previous stochastic dual averaging algorithms. In addition, our method constructs the final solution directly from the proximal mapping instead of averaging of all previous iterates. For widely used sparsity-inducing regularizers (e.g., $\ell_1$-norm), it has the advantage of encouraging sparser solutions. We further develop a multi-stage extension using the proposed algorithm as a subroutine, which achieves the uniformly-optimal rate $O(\frac{1}{N}+\exp\{-N\})$ for strongly convex loss.","Optimal Regularized Dual Averaging Methods for
Stochastic Optimization

Xi Chen
Machine Learning Department
Carnegie Mellon University
xichen@cs.cmu.edu

?
Qihang Lin
Javier Pena
Tepper School of Business
Carnegie Mellon University
{qihangl,jfp}@andrew.cmu.edu

Abstract
This paper considers a wide spectrum of regularized stochastic optimization problems where both the loss function and regularizer can be non-smooth. We develop
a novel algorithm based on the regularized dual averaging (RDA) method, that
can simultaneously achieve the optimal convergence rates for both convex and
strongly convex loss. In particular, for strongly convex loss, it achieves the optimal rate of O( N1 + N12 ) for N iterations, which improves the rate O( logNN ) for previous regularized dual averaging algorithms. In addition, our method constructs
the final solution directly from the proximal mapping instead of averaging of all
previous iterates. For widely used sparsity-inducing regularizers (e.g., `1 -norm),
it has the advantage of encouraging sparser solutions. We further develop a multistage extension using the proposed algorithm as a subroutine, which achieves the
uniformly-optimal rate O( N1 + exp{?N }) for strongly convex loss.

1

Introduction

Many risk minimization problems in machine learning can be formulated into a regularized stochastic optimization problem of the following form:
minx?X {?(x) := f (x) + h(x)}.

(1)

Here, the set of feasible solutions X is a convex set in Rn , which is endowed with a norm k ? k and
the dual norm k ? k? . The regularizer h(x) is assumed to be convex, but could be non-differentiable.
Popular examples of h(x) include `1 -norm and related
R sparsity-inducing regularizers. The loss
function f (x) takes the form: f (x) := E? (F (x, ?)) = F (x, ?)dP (?), where ? is a random vector
with the distribution P . In typical regression or classification tasks, ? is the input and response (or
class label) pair. We assume that for every random vector ?, F (x, ?) is a convex and continuous
function in x ? X . Therefore, f (x) is also convex. Furthermore, we assume that there exist
constants L ? 0, M ? 0 and ?
e ? 0 such that
L
?
e
kx ? yk2 ? f (y) ? f (x) ? hy ? x, f 0 (x)i ? kx ? yk2 + M kx ? yk,
2
2

?x, y ? X ,

(2)

where f 0 (x) ? ?f (x), the subdifferential of f . We note that this assumption allows us to adopt a
wide class of loss functions. For example, if f (x) is smooth and its gradient f 0 (x) = ?f (x) is
Lipschitz continuous, we have L > 0 and M = 0 (e.g., squared or logistic loss). If f (x) is nonsmooth but Lipschitz continuous, we have L = 0 and M > 0 (e.g., hinge loss). If ?
e > 0, f (x) is
strongly convex and ?
e is the so-called strong convexity parameter.
In general, the optimization problem in Eq.(1) is challenging since the integration in f (x) is computationally intractable for high-dimensional P . In many learning problems, we do not even know the
underlying distribution P but can only generate i.i.d. samples ? from P . A traditional approach is to
1

consider empirical loss minimization problem where the expectation in fP
(x) is replaced by its emm
1
pirical average on a set of training samples {?1 , . . . , ?m }: femp (x) := m
i=1 F (x, ?i ). However,
for modern data-intensive applications, minimization of empirical loss with an off-line optimization
solver could suffer from very poor scalability.
In the past few years, many stochastic (sub)gradient methods [6, 5, 8, 12, 14, 10, 9, 11, 7, 18] have
been developed to directly solve the stochastic optimization problem in Eq.(1), which enjoy low periteration complexity and the capability of scaling up to very large data sets. In particular, at the t-th
iteration with the current iterate xt , these methods randomly draw a sample ?t from P ; then compute the so-called ?stochastic subgradient? G(xt , ?t ) ? ?x F (xt , ?t ) where ?x F (xt , ?t ) denotes the
subdifferential of F (x, ?t ) with respect to x at xt ; and update xt using G(xt , ?t ). These algorithms
fall into the class of stochastic approximation methods. Recently, Xiao [21] proposed the regularized dual averaging (RDA) method and its accelerated version (AC-RDA) based on Nesterov?s
primal-dual method [17]. Instead of only utilizing a single stochastic subgradient G(xt , ?t ) of the
current iteration, it updates the parameter vector using the average of all past stochastic subgradients
{G(xi , ?i )}ti=1 and hence leads to improved empirical performances.
In this paper, we propose a novel regularized dual averaging method, called optimal RDA or ORDA,
which achieves the optimal expected convergence rate of E[?(b
x) ? ?(x? )], where x
b is the solution
?
from ORDA and x is the optimal solution of Eq.(1). As compared to previous dual averaging
methods, it has three main advantages:
1. For strongly convex f (x), ORDA improves the convergence rate of stochastic
dual aver 2

? 2 log N
log N
? +M 2
L
aging methods O( ?eN ) ? O( ?eN ) [17, 21] to an optimal rate O
+
?
2
?
eN
N
 
O ?e1N , where ? 2 is the variance of the stochastic subgradient, N is the number of iterations, and the parameters ?
e, M and L of f (x) are defined in Eq.(2).
2. ORDA is a self-adaptive and optimal algorithm for solving both convex and strongly convex f (x) with the strong convexity parameter ?
e as an input. When ?
e = 0, ORDA reduces
to a variant of AC-RDA in [21] with the optimal rate for solving convex f (x). Furthermore, our analysis allows f (x) to be non-smooth while AC-RDA requires the smoothness
of f (x).
e > 0, our algorithm achieves the optimal rate of
 For strongly convex f (x) with ?

1
?
eN while AC-RDA does not utilize the advantage of strong convexity.
3. Existing RDA methods [21] and many other stochastic gradient methods (e.g., [14, 10])
PN
PN
can only show the convergence rate for the averaged iterates: x
?N = t=1 %t xt / t=1 %t ,
where the {%t } are nonnegative weights. However, in general, the average iterates x
?N
cannot keep the structure that the regularizer tends to enforce (e.g., sparsity, low-rank,
etc). For example, when h(x) is a sparsity-inducing regularizer (`1 -norm), although xt
computed from proximal mapping will be sparse as t goes large, the averaged solution
could be non-sparse. In contrast, our method directly generates the final solution from the
proximal mapping, which leads to sparser solutions.
In addition to the rate of convergence, we also provide high probability bounds on the error of
objective values. Utilizing a technical lemma from [3], we could show the same high probability
bound as in RDA [21] but under a weaker assumption.
Furthermore, using ORDA
we develop
the multi-stage ORDA which obtains the

 2 as 2a subroutine,
p
+
exp{?
?
e
/LN
}
for
strongly convex f (x). Recall that ORDA
convergence rate of O ? ?e+M
N

 2
2
has the rate O ? ?e+M
+ NL2 for strongly convex f (x). The rate of muli-stage ORDA improves
N


p

the second term in the rate of ORDA from O NL2 to O exp{? ?
e/LN } and achieves the socalled ?uniformly-optimal ? rate [15]. Although the improvement is on the non-dominating term,
multi-stage ORDA is an optimal algorithm for both stochastic and deterministic optimization. In
particular, for deterministic strongly convex and smooth f (x) (M = 0), one can use the same algorithm but only replaces the stochastic subgradient G(x, ?) by the deterministic gradient ?f (x).
2
2
Then, the variance of the stochastic subgradient ? = 0. Now the term ? ?e+M
in the rate equals
N
to 0 and multi-stage ORDA becomes an optimal deterministic solver with the exponential rate
2

Algorithm 1 Optimal Regularized Dual Averaging Method: ORDA(x0 , N, ?, c)
Input Parameters: Starting point x0 ? X , the number of iterations N , constants ? ? L and c ? 0.
Parameters for f (x): Constants L, M and ?
e for f (x) in Eq. (2) and set ? = ?
e/? .
2
2
Initialization: Set ?t = t+2
; ?t = t+1
; ?t = c(t + 1)3/2 + ? ?; z0 = x0 .
Iterate for t = 0, 1, 2, . . . , N :
1. yt =

(1??t )(?+?t2 ?t )
x
?t2 ?t +(1??t2 )? t

+

(1??t )?t ?+?t3 ?t
z
?t2 ?t +(1??t2 )? t

2. Sample ?t from the distribution P (?) and compute the stochastic subgradient G(yt , ?t ).
P

t
G(yi ,?i )
3. gt = ?t ?t
i=0
?i
n
P

o
t
?V (x,yi )
4. zt+1 = arg minx?X hx, gt i + h(x) + ?t ?t
+ ?t ?t ?t+1 V (x, x0 )
i=0
?i
n


o
5. xt+1 = arg minx?X hx, G(yt , ?t )i + h(x) + ???2 + ??t V (x, yt )
t

Output: xN +1


p
O exp{? ?
e/LN } . This is the reason why such a rate is ?uniformly-optimal?, i.e., optimal
with respect to both stochastic and deterministic optimization.

2

Preliminary and Notations

In the framework of first-order stochastic optimization, the only available information of f (x) is the
stochastic subgradient. Formally speaking, stochastic subgradient of f (x) at x, G(x, ?), is a vectorvalued function such that E? G(x, ?) = f 0 (x) ? ?f (x). Following the existing literature, a standard
assumption on G(x, ?) is made throughout the paper : there exists a constant ? such that ?x ? X ,
E? (kG(x, ?) ? f 0 (x)k2? ) ? ? 2 .

(3)

A key updating step in dual averaging methods, the proximal mapping, utilizes the Bregman divergence. Let ?(x) : X ? R be a strongly convex and differentiable function, the Bregman divergence
associated with ?(x) is defined as:
V (x, y) := ?(x) ? ?(y) ? h??(y), x ? yi.

(4)

1
2
2 kxk2

together with V (x, y) = 12 kx ? yk22 . One may
scale ?(x) so that V (x, y) ? 21 kx ? yk2 for all

One typical and simple example is ?(x) =
refer to [21] for more examples. We can always
x, y ? X . Following the assumption in [10]: we assume that V (x, y) grows quadratically with the
parameter ? > 1, i.e., V (x, y) ? ?2 kx ? yk2 with ? > 1 for all x, y ? X . In fact, we could simply
choose ?(x) with a ? -Lipschitz continuous gradient so that the quadratic growth assumption will be
automatically satisfied.

3

Optimal Regularized Dual Averaging Method

In dual averaging methods [17, 21], the key proximal mapping step utilizes the average of all past
stochastic subgradients
to update the parameter
vector. In particular, it takes the form: zt+1 =
n
o
Pt
1
arg minx?X hgt , xi + h(x) + ?tt V (x, x0 ) , where ?t is the step-size and gt = t+1
i=0 G(zi , ?i ).
2

N
For strongly convex f (x), the current dual averaging methods achieve a rate of O( ? ?elog
N ), which
is suboptimal. In this section, we propose a new dual averaging algorithm which adapts to both
strongly and non-strongly convex f (x) via the strong convexity parameter ?
e and achieves optimal
rates in both cases. In addition, for previous dual averaging methods, to guarantee the convergence,
PN
the final solution takes the form: x
b = N1+1 t=0 zt and hence is not sparse in nature for sparsityinducing regularizers. Instead of taking the average, we introduce another proximal mapping and
generate the final solution directly from the second proximal mapping. This strategy will provide us
sparser solutions in practice. It is worthy to note that in RDA, zN has been proved to achieve the desirable sparsity pattern (i.e., manifold identification property) [13]. However, according to [13], the

3

convergence of ?(zN ) to the optimal ?(x? ) is established only under a more restrictive assumption
that x? is a strong local minimizer of ? relative to the optimal manifold and the convergence rate is
quite slow. Without this assumption, the convergence of ?(zN ) is still unknown.
The proposed optimal RDA (ORDA) method is presented in Algorithm 1. To simplify our notations,
we define the parameter ? = ?
e/? , which scales the strong convexity parameter ?
e by ?1 , where ? is
the quadratic growth constant. In general, the constant ? which defines the step-size parameter ?t
is set to L. However, we allow ? to be an arbitrary constant greater than or equal to L to facilitate
the introduction of the multi-stage ORDA in the later section. The parameter c is set to achieve the
optimal rates for both convex and strongly convex loss.? When ? > 0 (or equivalently, ?
e > 0), c is
)
?
.
Since
x
is
unknown
in practice,
set to 0 so that ?t ? ? ? ? ? L; while for ? = 0, c = ?? (?+M
?
2

V (x ,x0 )

one might replace V (x? , x0 ) in c by a tuning parameter.
Here, we make a few more explanations of Algorithm 1. In Step 1, the intermediate point yt is
a convex combination of xt and zt and when ? = 0, yt = (1 ? ?t )xt + ?t zt . The choice of the
combination
weights is inspired by [10]. Second, with our choice of ?t and ?t , it is easy to prove that
Pt 1
1
t
=
i=0 ?i
?t ?t . Therefore, gt in Step 3 is a convex combination of {G(yi , ?i )}i=0 . As compared
to RDA which uses the average of past subgradients, gt in ORDA is a weighted average of all
past stochastic subgradients and the subgradient from the larger iteration has a larger weight (i.e.,
2(i+1)
G(yi , ?i ) has the weight (t+1)(t+2)
). In practice, instead of storing all past stochastic subgradients,


gt?1
G(yt ,?t )
+
. We also note that
gt could be simply updated based on gt?1 : gt = ?t ?t ?t?1
?t?1
?t
since the error in the stochastic subgradient G(yt , ?t ) will affect the sparsity of xt+1 via the second
proximal mapping, to obtain stable sparsity recovery performances, it would be better to construct
the stochastic subgradient with a small batch of samples [21, 1]. This could help to reduce the noise
of the stochastic subgradient.
3.1

Convergence Rate

We present the convergence rate for ORDA. We start by presenting a general theorem without plugging the values of the parameters. To simplify our notations, we define ?t := G(yt , ?t ) ? f 0 (yt ).
Theorem 1 For ORDA, if we require c > 0 when ?
e = 0, then for any t ? 0:
?(xt+1 ) ? ?(x? ) ? ?t ?t ?t+1 V (x? , x0 ) +

t
?t ? t X

2 i=0

(k?i k? + M )2
?
? ?i

+

?i ?i
?

 + ?t ?t
? ?i L ?i

t
X
hx? ? zbi , ?i i
, (5)
?i
i=0

(1?? )?+? ? 2

?t ?
t
t t
where zbt = ?+?
zt , is a convex combination of yt and zt ; and zbt = zt when
2 yt +
?+?t ?t2
t ?t
? = 0. Taking the expectation on both sides of Eq.(5):

E?(xt+1 ) ? ?(x? ) ? ?t ?t ?t+1 V (x? , x0 ) + (? 2 + M 2 )?t ?t

t
X

1


i=0

?
? ?i

+

?i ?i
?

 .
? ?i L ? i

(6)

The proof of Theorem 1 is given in Appendix. In the next two corollaries, we establish the rates of
convergence in expectation for ORDA by choosing different values for c based on ?
e.
?

)
Corollary 1 For convex f (x) with ?
e = 0 , by setting c = ?? (?+M
and ? = L, we obtain:
?
2

E?(xN +1 ) ? ?(x? ) ?

V (x ,x0 )

p
4? LV (x? , x0 ) 8(? + M ) ? V (x? , x0 )
?
+
.
N2
N

(7)

Based on Eq.(6), the proof of Corollary 1 is straightforward with the details in Appendix. Since x?
is unknown in practice, one could set c by replacing V (x? , x0 ) in c with any value D? ? V (x? , x0 ).
By doing so, Eq.(7) remains valid after replacing all V (x? , x0 ) by D? . For convex f (x) with ?
e = 0,
the rate in Eq.(7) has achieved the uniformly-optimal rate according to [15]. In fact, if f (x) is a
deterministic and smooth function with ? = M = 0 (e.g., smooth empirical loss), one only needs
4

to change the stochastic subgradient G(yt , ?t ) to ?f (yt ). The resulting algorithm, which reduces to
?
,x0 )
Algorithm 3 in [20], is an optimal deterministic first-order method with the rate O( LV (x
).
N2
We note that the quadratic growth assumption of V (x, y) is not necessary for convex f (x).
If one doesn not assume this assumption
andreplaces the

o last step in ORDA by xt+1 =
?
?t
2
arg minx?X hx, G(yt , ?t )i + h(x) + 2?2 + 2 kx ? yt k , we can achieve the same rate as in
t
Eq.(7) but just removing all ? from the right hand side. But the quadratic growth assumption is
indeed required for showing the convergence for strongly convex f (x) as in the next corollary.
Corollary 2 For strongly convex f (x) with ?
e > 0, we set c = 0 and ? = L and obtain that:
4? LV (x? , x0 ) 4? (? 2 + M 2 )
+
.
(8)
N2
?N


 
N
1
, is optimal and better than the O log
rate for previous
The dominating term in Eq.(8), O ?N
?N
dual averaging methods. However, ORDA has not achieved the uniformly-optimal rate, which takes
p?
2
+M 2
the form of O( ? ?N
+exp(? L
N )). In particular, for deterministic smooth and strongly convex
f (x) (i.e., empirical loss with ? = M = 0), ORDA only achieves the rate of O( NL2 ) while the
p? 
optimal deterministic rate should be O exp(? L
N ) [16]. Inspired by the multi-restart technique
in [7, 11], we present a multi-stage extension of ORDA in Section 4 which achieves the uniformlyoptimal convergence rate.
E?(xN +1 ) ? ?(x? ) ?

3.2

High Probability Bounds

For stochastic optimization problems, another important evaluation criterion is the confidence
level of the objective value. In particular, it is of great interest to find (N, ?) as a monotonically decreasing function in both N and ? ? (0, 1) such that the solution xN +1 satisfies
Pr (?(xN +1 ) ? ?(x? ) ? (N, ?)) ? ?. In other words, we want to show that with probability
at least 1 ? ?, ?(xN +1 ) ? ?(x? ) < (N, ?). According to Markov inequality, for any  > 0,
?
?
Pr(?(xN +1 ) ? ?(x? ) ? ) ? E(?(xN +1)??(x )) . Therefore, we have (N, ?) = E?(xN +1?)??(x ) .
Under the basic assumption in Eq.(3), namely
?) ? f 0 (x)k2? ) ? ? 2 , and according to
? ? E? (kG(x,


 2

(?+M ) V (x ,x0 )
? +M 2
?
Corollary 1 and 2, (N, ?) = O
for
convex
f
(x),
and
(N,
?)
=
O
?N ?
N?
for strongly convex f (x).
However, the above bounds are quite loose. To obtain tighter bounds, we strengthen the basic
assumption of the stochastic
	to the ?light-tail? assumption [14]. In particular,
 subgradient in Eq. (3)
we assume that E exp kG(x, ?) ? f 0 (x)k2? /? 2
? exp{1}, ?x ? X . By further making the
boundedness assumption (kx? ? zbt k ? D) and utilizing?a technical lemma from [3], we obtain a


ln(1/?)D?
?
much tighter high probability bound with (N, ?) = O
for both convex and strongly
N
convex f (x). The details are presented in Appendix.

4

Multi-stage ORDA for Stochastic Strongly Convex Optimization

As we show in Section 3.1, for convex f (x), ORDA achieves the uniformly-optimal rate. However, for strongly convex f (x), although the dominating term of the convergence rate in Eq.(8) is
optimal, the overall rate is not uniformly-optimal. Inspired by the multi-stage stochastic approximation methods [7, 9, 11], we propose the multi-stage extension of ORDA in Algorithm 2 for
stochastic strongly convex optimization. For each stage 1 ? k ? K, we run ORDA in Algorithm
1 as a sub-routine for Nk iterations with the parameter ?t = c(t + 1)3/2 + ? ? with c = 0 and
? = ?k + L. Roughly speaking, we set Nk = 2Nk?1 and ?k = 4?k?1 . In other words, we double
the number of iterations for the next stage but reduce the step-size. The multi-stage ORDA has
achieved uniformly-optimal convergence rate as shown in Theorem 2 with the proof in Appendix.
The proof technique follows the one in [11]. Due this specialized proof technique, instead of showing E(?(xN )) ? ?(x? ) ? (N ) as in ORDA, we show the number of iterations N () to achieve the
-accurate solution: E(?(xN () )) ? ?(x? ) ? . But the two convergence rates are equivalent.
5

Algorithm 2 Multi-stage ORDA for Stochastic Strongly Convex Optimization
Initialization: x0 ? X , a constant V0 ? ?(x0 ) ? ?(x? ) and the number of stages K.
Iterate for k = 1, 2, . . . , K:
n q
o
k+9
(? 2 +M 2 )
1. Set Nk = max 4 ??L , 2 ??V
0
q
3/2
2k?1 ?(? 2 +M 2 )
2. Set ?k = Nk
? V0
3. Generate x
ek by calling the sub-routine ORDA(e
xk?1 , Nk , ? = ?k + L, c = 0)
Output: x
eK

Theorem 2 If we run multi-stage ORDA for K stages with K = log2 V0 for any given , we have
E(?(e
xK )) ? ?(x? ) ?  and the total number of iterations is upper bounded by:
s
 
K
X
?L
V0
1024? (? 2 + M 2 )
log2
+
.
(9)
N=
Nk ? 4
?

?
k=1

5

Related Works

In the last few years, a number of stochastic gradient methods [6, 5, 8, 12, 14, 21, 10, 11, 7, 4, 3] have
been developed to solve Eq.(1), especially for a sparsity-inducing h(x). In Table 1, we compare the
proposed ORDA and its multi-stage extension with some widely used stochastic gradient methods
using the following metrics. For the ease of comparison, we assume f (x) is smooth with M = 0.
1. The convergence rate for solving (non-strongly) convex f (x) and whether this rate has
achieved the uniformly-optimal (Uni-opt) rate.
2. The convergence rate for solving
convex f (x) and whether (1) the dominating

 strongly
term of rate is optimal, i.e., O

?2
?
eN

and (2) the overall rate is uniformly-optimal.

3. Whether the final solution x
b, on which the results of convergence are built, is generated
from the weighted average of previous iterates (Avg) or from the proximal mapping (Prox).
For sparsity-inducing regularizers, the solution directly from the proximal mapping is often
sparser than the averaged solution.
4. Whether an algorithm allows to use a general Bregman divergence in proximal mapping or
it only allows the Euclidean distance V (x, y) = 21 kx ? yk22 .
In Table 1, the algorithms in the first 7 rows are stochastic approximation algorithms where only
the current stochastic gradient is used at each iteration. The last 4 rows are dual averaging methods
where all past subgradients are used. Some algorithms in Table 1 make a more restrictive assumption
on the stochastic gradient: ?G > 0, EkG(x, ?)k2? ? G2 , ?x ? X . It is easy to verify that this
assumption implies our basic assumption in Eq.(3) by Jensen?s inequality.
As we can see from Table 1, the proposed ORDA possesses all good properties except that the
convergence rate for strongly convex f (x) is not uniformly-optimal. Multi-stage ORDA further
improves this rate to be uniformly-optimal. In particular, SAGE [8] achieves a nearly optimal rate
since the parameter D in the convergence rate is chosen such that E kxt ? x? k22 ? D for all t ? 0
and it could be much larger than V ? V (x? , x0 ). In addition, SAGE requires the boundedness of the
domain X , the smoothness of f (x), and only allows the Euclidean distance in proximal mapping.
As compared to AC-SA [10] and multi-stage AC-SA [11], our methods do not require the final
averaging step; and as shown in our experiments, ORDA has better empirical performances due
to the usage of all past stochastic subgradients. Furthermore, we improve the rates of RDA and
extend AC-RDA to an optimal algorithm for both convex and strongly convex f (x). Another highly
relevant work is [9]. Juditsky et al. [9] proposed multi-stage algorithms to achieve the optimal
strongly convex rate based on non-accelerated dual averaging methods. However, the algorithms in
[9] assume that ?(x) is a Lipschitz continuous function, i.e., the subgradient of ?(x) is bounded.
Therefore, when the domain X is unbounded, the algorithms in [9] cannot be directly applied.
6

FOBOS [6]
COMID [5]
SAGE [8]
AC-SA [10]

Convex f (x)
Rate
Uni-opt
 ? 
V
G
?
O
NO
 ?N 
V
G
O ?N
NO

 ?
?? D
LD
O
+ N 2 NEARLY

 ?N
?? V
O
+ LV
YES
N2
N

M-AC-SA [11] NA

NA

Epoch-GD [7] NA
 ? 
RDA [21]
O G?NV
 ?
AC-RDA [21] O ??NV +
 ?
ORDA
O ??NV +

NA

M-ORDA

NA

NO
LV
N2



YES

LV
N2



YES
NA

Strongly Convex f (x)
Rate
Opt
 2

G log N
O
NO
 2?eN 
G log N
O
NO

 2?eN
?
LD
O ?eN + N 2
YES

 2
?
LV
O ?eN + N 2
YES


q
2
?
e
N } YES
O ?e?N + exp{? L
 2
O ?eGN
YES
 2

G log N
O
NO
?
eN

Uni-opt

Final x
b Bregman

NO

Prox

NO

NO

Prox

YES

NO

Prox

NO

NO

Avg

YES

YES

Avg

YES

NO

Avg

NO

NO

Avg

YES

Avg

YES

Prox

YES

Prox

YES

NA
NA NA
 2

?
LV
O ?eN + N 2
YES NO


q
2
?
e
O ?e?N + exp{? L
N } YES YES

Table 1: Summary for different stochastic gradient algorithms. V is short for V (x? , x0 ); AC for ?accelerated?;
M for ?multi-stage? and NA stands for either ?not applicable? or ?no analysis of the rate?.
2

Recently, the paper [18] develops another stochastic gradient method which achieves the rate O( ?eGN )
for strongly convex f (x). However, for non-smooth f (x), it requires the averaging of the last a few
iterates and this rate is not uniformly-optimal.

6

Simulated Experiments

In this section, we conduct simulated experiments to demonstrate the performance of ORDA and
its multi-stage extension (M ORDA). We compare our ORDA and M ORDA (only for strongly
convex loss) with several state-of-the-art stochastic gradient methods, including RDA and AC-RDA
[21], AC-SA [10], FOBOS [6] and SAGE [8]. For a fair comparison, we compare all different
methods using solutions which have expected convergence guarantees. For all algorithms, we tune
the parameter related to step-size (e.g., c in ORDA for convex loss) within an appropriate range and
choose the one that leads to the minimum objective value.
In this experiment, we solve a sparse linear regression problem: minx?Rn f (x)+h(x) where f (x) =
?
1
T
2
2
2 Ea,b ((a x ? b) ) + 2 kxk2 and h(x) = ?kxk1 . The input vector a is generated from N (0, In?n )
T ?
and the response b = a x + , where x?i = 1 for 1 ? i ? n/2 and 0 otherwise and the noise
 ? N (0, 1). When ? = 0, th problem is the well known Lasso [19] and when ? > 0, it is known
as Elastic-net [22]. The regularization parameter ? is tuned so that a deterministic solver on all the
samples can correctly recover the underlying sparsity pattern. We set n = 100 and create a large
pool of samples for generating stochastic gradients and evaluating objective values. The number
of iterations N is set to 500. Since we focus on stochastic optimization instead of online learning,
we could randomly draw samples from an underlying distribution. So we construct the stochastic
gradient using the mini-batch strategy [2, 1] with the batch size 50. We run each algorithm for 100
times and report the mean of the objective value and the F1-score for sparsity recovery performance.
Pp
Pp
precision?recall
where precision =
F1-score is defined as 2 precision+recall
xi =1,x?
=1} /
xi =1} and
i=1 1{b
i=1 1{b
i
Pp
Pp
?
?
recall = i=1 1{bxi =1,xi =1} / i=1 1{xi =1} . The higher the F1-score is, the better the recovery
ability of the sparsity pattern. The standard deviations for both objective value and the F1-score in
100 runs are very small and thus omitted here due to space limitations.
We first set ? = 0 to test algorithms for (non-strongly) convex f (x). The result is presented in
Table 2 (the first two columns). We also plot the decrease of the objective values for the first 200
iterations in Figure 1. From Table 2, ORDA performs the best in both objective value and recovery
ability of sparsity pattern. For those optimal algorithms (e.g., AC-RDA, AC-SA, SAGE, ORDA),
they achieve lower final objective values and the rates of the decrease are also faster. We note that
for dual averaging methods, the solution generated from the (first) proximal mapping (e.g., zt in
7

Table 2: Comparisons in objective
value and F1-score.

28

31
RDA
AC?RDA
AC?SA
FOBOS
SAGE
ORDA

27
26
25
24
23

29
28
27
26

22

25

21

24

20

50

100

150

200

RDA
AC?RDA
AC?SA
FOBOS
SAGE
ORDA

30

Objective

?=1
Obj F1
21.57 0.67
21.12 0.67
21.01 0.67
21.19 0.84
21.09 0.73
20.97 0.87
20.98 0.88

Objective

?=0
Obj F1
RDA
20.87 0.67
AC-RDA 20.67 0.67
AC-SA
20.66 0.67
FOBOS 20.98 0.83
SAGE
20.65 0.82
ORDA
20.56 0.92
M ORDA N.A. N.A.

23

50

100

150

200

Iteration

Iteration

Figure 1: Obj for Lasso.

Figure 2: Obj for Elastic-Net.

ORDA) has almost perfect sparsity recovery performance. However, since here is no convergence
guarantee for that solution, we do not report results here.

Objective

Then we set ? = 1 to test algorithms for solving
strongly convex f (x). The results are presented
30
in Table 2 (the last two columns) and Figure
ORDA
M_ORDA
2 and 3. As we can see from Table 2, ORDA
28
and M ORDA perform the best. Although
M ORDA achieves the theoretical uniformly26
optimal convergence rate, the empirical per24
formance of M ORDA is almost identical to
that of ORDA. This observation is consistent
22
with our theoretical analysis since the improvement of the convergence rate only appears on
20
the non-dominating term. In addition, ORDA,
100
200
300
400
500
Iteration
M ORDA, AC-SA and SAGE with the convergence rate O( ?e1N ) achieve lower objective valFigure 3: ORDA v.s. M ORDA.
ues as compared to other algorithms with the
N
)
.
For
better
visualization,
we
do
rate O( log
?
eN
not include the comparison between M ORA and ORDA in Figure 2. Instead, we present the comparison separately in Figure 3. From Figure 3, the final objective values of both algorithms are very
close. An interesting observation is that, for M ORDA, each time when a new stage starts, it leads
to a sharp increase in the objective value following by a quick drop.

7

Conclusions and Future Works

In this paper, we propose a new dual averaging method which achieves the optimal rates for solving
stochastic regularized problems with both convex and strongly convex loss functions. We further
propose a multi-stage extension to achieve the uniformly-optimal convergence rate for strongly convex loss.
Although we study stochastic optimization problems in this paper, our algorithms can be easily
converted into online optimization approaches, where a sequence of decisions {xt }N
t=1 are generated
according to Algorithm 1 or 2. We often measure the quality of an online learning algorithm via the
PN
so-called regret, defined as RN (x? ) = t=1 (F (xt , ?t ) + h(xt )) ? (F (x? , ?t ) + h(x? )) . Given
the expected convergence rate in Corollary 1 and 2, the expected regret can be easily derived. For
PN
PN
1
?
example, for strongly convex f (x): ERN (x? ) ?
t=1 (E(?(xt )) ? ?(x )) ?
t=1 O( t ) =
O(ln N ). However, it would be a challenging future work to derive the regret bound for ORDA
instead of the expected regret. It would also be interesting to develop the parallel extensions of
ORDA (e.g., combining the distributed mini-batch strategy in [21] with ORDA) and apply them to
some large-scale real problems.

8

References
[1] A. Cotter, O. Shamir, N. Srebro, and K. Sridharan. Better mini-batch algorithms via accelerated
gradient methods. In Advances in Neural Information Processing Systems (NIPS), 2011.
[2] O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction
using mini-batches. Technical report, Microsoft Research, 2011.
[3] J. Duchi, P. L. Bartlett, and M. Wainwright. Randomized smoothing for stochastic optimization. arXiv:1103.4296v1, 2011.
[4] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and
stochastic optimization. In Conference on Learning Theory (COLT), 2010.
[5] J. Duchi, S. Shalev-Shwartz, Y. Singer, and A. Tewari. Composite objective mirror descent. In
Conference on Learning Theory (COLT), 2010.
[6] J. Duchi and Y. Singer. Efficient online and batch learning using forward-backward splitting.
Journal of Machine Learning Research, 10:2873?2898, 2009.
[7] E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for
stochastic strongly-convex optimization. In Conference on Learning Theory (COLT), 2011.
[8] C. Hu, J. T. Kwok, and W. Pan. Accelerated gradient methods for stochastic optimization and
online learning. In Advances in Neural Information Processing Systems (NIPS), 2009.
[9] A. Juditsky and Y. Nesterov. Primal-dual subgradient methods for minimizing uniformly convex functions. August 2010.
[10] G. Lan and S. Ghadimi. Optimal stochastic approximation algorithms for strongly convex
stochastic composite optimization, part i: a generic algorithmic framework. Technical report,
University of Florida, 2010.
[11] G. Lan and S. Ghadimi. Optimal stochastic approximation algorithms for strongly convex
stochastic composite optimization, part ii: shrinking procedures and optimal algorithms. Technical report, University of Florida, 2010.
[12] J. Langford, L. Li, and T. Zhang. Sparse online learning via truncated gradient. Journal of
Machine Learning Research, 10:777?801, 2009.
[13] S. Lee and S. J. Wright. Manifold identification of dual averaging methods for regularized
stochastic online learning. In International Conference on Machine Learning (ICML), 2011.
[14] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach
to stochastic programming. SIAM Journal on Optimization, 19(4):1574?1609, 2009.
[15] A. Nemirovski and D. Yudin. Problem complexity and method efficiency in optimization. John
Wiley New York, 1983.
[16] Y. Nesterov. Introductory lectures on convex optimization: a basic course. Kluwer Academic
Pub, 2003.
[17] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Programming, 120:221?259, 2009.
[18] A. Rakhlin, O. Shamir, and K. Sridharan. To average or not to average? making stochastic gradient descent optimal for strongly convex problems. In International Conference on Machine
Learning (ICML), 2012.
[19] R. Tibshirani. Regression shrinkage and selection via the lasso. J.R.Statist.Soc.B, 58:267?288,
1996.
[20] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. SIAM
Journal on Optimization (Submitted), 2008.
[21] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization.
Journal of Machine Learning Research, 11:2543?2596, 2010.
[22] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. J. R. Statist.
Soc. B, 67(2):301?320, 2005.

9

"
3309,2010,Worst-Case Linear Discriminant Analysis,"Dimensionality reduction is often needed in many applications due to the high dimensionality of the data involved. In this paper, we first analyze the scatter measures used in the conventional linear discriminant analysis~(LDA) model and note that the formulation is based on the average-case view. Based on this analysis, we then propose a new dimensionality reduction method called worst-case linear discriminant analysis~(WLDA) by defining new between-class and within-class scatter measures. This new model adopts the worst-case view which arguably is more suitable for applications such as classification. When the number of training data points or the number of features is not very large, we relax the optimization problem involved and formulate it as a metric learning problem. Otherwise, we take a greedy approach by finding one direction of the transformation at a time. Moreover, we also analyze a special case of WLDA to show its relationship with conventional LDA. Experiments conducted on several benchmark datasets demonstrate the effectiveness of WLDA when compared with some related dimensionality reduction methods.","Worst-Case Linear Discriminant Analysis

Yu Zhang and Dit-Yan Yeung
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
{zhangyu,dyyeung}@cse.ust.hk

Abstract
Dimensionality reduction is often needed in many applications due to the high
dimensionality of the data involved. In this paper, we first analyze the scatter
measures used in the conventional linear discriminant analysis (LDA) model and
note that the formulation is based on the average-case view. Based on this analysis,
we then propose a new dimensionality reduction method called worst-case linear
discriminant analysis (WLDA) by defining new between-class and within-class
scatter measures. This new model adopts the worst-case view which arguably is
more suitable for applications such as classification. When the number of training
data points or the number of features is not very large, we relax the optimization problem involved and formulate it as a metric learning problem. Otherwise,
we take a greedy approach by finding one direction of the transformation at a
time. Moreover, we also analyze a special case of WLDA to show its relationship
with conventional LDA. Experiments conducted on several benchmark datasets
demonstrate the effectiveness of WLDA when compared with some related dimensionality reduction methods.

1

Introduction

With the development of advanced data collection techniques, large quantities of high-dimensional
data are commonly available in many applications. While high-dimensional data can bring us more
information, processing and storing such data poses many challenges. From the machine learning
perspective, we need a very large number of training data points to learn an accurate model due
to the so-called ?curse of dimensionality?. To alleviate these problems, one common approach is
to perform dimensionality reduction on the data. An assumption underlying many dimensionality
reduction techniques is that the most useful information in many high-dimensional datasets resides
in a low-dimensional latent space. Principal component analysis (PCA) [8] and linear discriminant
analysis (LDA) [7] are two classical dimensionality reduction methods that are still widely used in
many applications. PCA, as an unsupervised linear dimensionality reduction method, finds a lowdimensional subspace that preserves as much of the data variance as possible. On the other hand,
LDA is a supervised linear dimensionality reduction method which seeks to find a low-dimensional
subspace that keeps data points from different classes far apart and those from the same class as
close as possible.
The focus of this paper is on the supervised dimensionality reduction setting like that for LDA. To set
the stage, we first analyze the between-class and within-class scatter measures used in conventional
LDA. We then establish that conventional LDA seeks to maximize the average pairwise distance
between class means and minimize the average within-class pairwise distance over all classes. Note
that if the purpose of applying LDA is to increase the accuracy of the subsequent classification task,
then it is desirable for every pairwise distance between two class means to be as large as possible and
every within-class pairwise distance to be as small as possible, but not just the average distances.
To put this thinking into practice, we incorporate a worst-case view to define a new between-class
1

scatter measure as the minimum of the pairwise distances between class means, and a new withinclass scatter measure as the maximum of the within-class pairwise distances over all classes. Based
on the new scatter measures, we propose a novel dimensionality reduction method called worst-case
linear discriminant analysis (WLDA). WLDA solves an optimization problem which simultaneously
maximizes the worst-case between-class scatter measure and minimizes the worst-case within-class
scatter measure. If the number of training data points or the number of features is not very large,
e.g., below 100, we propose to relax the optimization problem and formulate it as a metric learning
problem. In case both the number of training data points and the number of features are large, we
propose a greedy approach based on the constrained concave-convex procedure (CCCP) [24, 18] to
find one direction of the transformation at a time with the other directions fixed. Moreover, we also
analyze a special case of WLDA to show its relationship with conventional LDA. We will report
experiments conducted on several benchmark datasets.

2

Worst-Case Linear Discriminant Analysis

We are given a training set of ? data points, ? = {x1 , . . . , x? } ? ?? . Let ? be partitioned into
? ? 2 disjoint classes ?? , ? = 1, . . . , ?, where class ?? contains ?? examples. We perform linear
dimensionality reduction by finding a transformation matrix W ? ???? .
2.1

Objective Function

We first briefly review the conventional LDA. The between-class scatter matrix and within-class
scatter matrix are defined as
S? =

?
?
??
(m
? ? ? m)(
? m
? ? ? m)
? ?,
?

S? =

?=1

?
? 1
?
(x? ? m
? ? )(x? ? m
? ? )? ,
?
x ??

?=1 ?

?

??
where m
?? =
? = ?1 ?=1 x? is the
x? ??? x? is the class mean of the ?th class ?? and m
sample mean of all data points. Based on the scatter matrices, the between-class scatter measure and
within-class scatter measure are defined as
1
??

?

(
)
?? = tr W? S? W ,

(
)
?? = tr W? S? W ,

where tr(?) denotes the trace of a square matrix. LDA seeks to find the optimal solution of W that
maximizes the ratio ?? /?? as the optimality criterion.
??
By using the fact that m
? = ?1 ?=1 ?? m
? ? , we can rewrite S? as
S? =

?
?
1 ??
?? ?? (m
?? ?m
? ? )(m
?? ?m
? ? )? .
2?2 ?=1 ?=1

According to this and the definition of the within-class scatter measure, we can see that LDA tries
to maximize the average pairwise distance between class means {m
? ? } and minimize the average
within-class pairwise distance over all classes. Instead of taking this average-case view, our WLDA
model adopts a worst-case view which arguably is more suitable for classification applications.
We define the sample covariance matrix for the ?th class ?? as
S? =

1 ?
(x? ? m
? ? )(x? ? m
? ? )? .
?? x ??
?

(1)

?

Unlike LDA which uses the average of the distances between each class mean and the sample mean
as the between-class scatter measure, here we use the minimum of the pairwise distances between
class means as the between-class scatter measure:
{ (
)}
?? = min tr W? (m
?? ?m
? ? )(m
?? ?m
? ? )? W .
?,?

(2)

Also, we define the new within-class scatter measure as
{ (
)}
?? = max tr W? S? W ,
?

which is the maximum of the average within-class pairwise distances.
2

(3)

Similar to LDA, we define the optimality criterion of WLDA as the ratio of the between-class scatter
measure to the within-class scatter measure:
max
W

s.t.

?(W) =

??
??

W ? W = I? ,

(4)

where I? denotes the ? ? ? identity matrix. The orthonormality constraint in problem (4) is widely
used by many existing dimensionality reduction methods. Its role is to limit the scale of each column
of W and eliminate the redundancy among all columns of W.
2.2

Optimization Procedure

Since problem (4) is not easy to optimize with respect to W, we resort to formulate this dimensionality reduction problem as a metric learning problem [22, 21, 4]. We define a new variable
? = WW? which can be used to define a metric. Then we express ?? and ?? in terms of ? as
??

=

??

=

{ (
)}
min tr (m
?? ?m
? ? )(m
?? ?m
? ? )? ?
?,?
{ (
)}
max tr S? ? ,
?

due to a property of the matrix trace that tr(AB) = tr(BA) for any matrices A and B with proper
sizes. The orthonormality constraint in problem (4) is non-convex with respect to W and cannot be
expressed in terms of ?.
We define a set ?? as
{
}
?? = M? ? M? = WW? , W? W = I? , W ? ???? .

Apparently ? ? ?? . It has been shown in [16] that the convex hull of ?? can be precisely
expressed as a convex set ?? given by
{
}
?? = M? ? tr(M? ) = ?, 0 ? M? ? I? ,

where 0 denotes the zero vector or matrix of appropriate size and A ? B means that the matrix
B ? A is positive semidefinite. Each element in ?? is referred to as an extreme point of ?? .
Since ?? consists of all convex combinations of the elements in ?? , ?? is the smallest convex
set that contains ?? , and hence ?? ? ?? . Then problem (4) can be relaxed as
{ (
)}
min?,? tr S?? ?
{ (
?(?) =
)}
max? tr S? ?

max
?

tr(?) = ?, 0 ? ? ? I? ,

s.t.

(5)

where S?? = (m
?? ? m
? ? )(m
?? ? m
? ? )? . For notational simplicity, we denote the constraint set as
? = {? ? tr(?) = ?, 0 ? ? ? I? }. Table 1 shows an iterative algorithm for solving problem (5).
Table 1: Algorithm for solving optimization problem (5)
Input: {m
? ? }, {S? } and ?
1: Initialize ?(0) ;
2: For ? = 1, . . . , ?iter
2.1: Compute the ratio ?? from ?(??1) as: ?? = ?(?(??1) );
2.2: Solve the optimization problem
{ (
{ (
)}
)}
?(?) = arg max??? min?,? tr S?? ? ? ?? max? tr S? ? ;
2.3: If ??(?) ? ?(??1) ?? ? ? (here we set ? = 10?4 )
break;
Output: ?
We now present the solution of the optimization problem in step 2.2. It is equivalent to the following
problem
{
}
{
}
(
)
(
)
min ?? max tr S? ? ? min tr S?? ? .
???

?

?,?

3

(6)

{ (
)}
According to [3], we know that max? tr S? ? is a convex function because it is the maximum of
{ (
)}
several convex functions, and min?,? tr S?? ? is a concave function because it is the minimum of
several concave functions. Moreover, ?? is a positive scalar since ?? = ?(?(??1) ). So problem (6)
is a convex optimization problem. We introduce new variables ? and ? to simplify problem (6) as
?? ? ? ?
(
)
tr S? ? ? ?, ??
(
)
tr S?? ? ? ? > 0, ??, ?
tr(?) = ?, 0 ? ? ? I? .

min

?,?,?

s.t.

(7)

Note that problem (7) is a semidefinite programming (SDP) problem [19] which can be solved using
a standard SDP solver. After obtaining the optimal ?? , we can recover the optimal W? as the top ?
eigenvectors of ?? . In the following, we will prove the convergence of the algorithm in Table 1.
Theorem 1 For the algorithm in Table 1, we have ?(?(?) ) ? ?(?(??1) ).
{ (
{ (
)}
)}
Proof: We define ?(?) = min?,? tr S?? ? ? ?? max? tr S? ? . Then ?(?(??1) ) = 0 since
{ (
)}
min?,?

?? =
max?
(?)

?(?

tr S?? ?(??1)

{ (

tr S?

?(??1)

(?)
= arg max??? ?(?) and ?(??1) ? ?, we have
)} . Because ?

) ? ?(?(??1) ) = 0. This means
{ (
)}
min?,? tr S?? ?(?)
{ (
)} ? ?? ,
max? tr S? ?(?)

which implies that ?(?(?) ) ? ?(?(??1) ).

?

Theorem 2 For any ? ? ?, we have 0 ? ?(?) ?
value of S? .

??2tr(S? )
?=1 ????+1

where ?? is the ?th largest eigen-

Proof: It is obvious that ?(?) ? 0. The numerator of ?(?) can be upper-bounded as
{ (
)}
min tr S?? ? ?

?? ??

(
)
?=1 ?? ?? tr S?? ?
= 2tr(S? ?) ? 2tr(S? ).
?? ??
?=1
?=1 ?? ??

?=1

?,?

(8)

Moreover, the denominator of ?(?) can be lower-bounded as
(
)
??
?
?
{ (
?
)}
(
) ?
?=1 ?? tr S? ?
?? ?
????+1 ?
????+1 ,
max tr S? ? ?
= tr S? ? ?
??
?
?=1 ??
?=1
?=1

(9)

? ? is the ?th largest eigenvalue of ? and satisfies 0 ? ?
? ? ? 1 and ?? ?
?
where ?
?=1 ? = ? due to the
constraints ? on ?. By utilizing Eqs. (8) and (9), we can reach the conclusion.
?
From Theorem 2, we can see that ?(?) is bounded and our method is non-decreasing. So our
method can achieve a local optimum when converged.
2.3

Optimization in Dual Form

In the previous subsection, we need to solve the SDP problem in problem (7). However, SDP is
not scalable to high dimensionality ?. In many real-world applications to which dimensionality
reduction is applied, the number of data points ? is much smaller than the dimensionality ?. Under
such circumstances, speedup can be obtained by solving the dual form of problem (4) instead.
It is easy to show that the solution of problem (4) satisfies W = XA [14] where X = (x1 , . . . , x? )
is the data matrix and A ? ???? . Then problem (4) can be formulated as
max
A

s.t.

{ (
)}
min?,? tr A? X? S?? XA
{ (
)}
max? tr A? X? S? XA
A? KA = I? ,

4

(10)

where K = X? X is the linear kernel matrix. Here we assume that K is positive definite since the
data points are independent and identically distributed and ? is much larger than ?. We define a new
1
variable B = K 2 A and problem (10) can be reformulated as
{ (
)}
1
1
min?,? tr B? K? 2 X? S?? XK? 2 B
{ (
)}
1
1
max? tr B? K? 2 X? S? XK? 2 B

max
B

B? B = I ? .

s.t.

(11)

Note that problem (11) is almost the same as problem (4) and so we can use the same relaxation
? = BB? used to
technique above to solve problem (11). In the relaxed problem, the variable ?
define the metric in the dual form is of size ? ? ? which is much smaller than that (? ? ?) of ? in
the primal form when ? < ?. So solving the problem in the dual form is more efficient. Moreover,
the dual form facilitates kernel extension of our method.
2.4

Alternative Optimization Procedure

In case the number of training data points ? and the dimensionality ? are both large, the above
optimization procedures will be infeasible. Here we introduce yet another optimization procedure
based on a greedy approach to solve problem (4) when both ? and ? are large.
We find the first column of W by solving problem (4) where W is a vector, then find the second
column of W by assuming the first column is fixed, and so on. This procedure consists of ? steps.
In the ?th step, we assume that the first ? ? 1 columns of W have been obtained and we find the
?th column according to problem (4). We use W??1 to denote the matrix in which the first ? ? 1
columns are already known and the constraint in problem (4) becomes
?
w?? w? = 1, W??1
w? = 0.

?
?
w? = 0 does not
When ? = 1, W??1
can be viewed as an empty matrix and the constraint W??1
exist. So in the ?th step, we need to solve the following problem

min

w? ,?,?

s.t.

?
?
w?? S? w? + ?? ? ? ? 0, ??
? ? w?? (m
?? ?m
? ? )(m
?? ?m
? ? )? w? ? ??? ? 0, ??, ?
?, ? > 0
?
w?? w? ? 1, W??1
w? = 0,

(

?
W??1
S? W??1

)

(

(12)

?
W??1
(m
??

?

)

where ?? = tr
and ??? = tr
?m
? ? )(m
?? ?m
? ? ) W??1 . In the last
?
constraint of problem (12), we relax the constraint on w? as w? w? ? 1 to make it convex.
The function ?? is not convex with respect to (?, ?)? since the Hessian matrix is not positive semidefinite. So the objective function of problem (12) is non-convex. Moreover, the second constraint in
problem (12), which is the difference of two convex functions, is also non-convex. We rewrite the
objective function as
(? + 1)2
(? ? 1)2
?
=
?
,
?
4?
4?
2

which is also the difference of two convex functions since ? (?, ?) = (?+?)
for ? > 0 is convex
?
with respect to ? and ? according to [3]. Then we can use the constrained concave-convex procedure (CCCP) [24, 18] to optimize problem (12). More specifically, in the (? + 1)th iteration of
CCCP, we replace the non-convex parts of the objective function and the second constraint with
(?)
their first-order Taylor expansions at the solution {?(?) , ?(?) , w? } in the ?th iteration and solve the
following problem
min

w? ,?,?

s.t.

(? + 1)2
? ?? + ?2 ?
4?
w?? S? w? + ?? ? ? ? 0, ??
(?)

(?)

? ? 2(w? )? (m
?? ?m
? ? )(m
?? ?m
? ? )? w? + ??? ? ??? ? 0, ??, ?
?, ? > 0
?
w?? w? ? 1, W??1
w? = 0,

5

(13)

where ? =
(?+1)2
4? ,

i.e.,

(?)
(?)
?(?) ?1
and ??? = (w? )? (m
??
2?(?)
(?+1)2
? ?, and using the fact
4?

(?)

?m
? ? )(m
?? ?m
? ? )? w? . By putting an upper bound on
that

2



(? + 1)

? + 1
? ? (?, ? > 0) ? 

 ? ? + ?,
4?
??? 2

where ? ? ?2 denotes the 2-norm of a vector, we can reformulate problem (13) into a second-order
cone programming (SOCP) problem [12] which is more efficient than SDP:
min

w? ,?,?,?

s.t.

? ? ?? + ?2 ?
w?? S? w? + ?? ? ? ? 0, ??
(?)

(?)

? ? 2(w? )? (m
?? ?m
? ? )(m
?? ?m
? ? )? w? + ??? ? ??? ? 0, ??, ?



? + 1


 ? ? + ? with ?, ?, ? > 0
??? 2
?
?
w? w? ? 1, W??1
w? = 0.

2.5

(14)

Analysis

It is well known that in binary classification problems when both classes are normally distributed
with the same covariance matrix, the solution given by conventional LDA is the Bayes optimal
solution. We will show here that this property still holds for WLDA.
The objective function for WLDA in a binary classification problem is formulated as
w

w? (m
?1 ?m
? 2 )(m
?1 ?m
? 2 )? w
?
?
max{w S1 w, w S2 w}

s.t.

w ? ?? , w? w ? 1.

max

(15)

Here, similar to conventional LDA, the reduced dimensionality ? is set to 1. When the two classes
have the same covariance matrix, i.e., S1 = S2 , the problem degenerates to the optimization problem
of conventional LDA since w? S1 w = w? S2 w for any w and w is the solution of conventional
LDA.1 So WLDA also gives the same Bayes optimal solution as conventional LDA.
Since the scale of w does not affect the final solution in problem (15), we simplify problem (15) as
max
w

s.t.

w? (m
?1 ?m
? 2 )(m
?1 ?m
? 2 )? w
w? S1 w ? 1, w? S2 w ? 1.

(16)

Since problem (16) is to maximize a convex function, it is not a convex problem. We can still use
CCCP to optimize problem (16). In the (? + 1)th iteration of CCCP, we need to solve the following
problem
max
w

s.t.

(w(?) )? (m
?1 ?m
? 2 )(m
?1 ?m
? 2 )? w
w? S1 w ? 1, w? S2 w ? 1.

(17)

The Lagrangian is given by
? = ?(w(?) )? (m
?1 ?m
? 2 )(m
?1 ?m
? 2 )? w + ?(w? S1 w ? 1) + ?(w? S2 w ? 1),

where ? ? 0 and ? ? 0. We calculate the gradients of ? with respect to w and set them to 0 to
obtain
w = (2?S1 + 2?S2 )?1 (m
?1 ?m
? 2 )(m
?1 ?m
? 2 )? w(?) .

From this, we can see that when the algorithm converges, the optimal w? satisfies
w? ? (2?? S1 + 2? ? S2 )?1 (m
?1 ?m
? 2 ).

This is similar to the following property of the optimal solution in conventional LDA
w? ? S?1
?1 ?m
? 2 ) ? (?1 S1 + ?2 S2 )?1 (m
?1 ?m
? 2 ).
? (m
1

The constraint w? w ? 1 in problem (15) only serves to limit the scale of w.

6

However, in our method, ?? and ? ? are not fixed but learned from the following dual problem
min
?,?

s.t.

(

?
(m
?1 ?m
? 2 )(?S1 + ?S2 )?1 (m
?1 ?m
? 2) + ? + ?
4
? ? 0, ? ? 0,

(18)

)2

where ? = (m
?1 ?m
? 2 )? w(?) . Note that the first term in the objective function of problem (18)
is just the scaled optimality criterion of conventional LDA when we assume the within-class scatter
matrix S? to be S? = ?S1 + ?S2 . From this view, WLDA seeks to find a linear combination of S1
and S2 as the within-class scatter matrix to maximize the optimality criterion of conventional LDA
while controlling the complexity of the within-class scatter matrix as reflected by the second and
third terms of the objective function in problem (18).

3

Related Work

In [11], Li et al. proposed a maximum margin criterion for dimensionality
reduction
(
) by changing
the optimization problem of conventional LDA to: maxW tr W? (S? ? S? )W . The objective
function has a physical meaning similar to that of LDA which favors a large between-class scatter
measure and a small within-class scatter measure. However, similar to LDA, the maximum margin
criterion also uses the average distances to describe the between-class and within-class scatter measures. Kocsor et al. [10] proposed another maximum margin criterion for dimensionality reduction.
The objective function in [10] is identical to that of support vector machine (SVM) and it treats the
decision function in SVM as one direction in the transformation matrix W.
In [9], Kim et al. proposed a robust LDA algorithm to deal with data uncertainty in classification
applications by formulating the problem as a convex problem. However, in many applications, it is
not easy to obtain the information about data uncertainty. Moreover, its limitation is that it can only
handle binary classification problems but not more general multi-class problems.
The orthogonality constraint on the transformation matrix W has been widely used by dimensionality reduction methods, such as Foley-Sammon LDA (FSLDA) [6, 5] and orthogonal LDA [23].
The orthogonality constraint can help to eliminate the redundant information in W. This has been
shown to be effective for dimensionality reduction.

4

Experimental Validation

In this section, we evaluate WLDA empirically on some benchmark datasets and compare WLDA
with several related methods, including conventional LDA, trace-ratio LDA [20], FSLDA [6, 5], and
MarginLDA [11]. For fair comparison with conventional LDA, we set the reduced dimensionality
of each method compared to ? ? 1 where ? is the number of classes in the dataset. After dimensionality reduction, we use a simple nearest-neighbor classifier to perform classification. Our choice
of the optimization procedure follows this strategy: when the number of features ? or the number
of training data points ? is smaller than 100, the optimization method in Section 2.2 or 2.3 is used
depending on which one is smaller; otherwise, we use the greedy method in Section 2.4.
4.1

Experiments on UCI Datasets

Ten UCI datasets [1] are used in the first set of experiments. For each dataset, we randomly select
70% to form the training set and the rest for the test set. We perform 10 random splits and report
in Table 2 the average results across the 10 trials. For each setting, the lowest classification error is
shown in bold. We can see that WLDA gives the best result for most datasets. For some datasets,
e.g., balance-scale and hayes-roth, even though WLDA is not the best, the difference between it and
the best one is very small. Thus it is fair to say that the results obtained demonstrate convincingly
the effectiveness of WLDA.
4.2

Experiments on Face and Object Datasets

Dimensionality reduction methods have been widely used for face and object recognition applications. Previous research found that face and object images usually lie in a low-dimensional subspace
7

Table 2: Average classification errors
LDA [20].
Dataset
LDA
diabetes
0.3233
heart
0.2448
liver
0.4001
sonar
0.2806
spambase
0.1279
balance-scale 0.1193
iris
0.0244
hayes-roth
0.3125
waveform
0.1861
mfeat-factors 0.0732

on the UCI datasets. Here tr-LDA denotes the trace-ratio
tr-LDA
0.3143
0.2259
0.3933
0.2895
0.1301
0.1198
0.0267
0.3104
0.1865
0.0518

FSLDA
0.4039
0.4395
0.4365
0.3694
0.3093
0.1176
0.0622
0.3104
0.2261
0.0868

MarginLDA
0.4143
0.2407
0.5058
0.2806
0.1440
0.1150
0.0644
0.2958
0.2303
0.0817

WLDA
0.2996
0.2157
0.3779
0.2661
0.1260
0.1174
0.0211
0.3050
0.1671
0.0250

of the ambient image space. Fisherface (based on LDA) [2] is one representative dimensionality reduction method. We use three face databases, ORL [2], PIE [17] and AR [13], and one object
database, COIL [15], in our experiments. In the AR face database, 2,600 images of 100 persons
(50 men and 50 women) are used. Before the experiment, each image is converted to gray scale
and normalized to a size of 33 ? 24 pixels. The ORL face database contains 400 face images of
40 persons, each having 10 images. Each image is preprocessed to a size of 28 ? 23 pixels. In our
experiment, we choose the frontal pose from the PIE database with varying lighting and illumination conditions. There are about 49 images for each subject. Before the experiment, we resize each
image to a resolution of 32 ? 32 pixels. The COIL database contains 1,440 grayscale images with
black background for 20 objects with each object having 72 different images.
In face and object recognition applications, the size of the training set is usually not very large since
labeling data is very laborious and costly. To simulate this realistic situation, we randomly choose
4 images of a person or object in the database to form the training set and the remaining images
to form the test set. We perform 10 random splits and report the average classification error rates
across the 10 trials in Table 3. From the result, we can see that WLDA is comparable to or even
better than the other methods compared.

Table 3: Average classification errors on the face and object datasets. Here tr-LDA denotes the
trace-ratio LDA [20].
Dataset
LDA
tr-LDA FSLDA MarginLDA WLDA
ORL
0.1529 0.1042 0.0654
0.0536
0.0446
PIE
0.4305 0.2527 0.6715
0.2936
0.2469
AR
0.2498 0.1919 0.7726
0.4282
0.1965
COIL
0.2554 0.1737 0.1726
0.1653
0.1593

5

Conclusion

In this paper, we have presented a new supervised dimensionality reduction method by exploiting
the worst-case view instead of average-case view in the formulation. One interesting direction of
our future work is to extend WLDA to handle tensors for 2D or higher-order data. Moreover, we
will investigate the semi-supervised extension of WLDA to exploit the useful information contained
in the unlabeled data available in some applications.

Acknowledgement
This research has been supported by General Research Fund 621407 from the Research Grants
Council of Hong Kong.
8

References
[1] A. Asuncion and D.J. Newman. UCI machine learning repository, 2007.
[2] P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman. Eigenfaces vs. Fisherfaces: Recognition using class
specific linear projection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(7):711?
720, 1997.
[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, New York, NY, 2004.
[4] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon. Information-theoretic metric learning. In Proceedings of the Twenty-Fourth International Conference on Machine Learning, pages 209?216, Corvalis,
Oregon, USA, 2007.
[5] J. Duchene and S. Leclercq. An optimal transformation for discriminant and principal component analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 10(6):978?983, 1988.
[6] D. H. Foley and J. W. Sammon. An optimal set of discriminant vectors. IEEE Transactions on Computers,
24(3):281?289, 1975.
[7] K Fukunnaga. Introduction to Statistical Pattern Recognition. Academic Press, New York, 1991.
[8] I. T. Jolliffe. Principal Component Analysis. Springer-Verlag, New York, 2nd edition, 2002.
[9] S.-J. Kim, A. Magnani, and S. Boyd. Robust Fisher discriminant analysis. In Y. Weiss, B. Sch?olkopf,
and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 659?666. Vancouver,
British Columbia, Canada, 2006.
[10] A. Kocsor, K. Kov?acs, and C. Szepesv?ari. Margin maximizing discriminant analysis. In Proceedings of
the 15th European Conference on Machine Learning, pages 227?238, Pisa, Italy, 2004.
[11] H. Li, T. Jiang, and K. Zhang. Efficient and robust feature extraction by maximum margin criterion. In
S. Thrun, L. K. Saul, and B. Sch?olkopf, editors, Advances in Neural Information Processing Systems 16,
Vancouver, British Columbia, Canada, 2003.
[12] M. S. Lobo, L. Vandenberghe, S. Boyd, and H. Lebret. Applications of second-order cone programming.
Linear Algebra and its Applications, 284:193?228, 1998.
[13] A. M. Mart??nez and R. Benavente. The AR-face database. Technical Report 24, CVC, 1998.
[14] S. Mika, G. R?atsch, J. Weston, B. Sch?olkopf, A. J. Smola, and K.-R. M?uller. Constructing descriptive and
discriminative nonlinear features: Rayleigh coefficients in kernel feature spaces. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 25(5):623?633, 2003.
[15] S. A. Nene, S. K. Nayar, and H. Murase. Columbia object image library (COIL-20). Technical Report
005, CUCS, 1996.
[16] M. L. Overton and R. S. Womersley. Optimality conditions and duality theory for minimizing sums of
the largest eigenvalues of symmetric matrices. Math Programming, 62(2):321?357, 1993.
[17] T. Sim, S. Baker, and M. Bsat. The CMU pose, illumination and expression database. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 25(12):1615?1618, 2003.
[18] A. J. Smola, S. V. N. Vishwanathan, and T. Hofmann. Kernel methods for missing variables. In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, Barbados, 2005.
[19] L. Vandenberghe and S. Boyd. Semidefinite prgramming. SIAM Review, 38(1):49?95, 1996.
[20] H. Wang, S. Yan, D. Xu, X. Tang, and T. Huang. Trace ratio vs. ratio trace for dimensionality reduction.
In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,
pages 1?8, Minneapolis, Minnesota, USA, 2007.
[21] K. Q. Weinberger, J. Blitzer, and L. K. Saul. Distance metric learning for large margin nearest neighbor
classification. In Y. Weiss, B. Sch?olkopf, and J. Platt, editors, Advances in Neural Information Processing
Systems 18, pages 1473?1480, Vancouver, British Columbia, Canada, 2005.
[22] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. J. Russell. Distance metric learning with application to clustering
with side-information. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information
Processing Systems 15, pages 505?512, Vancouver, British Columbia, Canada, 2002.
[23] J.-P. Ye and T. Xiong. Computational and theoretical analysis of null space and orthogonal linear discriminant analysis. Journal of Machine Learning Research, 7:1183?1204, 2006.
[24] A. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915?936, 2003.

9

"
3592,2011,Prismatic Algorithm for Discrete D.C. Programming Problem,"In this paper, we propose the first exact algorithm for minimizing the difference of two submodular functions (D.S.), i.e., the discrete version of the D.C. programming problem. The developed algorithm is a branch-and-bound-based algorithm which responds to the structure of this problem through the relationship between submodularity and convexity. The D.S. programming problem covers a broad range of applications in machine learning because this generalizes the optimization of a wide class of set functions. We empirically investigate the performance of our algorithm, and illustrate the difference between exact and approximate solutions respectively obtained by the proposed and existing algorithms in feature selection and discriminative structure learning.","Prismatic Algorithm for Discrete D.C. Programming Problem

Yoshinobu Kawahara? and Takashi Washio
The Institute of Scientific and Industrial Research (ISIR)
Osaka University
8-1 Mihogaoka, Ibaraki-shi, Osaka 567-0047 JAPAN
{kawahara,washio}@ar.sanken.osaka-u.ac.jp

Abstract
In this paper, we propose the first exact algorithm for minimizing the difference of two
submodular functions (D.S.), i.e., the discrete version of the D.C. programming problem.
The developed algorithm is a branch-and-bound-based algorithm which responds to the
structure of this problem through the relationship between submodularity and convexity.
The D.S. programming problem covers a broad range of applications in machine learning. In fact, this generalizes any set-function optimization. We empirically investigate
the performance of our algorithm, and illustrate the difference between exact and approximate solutions respectively obtained by the proposed and existing algorithms in feature
selection and discriminative structure learning.

1 Introduction
Combinatorial optimization techniques have been actively applied to many machine learning applications, where submodularity often plays an important role to develop algorithms [10, 16, 27, 14,
15, 19, 1]. In fact, many fundamental problems in machine learning can be formulated as submoular
optimization. One of the important categories would be the D.S. programming problem, i.e., the
problem of minimizing the difference of two submodular functions. This is a natural formulation
of many machine learning problems, such as learning graph matching [3], discriminative structure
learning [21], feature selection [1] and energy minimization [24].
In this paper, we propose a prismatic algorithm for the D.S. programming problem, which is a
branch-and-bound-based algorithm responding to the specific structure of this problem. To the best
of our knowledge, this is the first exact algorithm to the D.S. programming problem (although there
exists an approximate algorithm for this problem [21]). As is well known, the branch-and-bound
method is one of the most successful frameworks in mathematical programming and has been incorporated into commercial softwares such as CPLEX [13, 12]. We develop the algorithm based
on the analogy with the D.C. programming problem through the continuous relaxation of solution
spaces and objective functions with the help of the Lov?asz extension [17, 11, 18]. The algorithm is
implemented as an iterative calculation of binary-integer linear programming (BILP).
Also, we discuss applications of the D.S. programming problem in machine learning and investigate empirically the performance of our method and the difference between exact and approximate
solutions through feature selection and discriminative structure-learning problems.
The remainder of this paper is organized as follows. In Section 2, we give the formulation of the
D.S. programming problem and then describe its applications in machine learning. In Section 3,
we give an outline of the proposed algorithm for this problem. Then, in Section 4, we explain the
details of its basic operations. And finally, we give several empirical examples using artificial and
real-world datasets in Section 5, and conclude the paper in Section 6.
Preliminaries and Notation: A set function f is called submodular if f (A) + f (B) ? f (A ?
B) + f (A ? B) for all A, B ? N , where N = {1, ? ? ? , n} [5, 7]. Throughout this paper, we denote
?

http://www.ar.sanken.osaka-u.ac.jp/?kawahara/

1

by f? the Lov?asz extension of f , i.e., a continuous function f? : Rn ? R defined by
?m?1
f?(p) = j=1 (?
pj ? p?j+1 )f (Uj ) + p?m f (Um ),
where Uj = {i ? N : pi ? p?j } and p?1 > ? ? ? > p?m are the m distinct elements of p?
[17, 18]. Also,
we denote by IA ? {0, 1}n the characteristic vector of a subset A ? N , i.e., IA = i?A ei where
ei is the i-th unit vector. Note, through the definition of the characteristic vector, any subset A ? N
has the one-to-one correspondence with the vertex of a n-dimensional cube D := {x ? Rn : 0 ?
xi ? 1(i = 1, . . . , n)}. And, we denote by (A, t)(T ) all combinations of a real value plus subset
whose corresponding vectors (IA , t) are inside or on the surface of a polytope T ? Rn+1 .

2

The D.S. Programming Problem and its Applications

Let f and g are submodular functions. In this paper, we address an exact algorithm to solve the D.S.
programming problem, i.e., the problem of minimizing the difference of two submodular functions:
min f (A) ? g(A).

A?N

(1)

As is well known, any real-valued function whose second partial derivatives are continuous everywhere can be represented as the difference of two convex functions [12]. As well, the problem (1)
generalizes any set-function optimization problem. Problem (1) covers a broad range of applications
in machine learning [21, 24, 3, 1]. Here, we give a few examples.
Feature selection using structured-sparsity inducing norms: Sparse methods for supervised
learning, where we aim at finding good predictors from as few variables as possible, have attracted
much interests from machine learning community. This combinatorial problem is known to be a
submodular maximization problem with cardinality constraint for commonly used measures such as
least-squared errors [4, 14]. And as is well known, if we replace the cardinality function with its
convex envelope such as l1 -norm, this can be turned into a convex optimization problem. Recently,
it is reported that submodular functions in place of the cardinality can give a wider family of polyhedral norms and may incorporate prior knowledge or structural constraints in sparse methods [1].
Then, the objective (that is supposed to be minimized) becomes the sum of a loss function (often,
supermodular) and submodular regularization terms.
Discriminative structure learning: It is reported that discriminatively structured Bayesian classifier often outperforms generatively structured one [21, 22]. One commonly used metric for discriminative structure learning would be EAR (explaining away residual) [2]. EAR is defined as the
difference of the conditional mutual information between variables by class C and non-conditional
one, i.e., I(Xi ; Xj |C) ? I(Xi ; Xj ). In structure learning, we repeatedly try to find a subset in
variables that minimize this kind of measures. Since the (symmetric) mutual information is a submodular function, obviously this problem leads the D.S. programming problem [21].
Energy minimization in computer vision: In computer vision, an image is often modeled with
a Markov random field, where each node represents a pixel. Let G = (V, E) be the undirected
graph, where a label xs ? L is assigned on each node. Then, many tasks in computer vision
can be naturally?formulated in terms
? of energy minimization where the energy function has the
form: E(x) = p?V ?p (xp ) + (p,q)?E ?(xp , xq ), where ?p and ?p,q are univariate and pairwise
potentials. In a pairwise potential for binarized energy (i.e., L = {0, 1}), submodularity is defined
as ?pq (1, 1) + ?pq (0, 0) ? ?pq (1, 0) + ?pq (0, 1) (see, for example, [26]). Based on this, any energy
function in computer vision can be written with a submodular function E1 (x) and a supermodular
function E2 (x) as E(x) = E1 (x) + E2 (x) (ex. [24]). Or, in case of binarized energy, even if such
explicit decomposition is not known, a non-unique decomposition to submodular and supermodular
functions can be always given [25].

3

Prismatic Algorithm for the D.S. Programming Problem

By introducing an additional variable t(? R), Problem (1) can be converted into the equivalent
problem with a supermodular objective function and a submodular feasible set, i.e.,
min

A?N,t?R

t ? g(A)

s.t. f (A) ? t ? 0.

2

(2)

Obviously, if (A? , t? ) is an optimal solution of Problem (2), then A? is an optimal solution of Problem (1)
and t? = f (A? ). The proposed algorithm is a realization
of the branch-and-bound scheme which responds to this
specific structure of the problem.
To this end, we first define a prism T (S) ? Rn+1 by
T = {(x, t) ? Rn ? R : x ? S},
where S is an n-simplex. S is obtained from the ndimensional cube D at the initial iteration (as described
in Section 4.1), or by the subdivision operation described
in the later part of this section (and the detail will be described in Section 4.2). The prism T has n + 1 edges that
are vertical lines (i.e., lines parallel to the t-axis) which
pass through the n + 1 vertices of S, respectively [11].

T

v

(0,1)

(1,1)

D
r

(1,0)

(0,0)

S2
S1
S

Figure 1: Illustration of the prismatic algorithm.

Our algorithm is an iterative procedure which mainly consists of two parts; branching and bounding,
as well as other branch-and-bound frameworks [13]. In branching, subproblems are constructed by
dividing the feasible region of a parent problem. And in bounding, we judge whether an optimal
solution exists in the region of a subproblem and its descendants by calculating an upper bound of
the subproblem and comparing it with an lower bound of the original problem. Some more details
for branching and bounding are described as follows.
Branching: The branching operation in our method is carried out using the property of a simplex.
That is, since, in a n-simplex, any r + 1 vertices
?p are not on a r ? 1-dimensional hyperplane for
r ? n, any n-simplex can be divided as S = i=1 Si , where p ? 2 and Si are n-simplices such
that each pair of simplices Si , Sj (i ?= j) intersects at most in common
?pboundary points (the way of
constructing such partition is explained in Section 4.2). Then, T = i=1 Ti , where Ti = {(x, t) ?
Rn ? R : x ? Si }, is a natural prismatic partition of T induced by the above simplical partition.
Bounding: For the bounding operation on Sk (resp., Tk ) at the iteration k, we consider a polyhe? where D
? = {(x, t) ? Rn ? R : x ? D, f?(x) ? t} is the
dral convex set Pk such that Pk ? D,
region corresponding to the feasible set of Problem (2). At the first iteration, such P is obtained as
P0 = {(x, t) ? Rn ? R : x ? S, t ? t?},
where t? is a real number satisfying t? ? min{f (A) : A ? N }. Here, t? can be determined by using
some existing submodular minimization solver [23, 8]. Or, at later iterations, more refined Pk , such
? is constructed as described in Section 4.4.
that P0 ? P1 ? ? ? ? ? D,
As described in Section 4.3, a lower bound ?(Tk ) of t ? g(A) on the current prism Tk can be
calculated through the binary-integer linear programming (BILP) (or the linear programming (LP))
using Pk , obtained as described above. Let ? be the lowest function value (i.e., an upper bound of
? found so far. Then, if ?(Tk ) ? ?, we can conclude that there is no feasible solution
t ? g(A) on D)
which gives a function value better than ? and can remove Tk without loss of optimality.
The pseudo-code of the proposed algorithm is described in Algorithm 1. In the following section,
we explain the details of the operations involved in this algorithm.

4

Basic Operations

Obviously, the procedure described in Section 3 involves the following basic operations:
1. Construction of the first prism: A prism needs to be constructed from a hypercube at first,
2. Subdivision process: A prism is divided into a finite number of sub-prisms at each iteration,
3. Bound estimation: For each prism generated throughout the algorithm, a lower bound for the
objective function t ? g(A) over the part of the feasible set contained in this prism is computed,
4. Construction of cutting planes: Throughout the algorithm, a sequence of polyhedral convex sets
? Each set Pj is generated by a cutting
P0 , P1 , ? ? ? is constructed such that P0 ? P1 ? ? ? ? ? D.
plane to cut off a part of Pj?1 , and
5. Deletion of non-optimal prisms: At each iteration, we try to delete prisms that contain no
feasible solution better than the one obtained so far.
3

?
Construct a simplex S0 ? D, its corresponding prism T0 and a polyhedral convex set P0 ? D.
Let ?0 be the best objective function value known in advance. Then, solve the BILP (5)
corresponding to ?0 and T0 , and let ?0 = ?(T0 , P0 , ?0 ) and (A?0 , t?0 ) be the point satisfying
?0 = t?0 ? g(A?0 ).
Set R0 ? T0 .
while Rk ?= ?
Select a prism Tk? ? Rk satisfying ?k = ?(Tk? ), (?
v k , t?k ) ? Tk? .
k ?
? then
if (?
v , tk ) ? D
Set Pk+1 = Pk .
else
Construct lk (x, t) according to (8), and set Pk+1 = {(x, t) ? Pk : lk (x, t) ? 0}.
Subdivide Tk? = T (Sk? ) into a finite number of subprisms Tk,j (j?Jk ) (cf. Section 4.2).
For each j ? Jk , solve the BILP (5) with respect to Tk,j , Pk+1 and ?k .
Delete all Tk,j (j?Jk ) satisfying (DR1) or (DR2). Let Mk denote the collection of
remaining prisms Tk,j (j ? Jk ), and for each T ? Mk set

1
2

3
4
5
6
7
8
9
10
11
12

?(T ) = max{?(Tk? ), ?(T, Pk+1 , ?k )}.
Let Fk be the set of new feasible points detected while solving BILP in Step 11, and set

13

?k+1 = min{?k , min{t ? g(A) : (A, t) ? Fk }}.
Delete all T ?Mk satisfying ?(T )??k+1 and let Rk be Rk?1 \ Tk ? Mk .
Set ?k+1 ? min{?(T ) : T ? Mk } and k ? k + 1.

14
15

Algorithm 1: Pseudo-code of the prismatic algorithm for the D.S programming problem.
4.1

Construction of the first prism

? can be constructed as follows.
The initial simplex S0 ? D (which yields the initial prism T0 ? D)
Now,
? let v and Av be a vertex of D and its corresponding subset in N , respectively, i.e., v =
i?Av ei . Then, the initial simplex S0 ? D can be constructed by
S0 = {x ? Rn : xi ? 1(i ? Av ), xi ? 0(i ? N \ Av ), aT x ? ?},
?
where a = i?N \Av ei ? i?Av ei and ? = |N \ Av |. The n + 1 vertices of S0 are v and the n
points where the hyperplane {x ? Rn : aT x = ?} intersects the edges of the cone {x ? Rn : xi ?
1(i ? Av ), xi ? 0(i ? N \ Av )}. Note this is just an option and any n-simplex S ? D is available.
?

4.2 Sub-division of a prism
Let Sk and Tk be the simplex and prism at k-th iteration in the algorithm, respectively. We denote Sk
as Sk = [v ik , . . . , v n+1
] := conv{v 1k , . . . , v n+1
} which is defined as the convex hull of its vertices
k
k
n+1
1
v k , . . . , v k . Then, any r ? Sk can be represented as
?n+1
?n+1
r = i=1 ?i v ik , i=1 ?i = 1, ?i ? 0 (i = 1, . . . , n + 1).
Suppose that r ?= v ik (i = 1, . . . , n + 1). For each i satisfying ?i > 0, let Ski be the subsimplex of
Sk defined by
i+1
n+1
Ski = [v 1k , . . . , v i?1
].
(3)
k , r, v k , . . . , v k
i
Then, the collection {Sk : ?i > 0} defines a partition of Sk , i.e., we have
?
j
i
i
?i >0 Sk = Sk , int Sk ? int Sk = ? for i ?= j [12].
In a natural way, the prisms T (Ski ) generated by the simplices Ski defined in Eq. (3) form a partition
of Tk . This subdivision process of prisms is exhaustive,
??i.e., for every nested (decreasing) sequence
of prisms {Tq } generated by this process, we have q=0 Tq = ? , where ? is a line perpendicular
to Rn (a vertical line) [11]. Although several subdivision process can be applied, we use a classical
bisection one, i.e., each simplex is divided into subsimplices by choosing in Eq. (3) as
r = (v ik1 + v ik2 )/2,
where ?v ik1 ? v ik2 ? = max{?v ik ? v jk ? : i, j ? {0, . . . , n}, i ?= j} (see Figure 1).
4

4.3

Lower bounds

Again, let Sk and Tk be the simplex and prism at k-th iteration in the algorithm, respectively. And,
let ? be an upper bound of t ? g(A), which is the smallest value of t ? g(A) attained at a feasible
?
point known so far in the algorithm. Moreover, let Pk be a polyhedral convex set which contains D
and be represented as
Pk = {(x, t) ? Rn ? R : Ak x + ak t ? bk },
(4)
m 1
where Ak is a real (m ? n)-matrix and ak , bk ? R . Now, a lower bound ?(Tk , Pk , ?) of t ? g(A)
? can be computed as follows.
over Tk ? D
First, let v ik (i = 1, . . . , n + 1) denote the vertices of Sk , and define I(Sk ) = {i ? {1, . . . , n + 1} :
v ik ? Bn } and
{
min{?, min{f?(v ik ) ? g?(v ik ) : i ? I(S)}}, if I(S) ?= ?,
?=
?,
if I(S) = ?.
For each i = 1, . . . , n + 1, consider the point (v ik , tik ) where the edge of Tk passing through v ik
intersects the level set {(x, t) : t ? g?(x) = ?}, i.e.,
tik = g?(v ik ) + ? (i = 1, . . . , n + 1).
Then, let us denote the uniquely defined hyperplane through the points (v ik , tik ) by H = {(x, t) ?
Rn ?R : pT x?t = ?}, where p ? Rn and ? ? R. Consider the upper and lower halfspace generated
by H, i.e., H+ = {(x, t) ? Rn ? R : pT x ? t ? ?} and H? = {(x, t) ? Rn ? R : pT x ? t ? ?}.
? ? H+ , then we see from the supermodularity of g(A) (the concavity of g?(x)) that
If Tk ? D
? ? min{t ? g(A) : (A, t) ? (A, t)(Tk ? H+ )}
min{t ? g(A) : (A, t) ? (A, t)(Tk ? D)}
? min{t ? g?(x) : (x, t) ? Tk ? H+ }
= tik ? g?(xik )(i = 1, . . . , n + 1) = ?.

Otherwise, we shift the hyperplane H (downward with respect to t) until it reaches a point z =
(x? , t? ) (? Tk ? Pk ? H? , x? ? Bn ) ((x? , t? ) is a point with the largest distance to H and the
? denote the resulting
corresponding pair (A, t) (since x? ? Bn ) is in (A, t)(Tk ? Pk ? H? )). Let H
? + the upper halfspace generated by H.
? Moreover, for each
supporting hyperplane, and denote by H
i = 1, . . . , n + 1, let z i = (v ik , t?ik ) be the point where the edge of T passing through v ik intersects
? Then, it follows (A, t)(Tk ? D)
? ? (A, t)(Tk ? Pk ) ? (A, t)(Tk ? H
? + ), and hence
H.
? > min{t ? g(A) : (A, t) ? (A, t)(Tk ? H
? + )}
min{t ? g(A) : (A, t) ? (A, t)(Tk ? D)}
= min{t?ik ? g?(v ik ) : i = 1, . . . , n + 1}.

Now, the above consideration leads to the following BILP in (?, x, t):
(?
)
?n+1
n+1
max
s.t. Ak x + ak t ? bk , x = i=1 ?i v ik , x ? Bn ,
i=1 ti ?i ? t
?,x,t
?n+1
i=1 ?i = 1, ?i ? 0 (i = 1, . . . , n + 1),

(5)

where A, a and b are given in Eq. (4).
? is empty.
Proposition 1. (a) If the system (5) has no solution, then intersection (A, t)(Tk ? D)
?n+1 ?
?
? ?
?
?
(b) Otherwise, let (? , x , t ) be an optimal solution of BILP (5) and c =
i=1 ti ?i ? t its
optimal value, respectively. Then, the following statements hold:
? ? (A, t)(H+ ).
(b1) If c? ? 0, then (A, t)(Tk ? D)
?n+1
?
(b2) If c > 0, then z = ( i=1 ?i v ik , t?k ), z i = (v ik , t?ik ) = (v ik , tik ? c? ) and t?ik ? g?(v ik ) =
? ? c? (i = 1, . . . , n + 1).
?n+1
Proof. First, we prove part (a). Since every point in Sk is uniquely representable as x = i=1 ?i v i ,
we see from Eq. (4) that the set (A, t)(Tk ? Pk ) coincide with the feasible set of problem (5).
? =?
Therefore, if the system (5) has no solution, then (A, t)(Tk ?Pk ) = ?, and hence (A, t)(Tk ? D)
T
?
(because D ? Pk ). Next, we move to part (b). Since the equation of H is p x ? t = ?, it follows
1

Note that Pk is updated at each iteration, which does not depend on Sk , as described in Section 4.4.

5

? and the point z amounts to solving the binary integer linear
that determining the hyperplane H
programming problem:
max pT x ? t

s.t. (x, t) ? Tk ? Pk , x ? Bn .

(6)

Here, we note that the objective of the above can be represented as
(?
)
?n+1
n+1
i
T i
pT x ? t = pT
i=1 ?i v k ? t =
i=1 ?i p v k ? t.
On the other hand, since (v ik , tik ) ? H, we have pT v ik ? tik = ? (i = 1, . . . , n + 1), and hence
?n+1
?n+1
pT x ? t = i=1 ?i (? + tik ) ? t = i=1 tik ?i ? t + ?.
Thus, the two BILPs (5) and (6) are equivalent. And, if ? ? denotes the optimal objective function
? is
value in Eq. (6), then ? ? = c? + ?. If ? ? ? ?, then it follows from the definition of H+ that H
?
obtained by a parallel shift of H in the direction H+ . Therefore, c ? 0 implies (A, t)(Tk ? Pk ) ?
? ? (A, t)(H+ ).
(A, t)(H+ ), and hence (A, t)(Tk ? D)
? = {(x, t) ? Rn ? R : pT x ? t = ? ? } and H = {(x, t) ? Rn ? R : pT x ? t = ?}
Since H
we see that for each intersection point (v ik , t?ik ) (and (v ik , tik )) of the edge of Tk passing through v ik
? (and H), we have pT v i ? t?i = ? ? and pT v i ? ti = ?, respectively. This implies that
with H
k
k
k
k
i
t?k = tik + ? ? ? ? = tik ? c? , and (using tik = g?(v ik ) + ?) that t?ik = g?(v ik ) + ? ? c? .
From the above, we see that, in the case (b1), ? constitutes a lower bound of (t?g(A)) wheres, in the
case (b2), such a lower bound is given by min{t?ik ? g?(v ik ) : i = 1, . . . , n + 1}. Thus, Proposition 1
provides the lower bound
{
+?,
if BILP (5) has no feasible point,
?,
if c? ? 0,
(7)
?k (Tk , Pk , ?) =
?
? ? c if c? > 0.
As stated in Section 4.5, Tk can be deleted from further consideration when ?k = ? or ?.
4.4

Outer approximation

? used in the preceding section is updated in each iteration, i.e.,
The polyhedral convex set Pk ? D
? The update from Pk to Pk+1
a sequence P0 , P1 , ? ? ? is constructed such that P0 ? P1 ? ? ? ? ? D.
(k = 0, 1, . . .) is done in a way which is standard for pure outer approximation methods [12]. That
is, a certain linear inequality lk (x, t) ? 0 is added to the constraint set defining Pk , i.e., we set
Pk+1 = Pk ? {(x, t) ? Rn ? R : lk (x, t) ? 0}.
The function lk (x, t) is constructed as follows. At iteration k, we have a lower bound ?k of t ?
g(A) as defined in Eq. (7), and a point (?
v k , t?k ) satisfying t?k ? g?(?
v k ) = ?k . We update the outer
? Then, we can set
?
approximation only in the case (?
v k , tk ) ?
/ D.
lk (x, t) = sTk [(x, t) ? z k ] + (f?(x?k ) ? t?k ),

(8)

where sk is a subgradient of f?(x) ? t at z k . The subgradient can be calculated as, for example,
stated in [9] (see also [7]).
? i.e.,
Proposition 2. The hyperplane {(x, t) ? Rn ? R : lk (x, t) = 0} strictly separates z k from D,
?
?
lk (z k ) > 0, and lk (x, t) ? 0 for (x, t) ? D.
? we have lk (z k ) = (f?(x? ) ? t? ). And, the latter inequality is
Proof. Since we assume that z k ?
/ D,
k
k
an immediate consequence of the definition of a subgradient.
4.5

Deletion rules

At each iteration of the algorithm, we try to delete certain subprisms that contain no optimal solution.
To this end, we adopt the following two deletion rules:
(DR1) Delete Tk if BILP (5) has no feasible solution.
6

[b

?



[b

?
b

b





Approx. (Supermodular-submodular)





















Approx. (Supermodular-sumodular)


Time [second]



b

Exact (Prismatic)

Approx. (Supermodular-sumodular)


Test Error

Training Error



Exact (Prismatic)

Exact (Prismatic)



b

?




b

?



?

?



?

?



b



?



?



?

Figure 2: Training errors, test errors and computational time versus ? for the prismatic algorithm
and the supermodular-sumodular procedure.
p
120
120
120
120

n
150
150
150
150

k
5
10
20
40

exact(PRISM)
1.8e-4 (192.6)
2.0e-4 (262.7)
7.3e-4 (339.2)
1.7e-3 (467.6)

SSP
1.9e-4 (0.93)
2.4e-4 (0.81)
7.8e-4 (1.43)
2.1e-3 (1.17)

greedy
1.8e-4 (0.45)
2.3e-4 (0.56)
8.3e-4 (0.59)
2.9e-3 (0.63)

lasso
1.9e-4 (0.78)
2.4e-4 (0.84)
7.7e-4 (0.91)
1.9e-3 (0.87)

Table 1: Normalized mean-square prediction errors of training and test data by the prismatic algorithm, the supermodular-submodular procedure, the greedy algorithm and the lasso.
(DR2) Delete Tk if the optimal value c? of BILP (5) satisfies c? ? 0.
The feasibility of these rules can be seen from Proposition 1 as well as the D.C. programing prob? = ?, i.e., the prism Tk
lem [11]. That is, (DR1) follows from Proposition 1 that in this case Tk ? D
is infeasible, and (DR2) from Proposition 1 and from the definition of ? that the current best feasible
solution cannot be improved in T .

5 Experimental Results
We first provide illustrations of the proposed algorithm and its solution on toy examples from feature
selection in Section 5.1, and then apply the algorithm to an application of discriminative structure
learning using the UCI repository data in Section 5.2. The experiments below were run on a 2.8
GHz 64-bit workstation using Matlab and IBM ILOG CPLEX ver. 12.1.
5.1 Application to feature selection
We compared the performance and solutions by the proposed prismatic algorithm (PRISM), the
supermodular-submodular procedure (SSP) [21], the greedy method and the LASSO. To this end,
we generated data as follows: Given p, n and k, the design matrix X ? Rn?p is a matrix of i.i.d.
Gaussian components. A feature set J of cardinality k is chosen at random and the weights on the
selected features are sampled from a standard multivariate Gaussian distribution. The weights on
other features are 0. We then take y = Xw + n?1/2 ?Xw?2 ?, where w is the weights on features
and ? is a standard Gaussian vector. In the experiment, we used the trace norm of the submatrix
1
corresponding to J, XJ , i.e., tr(XJT XJ )1/2 . Thus, our problem is minw?Rp 2n
?y ? Xw?22 + ? ?
T
1/2
T
tr(XJ XJ ) , where J is the support of w. Or equivalently, minA?V g(A) + ? ? tr(XA
XA )1/2 ,
2
where g(A) := minwA ?R|A| ?y ? XA wA ? . Since the first term is a supermodular function [4] and
the second is a submodular function, this problem is the D.S. programming problem.
First, the graphs in Figure 2 show the training errors, test errors and computational time versus ? for
PRISM and SSP (for p = 120, n = 150 and k = 10). The values in the graphs are averaged over 20
datasets. For the test errors, we generated another 100 data from the same model and applied the estimated model to the data. And, for all methods, we tried several possible regularization parameters.
From the graphs, we can see the following: First, exact solutions (by PRISM) always outperform
approximate ones (by SSP). This would show the significance of optimizing the submodular-norm.
That is, we could obtain the better solutions (in the sense of prediction error) by optimizing the
objective with the submodular norm more exactly. And, our algorithm took longer especially when
7

Data
Chess
German
Census-income
Hepatitis

Attr.
36
20
40
19

Class
2
2
2
2

exact (PRISM)
96.6 (?0.69)
70.0 (?0.43)
73.2 (?0.64)
86.9 (?1.89)

approx. (SSP)
94.4 (?0.71)
69.9 (?0.43)
71.2 (?0.74)
84.3 (?2.31)

generative
92.3 (?0.79)
69.1 (?0.49)
70.3 (?0.74)
84.2 (?2.11)

Table 2: Empirical accuracy of the classifiers in [%] with standard deviation by the TANs discriminatively learned with PRISM or SSP and generatively learned with a submodular minimization
solver. The numbers in parentheses are computational time in seconds.
? smaller. This would be because smaller ? basically gives a larger size subset (solution). Also,
Table 1 shows normalized-mean prediction errors by the prismatic algorithm, the supermodularsubmodular procedure, the greedy method and the lasso for several k. The values are averaged over
10 datasets. This result also seems to show that optimizing the objective with the submodular norm
exactly is significant in the meaning of prediction errors.
5.2

Application to discriminative structure learning

Our second application is discriminative structure learning using the UCI machine learning repository.2 Here, we used CHESS, GERMAN, CENSUS-INCOME (KDD) and HEPATITIS, which have
two classes. The Bayesian network topology used was the tree augmented naive Bayes (TAN) [22].
We estimated TANs from data both in generative and discriminative manners. To this end, we used
the procedure described in [20] with a submodular minimization solver (for the generative case), and
the one [21] combined with our prismatic algorithm (PRISM) or the supermodular-submodular procedure (SSP) (for the discriminative case). Once the structures have been estimated, the parameters
were learned based on the maximum likelihood method.
Table 2 shows the empirical accuracy of the classifier in [%] with standard deviation for these
datasets. We used the train/test scheme described in [6, 22]. Also, we removed instances with
missing values. The results seem to show that optimizing the EAR measure more exactly could
improve the performance of classification (which would mean that the EAR is significant as the
measure of discriminative structure learning in the sense of classification).

6

Conclusions

In this paper, we proposed a prismatic algorithm for the D.S. programming problem (1), which is the
first exact algorithm for this problem and is a branch-and-bound method responding to the structure
of this problem. We developed the algorithm based on the analogy with the D.C. programming
problem through the continuous relaxation of solution spaces and objective functions with the help
of the Lov?asz extension. We applied the proposed algorithm to several situations of feature selection
and discriminative structure learning using artificial and real-world datasets.
The D.S. programming problem addressed in this paper covers a broad range of applications in
machine learning. In future works, we will develop a series of the presented framework specialized
to the specific structure of each problem. Also, it would be interesting to investigate the extension
of our method to enumerate solutions, which could make the framework more useful in practice.
Acknowledgments
This research was supported in part by JST PRESTO PROGRAM (Synthesis of Knowledge for
Information Oriented Society), JST ERATO PROGRAM (Minato Discrete Structure Manipulation
System Project) and KAKENHI (22700147). Also, we are very grateful to the reviewers for helpful
comments.
2

http://archive.ics.uci.edu/ml/index.html

8

References
[1] F. Bach. Structured sparsity-inducing norms through submodular functions. In Advances in Neural Information Processing Systems 23, pages 118?126, 2010.
[2] J. A. Bilmes. Dynamic Bayesian multinets. In Proc. of the 16th Conf. on Uncertainty in Artificial Intelligence (UAI?00), pages 38?45, 2000.
[3] T. S. Caetano, J. J. McAuley, L. Cheng, Q. V. Le, and A. J. Smola. Learning graph matching. IEEE Trans.
on Pattern Analysis and Machine Intelligence, 31(6):1048?1058, 2009.
[4] A. Das and D. Kempe. Algorithms for subset selection in linear regression. In Proc. of the 40th annual
ACM symp. on Theory of computing (STOC?08), pages 45?54, 2008.
[5] J. Edmonds. Submodular functions, matroids, and certain polyhedra. In R. Guy, H. Hanani, N. Sauer, and
J. Sch?onheim, editors, Combinatorial structures and their applications, pages 69?87, 1970.
[6] N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian network classifier. 29:131?163, 1997.
[7] S. Fujishige. Submodular Functions and Optimization. Elsevier, 2 edition, 2005.
[8] S. Fujishige, T. Hayashi, and S. Isotani. The minimum-norm-point algorithm applied submodular function
minimization and linear programming. Technical report, Research Institute for Mathematical Sciences,
Kyoto University, 2006.
[9] E. Hazan and S. Kale. Beyond convexity: online submodular minimization. In Advances in Neural
Information Processing Systems 22, pages 700?708, 2009.
[10] S. Hoi, R. Jin, J. Zhu, and M. Lyu. Batch mode active learning and its application to medical image
classification. In Proc. of the 23rd Int?l Conf. on Machine learning (ICML?06), pages 417?424, 2006.
[11] R. Horst, T. Q. Phong, Ng. V. Thoai, and J. de Vries. On solving a D.C. programming problem by a
sequence of linear programs. Journal of Global Optimization, 1:183?203, 1991.
[12] R. Horst and H. Tuy. Global Optimization (Deterministic Approaches). Springer, 3 edition, 1996.
[13] T. Ibaraki. Enumerative approaches to combinatorial optimization. In J.C. Baltzer and A.G. Basel, editors,
Annals of Operations Research, volume 10 and 11. 1987.
[14] Y. Kawahara, K. Nagano, K. Tsuda, and J. A. Bilmes. Submodularity cuts and applications. In Advances
in Neural Information Processing Systems 22, pages 916?924. MIT Press, 2009.
[15] A. Krause and V. Cevher. Submodular dictionary selection for sparse representation. In Proc. of the 27th
Int?l Conf. on Machine learning (ICML?10), pages 567?574. Omnipress, 2010.
[16] A. Krause, H. B. McMahan, C. Guestrin, and A. Gupta. Robust submodular observation selection. Journal
of Machine Learning Research, 9:2761?2801, 2008.
[17] L. Lov?asz. Submodular functions and convexity. In A. Bachem, M. Gr?otschel, and B. Korte, editors,
Mathematical Programming ? The State of the Art, pages 235?257. 1983.
[18] K. Murota. Discrete Convex Analysis. Monographs on Discrete Math and Applications. SIAM, 2003.
[19] K. Nagano, Y. Kawahara, and S. Iwata. Minimum average cost clustering. In Advances in Neural Information Processing Systems 23, pages 1759?1767, 2010.
[20] M. Narasimhan and J. A. Bilmes. PAC-learning bounded tree-width graphical models. In Proc. of the
20th Ann. Conf. on Uncertainty in Artificial Intelligence (UAI?04), pages 410?417, 2004.
[21] M. Narasimhan and J. A. Bilmes. A submodular-supermodular procedure with applications to discriminative structure learning. In Proc. of the 21st Ann. Conf. on Uncertainty in Artificial Intelligence (UAI?05),
pages 404?412, 2005.
[22] F. Pernkopf and J. A. Bilmes. Discriminative versus generative parameter and structure learning of
bayesian network classifiers. In Proc. of the 22nd Int?l Conf. on Machine Learning (ICML?05), pages
657?664, 2005.
[23] M. Queyranne. Minimizing symmetric submodular functions. Math. Prog., 82(1):3?12, 1998.
[24] C. Rother, T. Minka, A. Blake, and V. Kolmogorov. Cosegmentation of image pairs by histogram
matching-incorporating a global constraint into mrfs. In Proc. of the 2006 IEEE Comp. Soc. Conf. on
Computer Vision and Pattern Recognition (CVPR?06), pages 993?1000, 2006.
[25] A. Shekhovtsov. Supermodular decomposition of structural labeling problem. Control Systems and Computers, 20(1):39?48, 2006.
[26] A. Shekhovtsov, V. Kolmogorov, P. Kohli, V. Hlav c, C. Rother, and P. Torr. Lp-relaxation of binarized
energy minimization. Technical Report CTU-CMP-2007-27, Czech Technical University, 2007.
[27] M. Thoma, H. Cheng, A. Gretton, H. Han, H. P. Kriegel, A. J. Smola, S. Y. Le Song Philip, X. Yan, and
K. Borgwardt. Near-optimal supervised feature selection among frequent subgraphs. In Proc. of the 2009
SIAM Conf. on Data Mining (SDM?09), pages 1076?1087, 2008.

9

"
2161,2006,Non-rigid point set registration: Coherent Point Drift,Abstract Missing,"Non-rigid point set registration: Coherent Point Drift
? Carreira-Perpin?
? an
Andriy Myronenko
Xubo Song
Miguel A.
Department of Computer Science and Electrical Engineering
OGI School of Science and Engineering
Oregon Health and Science University
Beaverton, OR, USA, 97006
{myron, xubosong, miguel}@csee.ogi.edu

Abstract
We introduce Coherent Point Drift (CPD), a novel probabilistic method for nonrigid registration of point sets. The registration is treated as a Maximum Likelihood (ML) estimation problem with motion coherence constraint over the velocity field such that one point set moves coherently to align with the second set.
We formulate the motion coherence constraint and derive a solution of regularized
ML estimation through the variational approach, which leads to an elegant kernel
form. We also derive the EM algorithm for the penalized ML optimization with
deterministic annealing. The CPD method simultaneously finds both the non-rigid
transformation and the correspondence between two point sets without making
any prior assumption of the transformation model except that of motion coherence. This method can estimate complex non-linear non-rigid transformations,
and is shown to be accurate on 2D and 3D examples and robust in the presence of
outliers and missing points.

1 Introduction
Registration of point sets is an important issue for many computer vision applications such as robot navigation, image guided surgery, motion tracking, and face recognition. In fact, it is the key
component in tasks such as object alignment, stereo matching, point set correspondence, image
segmentation and shape/pattern matching. The registration problem is to find meaningful correspondence between two point sets and to recover the underlying transformation that maps one point
set to the second. The ?points? in the point set are features, most often the locations of interest points
extracted from an image. Other common geometrical features include line segments, implicit and
parametric curves and surfaces. Any geometrical feature can be represented as a point set; in this
sense, the point locations is the most general of all features.
Registration techniques can be rigid or non-rigid depending on the underlying transformation model.
The key characteristic of a rigid transformation is that all distances are preserved. The simplest nonrigid transformation is affine, which also allows anisotropic scaling and skews. Effective algorithms
exist for rigid and affine registration. However, the need for more general non-rigid registration
occurs in many tasks, where complex non-linear transformation models are required. Non-linear
non-rigid registration remains a challenge in computer vision.
Many algorithms exist for point sets registration. A direct way of associating points of two arbitrary
patterns is proposed in [1]. The algorithm exploits properties of singular value decomposition and
works well with translation, shearing and scaling deformations. However, for a non-rigid transformation, the method performs poorly. Another popular method for point sets registration is the
Iterative Closest Point (ICP) algorithm [2], which iteratively assigns correspondence and finds the
least squares transformation (usually rigid) relating these point sets. The algorithm then redetermines the closest point set and continues until it reaches the local minimum. Many variants of ICP

have been proposed that affect all phases of the algorithm from the selection and matching of points
to the minimization strategy [3]. Nonetheless ICP requires that the initial pose of the two point sets
be adequately close, which is not always possible, especially when transformation is non-rigid [3].
Several non-rigid registration methods are introduced [4, 5]. The Robust Point Matching (RPM)
method [4] allows global to local search and soft assignment of correspondences between two point
sets. In [5] it is further shown that the RPM algorithm is similar to Expectation Maximization
(EM) algorithms for the mixture models, where one point set represents data points and the other
represents centroids of mixture models. In both papers, the non-rigid transform is parameterized by
Thin Plate Spline (TPS) [6], leading to the TPS-RPM algorithm [4]. According to regularization
theory, the TPS parametrization is a solution of the interpolation problem in 2D that penalizes the
second order derivatives of the transformation. In 3D the solution is not differentiable at point
locations. In four or higher dimensions the generalization collapses completely [7]. The M-step in
the EM algorithm in [5] is approximated for simplification. As a result, the approach is not truly
probabilistic and does not lead, in general, to the true Maximum Likelihood solution.
A correlation-based approach to point set registration is proposed in [8]. Two data sets are represented as probability densities, estimated using kernel density estimation. The registration is considered as the alignment between the two distributions that minimizes a similarity function defined by
L2 norm. This approach is further extended in [9], where both densities are represented as Gaussian
Mixture Models (GMM). Once again thin-plate spline is used to parameterize the smooth non-linear
underlying transformation.
In this paper we introduce a probabilistic method for point set registration that we call the Coherent
Point Drift (CPD) method. Similar to [5], given two point sets, we fit a GMM to the first point set,
whose Gaussian centroids are initialized from the points in the second set. However, unlike [4, 5, 9]
which assumes a thin-plate spline transformation, we do not make any explicit assumption of the
transformation model. Instead, we consider the process of adapting the Gaussian centroids from
their initial positions to their final positions as a temporal motion process, and impose a motion
coherence constraint over the velocity field. Velocity coherence is a particular way of imposing
smoothness on the underlying transformation. The concept of motion coherence was proposed in
the Motion Coherence Theory [10]. The intuition is that points close to one another tend to move
coherently. This motion coherence constraint penalizes derivatives of all orders of the underlying
velocity field (thin-plate spline only penalizes the second order derivative). Examples of velocity
fields with different levels of motion coherence for different point correspondence are illustrated in
Fig. 1.

(a)

(b)

(c)

(d)

Figure 1: (a) Two given point sets. (b) A coherent velocity field. (c, d) Velocity fields that are less
coherent for the given correspondences.
We derive a solution for the velocity field through a variational approach by maximizing the likelihood of GMM penalized by motion coherence. We show that the final transformation has an elegant
kernel form. We also derive an EM algorithm for the penalized ML optimization with deterministic
annealing. Once we have the final positions of the GMM centroids, the correspondence between
the two point sets can be easily inferred through the posterior probability of the Gaussian mixture
components given the first point set. Our method is a true probabilistic approach and is shown to
be accurate and robust in the presence of outliers and missing points, and is effective for estimation
of complex non-linear non-rigid transformations. The rest of the paper is organized as follows. In
Section 2 we formulate the problem and derive the CPD algorithm. In Section 3 we present the
results of CPD algorithm and compare its performance with that of RPM [4] and ICP [2]. In Section
4 we summarize the properties of CPD and discuss the results.

2 Method
Assume two point sets are given, where the template point set Y = (y 1 , . . . , yM )T (expressed as a
M ? D matrix) should be aligned with the reference point set X = (x1 , . . . , xN )T (expressed as a
N ? D matrix) and D is the dimension of the points. We consider the points in Y as the centroids
of a Gaussian Mixture Model, and fit it to the data points X by maximizing the likelihood function.
We denote Y0 as the initial centroid positions and define a continuous velocity function v for the
template point set such that the current position of centroids is defined as Y = v(Y 0 ) + Y0 .
PM
1
Consider a Gaussian-mixture density p(x) = m=1 M
p(x|m) with x|m ? N (ym , ? 2 ID ), where
Y represents D-dimensional centroids of equally-weighted Gaussians with equal isotropic covariance matrices, and X set represents data points. In order to enforce a smooth motion constraint, we
define the prior p(Y|?) ? exp (? ?2 ?(Y)), where ? is a weighting constant and ?(Y) is a function
that regularizes the motion to be smooth. Using Bayes theorem, we want to find the parameters Y by
maximizing the posteriori probability, or equivalently by minimizing the following energy function:
E(Y) = ?

N
X

log

n=1

M
X

e? 2 k
1

xn ?ym
?

m=1

k2 + ? ?(Y)
2

(1)

We make the i.i.d. data assumption and ignore terms independent of Y. Equation 1 has a similar
form to that of Generalized Elastic Net (GEN) [11], which has shown good performance in nonrigid image registration [12]; note that there we directly penalized Y, while here we penalize the
transformation v. The ? function represents our prior knowledge about the motion, which should be
smooth. Specifically, we want the velocity field v generated by template point set displacement to
be smooth. According to [13], smoothness is a measure of the ?oscillatory? behavior of a function.
Within the class of differentiable functions, one function is said to be smoother than another if it
oscillates less; in other words, if it has less energy at high frequency. The high frequency content of
a function can be measured by first high-pass filtering
the function, and then measuring the resulting
R
? ds, where v? indicates the Fourier
v (s)|2 /G(s)
power. This can be represented as ?(v) = Rd |?
?
transform of the velocity and G is some positive function that approaches zero as ksk ? ?. Here
? represents a symmetric low-pass filter, so that its Fourier transform G is real and symmetric.
G
Following this formulation, we rewrite the energy function as:

E(?
v) = ?

N
X

n=1

log

M
X

e? 2 k

m=1

1

xn ?ym
?

k2 + ?
2

Z

Rd

|?
v (s)|2
ds
?
G(s)

(2)

It can be shown using a variational approach (see Appendix A for a sketch of the proof) that the
function which minimizes the energy function in Eq. 2 has the form of the radial basis function:

v(z) =

M
X

wm G(z ? y0m )

(3)

m=1

We choose a Gaussian kernel form for G (note it is not related to the Gaussian form of the distribution chosen for the mixture model). There are several motivations for such a Gaussian choice:
? approaches zero as
First, it satisfies the required properties (symmetric, positive definite, and G
ksk ? ?). Second, a Gaussian low pass filter has the property of having the Gaussian form in
both frequency and time domain without oscillations. By choosing an appropriately sized Gaussian filter we have the flexibility to control the range of filtered frequencies and thus the amount of
spatial smoothness. Third, the choice of the Gaussian makes our regularization
to
R term equivalent
? ds,
the one in Motion Coherence Theory (MCT) [10]. The regularization term Rd |?
v (s)|2 /G(s)
with a Gaussian function for G, is equivalent to the sum of weighted squares of all order derivatives
R P
? 2m
m 2
of the velocity field Rd ?
m=1 m!2m (D v) [10, 13] , where D is a derivative operator so that
2m
2m
2m+1
2m
D v = ? v and D
v = ?(? v). The equivalence of the regularization term with that
of the Motion Coherence Theory implies that we are imposing motion coherence among the points
and thus we call our method the Coherent Point Drift (CPD) method. Detailed discussion of MCT
can be found in [10]. Substituting the solution obtained in Eq. 3 back into Eq. 2, we obtain

CPD algorithm:
? Initialize parameters ?, ?, ?
? Construct G matrix, initialize Y = Y0
? Deterministic annealing:
? EM optimization, until convergence:
? E-step: Compute P
? M-step: Solve for W from Eq. 7
? Update Y = Y0 + GW
? Anneal ? = ??
? Compute the velocity field: v(z) = G(z, ?)W
Figure 2: Pseudo-code of CPD algorithm.

E(W) = ?

N
X

log

n=1

M
X

e

?
?
? xn ?y0m ?PM wk G(y0k ?y0m ) ?2
k=1
?
? 12 ?
?
?
?

m=1

+


?
tr WT GW
2

where GM ?M is a square symmetric Gram matrix with elements gij = e
WM ?D = (w1 , . . . , wM )T is a matrix of the Gaussian kernel weights in Eq. 3.

? y ?y ?2
?
?
? 12 ? 0i ? 0j ?

(4)
and

Optimization. Following the EM algorithm derivation for clustering using Gaussian Mixture Model
[14], we can find the upper bound of the function in Eq. 4 as (E-step):
Q(W) =

N X
M
X

n=1 m=1

P old (m|xn )

2


?
kxn ? y0m ? G(m, ?)Wk
+ tr WT GW
2? 2
2

(5)

where P old denotes the posterior probabilities calculated using previous parameter values, and
G(m, ?) denotes the mth row of G. Minimizing the upper bound Q will lead to a decrease in the
value of the energy function E in Eq. 4, unless it is already at local minimum. Taking the derivative
of Eq. 5 with respect to W, and rewriting the equation in matrix form, we obtain (M-step)
?Q
1
= 2 G(diag (P1))(Y0 + GW) ? PX) + ?GW = 0
(6)
?
?2
?
?2
?W
?
? old
?
? old
?
1 ? ym ?xn ?
1 ? ym ?xn ?
?2?
?2?
PM
?
?
?
?
/ m=1 e
.
where P is a matrix of posterior probabilities with pmn = e
The diag (?) notation indicates diagonal matrix and 1 is a column vector of all ones. Multiplying
Eq. 6 by ? 2 G?1 (which exists for a Gaussian kernel) we obtain a linear system of equations:
(diag (P1)) G + ?? 2 I)W = PX ? diag (P1) Y0

(7)

Solving the system for W is the M-step of EM algorithm. The E step requires computation of the
posterior probability matrix P. The EM algorithm is guaranteed to converge to a local optimum
from almost any starting point. Eq. 7 can also be obtained directly by finding the derivative of Eq. 4
with respect to W and equating it to zero. This results in a system of nonlinear equations that can
be iteratively solved using fixed point update, which is exactly the EM algorithm shown above. The
computational complexity of each EM iteration is dominated by the linear system of Eq. 7, which
takes O(M 3 ). If using a truncated Gaussian kernel and/or linear conjugate gradients, this can be
reduced to O(M 2 ).
Robustness to Noise. The use of a probabilistic assignment of correspondences between point sets is
innately more robust than the binary assignment used in ICP. However, the GMM requires that each
data point be explained by the model. In order to account for outliers, we add an additional uniform
pdf component to the mixture model. This new
component
changes posterior probability
matrix P
?
?
?
?2
? yold ?xn ?2
? old
?
1 ? ym ?xn ?
m
?
2 D
? 21 ?
?
P
2
?
2?
?
?
?
?
M
in Eq. 7, which now is defined as pmn = e
/( (2??a ) + m=1 e
), where
a defines the support for the uniform pdf. The use of the uniform distribution greatly improves the
noise.
Free Parameters. There are three free parameters in the method: ?, ? and ?. Parameter ? represents
the trade off between data fitting and smoothness regularization. Parameter ? reflects the strength

of interaction between points. Small values of ? produce locally smooth transformation, while large
values of ? correspond to nearly pure translation transformation. The value of ? serves as a capture
range for each Gaussian mixture component. Smaller ? indicates smaller and more localized capture
range for each Gaussian component in the mixture model. We use deterministic annealing for ?,
starting with a large value and gradually reducing it according to ? = ??, where ? is annealing rate
(normally between [0.92 0.98]), so that the annealing process is slow enough for the algorithm to be
robust. The gradual reducing of ? leads to a coarse-to-fine match strategy. We summarize the CPD
algorithm in Fig. 2.

3 Experimental Results
We show the performance of CPD on artificial data with non-rigid deformations. The algorithm is
implemented in Matlab, and tested on a Pentium4 CPU 3GHz with 4GB RAM. The code is available
at www.csee.ogi.edu/?myron/matlab/cpd. The initial value of ? and ? are set to 1.0 in
all experiments. The starting value of ? is 3.0 and gradually annealed with ? = 0.97. The stopping
condition for the iterative process is either when the current change in parameters drops below a
threshold of 10?6 or the number of iterations reaches the maximum of 150.
CPD algorithm

RPM algorithm

ICP algorithm

Figure 3: Registration results for the CPD, RPM and ICP algorithms from top to bottom. The first
column shows template (?) and reference (+) point sets. The second column shows the registered
position of the template set superimposed over the reference set. The third column represents the
recovered underlying deformation . The last column shows the link between initial and final template
point positions (only every second point?s displacement is shown).
On average the algorithm converges in few seconds and requires around 80 iterations. All point sets
are preprocessed to have zero mean and unit variance (which normalizes translation and scaling).
We compare our method on non-rigid point registration with RPM and ICP. The RPM and ICP
implementations and the 2D point sets used for comparison are taken from the TPS-RPM Matlab
package [4].
For the first experiment (Fig. 3) we use two clean point sets. Both CPD and RPM algorithms produce
accurate results for non-rigid registration. The ICP algorithm is unable to escape a local minimum.
We show the velocity field through the deformation of a regular grid. The deformation field for RPM
corresponds to parameterized TPS transformation, while that for CPD represents a motion coherent
non-linear deformation. For the second experiment (Fig. 4) we make the registration problem more
challenging. The fish head in the reference point set is removed, and random noise is added. In
the template point set the tail is removed. The CPD algorithm shows robustness even in the area of

missing points and corrupted data. RPM incorrectly wraps points to the middle of the figure. We
have also tried different values of smoothness parameters for RPM without much success, and we
only show the best result. ICP also shows poor performance and is stuck in a local minimum.
For the 3D experiment (Fig. 5) we show the performance of CPD on 3D faces. The face surface is
defined by the set of control points. We artificially deform the control point positions non-rigidly
and use it as a template point set. The original control point positions are used as a reference point
set. CPD is effective and accurate for this 3D non-rigid registration problem.
CPD algorithm

RPM algorithm

ICP algorithm

Figure 4: The reference point set is corrupted to make the registration task more challenging. Noise
is added and the fish head is removed in the reference point set. The tail is also removed in the
template point set. The first column shows template (?) and reference (+) point sets. The second
column shows the registered position of the template set superimposed over the reference set. The
third column represents the recovered underlying deformation. The last column shows the link
between the initial and final template point positions.

4 Discussion and Conclusion
We intoduce Coherent Point Drift, a new probabilistic method for non-rigid registration of two point
sets. The registration is considered as a Maximum Likelihood estimation problem, where one point
set represents centroids of a GMM and the other represents the data. We regularize the velocity
field over the points domain to enforce coherent motion and define the mathematical formulation
of this constraint. We derive the solution for the penalized ML estimation through the variational
approach, and show that the final transformation has an elegant kernel form. We also derive the
EM optimization algorithm with deterministic annealing. The estimated velocity field represents the
underlying non-rigid transformation. Once we have the final positions of the GMM centroids, the
correspondence between the two point sets can be easily inferred through the posterior probability of

(a)

(b)

(c)

4

3

3
2.5

2.5
3

2
1.5

2
1.5

2

1

1
1

0.5

0.5

0

0

0

?0.5
?1

?0.5
?1

?1

?1.5
?2
?1.5

?1

?0.5

0

0.5

1

1.5

(d)

?2

?1

0

?1.5

2

1

2

2

?2
?1.5

?1

?0.5

0

?2
?1.5

0
0.5

1

1.5

(e)

?2

?1

?0.5

0

0

0.5

1

(f)

1.5

?2

Figure 5: The results of CPD non-rigid registration on 3D point sets. (a, d) The reference face and
its control point set. (b, e) The template face and its control point set. (c, f) Result obtained by
registering the template point set onto the reference point set using CPD.
the GMM components given the data. The computational complexity of CPD is O(M 3 ), where M
is the number of points in template point set. It is worth mentioning that the components in the point
vector are not limited to spatial coordinates. They can also represent the geometrical characteristic
of an object (e.g., curvature, moments), or the features extracted from the intensity image (e.g.,
color, gradient). We compare the performance of the CPD algorithm on 2D and 3D data against ICP
and RPM algorithms, and show how CPD outperforms both methods in the presence of noise and
outliers. It should be noted that CPD does not work well for large in-plane rotation. Typically such
transformation can be first compensated by other well known global registration techniques before
CPD algorithm is carried out. The CPD method is most effective when estimating smooth non-rigid
transformations.

Appendix A
E=?

N
X

log

n=1

M
X

e

m
? 12 k xn ?y
k
?

2

m=1

?
+
2

Z

Rd

|?
v (s)|2
ds
?
G(s)

(8)

Consider the function in Eq. 8, where ym = y0m + v(y0m ),R and y0m is the initial position of ym
point. v is a continuous velocity function and v(y0m ) = Rd v?(s)e2?i<y0m ,s> ds in terms of its
Fourier transform v?. The following derivation follows [13]. Substituting v into equation Eq. 8 we
obtain:

E(?
v) = ?

N
X

log

n=1

M
X

e

?
? xn ?y0m ?R d
R
? 12 ?
?

?2
?
?

v
?(s)e2?i<y0m ,s> ds ?
?

m=1

+

?
2

Z

Rd

|?
v (s)|2
ds
?
G(s)

(9)

In order to find the minimum of this functional we take its functional derivatives with respect to v?,
v)
d
so that ?E(?
??
v (t) = 0, ?t ? R :

N
X
?E(?
v)
=?
??
v (t)
n=1

?
2

Z

Rd

PM

m=1

e? 2 k
1

N
X
? |?
v (s)|2
ds = ?
?
??
v (t) G(s)
n=1

xn ?ym
?

2

k

1
? 2 (xn

PM

m=1

e? 2 k
1

m=1

PM

? ym )

e? 2 k ?
PM
1

R

??
v (s) 2?i<y0m ,s>
ds
v (t) e
Rd ??

xn ?ym
?

xn ?ym

m=1

k

k2

+

2

e

1
2?i<y0 ,t>
? 2 (xn ? ym )e
m 2
? 12 k xn ?y
k
?

+?

v?(?t)
=0
?
G(t)

We now define the coefficients amn =

?1
2

2

m
k xn ?y
k
?

PM

m=1

tive as:

?1
e 2

k

1
(xn ?ym )
?2
xn ?ym 2
?

k

, and rewrite the functional deriva-

M X
N
X
v?(?t)
v?(?t)
=?
(
amn )e2?i<y0m ,t> + ?
= 0 (10)
?
?
G(t)
G(t)
n=1 m=1
m=1 n=1
PN
?
Denoting the new coefficients wm = ?1 n=1 amn , and changing t to ?t, we multiply by G(t)
on
both sides of this equation, which results in:

?

N X
M
X

e

amn e2?i<y0m ,t> + ?

?
v?(t) = G(?t)

M
X

wm e?2?i<y0m ,t>

(11)

m=1

? is symmetric (so that its Fourier transform is real), and taking the inverse Fourier
Assuming that G
transform of the last equation, we obtain:
M
M
X
X
v(z) = G(z) ?
wm ?(z ? y0m ) =
wm G(z ? y0m )
(12)
m=1

m=1

Since wm depend on v through amn and ym , the wm that solve Eq. 12 must satisfy a self consistency
? results in a specific basis function G.
equation equivalent to Eq. 7. A specific form of regularizer G

Acknowledgment
This work is partially supported by NIH grant NEI R01 EY013093, NSF grant IIS?0313350
?
(awarded to X. Song) and NSF CAREER award IIS?0546857 (awarded to Miguel A.
Carreira-Perpi?na? n).

References
[1] G.L. Scott and H.C. Longuet-Higgins. An algorithm for associating the features of two images. Royal
Society London Proc., B-244:21?26, 1991.
[2] P.J. Besl and N. D. McKay. A method for registration of 3-d shapes. IEEE Trans. Pattern Anal. Mach.
Intell., 14(2):239?256, 1992.
[3] S. Rusinkiewicz and M. Levoy. Efficient variants of the ICP algorithm. Third International Conference
on 3D Digital Imaging and Modeling, page 145, 2001.
[4] H Chui and A. Rangarajan. A new algorithm for non-rigid point matching. CVPR, 2:44?51, 2000.
[5] H. Chui and A. Rangarajan. A feature registration framework using mixture models. IEEE Workshop on
Mathematical Methods in Biomedical Image Analysis (MMBIA), pages 190?197, 2000.
[6] F. L. Bookstein. Principal warps: Thin-plate splines and the decomposition of deformations. IEEE Trans.
Pattern Anal. Mach. Intell., 11(6):567?585, 1989.
[7] R Sibson and G. Stone. Comp. of thin-plate splines. SIAM J. Sci. Stat. Comput., 12(6):1304?1313, 1991.
[8] Y. Tsin and T. Kanade. A correlation-based approach to robust point set registration. ECCV, 3:558?569,
2004.
[9] B. Jian and B.C. Vemuri. A robust algorithm for point set registration using mixture of gaussians. ICCV,
pages 1246?1251, 2005.
[10] A.L. Yuille and N.M. Grzywacz. The motion coherence theory. Int. J. Computer Vision, 3:344?353, 1988.
? Carreira-Perpi?na? n, P. Dayan, and G. J. Goodhill. Differential priors for elastic nets. In Proc. of the
[11] M. A.
6th Int. Conf. Intelligent Data Engineering and Automated Learning (IDEAL?05), pages 335?342, 2005.
? Carreira-Perpi?na? n. Non-parametric image registration using general[12] A. Myronenko, X Song, and M. A.
ized elastic nets. Int. Workshop on Math. Foundations of Comp. Anatomy: Geom. and Stat. Methods in
Non-Linear Image Registration, MICCAI, pages 156?163, 2006.
[13] F. Girosi, M. Jones, and T. Poggio. Regularization theory and neural networks architectures. Neural
Computation, 7(2):219?269, 1995.
[14] C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, 1995.

"
291,1996,Learning Temporally Persistent Hierarchical Representations,Abstract Missing,"Learning temporally persistent
hierarchical representations

Suzanna Becker
Department of Psychology
McMaster University
Hamilton, Onto L8S 4K1
becker@mcmaster.ca

Abstract
A biologically motivated model of cortical self-organization is proposed. Context is combined with bottom-up information via a
maximum likelihood cost function. Clusters of one or more units
are modulated by a common contextual gating Signal; they thereby
organize themselves into mutually supportive predictors of abstract
contextual features. The model was tested in its ability to discover
viewpoint-invariant classes on a set of real image sequences of centered, gradually rotating faces. It performed considerably better
than supervised back-propagation at generalizing to novel views
from a small number of training examples.

1

THE ROLE OF CONTEXT

The importance of context effects l in perception has been demonstrated in many
domains. For example, letters are recognized more quickly and accurately in the
context of words (see e.g. McClelland & Rumelhart, 1981), words are recognized
more efficiently when preceded by related words (see e.g. Neely, 1991), individual
speech utterances are more intelligible in the context of continuous speech, etc. Further, there is mounting evidence that neuronal responses are modulated by context.
For example, even at the level of the LGN in the thalamus, the primary source of
visual input to the cortex, Murphy & Sillito (1987) have reported cells with ""endstopped"" or length-tuned receptive fields which depend on top-down inputs from
the cortex. The end-stopped behavior disappears when the top-down connections
are removed, suggesting that the cortico-thalamic connections are providing contextual modulation to the LGN. Moving a bit higher up the visual hierarchy, von der
Heydt et al. (1984) found cells which respond to ""illusory contours"", in the absence
of a contoured stimulus within the cells' classical receptive fields. These examples demonstrate that neuronal responses can be modulated by secondary sources
of information in complex ways, provided the information is consistent with their
expected or preferred input.
1 We use the term context rather loosely here to mean any secondary source of input.
It could be from a different sensory modality, a different input channel within the same
modality, a temporal history of the input, or top-down information.

Learning Temporally Persistent Hierarchical Representations

825

Figure 1: Two sequences of 48 by 48 pixel images digitized with an IndyCam and preprocessed with a Sobel edge filter. Eleven views of each of four to ten faces were used in the
simulations reported here. The alternate (odd) views of two of the faces are shown above.

Why would contextual modulation be such a pervasive phenomenon? One obvious
reason is that if context can influence processing, it can help in disambiguating or
cleaning up a noisy stimulus. A less obvious reason may be that if context can
influence learning, it may lead to more compact representations, and hence a more
powerful processing system. To illustrate, consider the benefits of incorporating
temporal history into an unsupervised classifier. Given a continuous sensory signal
as input, the classifier must try to discover important partitions in its training
data. If it can discover features that are temporally persistent, and thus insensitive
to transformations in the input, it should be able to represent the signal compactly
with a small set offeatures. FUrther, these features are more likely to be associated
with the identity of objects rather than lower-level attributes.
However, most classifiers group patterns together on the basis of spatial overlap.
This may be reasonable if there is very little shift or other form of distortion between
one time step and the next, but is not a reasonable assumption about the sensory
input to the cortex. Pre-cortical stages of sensory processing, certainly in the visual
system (and probably in other modalities), tend to remove low-order correlations in
space and time, e.g. with centre-surround filters. Consider the image sequences of
gradually rotating faces in Figure 1. They have been preprocessed by a simple edgefilter, so that successive views of the same face have relatively little pixel overlap. In
contrast, identical views of different faces may have considerable overlap. Thus, a
classifier such as k-means, which groups patterns based on their Euclidean distance,
would not be expected to do well at classifying these patterns. So how are people
(and in fact very young children) able to learn to classify a virtually infinite number
of objects based on relatively brief exposures? It is argued here that the assumption
of temporal persistence is a powerful constraining factor for achieving this, and is
one which may be used to advantage in artificial neural networks as well. Not only
does it lead to the development of higher-order feature analyzers, but it can result in
more compact codes which are important for applications like image compression.
Further, as the simulations reported here show, improved generalization may be
achieved by allowing high-level expectations (e.g. of class labels) to influence the
development of lower-level feature detectors.

2

THE MODEL

Competitive learning (for a review, see Becker & Plumbley, 1996) is considered
by many to be a reasonably strong candidate model of cortical learning. It can
be implemented, in its simplest form, by a Hebbian learning rule in a network

S. Becker

826

with lateral inhibition. However, a major limitation of competitive learning, and
the majority of unsupervised learning procedures (but see the Discussion section), is
that they treat the input as a set of independent identically distributed (iid) samples.
They fail to take into account context. So they are unable to take advantage of the
temporal continuity in signals. In contrast, real sensory signals may be better viewed
as discretely sampled, continuously varying time-series rather than iid samples.
The model described here extends maximum likelihood competitive learning
(MLCL) (Nowlan, 1990) in two important ways: (i) modulation by context, and
(ii) the incorporation of several ""canonical features"" of neocortical circuitry. The
result is a powerful framework for modelling cortical self-organization.
MLCL retains the benefits of competitive learning mentioned above. Additionally,
it is more easily extensible because it maximizes a global cost function:
L

=

t, [t, ~iYi(a)1

(1)

log

where the 7r/s are positive weighting coefficients which sum to one, and the
the clustering unit activations:

y/ a )

N(fl a ), Wi, ~i)

Yi'S

are
(2)

where j(a) is the input vector for pattern a, and NO is the probability of j(a) under
a Gaussian centred on the ith unit's weight vector, Wi, with covariance matrix
2: i . For simplicity, Nowlan used a single global variance parameter for all input
dimensions, and allowed it to shrink during learning. MLCL actually maximizes
the log likelihood (L) of the data under a mixture of Gaussians model, with mixing
proportions equal to the 7r'S. L can be maximized by online gradient ascent 2 with
learning rate E:
D..Wij

=

E

()L

=

E ""'""

~

()Wij

7ri Yi(a)
L:k 7rk Yk(a)

(I/ a ) -

Wij)

(3)

Thus, we have a Hebbian update rule with normalization of post-synaptic unit
activations (which could be accomplished by shunting inhibition) and weight decay.
2.1 Contextual modulation
To integrate a contextual information source into MLCL, our first extension is to
replace the mixing proportions (7r/s) by the outputs of contextual gating units (see
Figure 2). Now the 7r/s are computed by separate processing units receiving their
own separate stream of input, the ""context"". The role of the gating signals here
is analagous to that of the gating network in the (supervised) ""competing experts""
model (Jacobs et al., 1991),3 For the network shown in Figure 2, the context is
simply a time-delayed version of the outputs of a module (explained in the next subsection). However, more general forms of context are possible (see Discussion) . In
the simulations reported here, the context units computed their outputs according
to a softmax function of their weighted summed inputs Xi:
(a) _

7r .
Z

ex;(a)

- ---..,.--:L: j eXj(a)

(4)

We refer to the action of the gating units (the 7r/s) as modulatory because of the
2Nowlan (1990) used a slightly different online weight update rule that more closely
approximates the batch update rule of the EM algorithm (Dempster et al., 1977)
3 However , in the competing experts architecture, both the experts and gating network
receive a common source of input. The competing experts model could be thought of as
fitting a mixture model of the training signal.

827

Learning Temporally Persistent Hierarchical Representations

~~~~~
....ta

(f)

I....... .nta

Figure 2: The architecture used in the simulations reported here. Except where indicated,
the gating units received all their inputs across unit delay lines with fixed weights of 1. o.

multiplicative effect they have on the activities of the clustering units (the y/s).
This multiplicative interaction is built into the cost function (Equation 1), and
consequently, arises in the learning rule (Equation 3). Thus, clustering units are
encouraged to discover features that agree with the current context signal they
receive. If their context signal is weak or if they fail to capture enough of the
activation relative to the other clustering units, they will do very little learning.
Only if a unit's weight vector is sufficiently close to the current input vector and
it's corresponding gating unit is strongly active will it do substantial learning.
2.2 Modular, hierarchical architecture
Our second modification to MLCL is required to apply it to the architecture shown
in Figure 2, which is motivated by several ubiquitous features of the neocortex: a
laminar structure, and a functional organization into ""cortical clusters"" of spatially
nearby columns with similar receptive field properties (see e.g. Calvin, 1995). The
cortex, when flattened out, is like a large six-layered sheet. As Calvin (1995, pp.
269) succinctly puts it, "" ... the bottom layers are like a subcortical 'out' box, the
middle layer like an 'in' box, and the superficial layers somewhat like an 'interoffice' box connecting the columns and different cortical areas"". The middle and
superficial layer cells are analagous to the first-layer clustering units and gating
units respectively. Thus, we propose that the superficial cells may be providing the
contextual modulation. (The bottom layers are mainly involved in motor output
and are not included in the present model.) To induce a functional modularity in
our model analogous to cortical clusters, clustering units within the same module
receive a shared gating signal. The cost function and learning rule are now:
n
[m
1 I
~
log ~ 1r~a) l ~Yi/a)

L

=

~
E

i

1r(a) Yi .(a)

L..J

2: (~)

a

q1rq

1

(
(a)

(5)
)

Ik(a) -Wijk

(6)

rYqr

Thus, units in the same module form predictions y~j) of the same contextual feature

1r~a). Fortunately, there is a disincentive to all of them discovering identical weights:
they would then do poorly at modelling the input.

3

EXPERIMENTS

As a simple test of this model, it was first applied to a set of image sequences of
four centered, gradually rotating faces (see Figure 1), divided into training and test

828

S. Becker

no context, 4 faces:
context, 4 faces:
context, 10 faces:

Layer
Layer
Layer
Layer
Layer

1
1
2
1
2

Training Set
59.2 (2.4)
88.4 (3.9)
88.8 (4.0)
96.3 (1.2)
91.8 (2.4)

Test Set
65 (3.5)
74.5 (4.2)
72.7 (4.8)
71.0 (3.0)
70.2 (4.3)

Table 1: Mean percent (and standard error) correctly classified faces , across 10 runs,
for unsupervised clustering networks trained for 2000 iterations with a learning rate of
0.5, with and without temporal context. Layer 1: clustering units . Layer 2: gating units.
Performance was assessed as follows: Each unit was assigned to predict the face class for
which it most frequently won (was the most active). Then for each pattern, the layer's
activity vector was counted as correct if the winner correctly predicted the face identity.

sets by taking alternating views. It was predicted that the clustering units should
discover ""features"" such as individual views of specific faces. Further, different views
of the same face should be clustered together within a module because they will be
observed in the same temporal context, while the gating units should discover the
identity of faces, independent of viewpoint.
First, the baseline effect of the temporal context on clustering performance was
assessed by comparing the network shown in Figure 2 to the same network with the
input connections to the gating layer removed. The latter is equivalent to MLCL
with fixed, equal 7ri'S . The results are summarized in Table 1. As predicted, the
temporal context provides incentive for the clustering units to group successive
instances of the same face together, and the gating layer can therefore do very well
at classifying the faces with a much smaller number of units - i.e., independently of
viewpoint. In contrast, the clustering units without the contextual signal are more
likely to group together similar views of different people's faces .
Next, to explore the scaling properties of the model, a network like the one shown
in Figure 2 but with 10 modules was presented with a set of 10 faces, 11 views each.
As before, the odd-numbered views were trained on and the even-numbered views
were tested on. To achieve comparable performance to the smaller network, the
weights on the self-pointing connections on the gating units were increased from 1.0
to 3.0, which increased the time constant of temporal agveraging. The model then
had no difficulty scaling up to the larger training set size, as shown in Table 1.
Based on the unexpected success of this model, it's classification performance was
then compared against supervised back-propagation networks on the four face sequences. The first supervised network we tried was a simple recurrent network with
essentially the same architecture: one layer of Gaussian units followed by one layer
of recurrent soft max units with fixed delay lines. Over ten runs of each model,
although the unsupervised classifier did worse on the training set (it averaged 88%
while the supervised model always scored 100% correct), it outperformed the supervised model in its generalization ability by a considerable margin (it averaged
73% while the supervised model averaged 45% correct) .
Finally, a feedforward back-propagation network with sigmoid units was trained.
The following constraint on the hidden layer activations, hj(t): 4
hidden state cost = ,\ l:)hj(t) - hj(t - 1?2
j
4 As Geoff Hinton pointed out, the above constraint, if normalized by the variance,
maximizes the mutual information between hidden unit states at adjacent time steps.

829

Learning Temporally Persistent Hierarchical Representations

Test Set Performance

Training Set Performance
1000

U

Ql

u

Ql

BOO

0

0
U

60.0

u

60.0

C

C
Ql

BOO

L..
L..

L..
L..

U

100.0

Ql

a

400

L..

Ql

a...

U

a

400

L..

-4

Ql

a...

????_?? 2

20.0

---. 1

- - -. 1

O.

1000.0

2000.0

JOOO .O

Learning epoch

4000.0

1000.0

2000.0

JOOO.O

4000.0

Learning epoch

Figure 3: Learning curves, averaged over five runs , for feedforward supervised net with a
temporal smoothness constraint, for each of four levels of the parameter

>..

was added to the cost function to encourage temporal smoothness. As the results
in Figure 3 show, a feedforward network with no contextual input was thereby able
to perform as well as our unsupervised model when it was constrained to develop
hidden layer representations that clustered temporally adjacent patterns together.

4

DISCUSSION

The unsupervised model's markedly better ability to generalize stems from it's cost
function; it favors hidden layer features which contribute to temporally coherent
predictions at the output (gating) layer. Multiple views of a given object are therefore more likely to be detected by a given clustering unit in the unsupervised model,
leading to considerably improved interpolation of novel views. The poor generalization performance of back-propagation is not just due to overtraining, as the learning
curves in Figure 3 show. Even with early stopping, the network with the lowest
value of >. would not have done as well as the unsupervised network. There is simply no reason why supervised back-propagation should cluster temporally adjacent
views together unless it is explicitly forced to do so.
A ""contextual input"" stream was implemented in the simplest possible way in the
simulations reported here, using fixed delay lines. However, the model we have proposed provides for a completely general way of incorporating arbitrary contextual
information, and could equally well integrate other sources of input. The incoming
weights to the gating units could also be learned. In fact, the gating unit activities
actually represent the probabilities of each clustering unit's Gaussian model fitting
the data, conditioned on the temporal history; hence, the entire model could be
viewed as a Hidden Markov Model (Geoff Hinton, personal communication). However, current techniques for fitting HMMs are intractable if state dependencies span
arbitrarily long time intervals.
The model in its present implementation is not meant to be a realistic account of the
way humans learn to recognize faces . Viewpoint-invariant recognition is achieved,
if at all, in a hierarchical, multi-stage system. One could easily extend our model
to achieve this, by connecting together a sequence of networks like the one shown
in Figure 2, each having progressively larger receptive fields.
A number of other unsupervised learning rules have been proposed based on the assumption oftemporally coherent inputs (FOldiak, 1991; Becker, 1993; Stone, 1996).
Phillips et al. (1995) have proposed an alternative model of cortical self-organization
they call coherent Infomax which incorporates contextual modulation. In their
model, the outputs from one processing stream modulate the activity in another

830

s. Becker

stream, while the mutual information between the two streams is maximized.
A wide range of perceptual and cognitive abilities could be modelled by a network that can learn features of its primary input in particular contexts. These include multi-sensor fusion, feature segregation in object recognition using top-down
cues, and semantic disambiguation in natural language understanding. Finally, it
is widely believed that memories are stored rapidly in the hippocampus and related brain structures, and gradually incorporated into the slower-learning cortex
for long-term storage. The model proposed here may be able to explain how such
interactions between disparate information sources are learned.

Acknowledgements
This work evolved out of discussions with Ron Racine and Larry Roberts. Thanks to
Geoff Hinton for contributing several valuable insights, as mentioned in the paper,
and to Ken Seergobin for the face images. Software was developed using the Xerion
neural network simulation package from Hinton's lab, with programming assistance
from Lianxiang Wang. This work was supported by a McDonnell-Pew Cognitive
Neuroscience research grant and a research grant from the Natural Sciences and
Engineering Research Council of Canada.

References
Becker, S. (1993). Learning to categorize objects using temporal coherence. In S. J.
Hanson, J. D. Cowan, & C. L. Giles (Eds.), Advances in Neural Information Processing
Systems 5 (pp. 361-368). San Mateo, CA: Morgan Kaufmann.
Becker, S. & Plumbley, M. (1996) . Unsupervised neural network learning procedures for
feature extraction and classification. International Journal of Applied Intelligence, 6(3).
Calvin, W. H. (1995). Cortical columns, modules, and Hebbian cell assemblies. In M.
Arbib (Ed.), The handbook of brain theory and neural networks. Cambridge, MA: MIT
Press.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. Proceedings of the Royal Statistical Society, B-39:1-38.
Foldiak, P. (1991). Learning invariance from transformation sequences. Neural Computation, 3(2):194-200.
Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. (1991). Adaptive mixtures
of local experts. Neural Computation, 3(1):79-87.
McClelland, J. L. & Rumelhart, D. E. (1981). An interactive activation model of context
effects in letter perception, part I: An account of basic findings. Psychological Review,
88:375-407.
Murphy, C. & Sillito, A. M. (1987). Corticofugal feedback influences the generation of
length tuning in the visual pathway. Nature, 329:727-729.
Neely, J. (1991). Semantic priming effects in visual word recognition: A selective review of
current findings and theories. In D. Besner & G. W. Humphreys (Eds.), Basic processes
in reading: Visual Word Recognition (pp. 264-336). Hillsdale, NJ: Lawrence Erlbaum
Associates.
Nowlan, S. J. (1990). Maximum likelihood competitive learning. In D. S. Touretzky (Ed.),
Neural Information Processing Systems, Vol. 2 (pp. 574-582). San Mateo, CA: Morgan
Kaufmann.
Phillips, W. A., Kay, J., & Smyth, D. (1995). The discovery of structure by multi-stream
networks of local processors with contextual guidance. Network, 6:225-246 .
Stone, J. (1996). Learning perceptually salient visual parameters using spatiotemporal
smoothness constraints. Neural Computation, 8:1463-1492.
von der Heydt, R., Peterhans, E., & Baumgartner, G. (1984). Illusory contours and cortical
neural responses. Science, 224 :1260-1262.

"
3157,2009,Filtering Abstract Senses From Image Search Results,"We propose an unsupervised method that, given a word, automatically selects non-abstract senses of that word from an online ontology and generates images depicting the corresponding entities. When faced with the task of learning a visual model based only on the name of an object, a common approach is to find images on the web that are associated with the object name, and then train a visual classifier from the search result. As words are generally polysemous, this approach can lead to relatively noisy models if many examples due to outlier senses are added to the model. We argue that images associated with an abstract word sense should be excluded when training a visual classifier to learn a model of a physical object. While image clustering can group together visually coherent sets of returned images, it can be difficult to distinguish whether an image cluster relates to a desired object or to an abstract sense of the word. We propose a method that uses both image features and the text associated with the images to relate latent topics to particular senses. Our model does not require any human supervision, and takes as input only the name of an object category. We show results of retrieving concrete-sense images in two available multimodal, multi-sense databases, as well as experiment with object classifiers trained on concrete-sense images returned by our method for a set of ten common office objects.","Filtering Abstract Senses From Image Search Results

Kate Saenko1,2 and Trevor Darrell2
1
MIT CSAIL, Cambridge, MA
2
UC Berkeley EECS and ICSI, Berkeley, CA
saenko@csail.mit.edu, trevor@eecs.berkeley.edu

Abstract
We propose an unsupervised method that, given a word, automatically selects
non-abstract senses of that word from an online ontology and generates images
depicting the corresponding entities. When faced with the task of learning a visual model based only on the name of an object, a common approach is to find
images on the web that are associated with the object name and train a visual classifier from the search result. As words are generally polysemous, this approach
can lead to relatively noisy models if many examples due to outlier senses are
added to the model. We argue that images associated with an abstract word sense
should be excluded when training a visual classifier to learn a model of a physical
object. While image clustering can group together visually coherent sets of returned images, it can be difficult to distinguish whether an image cluster relates to
a desired object or to an abstract sense of the word. We propose a method that uses
both image features and the text associated with the images to relate latent topics
to particular senses. Our model does not require any human supervision, and
takes as input only the name of an object category. We show results of retrieving
concrete-sense images in two available multimodal, multi-sense databases, as well
as experiment with object classifiers trained on concrete-sense images returned by
our method for a set of ten common office objects.

1

Introduction

Many practical scenarios call for robots or agents which can learn a visual model on the fly given
only a spoken or textual definition of an object category. A prominent example is the Semantic
Robot Vision Challenge (SRVC)1 , which provides robot entrants with a text-file list of categories to
be detected shortly before the competition begins. More generally, we would like a robot or agent
to be able to engage in situated dialog with a human user and to understand what objects the user
is refering to. It is generally unreasonable to expect users to refer only to objects covered by static,
manually annotated image databases. We therefore need a way to find images for an arbitrary object
in an unsupervised manner.
A common approach to learning a visual model based solely on the name of an object is to find
images on the web that co-occur with the object name by using popular web search services, and
train a visual classifier from the search results. As words are generally polysemous (e.g. mouse)
and are often used in different contexts (e.g. mouse pad), this approach can lead to relatively noisy
models. Early methods used manual intervention to identify clusters corresponding to the desired
sense [2], or grouped together visually coherent sets of images using automatic image clustering
(e.g. [9]). However, image clusters rarely exactly align with object senses because of the large
variation in appearance within most categories. Also, clutter from abstract senses of the word that
1

http://www.semantic-robot-vision-challenge.org

1

Input Word: cup

Object Sense: drink container

cup

WISDOM

Online Dictionary

cup

search

Object Sense: trophy

?

? cup?(a?small?open?container usually?used?for?drinking;?usually?has?a?handle)?""he?
put?the?cup?back?in?the?saucer"";?""the?handle?of?the?cup?was?missing""

Abstract Sense: sporting event

? cup,?loving?cup (a?large?metal?vessel?with?two?handles?that?is?awarded?as?a?trophy
to?the?winner?of?a?competition)?""the?school?kept?the?cups?is?a?special?glass?case?

?

? cup (a?contest in?which?a?cup?is?awarded)??the?World?Cup?is?the?
world's?most?widely?watched?sporting?event.?

Figure 1:

WISDOM

separates the concrete (physical) senses from the abstract ones.

are not associated with a physical object can further complicate matters (e.g. mouse as in ?a timid
person?.) 2
To address these issues, we propose an unsupervised Web Image Sense DisambiguatiOn Model
(WISDOM), illustrated in Figure 1. Given a word, WISDOM automatically selects concrete senses
of that word from a semantic dictionary and generates images depicting the corresponding entities,
first finding coherent topics in both text and image domains, and then grounding the learned topics
using the selected word senses. Images corresponding to different visual manifestations of a single
physical sense are linked together based on the likelihood of their image content and surrounding
text (words in close proximity to the image link) being associated with the given sense.
We make use of a well-known semantic dictionary (WordNet [8]), which has been previously used
together with a text-only latent topic model to construct a probabilistic model of individual word
senses for use with online images [17]. We build on this work by incorporating a visual term, and
by using the Wordnet semantic hierarchy to automatically infer whether a particular sense describes
a physical entity or a non-physical concept. We show results of detecting such concrete senses in
two available multimodal (text and image), multi-sense databases: the MIT-ISD dataset [17], and
the UIUC-ISD dataset [14]. We also experiment with object classification in novel images, using
classifiers trained on the images collected via our method for a set of ten common objects.

2

Related Work

Several approaches to building object models from image search results have been proposed. Some
have relied on visual similarity, either selecting a single inlier image cluster based on a small validation set [9], or bootstrapping object classifiers from existing labeled images [13]. In [18] a classifier
based on text features (such as whether the keyword appears in the URL) was used re-rank the images before bootstrapping the image model. However, the text ranker was category-independent and
thus unable to learn words predictive of a specific word sense. An approach most similar to ours [2]
discovered topics in the textual context of images using Latent Dirichlet Allocation (LDA), however,
manual intervention by the user was required to sort the topics into positive and negative for each
category. Also, the combination of image and text features is used in some web retrieval methods
(e.g. [7]), however, our work is focused not on instance-based image retrieval, but on category-level
modeling.
Two recent papers have specifically addressed polysemous words. In Saenko and Darrell [17], the
use of dictionary definitions to train an unsupervised visual sense model was proposed. However, the
user was required to manually select the definition for which to build the model. Furthermore, the
2
While the first few pages of image search results returned by modern search engines generally have very
few abstract examples, possibly due to the sucess of reranking based on previous user?s click-through history,
results from farther down the list are much less uniform, as our experimental results show.

2

sense model did not incorporate visual features, but rather used the text contexts to re-rank images,
after which an image classifier was built on the top-ranked results. Loeff et al. [14] performed
spectral clustering in both the text and image domain and evaluated how well the clusters matched
different senses. However, as a pure clustering approach, this method cannot assign sense labels.
In the text domain, Yarowsky [20] proposed an unsupervised method for traditional word sense
disambiguation (WSD), and suggested the use of dictionary definitions as an initial seed. Also, Boiy
et al. [4] determined which words are related to a visual domain using hypothesis testing on a target
(visual) corpus, compared to a general (non-visual) corpus.
A related problem is modeling word senses in images manually annotated with words, such as the
caption ?sky, airplane? [1]. Models of annotated images assume that there is a correspondence
between each image region and a word in the caption (e.g. Corr-LDA, [5]). Such models predict
words, which serve as category labels, based on image content. In contrast, our model predicts a
category label based on all of the words in the web image?s text context, where a particular word
does not necessarily have a corresponding image region, and vice versa. In work closely related to
Corr-LDA, a People-LDA [11] model is used to guide topic formation in news photos and captions,
using a specialized face recognizer. The caption data is less constrained than annotations, including
non-category words, but still far more constrained than generic webpage text.

3

Sense-Grounding with a Dictionary Model

We wish to estimate the probability that an image search result embedded in a web page is one of
a concrete or abstract concept. First, we determine whether the web image is related to a particular
word sense, as defined by a dictionary. The dictionary model presented in [17] provides an estimate
of word sense based on the text associated with the web image. We will first describe this model,
and then extend it to include both an image component and an adaptation step to better reflect word
senses present in images.
The dictionary model [17] uses LDA on a large collection of text related to the query word to learn
latent senses/uses of the word. LDA [6] discovers hidden topics, i.e. distributions over discrete
observations (such as words), in the data. Each document is modeled as a mixture of topics z ?
{1, ..., K}. A given collection of M documents, each containing a bag of Nd words, is assumed
to be generated by the following process: First, we sample the parameters ?j of a multinomial
distribution over words from a Dirichlet prior with parameter ? for each topic j = 1, ..., K. For
each document d, we sample the parameters ?d of a multinomial distribution over topics from a
Dirichlet prior with parameter ?. Finally, for each word token i, we choose a topic zi from the
multinomial ?d , and then choose a word wi from the multinomial ?zi .
Since learning LDA topics directly from the images? text contexts can lead to poor results due to
the low quantity and irregular quality of such data, an additional dataset of text-only web pages
is created for learning, using regular web search. The dictionary model then uses the limited text
available in the WordNet entries to relate dictionary sense to latent text topics. For example, sense
1 of ?bass? contains the definition ?the lowest part of the musical range,? as well as the hypernym
(?pitch?) and other semantic relations. The bag-of-words extracted from such a semantic entry for
sense s ? {1, 2, ..., S} is denoted by the variable es = (e1 , e2 , ..., eEs ), where Es is the total number
of words. The dictionary model assumes that the sense is independent of the words conditioned on
the distribution of topics in the document. For a web image with an associated text document dt , the
conditional probability of sense is given by

P (s|dt ) =

K
X

P (s|z = j)P (z = j|dt ),

(1)

j=1

where the distribution of latent topics in the text context, P (z|dt ) is given by the ?dt variable,
computed by generalizing the learned LDA model to the (unseen) text contexts. The likelihood of
a sense given latent topic z = j is defined as the normalized average likelihood of words in the
3

dictionary entry es , 3
P (s|z) ?

Es
1 X
P (ei |z),
Es i=1

(2)

Incorporating Image Features. The dictionary model (1) does not take into account the image part
of the image/text pair. Here, we extend it to include an image term, which can potentially provide
complementary information. First, we estimate P (s|di ), or the probability of a sense given an image
di . Similar to the text-only case, we learn an LDA model consisting of latent topics v ? {1, ..., L},
using the visual bag-of-words extracted from the unlabeled images. The estimated ? variables give
P (v|di ). To compute the conditional probability of a sense given a visual topic, we marginalize the
joint P (s, v) across all image and associated text documents {di , dt } in the collection
P (s|v) ?

M
X

P (s|dt = k)P (v|di = k)

(3)

k=1

Note that the above assumes conditional independence of the sense and the visual topic given the
observations. Intuitively, this provides us with an estimate of the collocation of senses with visual
topic. We can now compute the probability of dictionary sense for a novel image di? as:
P (s|di? ) =

L
X

P (s|v = j)P (v = j|di? )

(4)

j=1

Finally, the joint text and image model is defined as the combination of the text-space and imagespace models via the sum rule,
P (s|di , dt ) = ?P (s|di ) + (1 ? ?)P (s|dt )

(5)

Our assumption in using the sum rule is that the combination can be modelled as a mixture of
experts, where the features of one modality are independent of sense given the other modality [3].
Adaptation. Recall that we can estimate ?dt for the unseen web image contexts by generalizing the
web-text LDA model using Gibbs sampling. However, web topics can be a poor match to image
search data (e.g. the ?genome research? topic of mouse.) Our solution is to adapt the web topics to
the image search data. We do this by fixing the z assignments of the web documents and sampling
the z?s of the image contexts for a few iterations. This procedure updates the topics to better reflect
the latent dimensions present in the image search data, without the overfitting effect mentioned
earlier.

4

Filtering out Abstract Senses

To our knowledge, no previous work has considered the task of detecting concrete vs. abstract senses
in general web images. We can do so by virtue of the multimodal sense grounding method presented
in the previous section. Given a set of senses for a paricular word, our task is to classify each sense
as being abstract or concrete. Fortunately, WordNet contains relatively direct metadata related to
the physicality of a word sense. In particular, one of the main functions of WordNet is to put words
in semantic relation to each other using the concepts of hyponym and hypernym. For example,
?scarlet? and ?crimson? are hyponyms of ?red?, while ?color? is a hypernym of ?red?. One can
follow the chain of direct hypernyms all the way to the top of the tree, ?entity?. Thus, we can detect
a concrete sense by examining its hypernym tree to see if it contains one of the following nodes:
?article?, ?instrumentality?,?article of clothing?, ?animal?, or ?body part?. What?s more, we can thus
restrict the model to specific types of physical entities: living things, artifacts, clothing, etc.
In addition, WordNet contains lexical file information for each sense, marking it as a state, or an animal, etc. For example, the sense ?mouse, computer mouse? is marked <artifact>. In this paper, we
classify a WordNet sense as being due to a concrete object when the lexical tag is one of <animal>,
<artifact>, <body>, <plant> and <act>. We exclude people and proper nouns in the experiments
in this paper, as well as prune away infrequent senses.
3
The average word likelihood was found to be a good indicator of how relevant a topic is to a sense. The
total word likelihood could be used, but it would allow senses with longer entries to dominate.

4

5

Data

We evaluated the outlined algorithms on three datasets: the five-word MIT-ISD dataset [17], the
three-word UIUC-ISD dataset [14], and OFFICE dataset of ten common office objects that we collected for the classification experiment. 4 All datasets had been collected automatically by issuing
queries to the Yahoo Image SearchTM engine and downloading the returned images and corresponding HTML web pages. For the MIT-ISD dataset, the query terms used were: BASS, FACE, MOUSE,
SPEAKER and WATCH. For the UIUC-ISD dataset, three basic query terms were used: BASS,
CRANE and SQUASH. To increase corpus size, the authors also used supplemental query terms
for each word. The search terms selected were those related to the concrete senses (e.g. ?construction cranes?, ?whooping crane?, etc.) Since these human-selected search terms require human
input, while our method only requires a list of words, we exclude them from our experiments. The
OFFICE dataset queries were: CELLPHONE, FORK, HAMMER, KEYBOARD, MUG, PLIERS,
SCISSORS, STAPLER, TELEPHONE, WATCH.
The images were labeled by a human annotator with all concrete senses for each word. The annotator
saw only the images, and not the surrounding text or any dictionary definitions. For the MIT-ISD
dataset, each concrete sense was labeled as core, related, and unrelated. Images where the object
was too small or too occluded were labeled as related. For the UIUC-ISD dataset, the labels for
each concrete sense were similarly core, related and unrelated. In addition, a people label was
used for unrelated images depicting faces or a crowd. 5 The OFFICE dataset was only labeled with
core and unrelated labels. We evaluated our models on two retrieval tasks: retrieval of only core
images of each sense, and retrieval of both core and related images. In the former case, core labels
were used as positive labels for each sense, with related, unrelated and people images labeled as
negative. In the latter case, core and related images were labeled as positive, and unrelated and
people as negative. Note that the labels were only used in testing, and not in training.
To provide training data for the web text topic model, we also collected an unlabeled corpus of textonly webpages for each word. These additional webpages were collected via regular web search for
the single-word search term (e.g. CRANE), and were not labeled.

6

Features

When extracting words from web pages, all HTML tags are removed, and the remaining text is
tokenized. A standard stop-word list of common English words, plus a few domain-specific words
like ?jpg?, is applied, followed by a Porter stemmer [16]. Words that appear only once and the
actual word used as the query are pruned. To extract text context words for an image, the image
link is located automatically in the corresponding HTML page. All word tokens in a 100-token
window surrounding the location of the image link are extracted. The text vocabulary size used for
the dictionary model ranges between 12K-20K words for the different search words.
To extract image features, all images are resized to 300 pixels in width and converted to grayscale.
Two types of local feature points are detected in the image: edge features [9] and scale-invariant
salient points. To detect edge points, we first perform Canny edge detection, and then sample a fixed
number of points along the edges from a distribution proportional to edge strength. The scales of the
local regions around points are sampled uniformly from the range of 10-50 pixels. To detect scaleinvariant salient points, we use the Harris-Laplace [15] detector with the lowest strength threshold
set to 10. Altogether, 400 edge points and approximately the same number of Harris-Laplace points
are detected per image. A 128-dimensional SIFT descriptor is used to describe the patch surrounding each interest point. After extracting a bag of interest point descriptors for each image, vector
quantization is performed. A codebook of size 800 is constructed by k-means clustering a randomly
chosen subset of the database (300 images per keyword), and all images are converted to bags of the
resulting visual words (cluster centers of the codebook.) No spatial information is included in the
image representation, rather it is treated as a bag-of-words.
4

The MIT-ISD and OFFICE datasets are available at http://people.csail.mit.edu/saenko
The UIUC-ISD dataset and its complete description can be obtained at
http://visionpc.cs.uiuc.edu/isd/index.html
5

5

(a) Text Model

(b) Image Model

Figure 2: The top 25 images returned by the text and the image models for mouse-4 (device).

7

Retrieval Experiments

In this section, we evaluate WISDOM on the task of retrieving concrete sense images from search
results. Below are the actual concrete senses that were automatically selected from WordNet by our
model for each word in the datasets:
MIT-ISD: bass-7 (instrument), bass-8 (fish), face-1 (human face), face-13 (surface), mouse-1 (rodent), mouse-4 (device), speaker-2 (loudspeaker), watch-1 (timepiece)
UIUC-ISD: bass-7 (instrument), bass-8 (fish), crane-4 (machine), crane-5 (bird), squash-1 (plant),
squash-3 (game)
OFFICE: cellphone-1 (mobile phone), fork-1 (utensil), hammer-2 (hand tool), keyboard-1 (any
keyboard), mug-1 (drinking vessel), mug-1 (drinking vessel), pliers-1 (tool), scissors-1
(cutting tool), stapler-1 (stapling device), telephone-1 (landline phone), watch-1 (timepiece)
We train a separate web text LDA model and a separate image LDA model for each word in the
dataset. The number of topics K is a parameter to the model that represents the dimensionality of
the latent space used by the model. We set K = 8 for all LDA models in the following experiments.
This was done so that the number of latent text topics is roughly equal to the number of senses. In the
image domain, it is less clear what the number of topics should be. Ideally, each topic would coincide
with a visually coherent class of images all belonging to the same sense. In practice, because images
of an object class on the web are extremely varied, multiple visual clusters are needed to encompass
a single visual category. Our experiments have shown that the model is relatively insensitive to
values of this parameter in the range of 8-32. To perform inference in LDA, we used the Gibbs
sampling approach of [10], implemented in the Matlab Topic Modeling Toolbox [19]. We used
symmetric Dirichlet priors with scalar hyperparameters ? = 50/K and ? = 0.01, which have the
effect of smoothing the empirical topic distribution, and 1000 iterations of Gibbs sampling.
Figure 2 shows the images that were assigned the highest probability for mouse-4 (computer device)
sense by the text-only model P (s|dt ) (Figure 2(a)), and by the image-only model P (s|di ) (Figure
2(b)). Both models return high-precision results, but somewhat different and complementary images. As we expected, the image model?s results are more visually coherent, while the text model?s
results are more visually varied.
Next, we evaluate retrieval of individual senses using the multimodal model (Eq. 5, with ? = 0.5)
and compare it to the Yahoo search engine baseline. This is somewhat unfair to the baseline, as here
we assume that our model knows which sense to retrieve (we will remove this assumption later.)
The recall-precision curves (RPCs) are shown in Figure 3. The figure shows the RPCs for each
word in the MIT-ISD (top row) and UIUC-ISD (bottom row) datasets, computed by thresholding
P (s|di , dt ). WISDOM?s RPCs are shown as the green curves. The blue curves are the RPCs obtained
by the original Yahoo image search retrieval order. For example, the top leftmost plot shows retrieval
of bass-7 (musical instrument). These results demonstrate that we are able to greatly improve the
retrieval of each concrete sense compared to the search engine.
6

(a) MIT-ISD data

(b) UIUC-ISD data

Figure 3: Recall-precision of each concrete sense (core labels) using the multimodal dictionary
model (green) and the search engine (blue), evaluated on two datasets.

(a) Core Senses, MIT-ISD

(b) Core Senses, UIUC-ISD

(c) Core+Related Senses, MIT-ISD

(d) Core+Related Senses, UIUC-ISD

Figure 4: Recall-precision of all concrete senses using WISDOM (green) and the search engine (blue).
WISDOM does fail to retrieve one sense ? face-13, defined as ?a vertical surface?. This is a highly
ambiguous sense visually, although it has an <artifact> lexical tag. One possibility for the future
is to exclude senses that are descendents of ?surface? as being too ambiguous. Also, preliminary
investigation indicates that weighting the text and image components of the model differently can
result in improved results; model weighting is therefore an important topic for future work.

Next, we evaluate the ability of WISDOM to filter out abstract senses. Here we no longer assume that
the correct senses are known. Figure 4 shows the result of filtering out the abstract senses, which is
done by evaluating the probability of any of the concrete senses in a given search result. The ground
truth labels used to compute these RPCs are positive if an image was labeled either with any core
sense (Fig.4 (a,b)), or with any core or related sense (Fig.4 (c,d)), and negative otherwise. These
results demonstrate that our model improves the retrieval of images of concrete (i.e. physical) senses
of words, without any user input except for the word itself. Figure 5 shows how the model filters out
certain images, including illustrations by an artist named Crane, from search results for CRANE.

8

Classification Experiments

We have shown that our method can improve retrieval of concrete senses, therefore providing higherprecision image training data for object recognition algorithms. We have conjectured that this leads
to better classification results; in this section, we provide some initial experiments to support this
claim. We collected a dataset of ten office objects, and trained ten-way SVM classifiers using
the vocabulary-guided pyramid match kernel over bags of local SIFT features implemented in the
LIBPMK library [12]. The training data for the SVM was either the first 100 images returned from
the search engine, or the top 100 images ranked by our model. Since we?re interested in objects, we
keep only the <artifact> senses that descend from ?instrumentality? or ?article?. Figure 6 shows
classification results on held-out test data, averaged over 10 runs on random 80% subsets of the
7

(a) Yahoo Image Search

(b) WISDOM

Figure 5: The top images returned by the search engine for CRANE, compared to our model.

Figure 6: Classification accuracy of ten objects in the OFFICE dataset.

data. Our method improves accuracy for most of the objects; in particular, classification of ?mug?
improves greatly due to the non-object senses being filtered out. This is a very difficult task, as
evidenced by the baseline performance; the average baseline accuracy is 27%. Training with our
method achieves 35% accuracy, a 25% relative improvement. We believe that this relative improvement is due to the higher precision of the training images and will persist even if the overall accuracy
were improved due to a better classifier.

9

Conclusion

We presented WISDOM, an architecture for clustering image search results for polysemous words
based on image and text co-occurrences and grounding latent topics according to dictionary word
senses. Our method distinguishes which senses are abstract from those that are concrete, allowing
it to filter out the abstract senses when constructing a classifier for a particular object of interest to
a situated agent. This can be of particular utility to a mobile robot faced with the task of learning
a visual model based only on the name of an object provided on a target list or spoken by a human
user. Our method uses both image features and the text associated with the images to relate estimated
latent topics to particular senses in a semantic database. WISDOM does not require any human
supervision, and takes as input only an English noun. It estimates the probability that a search result
is associated with an abstract word sense, rather than a sense that is tied to a physical object. We
have carried out experiments with image and text-based models to form estimates of abstract vs.
concrete senses, and have shown results detecting concrete-sense images in two multimodal, multisense databases. We also demonstrated a 25% relative improvement in accuracy when classifiers are
trained with our method as opposed to the raw search results.
Acknowledgments
This work was supported in part by DARPA, Google, and NSF grants IIS-0905647 and IIS-0819984.
8

References
[1] K. Barnard, K. Yanai, M. Johnson, and P. Gabbur. Cross modal disambiguation. In Toward Category-Level
Object Recognition, J. Ponce, M. Hebert, C. Schmidt, eds., Springer-Verlag LNCS Vol. 4170, 2006.
[2] T. Berg and D. Forsyth. Animals on the web. In Proc. CVPR, 2006.
[3] J. Bilmes and K. Kirchhoff. Directed graphical models of classifier combination: application to phone
recognition. In Proc. ICSLP, 2000.
[4] E. Boiy, K. Deschacht, and M.-F. Moens. Learning Visual Entities and Their Visual Attributes from Text
Corpora. In Proc. DEXA, 2008.
[5] D. Blei and M. Jordan. Modeling annotated data. In Proc. International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 127-134. ACM Press, 2003.
[6] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. J. Machine Learning Research, 3:993-1022,
2003.
[7] Z. Chen, L. Wenyin, F. Zhang and M. Li. Web mining for web image retrieval. J. of the American Society
for Information Science and Technology, 51:10, pages 831-839, 2001.
[8] C. Fellbaum. Wordnet: An Electronic Lexical Database. Bradford Books, 1998.
[9] R. Fergus, L. Fei-Fei, P. Perona, and A. Zisserman. Learning Object Categories from Google?s Image
Search. In Proc. ICCV 2005.
[10] T. Griffiths and M. Steyvers. Finding Scientific Topics. In Proc. of the National Academy of Sciences,
101 (suppl. 1), pages 5228-5235, 2004.
[11] V. Jain, E. Learned-Miller, A. McCallum. People-LDA: Anchoring Topics to People using Face Recognition. In Proc. ICCV, 2007.
[12] J. Lee. LIBPMK: A Pyramid Match Toolkit. MIT Tech Report MIT-CSAIL-TR-2008-17, available online
at http://hdl.handle.net/1721.1/41070. 2008
[13] J. Li, G. Wang, and L. Fei-Fei. OPTIMOL: automatic Object Picture collecTion via Incremental MOdel
Learning. In Proc. CVPR, 2007.
[14] N. Loeff, C. Ovesdotter Alm, D. Forsyth. Discriminating Image Senses by Clustering with Multimodal
Features. In Proc. ACL 2006.
[15] K. Mikolajczyk and C. Schmid. Scale and affine invariant interest point detectors. In Proc. IJCV, 2004.
[16] M. Porter, An algorithm for suffix stripping, Program, 14(3) pp 130-137, 1980.
[17] K, Saenko and T. Darrell. Unsupervised Learning of Visual Sense Models for Polysemous Words. In Proc.
NIPS, 2008.
[18] F. Schroff, A. Criminisi and A. Zisserman. Harvesting image databases from the web. In Proc. ICCV,
2007.
[19] M. Steyvers and T. Griffiths. Matlab Topic Modeling Toolbox.
http://psiexp.ss.uci.edu/research/software.htm
[20] D. Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. ACL, 1995.

9

"
5806,2016,Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning,"We study the sampling-based planning problem in Markov decision processes (MDPs) that we can access only through a generative model, usually referred to as Monte-Carlo planning. Our objective is to return a good estimate of the optimal value function at any state while minimizing the number of calls to the generative model, i.e. the sample complexity. We propose a new algorithm, TrailBlazer, able to handle MDPs with a finite or an infinite number of transitions from state-action to next states. TrailBlazer is an adaptive algorithm that exploits possible structures of the MDP by exploring only a subset of states reachable by following near-optimal policies. We provide bounds on its sample complexity that depend on a measure of the quantity of near-optimal states. The algorithm behavior can be considered as an extension of Monte-Carlo sampling (for estimating an expectation) to problems that alternate maximization (over actions) and expectation (over next states). Finally, another appealing feature of TrailBlazer is that it is simple to implement and computationally efficient.","Blazing the trails before beating the path:
Sample-efficient Monte-Carlo planning

Jean-Bastien Grill
Michal Valko
SequeL team, INRIA Lille - Nord Europe, France
jean-bastien.grill@inria.fr

michal.valko@inria.fr

R?mi Munos
Google DeepMind, UK?
munos@google.com

Abstract
You are a robot and you live in a Markov decision process (MDP) with a finite or an
infinite number of transitions from state-action to next states. You got brains and so
you plan before you act. Luckily, your roboparents equipped you with a generative
model to do some Monte-Carlo planning. The world is waiting for you and you
have no time to waste. You want your planning to be efficient. Sample-efficient.
Indeed, you want to exploit the possible structure of the MDP by exploring only a
subset of states reachable by following near-optimal policies. You want guarantees
on sample complexity that depend on a measure of the quantity of near-optimal
states. You want something, that is an extension of Monte-Carlo sampling (for
estimating an expectation) to problems that alternate maximization (over actions)
and expectation (over next states). But you do not want to StOP with exponential
running time, you want something simple to implement and computationally
efficient. You want it all and you want it now. You want TrailBlazer.

1

Introduction

We consider the problem of sampling-based planning in a Markov decision process (MDP) when a
generative model (oracle) is available. This approach, also called Monte-Carlo planning or MonteCarlo tree search (see e.g., [12]), has been popularized in the game of computer Go [7, 8, 15] and
shown impressive performance in many other high dimensional control and game problems [4]. In
the present paper, we provide a sample complexity analysis of a new algorithm called TrailBlazer.
Our assumption about the MDP is that we possess a generative model which can be called from any
state-action pair to generate rewards and transition samples. Since making a call to this generative
model has a cost, be it a numerical cost expressed in CPU time (in simulated environments) or a
financial cost (in real domains), our goal is to use this model as parsimoniously as possible.
Following dynamic programming [2], planning can be reduced to an approximation ofhthe (optimal)
i
P
t
value function, defined as the maximum of the expected sum of discounted rewards: E
t?0 ? rt ,
where ? ? [0, 1) is a known discount factor. Indeed, if an ?-optimal approximation of the value
function at any state-action pair is available, then the policy corresponding to selecting in each state
the action with the highest approximated value will be O (?/ (1 ? ?))-optimal [3].
Consequently, in this paper, we focus on a near-optimal approximation of the value function for
a single given state (or state-action pair). In order to assess the performance of our algorithm we
measure its sample complexity defined as the number of oracle calls, given that we guarantee its
consistency, i.e., that with probability at least 1 ? ?, TrailBlazer returns an ?-approximation of the
value function as required by the probably approximately correct (PAC) framework.
?

on the leave from SequeL team, INRIA Lille - Nord Europe, France

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

We use a tree representation to represent the set of states that are reachable from any initial state.
This tree alternates maximum (MAX) nodes (corresponding to actions) and average (AVG) nodes
(corresponding to the random transition to next states). We assume the number K of actions is finite.
However, the number N of possible next states is either finite or infinite (which may be the case
when the state space is infinite), and we will report results in both the finite N and the infinite case.
The root node of this planning tree represents the current state (or a state-action) of the MDP and its
value is the maximum (over all policies defined at MAX nodes) of the corresponding expected sum of
discounted rewards. Notice that by using a tree representation, we do not use the property that some
state of the MDP can be reached by different paths (sequences of states-actions). Therefore, this state
will be represented by different nodes in the tree. We could potentially merge such duplicates to form
a graph instead. However, for simplicity, we choose not to merge these duplicates and keep a tree,
which could make the planning problem harder. To sum up, our goal is to return, with probability
1 ? ?, an ?-accurate value of the root node of this planning tree while using as low number of calls
to the oracle as possible. Our contribution is an algorithm called TrailBlazer whose sampling
strategy depends on the specific structure of the MDP and for which we provide sample complexity
bounds in terms of a new problem-dependent measure of the quantity of near-optimal nodes. Before
describing our contribution in more detail we first relate our setting to what has been around.
1.1

Related work

In this section we focus on the dependency between ? and the sample complexity and all bound of
the style 1/?c are up to a poly-logarithmic multiplicative factor not indicated for clarity. Kocsis and
Szepesv?ri [12] introduced the UCT algorithm (upper-confidence bounds for trees). UCT is efficient
in computer Go [7, 8, 15] and a number of other control and game problems [4]. UCT is based on
generating trajectories by selecting in each MAX node the action that has the highest upper-confidence
bound (computed according to the UCB algorithm of Auer et al. [1]). UCT converges asymptotically to
the optimal solution, but its sample complexity can be worst than doubly-exponential in (1/?) for
some MDPs [13]. One reason for this is that the algorithm can expand very deeply the apparently
best branches but may lack sufficient exploration, especially when a narrow optimal path is hidden in
a suboptimal branch. As a result, this approach works well in some problems with a specific structure
but may be much worse than a uniform sampling in other problems.
On the other hand, a uniform planning approach is safe for all problems. Kearns et al. [11] generate a
sparse look-ahead tree based on expanding all MAX nodes and sampling a finite number of children
from AVG nodes up to a fixed depth that depends on the desired accuracy ?. Their sample complexity
is2 of the order of (1/?)log(1/?) , which is non-polynomial in 1/?. This bound is better than that for
UCT in a worst-case sense. However, as their look-ahead tree is built in a uniform and non-adaptive
way, this algorithm fails to benefit from a potentially favorable structure of the MDP.
An improved version of this sparse-sampling algorithm by Walsh et al. [17] cuts suboptimal branches
in an adaptive way but unfortunately does not come with an improved bound and stays non-polynomial
even in the simple Monte Carlo setting for which K = 1.
Although the sample complexity is certainly non-polynomial in the worst case, it can be polynomial in some specific problems. First, for the case of finite N , the sample complexity is polynomial and Sz?r?nyi et al. [16] show that a uniform sampling algorithm has complexity at most
(1/?)2+log(KN )/(log(1/?)) . Notice that the product KN represents the branching factor of the lookahead planning tree. This bound could be improved for problems with specific reward structure or
transition smoothness. In order to do this, we need to design non-uniform, adaptive algorithm that
captures the possible structure of the MDP when available, while making sure that in the worst case,
we do not perform worse than a uniform sampling algorithm.
The case of deterministic dynamics (N = 1) and rewards considered by Hren and Munos [10] has a
complexity of order (1/?)(log ?)/(log(1/?)) , where ? ? [1, K] is the branching factor of the subset of
near-optimal nodes.3 The case of stochastic rewards has been considered by Bubeck and Munos [5]
but with the difference that the goal was not to approximate the optimal value function but the value
of the best open-loop policy which consists in a sequence of actions independent of states. Their
sample complexity is (1/?)max(2,(log ?)/(log 1/?)) .
2
3

neglecting exponential dependence in ?
nodes that need to be considered in order to return a near-optimal approximation of the value at the root

2

In the case of general MDPs, Bu?soniu and Munos [6] consider the case of a fully known model of
the MDP. For any state-action, the model returns the expected reward and the set of all next states
(assuming N is finite) with their corresponding transition probabilities. In that case, the complexity is
(1/?)log ?/(log(1/?)) , where ? ? [0, KN ] can again be interpreted as a branching factor of the subset
of near-optimal nodes. These approaches use the optimism in the face of uncertainty principle whose
applications to planning have been have been studied by Munos [13]. TrailBlazer is different. It
is not optimistic by design: To avoid voracious demand for samples it does not balance the upperconfidence bounds of all possible actions. This is crucial for polynomial sample complexity in the
infinite case. The whole Section 3 shines many rays of intuitive light on this single and powerful idea.
The work that is most related to ours is StOP by Sz?r?nyi et al. [16] which considers the planning problem in MDPs with a generative model. Their complexity bound is of the order of
(1/?)2+log ?/(log(1/?))+o(1) , where ? ? [0, KN ] is a problem-dependent quantity. However, their ?
defined as lim??0 max(?1 , ?2 ) (in their Theorem 2) is somehow difficult to interpret as a measure of
the quantity of near-optimal nodes. Moreover, StOP is not computationally efficient as it requires to
identify the optimistic policy which requires computing an upper bound on the value of any possible
policy, whose number is exponential in the number of MAX nodes, which itself is exponential in the
planning horizon. Although they suggest (in their Appendix F) a computational improvement, this
version is not analyzed. Finally, unlike in the present paper, StOP does not consider the case N = ?
of an unbounded number of states.
1.2

Our contributions

Our main result is TrailBlazer, an algorithm with a bound on the number of samples required to
return a high-probability ?-approximation of the root node whether the number of next states N is
finite or infinite. The bounds use a problem-dependent quantity (? or d) that measures the quantity of
near-optimal nodes. We now summarize the results.
Finite number of next states (N < ?): The sample complexity of TrailBlazer is of the order
of4 (1/?)max(2,log(N ?)/ log(1/?)+o(1)) , where ? ? [1, K] is related to the branching factor of the set
of near-optimal nodes (precisely defined later).
Infinite number of next states (N = ?): The complexity of TrailBlazer is (1/?)2+d , where d
is a measure of the difficulty to identify the near-optimal nodes. Notice that d can be finite even if
the planning problem is very challenging.5 We also state our contributions in specific settings in
comparison to previous work.
? For the case N < ?, we improve over the best-known previous worst-case bound with
an exponent (to 1/?) of max(2, log(N K)/ log(1/?)) instead of 2 + log(N K)/ log(1/?)
reported by Sz?r?nyi et al. [16].
? For the case N = ?, we identify properties of the MDP (when d = 0) under which the
sample complexity is of order (in 1/?2 ). This is the case when there are non-vanishing actiongaps6 from any state along near-optimal policies or when the probability of transitionning to
nodes with gap ? is upper bounded by ?2 . This complexity bound is as good as MonteCarlo sampling and for this reason TrailBlazer is a natural extension of Monte-Carlo
sampling (where all nodes are AVG) to stochastic control problems (where MAX and AVG
nodes alternate). Also, no previous algorithm reported a polynomial bound when N = ?.
? In MDPs with deterministic transitions (N = 1) but stochastic rewards our bound is
(1/?)max(2,log ?/(log 1/?)) which is similar to the bound achieved by Bubeck and Munos [5]
in a similar setting (open-loop policies).
? In the evaluation case without control (K = 1) TrailBlazer behaves exactly as MonteCarlo sampling (thus achieves a complexity of 1/?2 ), even in the case N = ?.
? Finally TrailBlazer is easy to implement and is numerically efficient.

4

neglecting logarithmic terms in ? and ?
since when N = ? the actual branching factor of the set of reachable nodes is infinite
6
defined as the difference in values of best and second-best actions

5

3

2

Monte-Carlo planning with a generative model

Setup We operate on a planning tree T . Each node
1: Input: ?, ?
of T from the root down is alternatively either an
2: Set: ? ? ? 1/ max(2,log(1/?)) 

log(K)
average (AVG) or a maximum (MAX) node. For any
2 log (1??)
3:
Set:
?
?
2
log(?(1
?
?))
log(?/?)
node s, C [s] is the set of its children. We consider
4: Set: m ? (log(1/?) + ?)/((1 ? ?)2 ?2 )
trees T for which the cardinality of C [s] for any MAX
5: Use: ? and ? as global parameters
node s is bounded by K. The cardinality N of C [s]
6: Output:
for any AVG node s can be either finite, N < ?,
? ? call the root with parameters (m, ?/2)
or infinite. We consider both cases. TrailBlazer
applies to both situations. We provide performance
Figure 1: TrailBlazer
guarantees for a general case and possibly tighter,
N -dependent guarantees in the case of N < ?. We assume that we have a generative model of the
transitions and rewards: Each AVG node s is associated with a transition, a random variable ?s ? C [s]
and a reward, a random variable rs ? [0, 1].
Objective For any node s, we define the value function V [s] as the optimum over policies ? (giving a
successor to all MAX nodes) of the sum of discounted
expected rewards playing policy ?,
""
#

X

t
V [s] = sup E
? rst s0 = s, ? ,
?

t?0

where ? ? (0, 1) is the discount factor. If s is an AVG
node, V satisfies the following Bellman equation,
X
V [s] = E [rs ] + ?
p(s0 |s)V [s0 ] .
s0 ?C[s]

If s is a MAX node, then V [s] = maxs0 ?C[s] V [s0 ] .
The planner has access to the oracle which can be
called for any AVG node s to either get a reward r or a
transition ? which are two independent random variables identically distributed as rs and ?s respectively.

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:

Input: m, ?
Initialization: {Only executed on first call}
SampledNodes ? ?,
r?0
Run:
if ? ? 1/(1 ? ?) then
Output: 0
end if
if |SampledNodes| > m then
ActiveNodes ? SampledNodes(1 : m)
else
while |SampledNodes| < m do
? ? {new sample of next state}
SampledNodes.append(?)
r ? r+[new sample of reward]
end while
ActiveNodes ? SampledNodes
end if {At this point, |ActiveNodes| = m}
for all unique nodes s ? ActiveNodes do
k ? #occurrences of s in ActiveNodes
? ? call s with parameters (k, ?/?)
? ? ? + ?k/m
end for
Output: ?? + r/|SampledNodes|

With the notation above, our goal is to estimate the
value V [s0 ] of the root node s0 using the smallest
possible number of oracle calls. More precisely,
Figure 2: AVG node
given any ? and ?, we want to output a value ??,? such
that P [|??,? ? V [s0 ]| > ?] ? ? using the smallest
possible number of oracle calls n?,? . The number of calls is the sample complexity of the algorithm.
2.1

Blazing the trails with TrailBlazer

To fulfill the above objective, our TrailBlazer constructs a planning tree T which is, at any
time, a finite subset of the potentially infinite tree. Only the already visited nodes are in T and
explicitly represented in memory. Taking the object-oriented paradigm, each node of T is a persistent
object with its own memory which can receive and perform calls respectively from and to other
nodes. A node can potentially be called several times (with different parameters) during the run of
TrailBlazer and may reuse (some of) its stored (transition and reward) samples. In particular, after
node s receives a call from its parent, node s may perform internal computation by calling its own
children in order to return a real value to its parent.
Pseudocode of TrailBlazer is in Figure 1 along with the subroutines for MAX nodes in Figure 3 and
AVG nodes in Figure 2. A node (MAX or AVG) is called with two parameters m and ?, which represent
some requested properties of the returned value: m controls the desired variance and ? the desired
maximum bias. We now describe the MAX and AVG node subroutines.

4

MAX nodes A MAX node s keeps a lower and an
upper bound of its children values which with high
probability simultaneously hold at all times. It sequentially calls its children with different parameters in order to get more and more precise estimates
of their values. Whenever the upper bound of one
child becomes lower than the maximum lower bound,
this child is discarded. This process can stop in two
ways: 1) The set L of the remaining children shrunk
enough such that there is a single child b? left. In
this case, s calls b? with the same parameters that s
received and uses the output of b? as its own output.
2) The precision we have on the value of the remaining children is high enough. In this case, s returns
the highest estimate of the children in L. Note that
the MAX node is eliminating actions to identify the
best. Any other best-arm identification algorithm for
bandits can be adapted instead.

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

Input: m, ?
L ? all children of the node
`?1
while |L| >q
1 and U ? (1 ? ?)? do

log(K`/(??))+?/(???)+?+1
2
U ? 1??
`
for b ? L do
?b ? call b with (`, U?/(1 ? ?))
end for
n
h
io
2U
2U
L ? b : ?b + 1??
? supj ?j ? 1??
`?`+1
end while
if |L| > 1 then
Output: ? ? maxb?L ?b
else { L = {b? } }
b? ? arg maxb?L ?b
? ? call b? with (m, ??)
Output: ?
end if

Figure 3: MAX node
AVG nodes Every AVG node s keeps a list of all the
children that it already sampled and a reward estimate r ? R. Note that the list may contain the same
child multiple times (this is particularly true for N < ?). After receiving a call with parameters
(m, ?), s checks if ? ? 1/(1 ? ?). If this condition is verified, then it returns zero. If not, s considers
the first m sampled children and potentially samples more children from the generative model if
needed. For every child s0 in this list, s calls it with parameters (k, ?/?), where k is the number of
times a transition toward this child was sampled. It returns r + ??, where ? is the average of all the
children estimates.
Anytime algorithm TrailBlazer is naturally anytime. It can be called with slowly decreasing ?,
such that m is always increased only by 1, without having to throw away any previously collected
samples. Executing TrailBlazer with ?0 and then with ? < ?0 leads to the same amount of
computation as immediately running TrailBlazer with ?.
Practical considerations The parameter ? exists so the behavior depends only on the randomness
of oracle calls and the parameters (m, ?) that the node has been called with. This is a desirable
property because it opens the possibility to extend the algorithm to more general settings, for instance
if we have also MIN nodes. However, for practical purposes, we may set ? = 0 and modify the
definition of U in Figure 3 by replacing K with the number of oracle calls made so far globally.

3

Cogs whirring behind

Before diving into the analysis we explain the ideas behind TrailBlazer and the choices made.
Tree-based algorithm The number of policies the planner can consider is exponential in the
number of states. This leads to two major challenges. First, reducing the problem to multi-arm
bandits on the set of the policies would hurt. When a reward is collected from a state, all the policies
which could reach that state are affected. Therefore, it is useful to share the information between the
policies. The second challenge is computational as it is infeasible to keep all policies in memory.
These two problems immediately vanish with just how TrailBlazer is formulated. Contrary to
Sz?r?nyi et al. [16], we do not represent the policies explicitly or update them simultaneously to
share the information, but we store all the information directly in the planning tree we construct.
Indeed, by having all the nodes being separate entities that store their own information, we can share
information between policies without explicitly having to enforce it.
We steel ourselves for the detailed understanding with the following two arguments. They shed light
from two different angles on the very same key point: Do not refine more paths than you need to!
5

Delicate treatment of uncertainty First, we give intuition about the two parameters which measure the requested precision of a call. The output estimate ? of any call with parameters (m, ?)
verifies the following property (conditioned on a high-probability event),


h
i
? 2 ?2
?? E e?(??V[s]) ? exp ? + ?|?| +
, with ? 2 = O (1/m) and constant ?.
(1)
2
This awfully looks like the definition of ? being uncentered sub-Gaussian, except that instead of ? in
the exponential function, there is |?| and there is a ?-independent constant ?. Inequality 1 implies
that the absolute value of the bias of the output estimate ? is bounded by ?,


E [?] ? V [s]  ? ?.
As in the sub-Gaussian case, the second term 12 ? 2 ?2 is a variance term. Therefore, ? controls the
maximum bias of ? and 1/m control its sub-variance. In some cases, getting high-variance or
low-variance estimate matters less as it is going to be averaged later with other independent estimates
by an ancestor AVG node. In this case we prefer to query for high variance rather than a low one, in
order to decrease sample complexity.
From ? and ? it is possible to deduce a confidence bounds on |? ? V?[s]| by typically summing
the bias ? and a term proportional to the standard deviation ? = O (1/ m). Previous approaches
[16, 5] consider a single parameter, representing the width of this high-probability confidence interval.
TrailBlazer is different. In TrailBlazer, the nodes can perform high-variance and low-bias
queries but can also query for both low-variance and low-bias. TrailBlazer treats these two types
of queries differently. This is the whetstone of TrailBlazer and the reason why it is not optimistic.
Refining few paths In this part we explain the condition |SampledNodes| > m in Figure 2, which
is crucial for our approach and results. First notice, that as long as TrailBlazer encounters only AVG
nodes, it behaves just like Monte-Carlo sampling ? without the MAX nodes we would be just doing
a simple averaging of trajectories. However, when TrailBlazer encounters a MAX node it locally
uses more samples around this MAX node, temporally moving away from a Monte-Carlo behavior.
This enables TrailBlazer to compute the best action at this MAX node. Nevertheless, once this
best action is identified with high probability, the algorithm should behave again like Monte-Carlo
sampling. Therefore, TrailBlazer forgets the additional nodes, sampled just because of the MAX
node, and only keeps in memory the first m ones. This is done with the following line in Figure 2,
ActiveNodes ? SampledNodes(1 : m).
Again, while additional transitions were useful for some MAX node parents to decide which action
to pick, they are discarded once this choice is made. Note that they can become useful again if an
ancestor becomes unsure about which action to pick and needs more precision to make a choice. This
is an important difference between TrailBlazer and some previous approaches like UCT where all
the already sampled transitions are equally refined. This treatment enables us to provide polynomial
bounds on the sample complexity for some special cases even in the infinite case (N = ?).

4

TrailBlazer is good and cheap ? consistency and sample complexity

In this section, we start by our consistency result, stating that TrailBlazer outputs a correct value
in a PAC (probably approximately correct) sense. Later, we define a measure of the problem difficulty
which we use to state our sample-complexity results. We remark that the following consistency result
holds whether the state space is finite or infinite.
Theorem 1. For all ? and ?, the output ??,? of TrailBlazer called on the root s0 with (?, ?) verifies
P [|??,? ? V [s0 ]| > ?] < ?.
4.1

Definition of the problem difficulty

We now define a measure of problem difficulty that we use to provide our sample complexity
guarantees. We define a set of near-optimal nodes such that exploring only this set is enough to
compute an optimal policy. Let s0 be a MAX node of tree T . For any of its descendants s, let
c?s (s0 ) ? C [s0 ] be the child of s0 in the path between s0 and s. For any MAX node s, we define
??s (s0 ) = max0 V [x] ? V [c?s (s0 )] .
x?C[s ]

6

??s (s0 ) is the difference of the sum of discounted rewards stating from s0 between an agent playing
optimally and one playing first the action toward s and then optimally.
Definition 1 (near-optimality). We say that a node s of depth h is near-optimal, if for any even
depth h0 ,
0
? (h?h )/2
??s (sh0 ) ? 16
?(1 ? ?)
with sh0 the ancestor of s of even depth h0 . Let Nh be the set of all near-optimal nodes of depth h.
Remark 1. Notice that the subset of near-optimal nodes contains all required information to get
the value of the root. In the case N = ?, when p(s|s0 ) = 0 for all s and s0 , then our definition of
near-optimality nodes leads to the smallest subset in a sense we precise in Appendix C. We prove that
with probability 1 ? ?, TrailBlazer only explores near-optimal nodes. Therefore, the size of the
subset of near-optimal nodes directly reflects the sample complexity of TrailBlazer.
In Appendix C, we discuss the negatives of other potential definitions of near-optimality.
4.2

Sample complexity in the finite case

We first state our result where the set of the AVG children nodes is finite and bounded by N .
Definition 2. We define ? ? [1, K] as the smallest number such that
?C ?h,

|N2h | ? CN h ?h .

Notice that since the total number of nodes of depth 2h is bounded by (KN )h , ? is upper-bounded
by K, the maximum number of MAX?s children. However ? can be as low as 1 in cases when the set
of near-optimal nodes is small.
Theorem 2. There exists C > 0 and K such that for all ? > 0 and ? > 0, with probability 1 ? ?,
the sample-complexity of TrailBlazer (the number of calls to the generative model before the
algorithm terminates) is
log(N ?)

n(?, ?) ? C(1/?)max(2, log(1/?) +o(1)) (log(1/?) + log(1/?)) ,
?

where ? = 5 when log(N ?)/ log(1/?) ? 2 and ? = 3 otherwise.
This provides a problem-dependent sample-complexity bound, which already in theworst case
e (1/?)2+log(KN )/ log(1/?) [16]. This
(? = K) improves over the best-known worst-case bound O
bound gets better as ? gets smaller and is minimal when ? = 1. This is, for example, the case when
the gap (see definition given in Equation 2) at MAX nodes is uniformly lower-bounded by some ? > 0.
In this case, this theorem provides a bound of order (1/?)max(2,log(N )/ log(1/?)) . However, we will
show in Remark 2 that we can further improve this bound to (1/?)2 .
4.3

Sample complexity in the infinite case

Since the previous bound depends on N , it does not apply to the infinite case with N = ?. We now
provide a sample complexity result in the case N = ?. However, notice that when N is bounded,
then both results apply.
We first define gap ?(s) for any MAX node s as the difference between the best and second best arm,
?(s) = V [i? ] ?

max

i?C[s],i6=i?

V [i]

with i? = arg max V [i] .

(2)

i?C[s]

For any even integer h, we define a random variable S h taking values among MAX nodes of depth h,
in the following way. First, from every AVG nodes from the root to nodes of depth h, we draw a single
transition to one of its children according to the corresponding transition probabilities. This defines
a subtree with K h/2 nodes of depth h and we choose S h to be one of them uniformly at random.
Furthermore, for any even integer h0 < h we note Shh0 the MAX node ancestor of S h of depth h0 .
7

Definition 3. We define d ? 0 as the smallest d such that for all ? there exists a > 0 for which for
all even h > 0,
?
?


0
? (h?h )/2
h


1 ?(Sh0 )<16 ?(1??) ?
Y
? h/2

?
h
? ? a? ?dh
E?
K
1
S
?
N
h
0
?
?
? h?h
0
0?h <h
h0 ?0(mod 2)

If no such d exists, we set d = ?.
This definition of d takes into account the size of the near-optimality set (just like ?) but unlike ? it
also takes into account the difficulty to identify the near-optimal paths.
Intuitively, the expected number of oracle calls performed by a given AVG node s is proportional to:
(1/?2 ) ? (the product of the inverted squared gaps of the set of MAX nodes in the path from the root to
s) ? (the probability of reaching s by following a policy which always tries to reach s).
Therefore, a near-optimal path with a larger number of small MAX node gaps can be considered
difficult. By assigning a larger weight to difficult nodes, we are able to give a better characterization
of the actual complexity of the problem and provide polynomial guarantees on the sample complexity
for N = ? when d is finite.
Theorem 3. If d is finite then there exists C > 0 such that for all ? > 0 and ? > 0, the expected
sample complexity of TrailBlazer satisfies
3

E [n(?, ?)] ? C

(log(1/?) + log(1/?))
?
?2+d

Note that this result holds in expectation only, contrary to Theorem 2 which holds in high probability.
We now give an example for which d = 0, followed by a special case of it.
Lemma 1. If there exists c > 0 and b > 2 such that for any near-optimal AVG node s,
P [? (?s ) ? x] ? cxb ,
where the random variable ?s is a successor state from s drawn from the MDP?s transition probabilities, then d = 0 and consequently the sample complexity is of order 1/?2 .
Remark 2. If there exists ?min such that for any near-optimal MAX node s, ?(s) ? ?min then
b
d = 0 and the sample complexity is of order 1/?2 . Indeed, in this case as P [?s ? x] ? (x/?min )
for any b > 2 for which d = 0 by Lemma 1.

5

Conclusion

We provide a new Monte-Carlo planning algorithm TrailBlazer that works for MDPs where the
number of next states N can be either finite or infinite. TrailBlazer is easy to implement and
is numerically efficient. It comes packaged with a PAC consistency and two problem-dependent
sample-complexity guarantees expressed in terms of a measure (defined by ?) of the quantity of
near-optimal nodes or a measure (defined by d) of the difficulty to identify the near-optimal paths.
The sample complexity of TrailBlazer improves over previous worst-case guarantees. What?s
more, TrailBlazer exploits MDPs with specific structure by exploring only a fraction of the whole
search space when either ? or d is small. In particular, we showed that if the set of near-optimal nodes
2
e
have non-vanishing action-gaps, then the sample complexity is O(1/?
), which is the same rate as
Monte-Carlo sampling. This is a pretty decent evidence that TrailBlazer is a natural extension of
Monte-Carlo sampling to stochastic control problems.
Acknowledgements The research presented in this paper was supported by French Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council, a doctoral grant of ?cole Normale Sup?rieure in Paris,
Inria and Carnegie Mellon University associated-team project EduBand, and French National Research Agency
projects ExTra-Learn (n.ANR-14-CE24-0010-01) and BoB (n.ANR-16-CE23-0003)

8

References
[1] Peter Auer, Nicol? Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed
bandit problem. Machine Learning, 47(2-3):235?256, 2002.
[2] Richard Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, 1957.
[3] Dimitri Bertsekas and John Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific,
Belmont, MA, 1996.
[4] Cameron B. Browne, Edward Powley, Daniel Whitehouse, Simon M. Lucas, Peter I. Cowling,
Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton.
A survey of Monte Carlo tree search methods. IEEE Transactions on Computational Intelligence
and AI in Games, 4(1):1?43, 2012.
[5] S?bastien Bubeck and R?mi Munos. Open-loop optimistic planning. In Conference on Learning
Theory, 2010.
[6] Lucian Bu?soniu and R?mi Munos. Optimistic planning for Markov decision processes. In
International Conference on Artificial Intelligence and Statistics, 2012.
[7] R?mi Coulom. Efficient selectivity and backup operators in Monte-Carlo tree search. Computers
and games, 4630:72?83, 2007.
[8] Sylvain Gelly, Wang Yizao, R?mi Munos, and Olivier Teytaud. Modification of UCT with
patterns in Monte-Carlo Go. Technical report, Inria, 2006. URL https://hal.inria.fr/
inria-00117266.
[9] Arthur Guez, David Silver, and Peter Dayan. Efficient Bayes-adaptive reinforcement learning
using sample-based search. Neural Information Processing Systems, 2012.
[10] Jean-Francois Hren and R?mi Munos. Optimistic Planning of Deterministic Systems. In
European Workshop on Reinforcement Learning, 2008.
[11] Michael Kearns, Yishay Mansour, and Andrew Y. Ng. A sparse sampling algorithm for nearoptimal planning in large Markov decision processes. In International Conference on Artificial
Intelligence and Statistics, 1999.
[12] Levente Kocsis and Csaba Szepesv?ri. Bandit-based Monte-Carlo planning. In European
Conference on Machine Learning, 2006.
[13] R?mi Munos. From bandits to Monte-Carlo tree search: The optimistic principle applied to
optimization and planning. Foundations and Trends in Machine Learning, 7(1):1?130, 2014.
[14] David Silver and Joel Veness. Monte-Carlo planning in large POMDPs. In Neural Information
Processing Systems, 2010.
[15] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the
game of Go with deep neural networks and tree search. Nature, 529(7587):484?489, 2016.
[16] Bal?zs Sz?r?nyi, Gunnar Kedenburg, and R?mi Munos. Optimistic planning in Markov decision
processes using a generative model. In Neural Information Processing Systems, 2014.
[17] Thomas J Walsh, Sergiu Goschin, and Michael L Littman. Integrating sample-based planning
and model-based reinforcement learning. AAAI Conference on Artificial Intelligence, 2010.

9

"
3216,2010,A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction,"In this paper we propose an approximated learning framework for large scale graphical models and derive message passing algorithms for learning their parameters efficiently.  We first relate CRFs and structured SVMs  and show that in the CRF's primal a variant of the log-partition function, known as soft-max, smoothly approximates the hinge loss function of structured SVMs.  We then propose an intuitive approximation for structured prediction problems using Fenchel duality based on a local entropy approximation that computes the exact gradients of the approximated problem and is guaranteed to converge. Unlike existing approaches, this allow us to learn graphical models with cycles and very large number of parameters efficiently. We demonstrate the effectiveness of our approach  in an image denoising task. This task was previously solved by sharing parameters across cliques. In contrast, our algorithm is able to efficiently learn large number of parameters resulting in orders of magnitude better prediction.","A Primal-Dual Message-Passing Algorithm for
Approximated Large Scale Structured Prediction

Raquel Urtasun
TTI Chicago
rurtasun@ttic.edu

Tamir Hazan
TTI Chicago
hazan@ttic.edu

Abstract
In this paper we propose an approximated structured prediction framework for
large scale graphical models and derive message-passing algorithms for learning their parameters efficiently. We first relate CRFs and structured SVMs and
show that in CRFs a variant of the log-partition function, known as the soft-max,
smoothly approximates the hinge loss function of structured SVMs. We then
propose an intuitive approximation for the structured prediction problem, using
duality, based on a local entropy approximation and derive an efficient messagepassing algorithm that is guaranteed to converge. Unlike existing approaches, this
allows us to learn efficiently graphical models with cycles and very large number
of parameters.

1

Introduction

Unlike standard supervised learning problems which involve simple scalar outputs, structured prediction deals with structured outputs such as sequences, grids, or more general graphs. Ideally,
one would want to make joint predictions on the structured labels instead of simply predicting each
element independently, as this additionally accounts for the statistical correlations between label
elements, as well as between training examples and their labels. These properties make structured
prediction appealing for a wide range of applications such as image segmentation, image denoising,
sequence labeling and natural language parsing.
Several structured prediction models have been recently proposed, including log-likelihood models
such as conditional random fields (CRFs, [10]), and structured support vector machines (structured
SVMs) such as maximum-margin Markov networks (M3Ns [21]). For CRFs, learning is done by
minimizing a convex function composed of a negative log-likelihood loss and a regularization term.
Learning structured SVMs is done by minimizing the convex regularized structured hinge loss.
Despite the convexity of the objective functions, finding the optimal parameters of these models can
be computationally expensive since it involves exponentially many labels. When the label structure
corresponds to a tree, learning can be done efficiently by using belief propagation as a subroutine;
The sum-product algorithm is typically used in CRFs and the max-product algorithm in structured
SVMs. In general, when the label structure corresponds to a general graph, one cannot compute
the objective nor the gradient exactly, except for some special cases in structured SVMs, such as
matching and sub-modular functions [22]. Therefore, one usually resorts to approximate inference
algorithms, cf. [2] for structured SVMs and [20, 12] for CRFs. However, the approximate inference
algorithms are computationally too expensive to be used as a subroutine of the learning algorithm,
therefore they cannot be applied efficiently for large scale structured prediction problems. Also, it is
not clear how to define a stopping criteria for these approaches as the objective does not monotonically decrease since the objective and the gradient are both approximated. This might result in poor
approximations.
In this paper we propose an approximated structured prediction framework for large scale graphical
models and derive message-passing algorithms for learning their parameters efficiently. We relate
CRFs and structured SVMs, and show that in CRFs a variant of the log-partition function, known as
1

soft-max, smoothly approximates the hinge loss function of structured SVMs. We then propose an
intuitive approximation for the structured prediction problem, using duality, based on a local entropy
approximation and derive an efficient message-passing algorithm that is guaranteed to converge.
Unlike existing approaches, this allows us to learn efficiently graphical models with cycles and
very large number of parameters. We demonstrate the effectiveness of our approach in an image
denoising task. This task was previously solved by sharing parameters across cliques. In contrast,
our algorithm is able to efficiently learn large number of parameters resulting in orders of magnitude
better prediction.
In the remaining of the paper, we first relate CRFs and structured SVMs in Section 3, show our
approximate prediction framework in Section 4, derive a message-passing algorithm to solve the
approximated problem efficiently in Section 5, and show our experimental evaluation.

2

Regularized Structured Loss Minimization

Consider a supervised learning setting with objects x ? X and labels y ? Y. In structured prediction
the labels may be sequences, trees, grids, or other high-dimensional objects with internal structure.
Consider a function ? : X ? Y ? Rd that maps (x, y) pairs to feature vectors. Our goal is to
construct a linear prediction rule
y? (x) = argmax ? > ?(x, y)
y?Y
d

with parameters ? ? R , such that y? (x) is a good approximation to the true label of x. Intuitively
one would like to minimize the loss `(y, y? ) incurred by using ? to predict the label of x, given that
the true label is y. However, since the prediction is norm-insensitive this method can lead to over
fitting. Therefore the parameters ? are typically learned by minimizing the norm-dependent loss
X
? x, y) + C k?kp ,
`(?,
(1)
p
p
(x,y)?S

defined over a training set S. The function `? is a surrogate loss of the true loss `(y, y?). In this paper
we focus on structured SVMs and CRFs which are the most common structured prediction models.
The first definition of structured SVMs used the structured hinge loss [21]
n
o
`?hinge (?, x, y) = max `(y, y?) + ? > ?(x, y?) ? ? > ?(x, y)
y??Y

The structured hinge loss upper bounds the true loss function, and corresponds to a maximummargin approach that explicitly penalizes training examples (x, y) for which ? > ?(x, y) <
`(y, y? (x)) + ? > ?(x, y? (x)).
The second loss function that we consider is based on log-linear models, and is commonly used in
CRFs [10]. Let the conditional distribution be




X
1
p(?
y |x, y; ?) =
exp `(y, y?) + ? > ?(x, y?) ,
Z(x, y) =
exp `(y, y?) + ? > ?(x, y?)
Z(x, y)
y??Y

where `(y, y?) is a prior distribution and Z(x, y) the partition function. The surrogate loss function
is then the negative log-likelihood under the parameters ?
1
`?log (?, x, y) = ln
.
p(?
y |x, y; ?)
In structured SVMs and CRFs a convex loss function and a convex regularization are minimized.

3

One parameter extension of CRFs and Structured SVMs

In CRFs one aims to minimize the regularized negative log-likelihood of the conditional distribution
p(?
y |x, y; ?) which decomposes into the log-partition and the linear term ? > ?(x, y). Hence the
problem of minimizing the regularized loss in (1) with the loss function `?log can be written as
?
?
? X
?
C
(CRF)
min
ln Z(x, y) ? d> ? + k?kpp ,
?
? ?
p
(x,y)?S

2

where (x, y) ? S ranges over training pairs and d =
means.

P

(x,y)?S

?(x, y) is the vector of empirical

Structured SVMs aim at minimizing the regularized hinge loss `?hinge (?, x, y), which measures the
loss of the label y? (x) that most violates the training pair (x, y) ? S by more than `(y, y? (x)).
Since y? (x) is independent of the training label y, the structured SVM program takes the form:
?
?
? X
?
n
o
C
(structured SVM)
min
max `(y, y?) + ? > ?(x, y?) ? d> ? + k?kpp ,
?
? ?
y??Y
p
(x,y)?S

where (x, y) ? S ranges over the training pairs, and d is the vector of empirical means.
In the following we deal with both structured prediction tasks (i.e., structured SVMs and CRFs)
as two instances of
 the same framework,
 by extending the partition function to norms, namely
>
Z (x, y) = k exp `(y, y?) + ? ?(x, y?) k1/ , where the norm is computed for the vector ranging over y? ? Y. Using the norm formulation we move from the partition function, for  = 1, to the
maximum over the exponential function for  = 0. Equivalently, we relate the log-partition and the
max-function by the soft-max function
!
X
`(y, y?) + ? > ?(x, y?)
ln Z (x, y) =  ln
exp
(2)

y??Y

For  = 1 the soft-max function reduces to the log-partition function, and for  = 0 it reduces
to the max-function. Moreover, when  ? 0 the soft-max function is a smooth approximation of
the max-function, in the same way the `1/ -norm is a smooth approximation of the `? -norm. This
smooth approximation of the max-function is used in different areas of research [8]. We thus define
the structured prediction problem as
?
?
?
? X
C
(3)
(structured-prediction)
min
ln Z (x, y) ? d> ? + k?kpp ,
?
? ?
p
(x,y)?S

which is a one-parameter extension of CRFs and structured SVMs, i.e.,  = 1 and  = 0 respectively. Similarly to CRFs and structured SVMs [11, 16], one can use gradient methods to optimize
structured prediction. The gradient of ?r takes the form
X X
p (?
y |x, y; ?)?r (x, y?) ? dr + |?r |p?1 sign(?r ),
(4)
(x,y)?S

where

y?

1
p (?
y |x, y; ?) =
exp
Z (x, y)1/

`(y, y?) + ? > ?(x, y?)


!
(5)

is a probability distribution over the possible labels y? ? Y. When  ? 0 this probability distribution
gets concentrated around its maximal values, since all its elements are raised to the power of a very
large number (i.e., 1/). Therefore for  = 0 we get a structured SVM subgradient.
In many real-life applications the labels y ? Y are n-tuples, y = (y1 , ..., yn ), hence there are
exponentially many labels in Y. The feature maps usually describe relations between subsets of
label elements y? ? {y1 , ..., yn }, and local interactions on single label elements yv , namely
X
X
?r (x, y?1 , ..., y?n ) =
?r,v (x, y?v ) +
?r,? (x, y?? ).
(6)
v?Vr,x

??Er,x

Each feature ?r (x, y?) can be described by its factor graph Gr,x , a bipartite graph with one set of
nodes corresponding to Vr,x and the other set corresponds to Er,x . An edge connects a single label
node v ? Vr,x with a subset of label nodes ? ? Er,x if and only if yv ? y? . In the following we
consider the factor graph G = ?r Gr which is the union of all factor graphs. We denote by N (v)
and N (?) the set of neighbors of v and ? respectively, P
in the factor graph G. For clarity in the
n
presentation we consider fully factorized loss `(y, y?) = v=1 `v (yv , y?v ), although our derivation
naturally extends to any graphical model representing the interactions `(y, y?).
3

To compute the soft-max and the marginal probabilities, p (?
yv |x, y; ?) and p (?
y? |x, y; ?), exponentially many labels have to be considered. This is in general computationally prohibitive, and
thus one has to rely on inference and message-passing algorithms. When the factor graph has no
cycles inference can be efficiently computed using belief propagation, but in the presence of cycles
inference can only be approximated [25, 26, 7, 5, 13]. There are two main problems when dealing with graphs with cycles and approximate inference: efficiency and accuracy. For graphs with
cycles there are no guarantees on the number of steps the message-passing algorithm requires till
convergence, therefore it is computationally costly to run it as a subroutine. Moreover, as these
message-passing algorithms have no guarantees on the quality of their solution, the gradient and the
objective function can only be approximated, and one cannot know if the update rule decreased or
increased the structured prediction objective. In contrast, in this work we propose to approximate
the structured prediction problem and to efficiently solve the approximated problem exactly using
message-passing. Intuitively, we suggest a principled way to run the approximate inference updates
for few steps, while re-using the messages of previous steps to extract intermediate beliefs. These
beliefs are used to update ?r , although the intermediate beliefs may not agree on their marginal
probabilities. This allows us to efficiently learn graphical models with large number of parameters.

4

Approximate Structured Prediction

The structured prediction objective in (3) and its gradients defined in (4) cannot be computed efficiently for general graphs since both involve computing the soft-max function, ln Z (x, y), and
the marginal probabilities, p (?
yv |x, y; ?) and p (?
y? |x, y; ?), which take into account exponentially
many elements y? ? Y . In the following we suggest an intuitive approximation for structured prediction, based on its dual formulation.
Since the dual of the soft-max is the entropy barrier, it follows that the dual program for structured
prediction is governed by the entropy function of the probabilities px,y (?
y ). The following duality
formulation is known for CRFs when  = 1 with `22 regularization, and for structured SVM when
 = 0 with `22 regularization, [11, 21, 1]. Here we derive the dual program for every  and every `pp
regularization using conjugate duality:
Claim 1 The dual program of the structured prediction program in (3) takes the form
?
max
px,y (?
y )??Y

X
(x,y)?S

?H(px,y ) +

?

X
y?

C 1?q
px,y (?
y )`(y, y?)? ?
q



q



 X X


px,y (?
y )?(x, y?) ? d


 ,

(x,y)?S y??Y

q

where ?Y is the probability simplex over Y and H(px,y ) = ?

P

y ) ln px,y (?
y)
y? px,y (?

is the entropy.

Proof: In [6]
When  = 1 the CRF dual program reduces to the well-known duality relation between the loglikelihood and the entropy. When  = 0 we obtain the dual formulation of structured SVM which
emphasizes the duality relation between the max-function and the probability simplex. In general,
Claim 1 describes the relation between the soft-max function and the entropy barrier over the probability simplex.
The dual program in Claim 1 considers the probabilities px,y (?
y ) over exponentially many labels
y? ? Y, as well as their entropies H(px,y ). However, when we take into account the graphical
model imposed by the features, Gr,x , we observe that the linear terms in the dual formulation consider the marginals probabilities px,y (?
yv ) and px,y (?
y? ). We thus propose to replace the marginal
probabilities
with their
P
Pcorresponding beliefs, and to replace the entropy term by the local entropies
? c? H(bx,y,? ) +
v cv H(bx,y,v ) over the beliefs. Whenever , cv , c? ? 0, the approximated
dual is concave and it corresponds to a convex dual program. By deriving its dual we obtain our
approximated structured prediction, for which we construct an efficient algorithm in Section 5.

4

LBP-SGD
LBP-SMD
LBP-BFGS
MF-SGD
MF-SMD
MF-BFGS
Ours

I1
2.7344
2.7344
2.7417
3.0469
2.9688
3.0005
0.0488

Gaussian noise
I2
I3
2.4707 3.2275
2.4731 3.2324
2.4194 3.1299
3.0762 4,1382
3.0640 3.8721
2.7783 3.6157
0.0073 0.1294

I4
2.3193
2.3145
2.4023
2.9053
14.4360
2.4780
0.1318

I1
5.2905
5.2954
5.2148
10.0488
?
5.2661
0.0537

Bimodal noise
I2
I3
4.4751
6.8164
4.4678
6.7578
4.3994
6.0278
41.0718 29.6338
?
?
4.6167
6.4624
0.0244
0.1221

I4
7.2510
7.2583
6.6211
53.6035
?
7.2510
0.9277

Figure 1: Gaussian and bimodal noise: Comparison of our approach to loopy belief propagation and mean field approximations when optimizing using BFGS, SGD and SMD. Note that our
approach significantly outperforms all the baselines. MF-SMD did not work for Bimodal noise.
Theorem 1 The approximation of the structured prediction program in (3) takes the form
!
P
P
X
X
`v (yv , y?v ) + r:v?Vr,x ?r ?r,v (x, y?v ) ? ??N (v) ?x,y,v?? (?
yv )
exp
min
cv ln
?x,y,v?? ,?
cv
y?v
(x,y)?S,v
!
P
P
X
X
?? ) + v?N (?) ?x,y,v?? (?
yv )
C
r:??Er ?r ?r,? (x, y
p
exp
? d> ? ? k?kp
+
c? ln
c?
p
(x,y)?S,?

y??

Proof: In [6]

5

Message-Passing Algorithm for Approximated Structured Prediction

In the following we describe a block coordinate descent algorithm for the approximated structured
prediction program of Theorem 1. Coordinate descent methods are appealing as they optimize a
small number of variables while holding the rest fixed, therefore they are efficient and can be easily
parallelized. Since the primal program is lower bounded by the dual program, the primal objective
function is guaranteed to converge. We begin by describing how to find the optimal set of variables
related to a node v in the graphical model, namely ?x,y,v?? (?
yv ) for every ? ? N (v), every y?v and
every (x, y) ? S.
Lemma 1 Given a vertex v in the graphical model, the optimal ?x,y,v?? (?
yv ) for every ? ?
N (v), y?v ? Yv , (x, y) ? S in the approximated program of Theorem 1 satisfies
?
!?
P
P
X
?? ) + u?N (?)\v ?x,y,u?? (?
yu )
r:??Er,x ?r ?r,? (x, y
?
?x,y,??v (?
yv ) = c? ln ?
exp
c?
y?? \?
yv
?
?
X
X
c? ?
?x,y,v?? (?
yv ) =
`v (yv , y?v ) +
?r ?r,v (x, y?v ) +
?x,y,??v (?
yv )? ? ?x,y,??v (?
yv ) + cx,y,v??
c?v
r:v?Vr,x

1

??N (v)

P

for every constant cx,y,v?? , where c?v = cv + ??N (v) c? . In particular, if either  and/or c?
are zero then ?x,y,??v corresponds to the `? norm and can be computed by the max-function.
Moreover, if either  and/or c? are zero in the objective, then the optimal ?x,y,v?? can be computed
for any arbitrary c? > 0, and similarly for cv > 0.
Proof: In [6]
It is computationally appealing to find the optimal ?x,y,v?? (?
yv ). When the optimal value cannot be
found, one usually takes a step in the direction of the negative gradient and the objective function
needs to be computed to ensure that the chosen step size reduces the objective. Obviously, computing the objective function at every iteration significantly slows the algorithm. When the optimal
?x,y,v?? (?
yv ) can be found, the block coordinate descent algorithm can be executed efficiently in
distributed manner, since every ?x,y,v?? (?
yv ) can be computed independently. The only interactions
occur when computing the normalization step cx,y,v?? . This allows for easy computation in GPUs.
We now turn to describe how to change ? in order to improve the approximated structured prediction.
Since we cannot find the optimal ?r while holding the rest fixed, we perform a step in the direction
1

For numerical stability in our algorithm we set cx,y,v?? such that

5

P

y
?v

?x,y,v?? (?
yv ) = 0

of the negative gradient, when , c? , ci are positive, or in the direction of the subgradient otherwise.
We choose the step size ? to guarantee a descent on the objective.
Lemma 2 The gradient of the approximated structured prediction program in Theorem 1 with respect to ?r equals to
X
X
bx,y,v (?
yv )?r,v (x, y?v ) +
bx,y,? (?
y? )?r,? (x, y?? ) ? dr + C ? |?r |p?1 ? sign(?r ),
(x,y)?S,v?Vr,x ,?
yv

(x,y)?S,??Er,x ,?
y?

where
bx,y,v (?
yv ) ? exp

`v (yv , y?v ) +
P

bx,y,? (?
y? ) ?

exp

r:??Er,x

P

r:v?Vr,x

?r ?r,v (x, y?v ) ?

?r ?r,? (x, y?? ) +

cv
P

v?N (?)

P

??N (v)

?x,y,v?? (?
yv )

?x,y,v?? (?
yv )

!

!

c?

However, if either  and/or c? equal zero, then the beliefs bx,y,? (?
y? ) can be taken from the
set of probabilityndistributions over support of the max-beliefs, namely
y?? ) > 0 only if
o bx,y,? (?
P
P
?? ) + v?N (?) ?x,y,v?? (?
y??? ? argmaxy??
y? ) . Similarly for bx,y,v (?
yv? )
r:??Er,x ?r ?r,? (x, y
whenever  and/or cv equal zero.
Proof: In [6]
Lemmas 1 and 2 describe the coordinate descent algorithm for the approximated structured prediction in Theorem 1. We refer the reader to [6] for a summary of our algorithm.
The coordinate descent algorithm is guaranteed to converge, as it monotonically decreases the approximated structured prediction objective in Theorem 1, which is lower bounded by its dual program. However, convergence to the global minimum cannot be guaranteed in all cases. In particular,
for  = 0 the coordinate descent on the approximated structured SVMs is not guaranteed to converge
to its global minimum, unless one uses subgradient methods which are not monotonically decreasing. Moreover, even when we are guaranteed to converge to the global minimum, i.e., , c? , cv > 0,
the sequence of variables ?x,y,v?? (?
yv ) generated by the algorithm is not guaranteed to converge
to an optimal solution, nor to be bounded. As a trivial example, adding an arbitrary constant to the
variables, ?x,y,v?? (?
yv ) + c, does not change the objective value, hence the algorithm can generate
non-decreasing unbounded sequences. However, the beliefs generated by the algorithm are bounded
and guaranteed to converge to the solution of the dual approximated structured prediction problem.
Claim 2 The block coordinate descent algorithm in lemmas 1 and 2 monotonically reduces the
approximated structured prediction objective in Theorem 1, therefore the value of its objective is
guaranteed to converge. Moreover, if , c? , cv > 0, the objective is guaranteed to converge to the
global minimum, and its sequence of beliefs are guaranteed to converge to the unique solution of the
approximated structured prediction dual.
Proof: In [6]
The convergence result has a practical implication, describing the ways we can estimate the convergence of the algorithm, either by the primal objective, the dual objective or the beliefs. The
approximated structured prediction can also be used for non-concave entropy approximations, such
as the Bethe entropy, where c? > 0 and cv < 0. In this case the algorithm is well defined, and its
stationary points correspond to the stationary points of the approximated structured prediction and
its dual. Intuitively, this statement holds since the coordinate descent algorithm iterates over points
?x,y,v?? (?
yv ), ?r with vanishing gradients. Equivalently the algorithm iterates over saddle points
?x,y,v?? (?
yv ), bx,y,v (?
yv ), bx,y,? (?
y? ) and (?r , zr ) of the Lagrangian defined in Theorem 1. Whenever the dual program is concave these saddle points are optimal points of the convex primal, but for
non-concave dual the algorithm iterates over saddle points. This is summarized in the claim below:
Claim 3 Whenever the approximated structured prediction is non convex, i.e., , c? > 0 and cv < 0,
the algorithm in lemmas 1 and 2 is not guaranteed to converge, but whenever it converges it reaches
a stationary point of the primal and dual approximated structured prediction programs.
Proof: In [6]
6

Figure 2: Denoising results: Gaussian (left) and Bimodal (right) noise.

6

Experimental evaluation

We performed experiments on 2D grids since they are widely used to represent images, and have
many cycles. We first investigate the role of  in the accuracy and running time of our algorithm, for
fixed c? , cv = 1. We used a 10 ? 10 binary image and randomly generated 10 corrupted samples
flipping every bit with 0.2 probability. We trained the model using CRF, structured-SVM and our
approach for  = {1, 0.5, 0.01, 0}, ranging from approximated CRFs ( = 1) to approximated
structured SVM ( = 0) and its smooth version ( = 0.01). The runtime for CRF and structuredSVM is order of magnitudes larger than our method since they require exact inference for every
training example and every iteration of the algorithm. For the approximated structured prediction,
the runtimes are 323, 324, 326, 294 seconds for  = {1, 0.5, 0.01, 0} respectively. As  gets smaller
the runtime slightly increases, but it decreases for  = 0 since the `? norm is computed efficiently
using the max function. However,  = 0 is less accurate than  = 0.01; When the approximated
structured SVM converges, the gap between the primal and dual objectives was 1.3, and only 10?5
for  > 0. This is to be expected since the approximated structured SVM is non-smooth (Claim 2),
and we did not used subgradient methods to ensure convergence to the optimal solution.
We generated test images in a similar fashion while using the same  for training and testing. In
this setting both CRF and structured-SVM performed well, with 2 misclassifications. For the approximated structured prediction, we obtained 2 misclassifications for  > 0. We also evaluated the
quality of the solution using different values of  for training and inference [24]. When predicting
with smaller  than the one used for learning the results are marginally worse than when predicting
with the same . However, when predicting with larger , the results get significantly worse, e.g.,
learning with  = 0.01 and predicting with  = 1 results in 10 errors, and only 2 when  = 0.01.
The main advantage of our algorithm is that it can efficiently learn many parameters. We now compared in a 5 ? 5 dataset a model learned with different parameters for every edge and vertex (? 300
parameters) and a model learned with parameters shared among the vertices and edges (2 parameters
for edges and 2 for vertices) [9]. Using large number of parameters increases performance: sharing
parameters resulted in 16 misclassifications, while optimizing over the 300 parameters resulted in 2
errors. Our algorithm avoids overfitting in this case, we conjecture it is due to the regularization.
We now compare our approach to state-of-the-art CRF solvers on the binary image dataset of [9]
that consists of 4 different 64 ? 64 base images. Each base image was corrupted 50 times with each
type of noise. Following [23], we trained different models to denoise each individual image, using
40 examples for training and 10 for test. We compare our approach to approximating the conditional
likelihood using loopy belief propagation (LBP) and mean field approximation (MF). For each of
these approximations, we use stochastic gradient descent (SGD), stochastic meta-descent (SMD)
and BFGS to learn the parameters. We do not report pseudolikelihood (PL) results since it did not
work. The same behavior of PL was noticed by [23]. To reduce the computational complexity and
the chances of convergence, [9, 23] forced their parameters to be shared across all nodes such that
?i, ?i = ?n and ?i, ?j ? N (i), ?ij = ?e . In contrast, since our approach is efficient, we can exploit
the full flexibility of the graph and learn more than 10, 000 parameters. This is computationally
prohibitive with the baselines. We use the pixel values as node potentials and an Ising model with
only bias for the edge potentials, i.e., ?i,j = [1, ?1; ?1, 1]. For all experiments we use  = 1, and
p = 2. For the baselines, we use the code, features and optimal parameters of [23].
Under the first noise model, each pixel was corrupted via i.i.d. Gaussian noise with mean 0 and standard deviation of 0.3. Fig. 1 depicts test error in (%) for the different base images (i.e., I1 , . . . , I4 ).
Note that our approach outperforms considerably the loopy belief propagation and mean field approximations for all optimization criteria (BFGS, SGD, SMD). For example, for the first base image
the error of our approach is 0.0488%, which is equivalent to a 2 pixels error on average. In contrast
7

?2000

?2000

Primal
Dual

Primal
Dual

?3000

?3000

?4000

?4000

?5000

?5000

?6000

?6000

?7000

?7000

?8000

0

5

10

15

20

?8000

25

Iterations

0

5

10

15

20

25

Iterations

(Gaussian)

(Bimodal)

Figure 3: Convergence. Primal and dual train errors for I1 .
the best baseline gets 112 pixels wrong on average. Fig. 2 (left) depicts test examples as well as our
denoising results. Note that our approach is able to cope with large amounts of noise.
Under the second noise model, each pixel was corrupted with an independent mixture of Gaussians.
For each class, a mixture of 2 Gaussians with equal mixing weights was used, yielding the Bimodal
noise. The mixture model parameters were (0.08, 0.03) and (0.46, 0.03) for the first class and
(0.55, 0.02) and (0.42, 0.10) for the second class, with (a, b) a Gaussian with mean a and standard
deviation b. Fig. 1 depicts test error in (%) for the different base images. As before, our approach
outperforms all the baselines. We do not report MF-SMD results since it did not work. Denoised
images are shown in Fig. 2 (right). We now show how our algorithm converges in a few iterations.
Fig. 3 depicts the primal and dual training errors as a function of the number of iterations. Note that
our algorithm converges, and the dual and primal values are very tight after a few iterations.

7

Related Work

For the special case of CRFs, the idea of approximating the entropy function with local entropies
appears in [24, 3]. In particular, [24] proved that using a concave entropy approximation gives robust
prediction. [3] optimized the non-concave Bethe entropy c? = 1, cv = 1 ? |N (v)|, by repeatedly
maximizing its concave approximation, thus converging in few concave iterations. Our work differs
from these works in two aspects: we derive an efficient algorithm in Section 5 for the concave
approximated program (c? , cv > 0) and our framework and algorithm include structured SVMs, as
well as their smooth approximation when  ? 0.
Some forms of approximated structured prediction were investigated for the special cases of CRFs.
In [18] a similar program was used, but without the Lagrange multipliers ?x,y,v?? (?
yv ) and no
regularization, i.e., C = 0. As a result the local log-partition functions are unrelated, and efficient
counting algorithm can be used for learning. In [3] a different approximated program was derived for
c? = 1, cv = 0 which was solved by the BFGS convex solver. Our work is different as it considers
efficient algorithms for approximated structured prediction which take advantage of the graphical
model by sending messages along its edges. We show in the experiments that this significantly
improves the run-time of the algorithm. Also, our approximated structured prediction includes as
special cases approximated CRF, for  = 1, and approximated structured SVM, for  = 0. Moreover, we describe how to smoothly approximate the structured SVMs to avoid the shortcomings of
subgradient methods, by simply setting  ? 0 .
Some forms of approximated structured SVMs were dealt in [19] with the structured SMO algorithm. Independently, [14] presented an approximated structured SVMs program and a message
passing algorithm, which reduce to Theorem 1 and Lemma 1 with  = 0 and c? = 1, cv = 1.
However, in this algorithm the messages are not guaranteed to be bounded. They main difference of
[14] from our work is that they lack the dual formulation, which we use to prove that the structured
SVM smooth approximation, with  ? 0, is guaranteed to converge to optimum and that the dual
variables, i.e. the beliefs, are guaranteed to converge to the optimal beliefs. The relation between
the margin and the soft-max is similar to the one used in [17]. Independently, [4, 15] described the
connection between structured SVMs loss and CRFs loss. [15] also presented the one-parameter
extension of CRFs and structured SVMs described in (3).

8

Conclusion and Discussion

In this paper we have related CRFs and structured SVMs and shown that the soft-max, a variant of
the log-partition function, approximates smoothly the structured SVM hinge loss. We have also proposed an approximation for structured prediction problems based on local entropy approximations
and derived an efficient message-passing algorithm that is guaranteed to converge, even for general
graphs. We have demonstrated the effectiveness of our approach to learn graphs with large number
of parameters.We plan to investigate other domains of application such as image segmentation.
8

References
[1] M. Collins, A. Globerson, T. Koo, X. Carreras, and P.L. Bartlett. Exponentiated gradient algorithms for
conditional random fields and max-margin markov networks. JMLR, 9:1775?1822, 2008.
[2] T. Finley and T. Joachims. Training structural SVMs when exact inference is intractable. In ICML, pages
304?311. ACM, 2008.
[3] V. Ganapathi, D. Vickrey, J. Duchi, and D. Koller. Constrained approximate maximum entropy learning
of markov random fields. In UAI, 2008.
[4] K. Gimpel and N.A. Smith. Softmax-margin CRFs: Training log-linear models with cost functions. In
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the
Association for Computational Linguistics, pages 733?736. Association for Computational Linguistics,
2010.
[5] T. Hazan and A. Shashua. Norm-Product Belief Propagation: Primal-Dual Message-Passing for Approximate Inference. Arxiv preprint arXiv:0903.3127, 2009.
[6] T. Hazan and R. Urtasun. Approximated Structured Prediction for Learning Large Scale Graphical Models. Arxiv preprint arXiv:1006.2899, 2010.
[7] T. Heskes. Convexity arguments for efficient minimization of the Bethe and Kikuchi free energies. Journal
of Artificial Intelligence Research, 26(1):153?190, 2006.
[8] J.K. Johnson, D.M. Malioutov, and A.S. Willsky. Lagrangian relaxation for MAP estimation in graphical
models. In Proceedings of the Allerton Conference on Control, Communication and Computing. Citeseer,
2007.
[9] S. Kumar and M. Hebert. Discriminative Fields for Modeling Spatial Dependencies in Natural Images.
In Neural Information Processing Systems. MIT Press, Cambridge, MA, 2003.
[10] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models for segmenting
and labeling sequence data. In ICML, pages 282?289, 2001.
[11] G. Lebanon and J. Lafferty. Boosting and maximum likelihood for exponential models. NIPS, 1:447?454,
2002.
[12] A. Levin and Y. Weiss. Learning to Combine Bottom-Up and Top-Down Segmentation. In European
Conference on Computer Vision, 2006.
[13] T. Meltzer, A. Globerson, and Y. Weiss. Convergent message passing algorithms-a unifying view. In UAI,
2009.
[14] O. Meshi, D. Sontag, T. Jaakkola, and A. Globerson. Learning Efficiently with Approximate Inference
via Dual Losses. In Proc. ICML. Citeseer, 2010.
[15] P. Pletscher, C. Ong, and J. Buhmann. Entropy and Margin Maximization for Structured Output Learning.
Machine Learning and Knowledge Discovery in Databases, pages 83?98, 2010.
[16] N. Ratliff, J.A. Bagnell, and M. Zinkevich. Subgradient methods for maximum margin structured learning. In ICML Workshop on Learning in Structured Output Spaces, 2006.
[17] F. Sha and L.K. Saul. Large margin hidden Markov models for automatic speech recognition. Advances
in neural information processing systems, 19:1249, 2007.
[18] C. Sutton and A. McCallum. Piecewise training for structured prediction. Machine Learning, 77(2):165?
194, 2009.
[19] B. Taskar. Learning structured prediction models: a large margin approach. PhD thesis, Stanford, CA,
USA, 2005. Adviser-Koller, Daphne.
[20] B. Taskar, P. Abbeel, and D. Koller. Discriminative probabilistic models for relational data. In UAI, pages
895?902. Citeseer, 2002.
[21] B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. NIPS, 16:51, 2004.
[22] B. Taskar, S. Lacoste-Julien, and M. I. Jordan. Structured prediction, dual extragradient and Bregman
projections. JMLR, 7:1653?1684, 2006.
[23] S. Vishwanathan, N. Schraudolph, M. Schmidt, and K. Murphy. Accelerated Training of Conditional
Random Fields with Stochastic Meta-Descent . In International Conference in Machine Learning, 2006.
[24] M.J. Wainwright. Estimating the Wrong Graphical Model: Benefits in the Computation-Limited Setting.
JMLR, 7:1859, 2006.
[25] M.J. Wainwright and M.I. Jordan. Graphical models, exponential families, and variational inference.
R in Machine Learning, 1(1-2):1?305, 2008.
Foundations and Trends
[26] J.S. Yedidia, W. T. Freeman, and Y. Weiss. Constructing free-energy approximations and generalized
belief propagation algorithms. Transactions on Information Theory, 51(7):2282?2312, 2005.

9

"
2538,2007,Experience-Guided Search: A Theory of Attentional Control,Abstract Missing,"Experience-Guided Search:
A Theory of Attentional Control

Michael C. Mozer
Department of Computer Science and
Institute of Cognitive Science
University of Colorado
mozer@colorado.edu

David Baldwin
Department of Computer Science
Indiana University
Bloomington, IN 47405
baldwind@indiana.edu

Abstract
People perform a remarkable range of tasks that require search of the visual environment for a target item among distractors. The Guided Search model (Wolfe,
1994, 2007), or GS, is perhaps the best developed psychological account of human visual search. To prioritize search, GS assigns saliency to locations in the
visual field. Saliency is a linear combination of activations from retinotopic maps
representing primitive visual features. GS includes heuristics for setting the gain
coefficient associated with each map. Variants of GS have formalized the notion
of optimization as a principle of attentional control (e.g., Baldwin & Mozer, 2006;
Cave, 1999; Navalpakkam & Itti, 2006; Rao et al., 2002), but every GS-like model
must be ?dumbed down? to match human data, e.g., by corrupting the saliency map
with noise and by imposing arbitrary restrictions on gain modulation. We propose
a principled probabilistic formulation of GS, called Experience-Guided Search
(EGS), based on a generative model of the environment that makes three claims:
(1) Feature detectors produce Poisson spike trains whose rates are conditioned on
feature type and whether the feature belongs to a target or distractor; (2) the environment and/or task is nonstationary and can change over a sequence of trials;
and (3) a prior specifies that features are more likely to be present for target than
for distractors. Through experience, EGS infers latent environment variables that
determine the gains for guiding search. Control is thus cast as probabilistic inference, not optimization. We show that EGS can replicate a range of human data
from visual search, including data that GS does not address.

1

Introduction

Human visual activity often involves search. We search for our keys on a cluttered desk, a face in
a crowd, an exit sign on the highway, a key paragraph in a paper, our favorite brand of cereal at
the supermarket, etc. The flexibility of the human visual system stems from the endogenous (or
internal) control of attention, which allows for processing resources to be directed to task-relevant
regions and objects in the visual field. How is attention directed based on an individual?s goals? To
what sort of features of the visual environment can attention be directed? These two questions are
central to the study of how humans explore their environment.
Visual search has traditionally been studied in the laboratory using cluttered stimulus displays containing artificial objects. The objects are defined by a set of primitive visual features, such as
color, shape, and size. For example, an experimental task might be to search for a red vertical line
segment?the target?among green verticals and red horizontals?the distractors. Performance is
typically evaluated as the response latency to detect the presence or absence of a target with high
accuracy. The efficiency of visual search is often characterized by the search slope?the increase
1

visual
image
horizontal

vertical

green

noise
process

top-down
gains

primitive-feature
contrast maps
red

Figure 1: The architecture of Guided
Search

saliency
map

bottom-up activations

+

attentional
state

attentional
selection

in response latency with each additional distractor in the display. An inefficient search can often
require an additional 25?35 ms/item (or more, if eye movements are required).
Many computational models of visual search have been proposed to explain data from the burgeoning experimental literature (e.g., Baldwin & Mozer, 2006; Cave, 1999; Itti & Koch, 2001; Mozer,
1991; Navalpakkam & Itti, 2006; Sandon, 1990; Wolfe, 1994). Despite differences in their details,
they share central assumptions, perhaps most plainly emphasized by Wolfe?s (1994) Guided Search
2.0 (GS) model. We describe the central assumptions of GS, taking some liberty in ignoring details
and complications of GS that obfuscate the similarities within this class of models and that are not
essential for the purpose of this paper.1
As depicted Figure 1, GS posits that primitive visual features are detected across the retina in parallel
along dimensions such as color and orientation, yielding a set of feature activity maps. Feature
activations are scalars in [0, 1]. The feature maps represent each dimension via a coarse coding.
That is, the maps for a particular dimension are highly overlapping and broadly tuned. For example,
color might be represented by maps tuned to red, green, blue, and yellow; orientation might be
represented by maps tuned to left, right, steep, and shallow-sloped edges. The feature activity maps
are passed through a differencing mechanism that enhances local contrast and texture discontinuities,
yielding a bottom-up activation.
The bottom-up activations from all feature maps are combined to form a saliency map in which
activation at a location indicates the priority of that location for the task at hand. Attention is directed
to locations in order from most salient to least, and the object at each location is identified. GS
supposes that response latency is linear in the number of locations that need to be searched before
a target is found. (The model includes rules for terminating search if no target is found after a
reasonable amount of effort.)
Consider the task of searching for a red vertical bar among green vertical bars and red horizontal
bars. Ideally, attention should be drawn to red and vertical bars, not to green or horizontal bar. To
allow for guidance of attention, GS posits that a weight or top-down gain is associated with each
feature map, and the contribution of given feature map to the saliency map is scaled by the gain. It
is the responsibility of control processes to determining gains that emphasize task-relevant features.
Although gain modulation is a sensible means of implementing goal-directed action, it yields behavior than is more efficient than people appear to be. To elaborate, consider again the task of
searching for a red vertical. If the gains on the red and vertical maps are set to 1, and the gains on
green and horizontal maps are set to 0, then a target (red vertical) will have two units of activation
in the saliency map, whereas each distractor (red horizontal or green vertical) will have only one
unit of activation. Because the target is the most salient item and GS assumes that response time is
monotonically related to the saliency ranking of the target, the target should be located quickly, in a
time independent of the number of distractors. In contrast, human response times increase linearly
with the number of distractors in conjunction search.
To reduce search efficiency, GS assumes noise corruption of the saliency map. In the case of GS, the
signal-to-noise ratio is roughly 2:1. Baldwin and Mozer (2006) also require noise corruption for the
same reason, although the corruption is to the low-level feature representation not the saliency map.
Although Navalpakkam and Itti (2006) do not explicitly introduce noise in their model, they do so
implicitly via a selection rule that the probability of attending an item is proportional to its saliency.
To further reduce search efficiency, GS includes a complex set of rules that limit gain control. For example, gain modulation is allowed for only one feature map per dimension. Other attentional models
1
Although Guided Search has undergone refinement (Wolfe, 2007), the key claims summarized here are
unchanged. Recent extensions to GS consider eye movements and acuity changes with retinal eccentricity.

2

place similar, somewhat
P arbitrary limitations on gain modulation. Baldwin and Mozer (2006) impose the restriction i |gi ? 1| < c, where gi isP
the gain of feature map i and c is a constant.
Navalpakkam and Itti (2006) prefer the constraints i gi = c and gi > 0.
Finally, we mention one other key property the various models have in common. Gain tuning is
cast as an optimization problem: the goal of the model is to adjust the gains so as to maximize the
target saliency relative to distractor saliency for the task at hand. Baldwin and Mozer (2006) define
the criterion in terms of the target saliency ranking. Navalpakkam and Itti (2006) use the expected
target to distractor saliency ratio. Wolfe (1994) sets gains according to rules that he describes as
performing optimization.

2

Experience-Guided Search

The model we introduce in this paper makes three contributions over the class of Guided Search
models previously proposed. (1) GS uses noise or nondeterminism to match human data. In reality,
noise and nondeterminism serve to degrade the model?s performance over what it could otherwise
be. In contrast, all components of our model are justified on computational grounds, leading to a
more elegant, principled account. (2) GS imposes arbitrary limitations on gain modulation that also
result in the model performing worse than it otherwise could. Although limitations on gain modulation might be neurobiologically rationalized, a more elegant account would characterize these
limitations in terms of trade offs: constraints on gain modulation may limit performance, but they
yield certain benefits. Our model offers such a trade-off account. (3) In GS, attentional control is
achieved by tuning gains to optimize performance. In contrast, our model is designed to infer the
structure of its environment through experience, and gain modulation is a byproduct of this inference. Consequently, our model has no distinct control mechanism, leading to a novel perspective on
executive control processes in the brain.
Our approach begins with the premise that attention is fundamentally task based: a location in the
visual field is salient if a target is likely at that location. We define saliency as the target probability,
P (Tx = 1|Fx ), where Fx is the local feature activity vector at retinal location x and Tx is a binary
random variable indicating if location x contains a target. Torralba et al. (2006) and Zhang and
Cottrell (submitted) have also suggested that saliency should reflect target probability, although they
propose approaches to computing the target probability very different from ours. Our approach is to
compute the target probability using statistics obtained from recent experience performing the task.
Consequently, we refer to our model as experience-guided search or EGS.
To expand P (Tx |Fx ), we make the naive-Bayes assumption that the feature activities are independent of one another, yielding
Q
P1
Q
P (Tx |Fx , ?) = P (Tx ) i P (Fxi |Tx , ?)/ t=0 P (Tx = t) i P (Fxi |Tx = t, ?),
(1)
where ? is a vector of parameters that characterize the current stimulus environment in the current
task, and Fxi encodes the activity of feature i. Consider Fxi to be a rate-coded representation of
a neural spike train. Specifically, Fxi denotes the count of the number of spikes that occurred in a
window of n time intervals, where at most one spike can occur in each interval.
We propose a generative model of the environment in which Fxi is a binomial random variable,
Fxi |{Tx = t, ?} ? Binomial(?it , n), where a spike rate ?it is associated with feature i for target
(t = 1) and nontarget (t = 0) items. As n becomes large?i.e., the spike count is obtained over
a larger period of time?the binomial is well approximated by a Gaussian: Fxi |{Tx = t, ?} ?
N (n?it , n?it (1 ? ?it )). Using the Gaussian approximation, Equation 1 can be rewritten in the form
n
of a logistic function: P (Tx |Fx , ?) = 1/(1 + e?(rx + 2 sx ) ), where




1
XX
P (Tx = 1)
1X
?i1 (1 ? ?i1 )
1 ? 2t
rx = ln
?
ln
and sx =
(f?xi ??it )2 (2)
P (Tx = 0)
2 i
?i0 (1 ? ?i0 )
?
(1
?
?
)
it
it
i t=0
and f?xi = fxi /n denotes the observed spike rate for a feature detector.
Because of the logistic relationship, P (Tx |Fx , ?) is monotonic in rx + n2 sx . Consequently, if attentional priority is given to locations in order of their target probability, P (Tx |Fx , ?), then it is
3

equivalent to rank using rx + n2 sx . Further, if we assume that the target is equally likely in any
location, then rx is constant across locations, and sx can substitute for P (Tx |Fx , ?) as an equivalent
measure of saliency.
This saliency measure, sx , makes intuitive sense. Saliency at a location increases if feature i?s
activity is distant from the mean activity observed in the past for a distractor (?i0 ) and decreases if
feature i?s activity is distant from the mean activity observed in the past for a target (?i1 ). These
saliency increases (decreases) are scaled by the variance of the distractor (target) activities, such that
high-variance features have less impact on saliency.
Expanding the numerator terms in the definition of sx (Equation 2), we observe that sx can be written
2
as a linear combination of terms involving the feature activities, f?xi , and the squared activities, f?xi
(along with a constant term that can be ignored for ranking by saliency).
The
saliency
measure
P
sx in EGS is thus quite similar to the saliency measure in GS, sGS
= i gi f?xi . The differences
x
are: first, EGS incorporates quadratic terms, and second, gain coefficients of EGS are not free
parameters but are derived from statistics of targets and distractors in the current task and stimulus
environment. In this fact lies the virtue of EGS relative to GS: The control parameters are obtained
not by optimization, but are derived directly from statistics of the environment.
2.1

Uncertainty in the Environment Statistics

The model parameters, ?, could be maximum likelihood estimates obtained by observing target
and distractor activations over a series of trials. That is, suppose that each item in the display is
identified as a target or distractor. The set of activations of feature i at all locations containing a
target could be used to estimate ?i1 , and likewise with locations containing a distractor to estimate
?i0 . Alternatively, one could adopt a Bayesian approach and treat ?it as a random variable, whose
uncertainty is reduced by the evidence obtained on each trial. Because feature spike rates lie in [0, 1],
we define ?it as a beta random variable, ?it ? Beta(?it , ?it ).
0
This Bayesian approach also allows us to specify priors over ?it in terms of imaginary counts, ?it
0
and ?it . For example, in the absence of any task experience, a conservative assumption is that all
feature activations are predictive of a target, i.e., ?i1 should be drawn from a distribution biased
toward 1, and ?i0 should be drawn from a distribution biased toward 0.

To
R compute the target probabilities, we must marginalize over ?, i.e., P (Tx |Fx ) =
P (Tx |Fx , ?)p(?)d?. Unfortunately, this integral is impossible to evaluate analytically. We in?
stead compute the expectation of sx over ?, s?x ? E? (sx ), which has the solution
s?x =

1
XX
i



(?it + ?it ? 1)(?it + ?it ? 2) ?2
2(?it + ?it ? 1) ?
?it
(1 ? 2t)
fxi ?
fxi +
(?it ? 1)(?it ? 1)
?it ? 1
?it ? 1
t=0


(3)

Note that, like the expression for sx , s?x is a weighted sum of linear and quadratic feature-activity
terms. When ?it and ?it are large, the distribution of ?it is sharply peaked, and s?x approaches sx
with ?it = ?it /(?it + ?it ). When this condition is satisfied, ranking by s?x is equivalent to ranking
by P (Tx |Fx ). Although the equivalence is not guaranteed for smaller ?it and ?it , we have found
the equivalence to hold in empirical tests. Indeed, in our simulations, we find that defining saliency
as either sx or s?x yields similar results, reinforcing the robustness of our approach.
2.2

Modeling the Stimulus Environment

The parameter vectors ? and ? maintain a model of the stimulus environment in the context of the
current task. Following each trial, these parameters must be updated to reflect the statistics of the
trial. We assume that following a trial, each item in the display has been identified as either a target
or distractor. (All other adaptive attention models such as GS make this assumption.)
Consider a location x that has been labeled as type t (1 for target, 0 for distractor), and some feature
i at that location, Fxi . We earlier characterized Fxi as a binomial random variable reflecting a
spike count; that is, during n time intervals, fxi spikes are observed. Each time interval provides
evidence as to the value ?it . Given prior distribution ?it ? Beta(?it , ?it ), the posterior is ?it |Fxi ?
Beta(?it + fxi , ?it + n ? fxi ). However, to limit the evidence provided from each item, we scale it
4

by a factor of n. When all locations are considered, the resulting posterior is:


P
P
?it |Fi ? Beta ?it + x??t f?xi , ?it + x??t 1 ? f?xi

(4)

where Fi is feature map i and ?t is the set of locations containing elements of type t.
With the approach we?ve described, evidence concerning the value of ?it accumulates over a sequence of trials. However, if an environment is nonstationary, this build up of evidence is not
adaptive. We thus consider a switching model of the environment that specifies with probability ?,
the environment changes and all evidence should be discarded. The consequence of this assumption
0
0
is that the posterior distribution is a mixture of Equation 4 and the prior distribution, Beta(?it
, ?it
).
Modeling the mixture distribution is problematic because the number of mixture components grows
linearly with the number of trials. We could approximate the mixture distribution by the beta distribution that best approximates the mixture, in the sense of Kullback-Leibler divergence. However,
we chose to adopt a simpler, more intuitive solution: to interpolate between the two distributions.
The update rule we use is therefore
""
#
""
#!
X
X
0
0
?
?
?it |Fi ? Beta ?? + (1 ? ?) ?it +
fxi , ?? + (1 ? ?) ?it +
1 ? fxi
. (5)
it

it

x??t

3

x??t

Simulation Methodology

We present a step-by-step description of how the model runs to simulate experimental subjects performing a visual search task. We start by generating a sequence of experimental trials with the
0
0
properties studied in an experiment. The model is initialized with ?it = ?it
and ?it = ?it
. On each
simulation trial, the following sequence occurs. (1) Feature extraction is performed on the display
to obtain firing rates, f?xi for each location x and feature type i. (2) Saliency, s?x , is computed for
each location according to Equation 3. (3) The saliency rank of each location is assessed, and the
number of locations that need to be searched in order to identify the target is assumed to be equal to
the target rank. Response time should then be linear in target rank. (4) Following each trial, target
and distractor statistics, ?it and ?it , are updated according to Equation 5.
0
0
EGS has potentially many free parameters: {?it
} and {?i1
}, and ?. However, with no reason to
believe that one feature behaves differently than another, we assign all the features the same priors.
0
0
0
0
Further, we impose symmetry such that ?i0
= ?j1
= ? and ?i1
= ?j0
= ? for all i and j, reducing
the total number of free parameters to three.

Because we are focused on the issue of attentional control, we wanted to sidestep other issues, such
as feature extraction. Consequently, EGS uses the front-end preprocessing of GS. GS takes as input
an 8 ? 8 array of locations, each of which can contain a single colored bar. As described earlier, GS
analyzes the input via four broadly tuned features for color, and four for orientation. After a local
contrast-enhancement operator, GS yields activation values in [0, 1] at each of 8 ? 8 locations for
each of eight feature dimensions. We treat the activation produced by GS for feature i at location
x as the firing rate f?xi needed to simulate EGS. Like GS, the response time of EGS is linear in the
target ranking. A scaling factor is required to convert rank to response time; we chose 25 msec/item,
which is a fourth free parameter of GS.

4

Results

We simulated EGS on a series of tasks that Wolfe (1994) used to evaluate GS. Because GS is limited
to processing displays containing colored, oriented lines, some of the tasks constructed by Wolfe did
not have an exact correspondence in the experimental literature. Rather, Wolfe, the leading expert
in visual search, identified key findings that he wanted GS to replicate. Because EGS shares frontend processing with GS, EGS is limited to the same set of tasks as GS. Consequently, we present a
comparison of GS and EGS.
We began by replicating Wolfe?s results on GS. This replication was nontrivial, because GS contains
many parameters, rules, and special cases, and published descriptions of GS do not provide a crisp
5

algorithmic description of the model. To implement EGS, we simply removed much of the complexity of GS?including the distinction between bottom-up and top-down weights, heuristics for
setting the weights, and the injection of high-amplitude noise into the saliency map?and replaced
it with Equations 3 and 5.
Each simulation begins with a sequence of 100 practice trials, followed by a sequence of 1000 trials
for each blocked condition. Displays on each trial are generated according to the constraints of
the task with random variation with respect to unconstrained aspects of the task (e.g., locations of
display elements, distractor identities, etc.). In typical search tasks, the participant is asked to detect
the presence or absence of a target. We omit results for target-absent trials, since GS and EGS make
identical predictions for these trials.
The qualitative performance of EGS does not depend on its free parameters when two conditions
are met: ? > 0 and ? > ?. The latter condition yields E[?i1 ] > E[?i0 ] for all i, and corresponds
to the bias that features are more likely to be present for a target than for a distractor. This bias is
rational in order to prevent cognition from suppressing information that could potentially be critical
to behavior. All simulation results reported here used ? = 0.3, ? = 25, and ? = 10.
Figure 2 shows simulation results on six sets of tasks, labeled A?F. The first and third columns (thin
lines) are data from our replication of GS; the second and fourth columns (thick lines) are data from
our implementation of EGS. The key feature to note is that results from EGS are qualitatively and
quantitatively similar to results from GS. As should become clear when we explain the individual
tasks, EGS probably produces a better qualitative fit to the human data. (Unfortunately, it is not
feasible to place the human data side-by-side with the simulation results. Although the six sets of
tasks were chosen by Wolfe to represent key experiments in the literature, most are abstractions
of the original experimental tasks because the retina of GS?and its descendent EGS?is greatly
simplified and cannot accommodate the stimulus arrays used in human studies. Thus, Wolfe never
intended to quantitatively model specific experimental studies.)
We briefly describe the six tasks. The first four involve displays of a homogeneous color, and search
for a target orientation among distractors of different orientations. Task A explores search for a
vertical (defined as 0? ) target among homogeneous distractors of a different orientation. The graph
plots the slope of the line relating display size to response latency, as a function of the distractor
orientation. Search slopes become more efficient as the target-distractor similarity decreases. Task
B explores search for a target among two types of distractors as a function of display size. The
distractors are 100? apart, and the target is 40? and 60? from the distractors, but in one case the
target differs from the distractors in that it is the only nearly vertical item, allowing pop out via the
vertical feature detector. Note that pop out is not wired into EGS, but emerges because EGS identifies
vertical-feature activity as a reliable predictor of the target. Task C examines search efficiency for
a target among heterogeneous distractors, for two target orientations and two degrees of targetdistractor similarity. Search is more efficient when the target and distractors are dissimilar. (EGS
obtains results better matched to the human data than GS.) Task D explores an asymmetry in search:
it is more efficient to find a tilted bar among verticals than a vertical among tilted. This effect arises
from the same mechanism that yielded efficient search in task B: a unique feature is highly activated
when the target is tilted but not when it is vertical. And search is better guided to features that are
present than to features that are absent in EGS, due to the ? priors. Task E involves conjunction
search. The target is a red vertical among green vertical and red tilted distractors. The red item?s
tilt can be either 90? (i.e., horizontal) or 40? . Both distractor environments yield inefficient search,
but?consistent with human data?conjunction searches can vary in their relative difficulty.
Task F examines search efficiency for a red vertical among red 60? and yellow vertical distractors,
as a function of the ratio of the two distractor types. The result shows that search can be guided:
response times become faster as either the target color or target orientation becomes sparse, because
a relatively unique feature serves as a reliable cue to the target. Figure 3a depicts how EGS adapts
differently for the extreme conditions in which the distractors are mostly vertical (dark bars) or
mostly red (light bars). The bars represent E[?i0 ]; the lower the value, the more a feature is viewed as
reliably discriminating targets and distractors. (E[?i1 ] is independent of the experimental condition.)
When distractors are mostly vertical, the red feature is a better cue, and vice versa. The standard
explanation for this phenomenon in the psychological literature is that subjects operate in two stages,
first filtering out based on the more discriminative feature, and then serially searching the remaining

6

(D)
Feature Search Asymmetry

5
0

10
5
0

1000
800
600
400

10 20 30 40 50
Distractor Orientation

10 20 30 40 50
Distractor Orientation

1200
RT (msec)

10

1200

15

10

400

800
T: 10?; D: ?30?; 70?

600

T: 20?; D: ?20?; 80?
T: 10?; D: ?50?; 50?

400
10

20 30 40
Display Size

10

800
600

20 30 40
Display Size

10

400
8
12
Display Size

1000

T: 0?; D: ?40?; 40?

600
500

T: 20?; D: 0?; 40?

T: 20?; D: ?20?; 60?

400
4

RT (msec)

RT (msec)

RT (msec)

500

10

20 30
40
Display Size

800

T: 0? R; D: 0? G; 40? R

600

20 30 40
Display Size

T: 0? R; D: 0? G; 90? R

10

20 30 40
Display Size

T: 0?; D: ?20?; 20?

700

600

T: 20?; D: 0?

(F) Conjunction Search
Varying Distractor Ratio

(C)
Target?Distractor Similarity
700

600

1000
RT (msec)

600

1000
RT (msec)

RT (msec)

RT (msec)

1000

800

20 30
40
Display Size

T: 0?; D: 20?

800

(E) Conjunction Search
Varying Distractor Confusability

(B)
Categorical Search
1000

1000

400

4

8
12
Display Size

800

RT (msec)

15

RT (msec)

RT Slope (msec/item)

RT Slope (msec/item)

(A) Vertical Bar Among
Homogeneous Distractors

600
0
.25 .50 .75
1
Proportion of Red Distractors

1000
800
600
0
.25 .50 .75
1
Proportion of Red Distractors

Figure 2: Simulation results on six sets of tasks, labeled A?F, for GS (thin lines, 1st and 3d columns)
and EGS (thick lines, 2nd and 4th columns). Simulation details are explained in the text.
items. EGS provides a single-stage account that does not need to invoke specialized mechanisms for
adaptation to the environment, because all attentional control is adaptation of this sort.
To summarize, EGS predicts the key factors in visual search that determine search efficiency. Most
efficient search is for a target defined by the presence of a single categorical feature among homogeneous distractors that do not share the categorical feature. Least efficient search is for target and
distractors that share features (e.g., T among L?s, or red verticals among red horizontals and green
verticals) and/or when distractors are heterogeneous.
Wolfe, Cave, & Franzel (1989) conducted an experiment to demonstrate that people can benefit
from guidance. This experiment, which oddly has never been modeled by GS, involves search for a
conjunction target defined by a triple of features, e.g., a big red vertical bar. The target might be presented among heterogeneous distractors that share two features with it, such as a big red horizontal
bar, or distractors that share only one feature with it, such as a small green vertical bar. Performance
in these two conditions, denoted T3-D2 and T3-D1, respectively, is compared to performance in
a standard conjunction search task, denoted T2-D1, involving targets defined by two features and
sharing one feature with each distractor. Wolfe et al. reasoned that if search can be guided, saliency
at a location should be proportional to the number of target-relevant features at that location, and the
ratio of target to distractor salience should be x/y in condition Tx-Dy. Because x > y, the target is
always more salient than any distractor, but GS assumes less efficient search due to noise corruption
of the saliency map, thereby predicting search slopes that are inversely related to x/y. The human
data show exactly this pattern, producing almost flat search slopes for T3-D1. EGS replicates the
human data (Figure 3b) without employing GS?s arbitrary assumption that prioritization is corrupted
by noise. Instead, x/y reflects the amount of evidence available on each trial about features that discriminate targets from distractors. Essentially, EGS suggests that x/y determines the availability of
discriminative statistics in the environment. Thus, the limitation is on learning, not on performance.
7

5

0.2

(a)

mostly vert distractors
mostly red distractors

0.15
0.1
0.05
0

vertical

Feature

1000

T3?D2
T2?D1
T3?D1

800
600
400
0

red

(b)

1200

Reaction Time

0.25

Activation

Figure 3: (a) Values
of E[?i0 ] in task F. (b)
EGS performance on
the
triple-conjunction
task of Wolfe, Cave, &
Franzel (1989)

10

20

Display Size

30

40

Discussion

We presented a model, EGS, that guides visual search via statistics collected over the course of
experience in a task environment. The primary contributions of EGS are as follows. First, EGS is a
significantly more elegant and parsimonious theory than its predecessors. In contrast to EGS, GS is a
complex model under the hood with many free parameters and heuristic assumptions. We and other
groups have spent many months reverse engineering GS to determine how exactly it works, because
published descriptions do not have the specificity of an algorithm. Second, to explain human data,
GS and its ancestors are ?retarded? by injecting noise or arbitrarily limiting gains. Although it may
ultimately be determined that the brain suffers from these conditions, one would prefer theories
that cast performance of the brain as ideal or rational. EGS achieves this objective via explicit
assumptions about the generative model of the environment embodied by cognition. In particular,
the dumbing-down of GS and its variants is replaced in EGS by the claim that environments are
nonstationary. If the environment can change from one trial to the next, the cognitive system does
well not to turn up gains on one feature dimension at the expense of other feature dimensions. The
result is a sensible trade off: attentional control can be rapidly tuned as the task or environment
changes, but this flexibility restricts EGS?s search efficiency when the task and environment remain
constant. Third, EGS suggests a novel perspective on attentional control, and executive control more
generally. All other modern perspectives we are aware of treat control as optimization, whereas in
EGS, control arises directly from statistical inference on the task environment. Our current research
is exploring the implications of this intriguing perspective.
Acknowledgments
This research was supported by NSF BCS 0339103 and NSF CSE-SMA 0509521. Support for the second
author comes from an NSF Graduate Fellowship.

References
Baldwin, D., & Mozer, M. C. (2006). Controlling attention with noise: The cue-combination model of visual
search. In R. Sun & N. Miyake (Eds.), Proc. of the 28th Ann. Conf. of the Cog. Sci. Society (pp. 42-47).
Hillsdale, NJ: Erlbaum.
Cave, K. R. (1999). The FeatureGate model of visual selection. Psychol. Res., 62, 182?194.
Itti, L., & Koch, C. (2001). Computational modeling of visual attention. Nature Rev. Neurosci., 2, 194?203.
Mozer, M. C. (1991). The perception of multiple objects: A connectionist approach. Cambridge, MA: MIT.
Navalpakkam, V., & Itti, L. (2006). Optimal cue selection strategy. In Advances in Neural Information Processing Systems Vol. 19 (pp. 1-8). Cambridge, MA: MIT Press.
Rao, R., Zelinsky, G., Hayhoe, M., & Ballard, D. (2002). Eye movements in iconic visual search. Vis. Res., 42,
1447?1463.
Sandon, P. A. (1990). Simulating visual attention. Journal of Cog. Neuro., 2, 213?231. Sandon, 1990
Torralba, A., Oliva, A., Castelhano, M.S., & Henderson, J. M. (2006). Contextual guidance of eye movements
and attention in real-world scenes: The role of global features on objects search. Psych. Rev., 113, 766?786.
Wolfe, J. M., Cave, K. R., & Franzel, S. L. (1989). Guided search: An alternative to the feature integration
model for visual search. Jnl. Exp. Psych.: Hum. Percep. & Perform., 15, 419?433.
Wolfe, J. M. (1994). Guided Search 2.0: A revised model of visual search. Psych. Bull. & Rev., 1, 202?238.
Wolfe, J. M. (2007). Guided Search 4.0: Current progress with a model of visual search. In. W. Gray (Ed.),
Integrated Models of Cognitive Systems. NY: Oxford.
Zhang, L., & Cottrell, G. W. (submitted). Probabilistic search: A new theory of visual search. Submitted for
publication.

8

"
5152,2015,Hessian-free Optimization for Learning Deep Multidimensional Recurrent Neural Networks,"Multidimensional recurrent neural networks (MDRNNs) have shown a remarkable performance in the area of speech and handwriting recognition. The performance of an MDRNN is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by using Hessian-free (HF) optimization. Given that connectionist temporal classification (CTC) is utilized as an objective of learning an MDRNN for sequence labeling, the non-convexity of  CTC poses a problem when applying HF to the network. As a solution, a convex approximation of CTC is formulated and its relationship with the EM algorithm and the Fisher information matrix is discussed. An MDRNN up to a depth of 15 layers is successfully trained using HF, resulting in an improved performance for sequence labeling.","Hessian-free Optimization for Learning
Deep Multidimensional Recurrent Neural Networks

Minhyung Cho
Chandra Shekhar Dhir
Jaehyung Lee
Applied Research Korea, Gracenote Inc.
{mhyung.cho,shekhardhir}@gmail.com
jaehyung.lee@kaist.ac.kr

Abstract
Multidimensional recurrent neural networks (MDRNNs) have shown a remarkable performance in the area of speech and handwriting recognition. The performance of an MDRNN is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by using Hessian-free (HF)
optimization. Given that connectionist temporal classification (CTC) is utilized as
an objective of learning an MDRNN for sequence labeling, the non-convexity of
CTC poses a problem when applying HF to the network. As a solution, a convex
approximation of CTC is formulated and its relationship with the EM algorithm
and the Fisher information matrix is discussed. An MDRNN up to a depth of 15
layers is successfully trained using HF, resulting in an improved performance for
sequence labeling.

1

Introduction

Multidimensional recurrent neural networks (MDRNNs) constitute an efficient architecture for
building a multidimensional context into recurrent neural networks [1]. End-to-end training of
MDRNNs in conjunction with connectionist temporal classification (CTC) has been shown to
achieve a state-of-the-art performance in on/off-line handwriting and speech recognition [2, 3, 4].
In previous approaches, the performance of MDRNNs having a depth of up to five layers, which
is limited as compared to the recent progress in feedforward networks [5], was demonstrated. The
effectiveness of MDRNNs deeper than five layers has thus far been unknown.
Training a deep architecture has always been a challenging topic in machine learning. A notable
breakthrough was achieved when deep feedforward neural networks were initialized using layerwise pre-training [6]. Recently, approaches have been proposed in which supervision is added to
intermediate layers to train deep networks [5, 7]. To the best of our knowledge, no such pre-training
or bootstrapping method has been developed for MDRNNs.
Alternatively, Hesssian-free (HF) optimization is an appealing approach to training deep neural
networks because of its ability to overcome pathological curvature of the objective function [8].
Furthermore, it can be applied to any connectionist model provided that its objective function is
differentiable. The recent success of HF for deep feedforward and recurrent neural networks [8, 9]
supports its application to MDRNNs.
In this paper, we claim that an MDRNN can benefit from a deeper architecture, and the application of
second order optimization such as HF allows its successful learning. First, we offer details of the development of HF optimization for MDRNNs. Then, to apply HF optimization for sequence labeling
tasks, we address the problem of the non-convexity of CTC, and formulate a convex approximation.
In addition, its relationship with the EM algorithm and the Fisher information matrix is discussed.
Experimental results for offline handwriting and phoneme recognition show that an MDRNN with
HF optimization performs better as the depth of the network increases up to 15 layers.
1

2

Multidimensional recurrent neural networks

MDRNNs constitute a generalization of RNNs to process multidimensional data by replacing the
single recurrent connection with as many connections as the dimensions of the data [1]. The network
can access the contextual information from 2N directions, allowing a collective decision to be made
based on rich context information. To enhance its ability to exploit context information, long shortterm memory (LSTM) [10] cells are usually utilized as hidden units. In addition, stacking MDRNNs
to construct deeper networks further improves the performance as the depth increases, achieving the
state-of-the-art performance in phoneme recognition [4]. For sequence labeling, CTC is applied as
a loss function of the MDRNN. The important advantage of using CTC is that no pre-segmented
sequences are required, and the entire transcription of the input sample is sufficient.
2.1

Learning MDRNNs

A d-dimensional MDRNN with M inputs and K outputs is regarded as a mapping from an input
sequence x ? RM ?T1 ?????Td to an output sequence a ? (RK )T of length T , where the input data for
M input neurons are given by the vectorization of d-dimensional data, and T1 , . . . , Td is the length
of the sequence in each dimension. All learnable weights and biases are concatenated to obtain a
parameter vector ? ? RN . In the learning phase with fixed training data, the MDRNN is formalized
as a mapping N : RN ? (RK )T from the parameters ? to the output sequence a, i.e., a = N (?).
The scalar loss function is defined over the output sequence as L : (RK )T ? R. Learning an
MDRNN is viewed as an optimization of the objective L(N (?)) = L ? N (?) with respect to ?.
The Jacobian JF of a function F : Rm ? Rn is the n ? m matrix where each element is a partial
derivative of an element of output with respect to an element of input. The Hessian HF of a scalar
function F : Rm ? R is the m ? m matrix of second-order partial derivatives of the output with
respect to its inputs. Throughout this paper, a vector sequence is denoted by boldface a, a vector at
time t in a is denoted by at , and the k-th element of at is denoted by atk .

3

Hessian-free optimization for MDRNNs

The application of HF optimization to an MDRNN is straightforward if the matching loss function [11] for its output layer is adopted. However, this is not the case for CTC, which is necessarily
adopted for sequence labeling. Before developing an appropriate approximation to CTC that is compatible with HF optimization, we discuss two considerations related to the approximation. The first
is obtaining a quadratic approximation of the loss function, and the second is the efficient calculation
of the matrix-vector product used at each iteration of the conjugate gradient (CG) method.
HF optimization minimizes an objective by constructing a local quadratic approximation for the
objective function and minimizing the approximate function instead of the original one. The loss
function L(?) needs to be approximated at each point ?n of the n-th iteration:
1 >
Qn (?) = L(?n ) + ?? L|>
(1)
?n ?n + ?n G?n ,
2
where ?n = ? ? ?n is the search direction, i.e., the parameters of the optimization, and G is a
local approximation to the curvature of L(?) at ?n , which is typically obtained by the generalized
Gauss-Newton (GGN) matrix as an approximation of the Hessian.
HF optimization uses the CG method in a subroutine to minimize the quadratic objective above for
utilizing the complete curvature information and achieving computational efficiency. CG requires
the computation of Gv for an arbitrary vector v, but not the explicit evaluation of G. For neural
networks, an efficient way to compute Gv was proposed in [11], extending the study in [12]. In
section 3.2, we provide the details of the efficient computation of Gv for MDRNNs.
3.1

Quadratic approximation of loss function

The Hessian matrix, HL?N , of the objective L (N (?)) is written as
>
HL?N = JN
HL JN +

KT
X
i=1

2

[JL ]i H[N ]i ,

(2)

where JN ? RKT ?N , HL ? RKT ?KT , and [q]i denotes the i-th component of the vector q.
An indefinite Hessian matrix is problematic for second-order optimization, because it defines an
unbounded local quadratic approximation [13]. For nonlinear systems, the Hessian is not necessarily
positive semidefinite, and thus, the GGN matrix is used as an approximation of the Hessian [11, 8].
The GGN matrix is obtained by ignoring the second term in Eq. (2), as given by
>
GL?N = JN
HL J N .

(3)

The sufficient condition for the GGN approximation to be exact is that the network makes a perfect
prediction for every given sample, that is, JL = 0, or [N ]i stays in the linear region for all i, that is,
H[N ]i = 0.
GL?N has less rank than KT and is positive semidefinite provided that HL is. Thus, L is chosen to
be a convex function so that HL is positive semidefinite. In principle, it is best to define L and N
such that L performs as much of the computation as possible, with the positive semidefiniteness of
HL as a minimum requirement [13]. In practice, a nonlinear output layer together with its matching
loss function [11], such as the softmax function with cross-entropy loss, is widely used.
3.2

Computation of matrix-vector product for MDRNN

>
HL JN v, amounts to the seThe product of an arbitrary vector v by the GGN matrix, Gv = JN
quential multiplication of v by three matrices. First, the product JN v is a Jacobian times vector
and is therefore equal to the directional derivative of N (?) along the direction of v. Thus, JN v can
be written using a differential operator JN v = Rv (N (?)) [12] and the properties of the operator
can be utilized for efficient computation. Because an MDRNN is a composition of differentiable
components, the computation of Rv (N (?)) throughout the whole network can be accomplished by
repeatedly applying the sum, product, and chain rules starting from the input layer. The detailed
derivation of the R operator to LSTM, normally used as a hidden unit in MDRNNs, is provided in
appendix A.

Next, the multiplication of JN v by HL can be performed by direct computation. The dimension
of HL could at first appear problematic, since the dimension of the output vector used by the loss
function L can be as high as KT , in particular, if CTC is adopted as an objective for the MDRNN.
If the loss function can be expressed as the sum of individual loss functions with a domain restricted
in time, the computation can be reduced significantly. For example, with the commonly used crossentropy loss function, the KT ? KT matrix HL can be transformed into a block diagonal matrix
with T blocks of a K ? K Hessian matrix. Let HL,t be the t-th block in HL . Then, the GGN matrix
can be written as
X
>
GL?N =
JN
HL,t JNt ,
(4)
t
t

where JNt is the Jacobian of the network at time t.
>
is calculated using the backFinally, the multiplication of a vector u = HL JN v by the matrix JN
propagation through time algorithm by propagating u instead of the error at the output layer.

4

Convex approximation of CTC for application to HF optimization

Connectioninst temporal classification (CTC) [14] provides an objective function of learning an
MDRNN for sequence labeling. In this section, we derive a convex approximation of CTC inspired
by the GGN approximation according to the following steps. First, the non-convex part of the
original objective is separated out by reformulating the softmax part. Next, the remaining convex
part is approximated without altering its Hessian, making it well matched to the non-convex part.
Finally, the convex approximation is obtained by reuniting the convex and non-convex parts.
4.1

Connectionist temporal classification

CTC is formulated as the mapping from an output sequence of the recurrent network, a ? (RK )T ,
to a scalar loss. The output activations at time t are normalized using the softmax function
ykt = P

exp(atk )
t ,
k0 exp(ak0 )
3

(5)

where ykt is the probability of label k given a at time t.
The conditional probability of the path ? is calculated by the multiplication of the label probabilities
at each timestep, as given by
T
Y
p(?|a) =
y?t t ,
(6)
t=1

where ?t is the label observed at time t along the path ?. The path ? of length T is mapped to a
label sequence of length M ? T by an operator B, which removes the repeated labels and then
the blanks. Several mutually exclusive paths can map to the same label sequence. Let S be a set
containing every possible sequence mapped by B, that is, S = {s|s ? B(?) for some ?} is the
image of B, and let |S| denote the cardinality of the set.
The conditional probability of a label sequence l is given by
X
p(l|a) =
p(?|a),

(7)

??B?1 (l)

which is the sum of probabilities of all the paths mapped to a label sequence l by B.
The cross-entropy loss assigns a negative log probability to the correct answer. Given a target
sequence z, the loss function of CTC for the sample is written as
L(a) = ? log p(z|a).

(8)

From the description above, CTC is composed of the sum of the product of softmax components.
The function ? log(ykt ), corresponding to the softmax with cross-entropy loss, is convex [11].
Therefore, ykt is log-concave. Whereas log-concavity is closed under multiplication, the sum of
log-concave functions is not log-concave in general [15]. As a result, the CTC objective is not
convex in general because it contains the sum of softmax components in Eq. (7).
4.2

Reformulation of CTC objective function

We reformulate the CTC objective Eq. (8) to separate out the terms that are responsible for the nonconvexity of the function. By reformulation, the softmax function is defined over the categorical
label sequences.
By substituting Eq. (5) into Eq. (6), it follows that
exp(b? )
,
0
? 0 ?all exp(b? )

p(?|a) = P
where b? =
as

P

t

(9)

at?t . By substituting Eq. (9) into Eq. (7) and setting l = z, p(z|a) can be re-written
P

exp(b? )

exp(fz )
,
(10)
exp(fz0 )
P

where S is the set of every possible label sequence and fz = log
??B?1 (z) exp(b? ) is the logp(z|a) =

??B?1 (z)

P

??all exp(b? )

=P

z0 ?S

sum-exp function1 , which is proportional to the probability of observing the label sequence z among
all the other label sequences.
With the reformulation above, the CTC objective can be regarded as the cross-entropy loss with the
softmax output, which is defined over all the possible label sequences. Because the cross-entropy
loss function matches the softmax output layer [11], the CTC objective is convex, except the part
that computes fz for each of the label sequences. At this point, an obvious candidate for the convex
approximation of CTC is the GGN matrix separating the convex and non-convex parts.
Let the non-convex part be Nc and the convex part be Lc . The mapping Nc : (RK )T ? R|S| is
defined by
Nc (a) = F = [fz1 , . . . , fz|S| ]> ,
(11)
1

f (x1 , . . . , xn ) = log(ex1 + ? ? ? + exn ) is the log-sum-exp function defined on Rn

4

where fz is given above, and |S| is the number of all the possible label sequences. For given F as
above, the mapping Lc : R|S| ? R is defined by
!
X
exp(fz )
Lc (F ) = ? log P
= ?fz + log
exp(fz0 ) ,
(12)
0
z0 ?S exp(fz )
0
z ?S

where z is the label sequence corresponding to a. The final reformulation for the loss function of
CTC is given by
L(a) = Lc ? Nc (a).
(13)
4.3

Convex approximation of CTC loss function

The GGN approximation of Eq. (13) immediately gives a convex approximation of the Hessian for
>
CTC as GLc ?Nc = JN
HLc JNc . Although HLc has the form of a diagonal matrix plus a rank-1
c
matrix, i.e., diag(Y ) ? Y Y > , the dimension of HLc is |S| ? |S|, where |S| becomes exponentially
large as the length of the sequence increases. This makes the practical calculation of HLc difficult.
On the other hand, removing the linear team
P ?fz from Lc(F ) in Eq. (12) does not alter its Hessian.
0
The resulting formula is Lp (F ) = log
z0 ?S exp(fz ) . The GGN matrices of L = Lc ? Nc
and M = Lp ? Nc are the same, i.e., GLc ?Nc = GLp ?Nc . Therefore, their Hessian matrices are
approximations of each other. The condition that the two Hessian matrices, HL and HM , converges
to the same matrix is discussed below.
P
P
Interestingly, M is given as a compact formula M(a) = Lp ? Nc (a) = t log k exp(atk ), where
atk is the output unit k at time t. Its Hessian HM can be directly computed, resulting in a block
diagonal matrix. Each block is restricted in time, and the t-th block is given by
>

HM,t = diag(Y t ) ? Y t Y t ,

(14)

t >
] and ykt is given in Eq. (5). Because the Hessian of each block is positive
where Y t = [y1t , . . . , yK
semidefinite, HM is positive semidefinite. A convex approximation of the Hessian of an MDRNN
using the CTC objective can be obtained by substituting HM for HL in Eq. (3). Note that the
resulting matrix is block diagonal and Eq. (4) can be utilized for efficient computation.

Our derivation can be summarized as follows:
1. HL = HLc ?Nc is not positive semidefinite.
2. GLc ?Nc = GLp ?Nc is positive semidefinite, but not computationally tractable.
3. HLp ?Nc is positive semidefinite and computationally tractable.
4.4

Sufficient condition for the proposed approximation to be exact

PKT
From Eq. (2), the condition HLc ?Nc = HLp ?Nc holds if and only if
i=1 [JLc ]i H[Nc ]i =
PKT
i=1 [JLp ]i H[Nc ]i . Since JLc 6= JLp in general, we consider only the case of H[Nc ]i = 0 for
all i, which corresponds to the case where Nc is a linear mapping.
[Nc ]i contains a log-sum-exp function mapping from paths to a label sequence. Let l be the label
sequence corresponding to [Nc ]i ; then, [Nc ]i = fl (. . . , b? , . . . ) for ? ? B ?1 (l). If the probability
of one path ? 0 is sufficiently large to ignore all the other paths, that is, exp(b?0 )  exp(b? ) for
? ? {B ?1 (l)\? 0 }, it follows that fl (. . . , b?0 , . . . ) = b?0 . This is a linear mapping, which results in
H[Nc ]i = 0.
In conclusion, the condition HLc ?Nc = HLp ?Nc holds if one dominant path ? ? B ?1 (l) exists such
that fl (. . . , b? , . . . ) = b? for each label sequence l.
4.5

Derivation of the proposed approximation from the Fisher information matrix

The identity of the GGN and the Fisher information matrix [16] has been shown for the network
using the softmax with cross-entropy loss [17, 18]. Thus, it follows that the GGN matrix of Eq. (13)
is identical to the Fisher information matrix. Now, we show that the proposed matrix in Eq. (14)
5

is derived from the Fisher information matrix under the condition given in section 4.4. The Fisher
information matrix of an MDRNN using CTC is written as
""
""
#
> 
#
? log p(l|a)
? log p(l|a)
>
F = Ex JN El?p(l|a)
JN ,
(15)
?a
?a
where a = a(x, ?) is the KT -dimensional output of the network N . CTC assumes output probabilities at each timestep to be independent of those at other timesteps [1], and therefore, its Fisher
information matrix is given as the sum of every timestep. It follows that
#
""
""
> 
#
X
? log p(l|a)
? log p(l|a)
>
JNt .
(16)
F = Ex
JNt El?p(l|a)
?at
?at
t
Under the condition in section 4.4, the Fisher information matrix is given by
""
#
X
>
t
t t>
F = Ex
JNt (diag(Y ) ? Y Y )JNt ,

(17)

t

which is the same form as Eqs. (4) and (14) combined. See appendix B for the detailed derivation.
4.6

EM interpretation of the proposed approximation

The goal of the Expectation-Maximization (EM) algorithm is to find the maximum likelihood solution for models having latent variables [19]. Given an input sequence x, and
P its corresponding
target label sequence z, the log likelihood of z is given by log p(z|x, ?) = log ??B?1 (z) p(?|x, ?),
where ? represents the model parameters. For each observation x, we have a corresponding latent
variable q which is a 1-of-k binary vector where k is the number
Pof all the paths mapped to z. The
log likelihood can be written in terms of q as log p(z, q|x, ?) = ??B?1 (z) q?|x,z log p(?|x, ?). The
? and repeats the following process until convergence.
EM algorithm starts with an initial parameter ?,
Expectation step calculates: ??|x,z =

P

?
p(?|x,?)
? .
p(?|x,?)

??B?1 (z)

Maximization step updates: ?? = argmax? Q(?),

where Q(?) =

P

??B?1 (z)

??|x,z log p(?|x, ?).

In the context of CTC and RNN, p(?|x, ?) is given as p(?|a(x, ?)) as in Eq. (6), where a(x, ?) is
the KT -dimensional output of the neural network. Taking the second-order derivative of log p(?|a)
t
t
t t>
t
with respect
P to a gives diag(Y )?Y Y , with Y as in Eq. (14). tBecause this term is independent
of ? and ??B?1 (z) ??|x,z = 1, the Hessian of Q with respect to a is given by
>

HQ,t = diag(Y t ) ? Y t Y t ,

(18)

which is the same as the convex approximation in Eq. (14).

5

Experiments

In this section, we present the experimental results for two different sequence labeling tasks, offline
handwriting recognition and phoneme recognition. The performance of Hessian-free optimization
for MDRNNs with the proposed matrix is compared with that of stochastic gradient descent (SGD)
optimization on the same settings.
5.1

Database and preprocessing

The IFN/ENIT Database [20] is a database of handwritten Arabic words, which consists of 32,492
images. The entire dataset has five subsets (a, b, c, d, e). The 25,955 images corresponding to the
subsets (b ? e) were used for training. The validation set consisted of 3,269 images corresponding
to the first half of the sorted list in alphabetical order (ae07 001.tif ? ai54 028.tif) in set a. The
remaining images in set a, amounting to 3,268, were used for the test. The intensity of pixels was
centered and scaled using the mean and standard deviation calculated from the training set.
6

The TIMIT corpus [21] is a benchmark database for evaluating speech recognition performance.
The standard training, validation, and core datasets were used. Each set contains 3,696 sentences,
400 sentences, and 192 sentences, respectively. A mel spectrum with 26 coefficients was used as
a feature vector with a pre-emphasis filter, 25 ms window size, and 10 ms shift size. Each input
feature was centered and scaled using the mean and standard deviation of the training set.
5.2

Experimental setup

For handwriting recognition, the basic architecture was adopted from that proposed in [3]. Deeper
networks were constructed by replacing the top layer with more layers. The number of LSTM cells
in the augmented layer was chosen such that the total number of weights between the different
networks was similar. The detailed architectures are described in Table 1, together with the results.
For phoneme recognition, the deep bidirectional LSTM and CTC in [4] was adopted as the basic
architecture. In addition, the memory cell block [10], in which the cells share the gates, was applied
for efficient information sharing. Each LSTM block was constrained to have 10 memory cells.
According to the results, using a large value of bias for input/output gates is beneficial for training
deep MDRNNs. A possible explanation is that the activation of neurons is exponentially decayed
by input/output gates during the propagation. Thus, setting large bias values for these gates may
facilitate the transmission of information through many layers at the beginning of the learning. For
this reason, the biases of the input and output gates were initialized to 2, whereas those of the forget
gates and memory cells were initialized to 0. All the other weight parameters of the MDRNN were
initialized randomly from a uniform distribution in the range [?0.1, 0.1].
The label error rate was used as the metric for performance evaluation, together with the average
loss of CTC in Eq. (8). It is defined by the edit distance, which sums the total number of insertions,
deletions, and substitutions required to match two given sequences. The final performance, shown
in Tables 1 and 2, was evaluated using the weight parameters that gave the best label error rate on
the validation set. To map output probabilities to a label sequence, best path decoding [1] was used
for handwriting recognition and beam search decoding [4, 22] with a beam width of 100 was used
for phoneme recognition. For phoneme recognition, 61 phoneme labels were used during training
and decoding, and then, mapped to 39 classes for calculating the phoneme error rate (PER) [4, 23].
For phoneme recognition, the regularization method suggested in [24] was used. We applied Gaussian weight noise of standard deviation ? = {0.03, 0.04, 0.05} together with L2 regularization of
strength 0.001. The network was first trained without noise, and then, it was initialized to the weights
that gave the lowest CTC loss on the validation set. Then, the network was retrained with Gaussian
weight noise [4]. Table 2 presents the best result for different values of ?.
5.2.1

Parameters

For HF optimization, we followed the basic setup described in [8], but different parameters were
utilized. Tikhonov damping was used together with Levenberg-Marquardt heuristics. The value of
the damping parameter ? was initialized to 0.1, and adjusted according to the reduction ratio ? (multiplied by 0.9 if ? > 0.75, divided by 0.9 if ? < 0.25, and unchanged otherwise). The initial search
direction for each run of CG was set to the CG direction found by the previous HF optimization
iteration decayed by 0.7. To ensure that CG followed the descent direction, we continued to perform
a minimum 5 and maximum 30 of additional CG iterations after it found the first descent direction.
We terminated CG at iteration i before reaching the maximum iteration if the following condition
was satisfied: (?(xi ) ? ?(xi?5 ))/?(xi ) < 0.005 , where ? is the quadratic objective of CG without offset. The training data were divided into 100 and 50 mini-batches for the handwriting and
phoneme recognition experiments, respectively, and used for both the gradient and matrix-vector
product calculation. The learning was stopped if any of two criteria did not improve for 20 epochs
and 10 epochs in handwriting and phoneme recognition, respectively.
For SGD optimization, the learning rate  was chosen from {10?4 , 10?5 , 10?6 }, and the momentum
? from {0.9, 0.95, 0.99}. For handwriting recognition, the best performance obtained using all the
possible combinations of parameters is presented in Table 1. For phoneme recognition, the best
parameters out of nine candidates for each network were selected after training without weight noise
based on the CTC loss. Additionally, the backpropagated error in LSTM layer was clipped to remain
7

in the range [?1, 1] for stable learning [25]. The learning was stopped after 1000 epochs had been
processed, and the final performance was evaluated using the weight parameters that showed the best
label error rate on the validation set. It should be noted that in order to guarantee the convergence,
we selected a conservative criterion as compared to the study where the network converged after 85
epochs in handwriting recognition [3] and after 55-150 epochs in phoneme recognition [4].
5.3

Results

Table 1 presents the label error rate on the test set for handwriting recognition. In all cases, the
networks trained using HF optimization outperformed those using SGD. The advantage of using HF
is more pronounced as the depth increases. The improvements resulting from the deeper architecture
can be seen with the error rate dropping from 6.1% to 4.5% as the depth increases from 3 to 13.
Table 2 shows the phoneme error rate (PER) on the core set for phoneme recognition. The improved
performance according to the depth can be observed for both optimization methods. The best PER
for HF optimization is 18.54% at 15 layers and that for SGD is 18.46% at 10 layers, which are
comparable to that reported in [4], where the reported results are a PER of 18.6% from a network
with 3 layers having 3.8 million weights and a PER of 18.4% from a network with 5 layers having
6.8 million weights. The benefit of a deeper network is obvious in terms of the number of weight
parameters, although this is not intended to be a definitive performance comparison because of
the different preprocessing. The advantage of HF optimization is not prominent in the result of
the experiments using the TIMIT database. One explanation is that the networks tend to overfit
to a relatively small number of the training data samples, which removes the advantage of using
advanced optimization techniques.
Table 1: Experimental results for Arabic offline handwriting recognition. The label error rate is
presented with the different network depths. AB denotes a stack of B layers having A hidden
LSTM cells in each layer. ?Epochs? is the number of epochs required by the network using HF
optimization so that the stopping criteria are fulfilled.  is the learning rate and ? is the momentum.
NETWORKS

2-10-50
2-10-213
2-10-146
2-10-128
2-10-1011
2-10-913

DEPTH

WEIGHTS

3
5
8
10
13
15

159,369
157,681
154,209
154,153
150,169
145,417

HF (%)
6.10
5.85
4.98
4.95
4.50
5.69

EPOCHS

77
90
140
109
84
84

SGD (%)
9.57
9.19
9.67
9.25
10.63
12.29

{, ?}
{10?4 ,0.9}
{10?5 ,0.99}
{10?4 ,0.95}
{10?4 ,0.95}
{10?4 ,0.9}
{10?5 ,0.99}

Table 2: Experimental results for phoneme recognition using the TIMIT corpus. PER is presented
with the different MDRNN architectures (depth ? block ? cell/block). ? is the standard deviation
of Gaussian weight noise. The remaining parameters are the same as in Table 1.
NETWORKS

WEIGHTS

3 ? 20 ? 10
5 ? 15 ? 10
8 ? 11 ? 10
10 ? 10 ? 10
13 ? 9 ? 10
15 ? 8 ? 10
3 ? 250 ? 1?
5 ? 250 ? 1?

771,542
795,752
720,826
755,822
806,588
741,230
3.8M
6.8M

HF (%)
20.14
19.18
19.09
18.79
18.59
18.54

EPOCHS

22
30
29
60
93
50

{?}
{0.03}
{0.05}
{0.05}
{0.04}
{0.05}
{0.04}

SGD (%)
20.96
20.82
19.68
18.46
18.49
19.09
18.6
18.4

{, ?, ?}
{10?5 , 0.99, 0.05 }
{10?4 , 0.9, 0.04 }
{10?4 , 0.9, 0.04 }
{10?5 , 0.95, 0.04 }
{10?5 , 0.95, 0.04 }
{10?5 , 0.95, 0.03 }
{10?4 , 0.9, 0.075 }
{10?4 , 0.9, 0.075 }

? The results were reported by Graves in 2013 [4].

6

Conclusion

Hessian-free optimization as an approach for successful learning of deep MDRNNs, in conjunction
with CTC, was presented. To apply HF optimization to CTC, a convex approximation of its objective
function was explored. In experiments, improvements in performance were seen as the depth of the
network increased for both HF and SGD. HF optimization showed a significantly better performance
for handwriting recognition than did SGD, and a comparable performance for speech recognition.
8

References
[1] Alex Graves. Supervised sequence labelling with recurrent neural networks, volume 385. Springer, 2012.
[2] Alex Graves, Marcus Liwicki, Horst Bunke, J?urgen Schmidhuber, and Santiago Fern?andez. Unconstrained on-line handwriting recognition with recurrent neural networks. In Advances in Neural Information Processing Systems, pages 577?584, 2008.
[3] Alex Graves and J?urgen Schmidhuber. Offline handwriting recognition with multidimensional recurrent
neural networks. In Advances in Neural Information Processing Systems, pages 545?552, 2009.
[4] Alex Graves, Abdel-ranhman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent
neural networks. In Proceedings of ICASSP, pages 6645?6649. IEEE, 2013.
[5] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua
Bengio. Fitnets: Hints for thin deep nets. CoRR, abs/1412.6550, 2014. URL http://arxiv.org/
abs/1412.6550.
[6] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504?507, 2006.
[7] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pages 1?9, 2015.
[8] James Martens. Deep learning via Hessian-free optimization. In Proceedings of the 27th International
Conference on Machine Learning, pages 735?742, 2010.
[9] James Martens and Ilya Sutskever. Learning recurrent neural networks with Hessian-free optimization.
In Proceedings of the 28th International Conference on Machine Learning, pages 1033?1040, 2011.
[10] Sepp Hochreiter and J?urgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735?
1780, 1997.
[11] Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural
Computation, 14(7):1723?1738, 2002.
[12] Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural Computation, 6(1):147?160, 1994.
[13] James Martens and Ilya Sutskever. Training deep and recurrent networks with Hessian-free optimization.
In Neural Networks: Tricks of the Trade, pages 479?535. Springer, 2012.
[14] Alex Graves, Santiago Fern?andez, Faustino Gomez, and J?urgen Schmidhuber. Connectionist temporal
classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of
the 23rd International Conference on Machine Learning, pages 369?376, 2006.
[15] Stephen Boyd and Lieven Vandenberghe, editors. Convex Optimization. Cambridge University Press,
2004.
[16] Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251?276,
1998.
[17] Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. In International
Conference on Learning Representations, 2014.
[18] Hyeyoung Park, S-I Amari, and Kenji Fukumizu. Adaptive natural gradient learning algorithms for various stochastic models. Neural Networks, 13(7):755?764, 2000.
[19] Christopher M. Bishop, editor. Pattern Recognition and Machine Learning. Springer, 2007.
[20] Mario Pechwitz, S Snoussi Maddouri, Volker M?argner, Noureddine Ellouze, and Hamid Amiri.
IFN/ENIT-database of handwritten arabic words. In Proceedings of CIFED, pages 129?136, 2002.
[21] DARPA-ISTO. The DARPA TIMIT acoustic-phonetic continuous speech corpus (TIMIT). In speech disc
cd1-1.1 edition, 1990.
[22] Alex Graves. Sequence transduction with recurrent neural networks. In ICML Representation Learning
Workshop, 2012.
[23] Kai-Fu Lee and Hsiao-Wuen Hon. Speaker-independent phone recognition using hidden markov models.
IEEE Transactions on Acoustics, Speech and Signal Processing, 37(11):1641?1648, 1989.
[24] Alex Graves. Practical variational inference for neural networks. In Advances in Neural Information
Processing Systems, pages 2348?2356, 2011.
[25] Alex Graves. Rnnlib: A recurrent neural network library for sequence learning problems, 2008.

9

"
1350,2002,Morton-Style Factorial Coding of Color in Primary Visual Cortex,Abstract Missing,"Morton-Style Factorial Coding of Color in
Primary Visual Cortex

Javier R. Movellan
Institute for Neural Computation
University of California San Diego
La Jolla, CA 92093-0515
movellan@inc.ucsd.edu

Thomas Wachtler
Sloan Center for Theoretical Neurobiology
The Salk Institute
La Jolla, CA 92037, USA
thomas@salk.edu

Thomas D. Albright
Howard Hughes Medical Institute
The Salk Institute
La Jolla, CA 92037, USA
tom@salk.edu

Terrence Sejnowski
Computational Neurobiology Laboratory
The Salk Institute
La Jolla, CA 92037, USA
terry@salk.edu

Abstract
We introduce the notion of Morton-style factorial coding and illustrate
how it may help understand information integration and perceptual coding in the brain. We show that by focusing on average responses one
may miss the existence of factorial coding mechanisms that become only
apparent when analyzing spike count histograms. We show evidence
suggesting that the classical/non-classical receptive field organization in
the cortex effectively enforces the development of Morton-style factorial
codes. This may provide some cues to help understand perceptual coding in the brain and to develop new unsupervised learning algorithms.
While methods like ICA (Bell & Sejnowski, 1997) develop independent
codes, in Morton-style coding the goal is to make two or more external
aspects of the world become independent when conditioning on internal
representations.
In this paper we introduce the notion of Morton-style factorial coding and illustrate how it
may help analyze information integration and perceptual organization in the brain. In the
neurosciences factorial codes are often studied in the context of mean tuning curves. A
tuning curve is called separable if it can be expressed as the product of terms selectively
influenced by different stimulus dimensions. Separable tuning curves are taken as evidence of factorial coding mechanisms. In this paper we show that by focusing on average
responses one may miss the existence of factorial coding mechanisms that become only
apparent when analyzing spike count histograms.
Morton (1969) analyzed a wide variety of psychophysical experiments on word perception
and showed that they could be explained using a model in which stimulus and context
have separable effects on perception. More precisely, in Mortons? model the joint effect of
stimulus and context on a perceptual representation can be obtained by multiplying terms

selectively controlled by stimulus and by context, i.e.,

	
 	

	
 
 
  
  
	
  
 	
(1)

	
 is the empirical probability of perceiving the perceptual alternative  in
where
 in context  ,   
	
 represents the support of stimulus  for percept
response

 the support
 and ! to	stimulus

of the context for percept . Massaro (1987b, 1987a, 1989a) has

shown that this form of factorization describes accurately a wide variety of psychophysical
studies in domains such as word recognition, phoneme recognition, audiovisual speech
recognition, and recognition of facial expressions.
Morton-style factorial codes used to be taken as evidence for a feedforward coding mechanism (Massaro, 1989b) but Movellan & McClelland (2001) showed that neural networks
with feedback connections can develop factorial codes when they follow an architectural
constraint named ?channel separability?. Channel separability is defined as follows: First
we identify the neurons which have a direct influence on the observed responses (e.g., the
set of neurons that affect an electrode). For a given set of response units, the stimulus
chanel is defined as the set of units modulated by the stimulus provided the response specification units are excised from the rest of the network. The context channel is the set of
units modulated by the context provided the response units are excised from the rest of
the networks. Two channels are called separable if they have no units in common. Channel separability implies that the influences of an information source upon the channel of
another information source should be mediated via the response specification units (see
Figure 1). While the models used in Movellan and McClelland (2001) are a simplification of actual neural circuits, the analysis suggests that the form of separability expressed
in the the Morton-Massaro model may be a useful paradigm for the study of information
integration in the brain. Indeed it is quite remarkable that the functional organization of
cortex into classical/non-classical receptive fields provides a separable architecture (See
Figure 1). Such organization may be nature?s way of enforcing Morton-style perceptual
coding. In this paper we present evidence in favor of this view by investigating how color
is encoded in primary visual cortex.
It is well known that stimuli of equal chromaticity can evoke different color percepts, depending on the visual context (Wesner & Shevell, 1992; Brown & MacLeod, 1997). Context dependent responses to color stimuli have been found in V4 (Zeki, 1983). More recently the last three authors of this article investigated the chromatic tuning properties of
V1 cells in response to stimuli presented in different chromatic contexts (Wachtler, Sejnowski, & Albright, 2003). The experiment showed that the background color, outside
the cell?s classical receptive field, had a significant effect on the response to colors inside
the receptive field. No attempt was made to model the form of such influence. In this
paper we analyze quantitatively the results of that experiment and show that a large proportion of these neurons, adhered to the Morton-Massaro law, i.e., stimulus and context had a
separable influence on the spike count histograms of these cells.

1 Methods
The animal preparation and methods of this experiment are described in Wachtler et al.
(in press) in great detail. Here we briefly describe the portion of the experiment relevant
to us. Two adult female rhesus monkeys were used in the study. Extracellular potentials
from single isolated neurons were recorded from two macaque monkeys. The monkeys
were awake and were required to fixate a small fixation target for the duration of each trial
(2500 ms.). Amplified electrical activity from the cortex was passed to a data acquisition
system for spike detection and sorting. Once a neuron was isolated, its receptive field was
determined using flashed and moving bars of different size, orientation, and color. All the

     
      

 


Background Channel 
 
 

 
 
Response Specification 

Response

Stimulus Channel

Response
Specification
Units

Stimulus
Relays

Context
Relays

Stimulus
Sensors

Context
Sensors

Stimulus

Context

Input





 
 



 




















Electrode

 
 

 
 




	

	

	

	










	
	
	  
 
 
 
 
 
 
 
 
  
	

	

	

	

	
	
	
	
	










	
	
	









	

	

	


	
	 

	 

	 
	
	
	  
 
 
 
 
 
 
 
 
  

	

Background

Stimulus





 
 



 








































 
 
 
 
Background

Figure 1: Left: A network with separable context and stimulus processing channels. Right:
The arrows connecting the stimulus to the unit in the center represent the classical receptive
field of that unit. External inputs affecting the classical receptive field are called ?stimuli?
and all the other inputs are called ?background?. In this preparation the stimulus and background channels are separable.
neurons recorded had receptive fields at eccentricities between  and  .
Once the receptive fields were located, the color tuning of the neurons was mapped by
flashing 8 stimuli of different chromaticity. The stimuli were homogenous color squares,
centered on and at least twice as large as the receptive field of the neuron under study. They
were flashed for 500 ms. Chromaticity was defined in a color space similar to the one used
in Derrington, Krauskopf, and Lennie (1984). Cone excitations were calculated on the basis
of the human cone fundamentals proposed by Stockman (Stockman, MaCleod, & Johnson,
1993). The origin of the color space corresponded to a homogeneous gray background to
which the animal had been adapted (luminance 48 cd/m  ). The three coordinate axis of the
color space corresponded to L versus M-cone contrast, S-cone contrast, and achromatic luminance. The 8 color stimuli were isoluminant with the gray background, had a fixed color
contrast (distance from origin of color space) and had chromatic directions corresponding
to polar angles  .
After several presentations of the stimuli, the chromatic directions for which the neurons
showed a clear response were determined, and one of them was selected as the second
background condition. In the second condition, the color of the background changed during
stimulus presentation (i.e., for 500 ms) to a different color. This color was isoluminant with
the gray background, was in the direction of a stimulus color to which the cell showed clear
response, but was of lower chromatic contrast than the stimulus colors. In subsequent trials
combinations of the 8 stimulus and 2 background conditions were presented in random
order.
For each trial we recored the number of spikes in a 100 ms window starting 50 ms after
stimulus onset. This time window was chosen because color tuning was usually more
pronounced in the first response phase as compared to later periods of the response and
because it maximized the effects of context. Data were recorded for a total of 94 units. Of
these, 20 neurons were selected for having the strongest background effect and a minimum
of 16 trials per condition. No other criteria were used for the selection of these neurons.

2 Results
Figure 2 shows example tuning curves of 4 different neurons. The thick lines represent
the average response for a particular color stimulus in the plane defined by the first two
chromatic axis. The dark curve represents responses for the gray background condition.
The light curve represents responses for the color background condition. The boxes around
the tuning curves represent average response rates as a function of stimulus onset for the
two background conditions.
Testing whether a code is factorial is like testing for the absence of interaction terms in
Analysis of Variance (ANOVA). The complexity (i.e., degrees of freedom) of an ANOVA
model without interaction terms is identical to the complexity of the Morton-Massaro
model. When testing for interaction effects we analyze whether the addition of interaction terms provides significant improvement on data fit over a simple additive model. In
our case we investigate whether the addition of non-factorial terms provides a significant
improvement on data fit over the factorial Morton-Massaro model. For each neuron there
were 8 stimulus conditions, 2 background conditions, and 10 response alternatives, one per
bin in the spike count histogram. The probabilities of the spike count histogram add up to

 independent probability estimates per neuron.
one thus, there is a total of
In this case the Morton-Massaro model requires
parameters

(Movellan & McClelland, 2001), thus there is a total of 63 nonfactorial terms.

  


	
 
   


 
 



For each neuron we fitted Morton-Massaro?s model and performed a standard likelihood
test to see whether the additional nonfactorial terms improved data fit significantly (i.e.,
whether the deviations from the Morton-Massaro factorial model where significant). We
found that of the 20 neurons only 5 showed significant deviations from the Morton-Massaro
 ). While the Morton-Massaro
model (chi-square test, 63 degrees of freedom,
model had 81 parameters many of them were highly redundant. We also evaluated a 30 parameter version of the model by performing PCA independently on the stimulus and on the
context parameters of the full model and deleting coefficients with small eigenvalues. The
30 parameter model provided fits almost indistinguishable from the 81 parameter model. In
this case only 4 neurons showed significant deviations from the model (chi-square, 124 df,
 ). On a pool of 20 neurons compliant with the Morton-Massaro model one would
expect the test to mistakenly reject 1 neuron by chance. Rejection of 4 or more neurons out
of 20 is not inconsistent with the idea that all the neurons were in fact compliant with the
Morton-Massaro model (
 , binomial test).

 	 	

 	 	

 	 	

Figure 2 shows the obtained and predicted spike count histograms for a typical neuron. The
top row represents the 8 stimulus conditions with gray background. The bottom row shows
the 8 conditions with color background. Lines represent spike count histograms predicted
by the Morton-Massaro model, dots represent obtained spike count histograms.
In order to test the statistical power of the likelihood-ratio test, we generated 20 neurons
with random histograms. The histograms were unimodal, with peak response randomly
selected between 0 and 9, with fall-offs similar to those found in the actual neurons and
with the same number of observations per condition as in the actual neurons. We then fitted
the 81-parameter Morton-Massaro model to each of these neurons and tested it using a
likelihood ratio test. All the simulated neurons exhibited statistically significant deviations
 ) suggesting that the test was quite sensitive.
from the model (chi-square, 63 df,

 	 	

Finally, for comparison purposes we tested a model of information integration that uses the
same number of parameters as the Morton-Massaro model but in which the stimulus and
context terms are are combined additively instead of multiplicatively, i.e.,

 
  		
  
   		
 
  
	
  

 

(2)

Figure 2: Effect of the stimulus and background on the chromatic mean tuning curves of
4 neurons. The thick dark and light lines show mean responses in the isoluminant plane
(x axis: L-M cone variation; y axis: S cone variation) for the two background conditions.
Black: gray background; Light: colored background. The 8 boxes around each tuning
curve shows the average response rate as a function of the time from stimulus onset for the
two background conditions.

Figure 3: Predicted (lines) and obtained (dots) spike count histograms for a typical neuron.
The horizontal axis represents spike counts in a 100 ms. window. The vertical axis represents probabilities. Each row represents a different background condition. Each column
represents a different stimulus condition.

  	 	

After fitting the new model, we performed a likelihood-ratio test. 80 % of the neurons
 ).
showed significant deviations from this model (chi-square, 63 df,

3 Relation to Tuning Curve Separability
In neuroscience separability is commonly studied in the context of mean tuning curves. For
example, a tuning curve is called (multiplicatively) separable if the conditional expected
value of a neuron?s response can be decomposed as the product of two different factors each
selectively influenced by a single stimulus dimension. An important aspect of the MortonMassaro model is that it applies to entire response histograms, not to expected values. If the
Morton-Massaro model holds, then separability appears in the following sense: If we are
allowed to see the response histograms for all the stimuli in background condition A and
the response histogram for a reference stimulus in background condition B, then it should
be possible to predict the response histograms for any stimulus in background condition B.
For example, by looking at the top row of Figure 1 and one of the cells of the bottom row
of Figure 1, it should be possible to reproduce all the other cells in the bottom row.

Obviously if we can predict response histograms then we can also predict tuning curves,
since they are based on averages of response histograms. Most importantly, there are forms
of separability of the tuning curve that become only apparent when studying the entire
response histogram. Figure 4 illustrates this fact with an example. The curve shows the
tuning curves of a particular neuron from an experiment fitted using the Morton-Massaro
model. These curves were obtained by fitting the entire spike count histograms for each
stimulus and background condition, and then obtaining the mean response for the predicted
histograms. The large open circles represent the obtained average responses. The dots
represent 95 % confidence intervals around those responses. Note that the two tuning
curves do not appear separable in a discernable way (it is not possible to predict curve B by
looking at curve A and a single point of curve B). Separability becomes only apparent when
the entire histogram is analyzed, not just the tuning curves based on response averages.

Figure 4: Tuning curves for a typical neuron as predicted by the Morton-Massaro model.
The two curves represent the average response of the neuron to isoluminant stimulus, for
two different background conditions. The elongated curve corresponds to the homogenous
gray background and the circular curve to the colored background. The open dots are the
obtained mean responses. The dots represent 95 % confidence interval of those responses.
Note that the predicted curves do not appear separable in a classic sense. However since
they are generated by Morton?s model the underlying code is factorial. This becomes apparent only when one looks at spike count histograms, not just mean tuning curves.

4 Discussion
We introduced the notion of Morton-style factorial coding and illustrated how it may help
analyze information integration and perceptual organization in the brain. We showed that
by focusing on average responses one may miss the existence of factorial coding mechanisms that become only apparent when analyzing spike count histograms. The results of
our study suggest that V1 represents color using a Morton-style factorial code. This may
provide some cues to help understand perceptual coding in the brain and to develop new
unsupervised learning algorithms. While methods like ICA (Bell & Sejnowski, 1997) develop independent codes, in Morton-style coding the goal is to make two or more external
aspects of the world become independent when conditioning on internal representations.
Morton-style coding is optimal when the statistics of stimulus and background exhibit a
particular property: when conditioning on each possible response category (i.e., spike
counts) the empirical likelihood ratios of stimulus and background factorize. Our study
suggests that Morton coding of color in natural scenes should be optimal or approximately
optimal, a prediction that can be tested via statistical analysis of color in natural scenes.

Acknowledgments
This project was supported by NSF?s grant ITR IIS-0223052.

5 References
Bell, A., & Sejnowski, T. (1997). The ?independent components? of natural scenes are edge filters.
Vision Research, 37(23), 3327?3338.
Brown, R. O., & MacLeod, D. I. A. (1997). Color appearance depends on the variance of surround
colors. Current Biology, (7), 844?849.
Derrington, A. M., Krauskopf, J., & Lennie, P. (1984). Chromatic mechanisms in lateral geniculate
nucleus of macaque. Journal of Physiology, 357, 241?265.
Domingos, P., & Pazzani, M. (1997). On the optimality of the simple Bayesian classifier under
zero-one loss. Journal of Machine Learning, 29, 103?130.
Massaro, D. W. (1987a). Categorical perception: A fuzzy logical model of categorization behavior.
In S. Harnad (Ed.), Categorical perception. Cambridge,England: Cambridge University Press.
Massaro, D. W. (1987b). Speech perception by ear and eye: A paradigm for psychological research.
Hillsdale, NJ: Erlbaum.
Massaro, D. W. (1989a). Perceiving talking faces. Cambridge, Massachusetts: MIT Press.
Massaro, D. W. (1989b). Testing between the TRACE model and the fuzzy logical model of speech
perception. Cognitive Psychology, 21, 398?421.
Morton, J. (1969). The interaction of information in word recognition. Psychological Review, 76,
165?178.
Movellan, J. R., & McClelland, J. L. (2001). The Morton-Massaro law of information integration:
Implications for models of perception. Psychological Review, (1), 113?148.
Stockman, A., MaCleod, D. I. A., & Johnson, N. E. (1993). Spectral sensitivities of the human cones.
Journal of the Optical Society of America A, (10), 2491?2521.
Wachtler, T., Sejnowski, T. J., & Albright, T. D. (2003). Representation of color stimuli in awake
macaque primary visual cortex. Neuron, 37, 1?20.
Wesner, M. F., & Shevell, S. K. (1992). Color perception within a chromatic context: Changes in
red/green equilibria caused by noncontiguous light. Vision Research, (32), 1623?1634.
Zeki, S. (1983). Colour coding in cerebral cortex: the responses of wavelength selective and colourcoded cells in monkey visual cortex to changes in wavelenght composition. Neuroscience, 9,
767?781.

"
4545,2013,Density estimation from unweighted k-nearest neighbor graphs: a roadmap,"Consider an unweighted k-nearest neighbor graph   on n points that have been sampled i.i.d. from some unknown density p on R^d. We prove how one can estimate the density p just from the unweighted adjacency matrix of the graph, without knowing the points themselves or their distance or similarity scores. The key insights are that local differences in link numbers can be used to estimate some local function of p, and that integrating this function along shortest paths leads to an estimate of the underlying density.","Density estimation from unweighted k-nearest
neighbor graphs: a roadmap

Ulrike von Luxburg
and
Morteza Alamgir
Department of Computer Science
University of Hamburg, Germany
{luxburg,alamgir}@informatik.uni-hamburg.de

Abstract
Consider an unweighted k-nearest neighbor graph on n points that have been sampled i.i.d. from some unknown density p on Rd . We prove how one can estimate
the density p just from the unweighted adjacency matrix of the graph, without
knowing the points themselves or any distance or similarity scores. The key insights are that local differences in link numbers can be used to estimate a local
function of the gradient of p, and that integrating this function along shortest paths
leads to an estimate of the underlying density.

1

Introduction

The problem. Consider an unweighted k-nearest neighbor graph that has been built on a random
sample X1 , ..., Xn from some unknown density p on Rd . Assume we are given the adjacency matrix
of the graph, but we do not know the point locations X1 , ...., Xn or any distance or similarity scores
between the points. Is it then possible to estimate the underlying density p, just from the adjacency
matrix of the unweighted graph?
Why is this problem interesting for machine learning? Machine learning algorithms on graphs
are abundant, ranging from graph clustering methods such as spectral clustering over label propagation methods for semi-supervised learning to dimensionality reduction methods and manifold
algorithms. In the majority of applications, the graphs that are used as input are similarity graphs:
Given a set of abstract ?objects? X1 , ..., Xn we first compute pairwise similarities s(Xi , Xj ) according to some suitable similarity function and then build a k-nearest neighbor graph (kNN graph for
short) based on this similarity function. The intuition is that the edges in the graph encode the local
information given by the similarity function, whereas the graph as a whole reveals global properties
of the data distribution such as cluster properties, high- and low-density regions, or manifold structure. From a computational point of view, kNN graphs are convenient because they lead to a sparse
representation of the data ? even more so when the graph is unweighted. From a statistical point of
view the key question is whether this sparse representation still contains all the relevant information
about the original data, in particular the information about the underlying data distribution. It is easy
to see that for suitably weighted kNN graphs this is the case: the original density can be estimated
from the degrees in the graph. However, it is completely unclear whether the same holds true for
unweighted kNN graphs.
Why is the problem difficult? The naive attempt to estimate the density from vertex degrees
obviously has to fail in unweighted kNN graphs because all vertex degrees are (about) k. Moreover,
unweighted kNN graphs are invariant with respect to rescaling of the underlying distribution by
a constant factor (e.g., the unweighted kNN graph on a sample from the uniform distribution on
[0, 1]2 is indistinguishable from a kNN graph on a sample from the uniform distribution on [0, 2]2 ).
So all we can hope for is an estimate of the density up to some multiplicative constant that cannot
be determined from the kNN graph alone. The main difficulty, however, is that a kNN graph ?looks
1

the same? in every small neighborhood. To see this, consider the case where the underlying density
is continuous, hence approximately constant in small neighborhoods. Then, if n is large and k/n is
small, local neighborhoods in the kNN graph are all going to look like kNN graphs from a uniform
distribution. This intuition raises an important issue. It is impossible to estimate the density in
an unweighted kNN graph by local quantities alone. We somehow have to make use of global
properties if we want to be successful. This makes the problem very different and much harder than
more standard density estimation problems.
Our solution. We show that it is indeed possible to estimate the underlying density from an
unweighted kNN graph. The construction is fairly involved. In a first step we estimate a pointwise
function of the gradient of the density, and in a second step we integrate these estimates along
shortest paths in the graph to end up with an approximation of the log-density. Our estimate works
as long as the kNN graph is reasonably dense (k d+2 /(n2 logd n) ? ?). However, it fails in the
more important sparser regime (e.g., k ? log n). Currently we do not know whether this is due to a
suboptimal proof or whether density estimation is generally impossible in the sparse regime.

2

Notation and assumptions

Underlying space. Let X ? Rd be a compact subset of Rd . Denote by ?X the topological boundary
of X . For ? > 0 define the ?-interior X? := {x ? X  d(x, ?X ) ? ?}. We assume that X is ?full
dimensional? in the sense that there exists some ?0 > 0 such that X?0 is non-empty and connected.
By ?d we denote the volume of a d-dimensional unit ball, and by vd the volume of the intersection
of two d-dimensional unit balls whose centers have distance 1.
Density. Let p be a continuously differentiable density on X . We assume that there exist constants
pmin and pmax such that 0 < pmin ? p(x) ? pmax < ? for all x ? X .
Graph. Given an i.i.d. sample Xn := {X1 , ..., Xn } from p, we build a graph Gn = (Vn , En ) with
Vn = Xn . We connect Xi by a directed edge to Xj if Xj is among the k-nearest neighbors of Xi .
The resulting graph is called the directed, unweighted kNN graph (in the following, we will often
drop the words ?directed? and ?unweighted?). By r(x) := rn,k (x) we denote the Euclidean distance
of a point x to its kth nearest neighbor. For any vertex x ? V we define the sets

In(x) := Inn,k (x) := {y ? Xn  (y, x) ? En }
(source points of in-links to x)


Out(x) := Outn,k (x) := {y ? Xn (x, y) ? En }
(target points of out-links from x).
To increase readability we often omit the indices n and k. For a finite set S we denote by |S| its
number of elements.
Paths. For a rectifiable path ? : [0, 1] ? X we define its p-weighted length as
Z
Z 1
`p (?) :=
p1/d (x) ds :=
p1/d (?(t))|? 0 (t)| dt
?

0

(recall the notational convention of writing ?ds? in a line integral). For two points x, y ? X we
define their p-weighted distance as Dp (x, y) = inf ? `p (?) where the infimum is taken over all
rectifiable paths ? that connect x to y. As a consequence of the compactness of X , a minimizing
path that realizes Dp always exists (cf. Burago et al., 2001, Section 2.5.2). We call such a path a
Dp -shortest path. Under the given assumptions on p, the Dp -shortest path between any two points
x, y ? X?0 is smooth.
In an unweighted graph, define the length of a path as its number of edges. For two vertices
x, y denote by Dsp (x, y) their shortest path distance in the graph. It has been proved in Alamgir
and von Luxburg (2012) that for unweighted, undirected kNN graphs, (k/(n?d ))1/d Dsp (x, y) ?
Dp (x, y) almost surely as n ? ? and k ? ? appropriately slowly. The proofs extend directly to
the case of directed kNN graphs.

3

Warmup: the 1-dimensional case

To gain some intuition about the problem and its solution, let us consider the 1-dimensional case
X ? R. For any given point x ? Xn we define the following sets:


Left1 (x) := |{y ? Out(x)  y < x}|
and
Right1 (x) := |{y ? Out(x)  y > x}|.
2

t
en
ng

R

at x

Rp'(x)

D

en
si
ty

Ta

e
o th

ce t
spa
t
n
ge
T an

Rp'(x)

sity
d en

xl

x
Left 1

xr

Left
d

Right1

R

Right

d

Figure 1: Geometric argument (left: 1-dimensional case, right: 2-dimensional case). The difference
Right ? Left is approximately proportional to the volume of the grey-shaded area.
The intuition to estimate the density from the directed kNN graph is the following. Consider a point
x in a region where the density has positive slope. The set Out(x) is approximately symmetric
around x, that is it has the form Out(x) = Xn ? [x ? R, x + R] for some R > 0. When the density
has an increasing slope at x, there tend to be less sample points in [x?R, x] than in [x, x+R], so the
set Right1 (x) tends to contain more sample points than the set Left1 (x). This is the effect we want
to exploit. The difference Right1 (x)?Left1 (x) can be approximated by n?(P ([x, x+R])?P ([x?
R, x])), and by a simple geometric argument one can see that the latter probability is approximately
R2 p0 (x). See Figure 1 (left side) for an illustration. By standard concentration arguments one can
see that if n is large enough and k chosen appropriately, then R ? k/(2np(x)). Plugging these
two things together shows that Right1 (x) ? Left1 (x) ? (k 2 /(4n2 )) ? p0 (x)/p2 , hence gives an
estimate of p0 (x)/p2 (x). But we are not there yet: it is impossible to directly turn an estimate of
p0 (x)/p2 (x) into an estimate of p(x). This is in accordance with the intuition we mentioned above:
one cannot estimate the density by just looking at a local neighborhood of x in the kNN graph.
Here is now the key trick to introduce a global component to the estimate. We fix one data point
X0 that is going to play the role of an anchor point. To estimate the density at a particular data
point Xs , we now sum the estimates p0 (x)/p2 (x) over all data points x that sit between X0 and Xs .
This corresponds to integrating the function p0 (x)/p2 (x) over the interval [X0 , Xs ] with respect to
the underlying density p, which in turn corresponds to integrating the function p0 (x)/p(x) over the
interval [X0 , Xs ] with respect to the standard Lebesgue measure. This latter integral is well known,
its primitive is log p(x). Hence, for each data point Xs we get an estimate of log p(Xs ) ? log p(X0 ).
Then we exponentiate and arrive at an estimate of c ? p(x), where c = 1/p(X0 ) plays the role of an
unknown constant.

4

A hypothetical estimate in the d-dimensional case

We now generalize our approach to the d-dimensional setting. There are two main challenges: First,
we need to replace the integral over all sample points between X0 and Xs by something more general
in Rd . Our idea is to consider an integral along a path between X0 and Xs , specifically along a path
that corresponds to a shortest path in the graph Gn . Second, we need a generalization of the concept
of what are ?left? and ?right? out-links. Our idea is to use the shortest path as reference. For a point
x on the shortest path between X0 and Xs , the ?left points? of x should be the ones that are on or
close to the subpath from X0 to x and ?right points? the ones on or close to the path from x to Xs .
4.1

Gradient estimates based on link differences

Fix a point x on a simple, continuously differentiable path ? and let T (x) be its tangent vector.
Consider h(y) = hw, yi + b with normal
vector w := T (x), where the offset b has been chosen such

that the hyperplane H := {y ? Rd  h(y) = 0} goes through x. Define

Leftd (x) := Leftd,n,k (x) := |{x ? Out(x)  h(x) ? 0}|

Rightd (x) := Rightd,n,k (x) := |{x ? Out(x)  h(x) > 0}|.
3

Out(x)
H
Out(x)
x
Left
d

In(xr )

In(x l )
path ?

path ?
Right

x
r

xl
Left

d

?

Right

?

Figure 2: Definitions of ?left? and ?right?in the d-dimensional case.
See Figure 2 (left side) for an illustration. This definition is a direct generalization of the definition
of Left1 und Right1 in the 1-dimensional case. It is not yet the end of the story, as the quantities
Leftd and Rightd cannot be evaluated based on the kNN graph alone, but it is a good starting point to
develop the necessary proof concepts. In this section we prove the consistency of a density estimate
based on Leftd and Rightd . In Section 5 we will further generalize the definition to our final estimate.
Theorem 1 (Estimate related to the gradient) Let X and p satisfy the assumptions in Section 2.
Let ? be a differentiable, regular, simple path in X?0 and x a sample point on this path. Let T be
the tangent direction of ? at x and p0T (x) the directional derivative of the density p in direction T at
point x. Then, if n ? ?, k ? ?, k/n ? 0, k d+2 /n2 ? ?,
1/d

2n1/d ?d 
p0T (x)
a.s.
Rightd,n,k (x) ? Leftd,n,k (x) ??
(d+1)/d
k
p(x)(d+1)/d

If k d+2 /(n2 logd n) ? ? the convergence even holds uniformly over all sample points x ? Xn .
Proof sketch. The key problem in the proof is that the difference Rightd ?Leftd is of a much smaller
order of magnitude than Rightd and Leftd themselves, so controlling the deviations of Rightd ?Leftd
is somewhat tricky. Conditioned on rout (x) =: r, Rightd ? Bin(k, ?r ) and Leftd ? Bin(k, ?l ),
where ?r = P (right half ball)/P (ball) and ?l?analogously (cf. Figure 2). By Hoeffding?s inequality, Rightd ? Leftd ? E(Rightd ? Leftd ) ? ?( k) with high probability. Note that ?l and ?r tend to
be close to 1/2, thus Hoeffding?s inequality is reasonably tight. A simple geometric argument shows
that if the density in a neighborhood of x is linear, then E(Rightd ? Leftd ) = n ? rd ?d /2 ? rp0T (x)
(n times the probability mass of the grey area in Figure 1). A similar argument holds approximately
if the density is just differentiable at x. A standard concentration argument for the out-radius shows
that with high probability, rout (x) can be approximated by (k/(n?d p(x)))1/d . Combining all results
we obtain that with high probability,
1/d
 n1/d 
2n1/d ?d
p0T (x)
(Right
?
Left
)
=
?
?
.
d
d
k (d+1)/d
p(x)(d+1)/d
k 1/2+1/d

Convergence takes place if the noise term on the right hand side goes to 0 and the ?high probability?
converges to 1, which happens under the conditions on n and k stated in the theorem.
,
4.2

Integrating the gradient estimates along the shortest path

To deal with the integration part, let us recap some standard results about line integrals.
Proposition 2 (Line integral) Let ? : [0, 1] ? Rd be a simple, continuously differentiable path
from x0 = ?(0) to x1 = ?(1) parameterized by arc length. For a point x = ?(t) on the path, denote
by T (x) the tangent vector to ? at x, and by p0T (x) the directional derivative of p in the tangent
direction T . Then
Z 0
pT (x)
ds = log(p(x1 )) ? log(p(x0 )).
? p(x)
4

Proof. We define the vector field
?p/?x1
...
?p/?xd

p0 (x)
1
F : R ? R , x 7?
=
p(x)
p(x)
d

d

!
.

Observe that F is a continuous gradient field with primitive V : Rd ? R, x 7? log(p(x)). Now
consider the line integral of F along ?:
Z
Z 1D
Z 1
D
E
E
1
def
0
p0 (?(t)), ? 0 (t) dt.
(1)
F (x) dx =
F (?(t)), ? (t) dt =
?
0
0 p(?(t))
Note that ? 0 (t) is the tangent vector T (x) of the path ? at point x = ?(t). Hence, the scalar product
hp0 (?(t)), ? 0 (t)i coincides with the directional derivative of p in direction T , so the right hand side
of Equation (1) coincides with the left hand side of the equation in the proposition. On the other
hand, it is well known that the line integral over a gradient field only depends on the starting and
end point of ? and is given by
Z
F (x) dx = V (x1 ) ? V (x0 ).
?

This coincides with the right hand side of the equation in the proposition.

,

Now we consider the finite sample case. The goal is to approximate the integral along the continuous
path ? by a sum along a path ?n in the kNN graph Gn . To achieve this, we need to construct a
sequence of paths ?n in Gn such that ?n converges to some well-defined path ? in the underlying
space and the lengths of ?n in Gn converge to `p (?). To this end, we are going to consider paths ?n
which are shortest paths in the graph.
Adapting the proof of the convergence of shortest paths in unweighted kNN graphs (Alamgir and
von Luxburg, 2012) we can derive the following statement for integrals along shortest paths.
Proposition 3 (Integrating a function along a shortest path) Let X and p satisfy the assumptions
in Section 2. Fix two sample points in X?0 , say X0 and Xs , and let ?n be a shortest path between
X0 and Xs in the kNN graph Gn . Let ? ? X be a path that realizes Dp (X0 , Xs ). Assume that it
is unique and is completely contained in X?0 . Let g : X ? R be a continuous function. Then, as
n ? ?, k 1+? /n ? 0 (for some small ? > 0), k/ log n ? ?,

1/d X
Z
k
?
g(x) ??
g(x)p(x)1/d ds a.s.
n?d
?
x??
n

Note that if g(x)p1/d (x) can be written in the form hF (?(t)), ? 0 (t)i, then the same statement even
holds if the shortest Dp -path is not unique, because the path integral then only depends on start and
end point. This is the case for our particular function of interest, g(x) = p0T (x)/p1+1/d (x).
4.3

Combining everything to obtain a density estimate

Theorem 4 (Density estimate) Let X and p satisfy the assumptions in Section 2, let X0 ? X?0 be
any fixed sample point. For another sample point Xs , let ?n be a shortest path between X0 and Xs
in the kNN graph Gn . Assume that there exists a path ? that realizes Dp (x, y) and that is completely
contained in X?0 . Then, as n ? ?, k ? ?, k/n ? 0, k d+2 /(n2 logd n) ? ?,
2 X
(Rightd,n,k (x) ? Leftd,n,k (x)) ?? log p(Xs ) ? log p(X0 ) a.s.
k x??
n

Proof sketch. By Proposition 2,
Z
log(p(Xs )) ? log(p(X0 )) =
?

p0T (x)
ds =
p(x)
5

Z
?

p0T (x)
p(x)1/d ds.
p(x)(d+1)/d

According to Proposition 3, the latter can be approximated by

1/d X
k
p0T (x)
n?d
p(x)(d+1)/d
x??
n

where ?n is a shortest path between X0 and Xs in the kNN graph. Proposition 1 shows that this
quantity gets estimated by

1/d
 2 X

X
k
n1/d
1/d
? 2?d
Rightd (x) ? Leftd (x) =
Rightd (x) ? Leftd (x) .
(d+1)/d
n?d
k x??
k
x??
n

n

,

5

The final d-dimensional density estimate

In this section, we finally introduce an estimate that solely uses quantities available from the kNN
graph. Let x be a vertex on a shortest path ?n,k in the kNN graph Gn . Let xl and xr be the
predecessor and successor vertices of x on this path (in particular, xl and xr are sample points as
well). Define
Left?n,k (x) := | Out(x) ? In(xl )|

and

Right?n,k (x) := | Out(x) ? In(xr )|.

See Figure 2 (right side) for an illustration. On first glance, these sets look quite different from
Leftd and Rightd . But the intuition is that whenever we find two sets on the left and right side of
x that have approximately the same volume, then the difference Left?n,k ? Right?n,k should be a
function of p0T (x). For a second intuition consider the special case d = 1 and recall the definition of
R of Section 3. One can show that in expectation, [x ? R, x] coincides with Out(x) ? In(xl ) and
[x, x + R] with Out(x) ? In(xr ), so in case d = 1 the definitions coincide in expectation with the
ones in Section 3. Another insight is that the set Left?n,k (x) counts the number of directed paths of
length 2 from x to xl , and Right?n,k (x) analogously.
We conjecture that the difference Right?n,k ? Left?n,k can be used as before to construct a density
estimate. Specifically, if ?n,k is a shortest path from the anchor point X0 to Xs , we believe that
under similar conditions on k and n as before,
?d X
Right?n,k (x) ? Left?n,k (x)
(?)
k?d x??
n

is a consistent estimator of the quantity log p(Xs ) ? log p(X0 ). Our simulations in Section 6 show
that the estimate works, even surprisingly well. So far we do not have a formal proof yet, due
to two technical difficulties. The first problem is that the set In(xl ) is not a ball, but an ?eggshaped? set. As n ? ?, one can sandwich In(x) between two concentric balls that converge to
each other, but this approximation is too weak to carry the proof. To compute the expected value
E(Right?n,k (x) ? Left?n,k (x)) we would have to integrate the intersection of the ?egg? In(xl ) with
the ball Out(x), and so far we have no closed form solution. The second difficulty is related to the
shortest path in the graph. While it is clear that ?most edges? in this path have approximately the
maximal length (that is, (k/(n?d p(x))1/d for an edge in the neighborhood of x), this is not true for
all edges. Intuitively it is clear that the contribution of the few violating edges will be washed out in
the integral along the shortest path, but we don?t have a formal proof yet.
What we can prove is the following weaker version. Consider a Dp -shortest path ? ? Rd and a point
x on this path with out-radius rout (x). Define the points xl and xr as the two points where the path ?
enters resp. leaves the ball B(x, rout (x)), and define the sets Ln,k := Out(x) ? B(xl , rout (x)) and
1/d
1/d P
Rn,k := Out(x) ? B(xr , rout (x)). Then it can be proved that (?d )/(k?d ) x??n Rn,k (x) ?
Ln,k (x) ? log p(Xs ) ? log p(X0 ). The proof is similar to the one in Section 4 . It circumvents the
problems mentioned above by using well defined balls instead of In-sets and the continuous path ?
rather than the finite sample shortest path ?n , but the quantities cannot be estimated from the kNN
graph alone.
6

6

Simulations

As a proof of concept, we ran simple experiments to evaluate the behavior of estimator (?). We
draw n = 2000 points according to a couple of simple densities on R, R2 and R10 , then we build
the directed, unweighted kNN graph with k = 50. We fix a random point as anchor point X0 ,
compute the quantities Right?n,k and Left?n,k for all sample points, and then sum the differences
Right?n,k ? Left?n,k along shortest paths to X0 . Rescaling by the constant ?n /(kvd ) and exponentiating then leads to our estimate for p(x)/p(X0 ). In order to nicely plot our results, we multiply
the resulting estimate by p(X0 ) to get rid of the scaling constant (this step would not be possible in
applications, but it merely serves for illustration purposes). The results are shown in Figure 3. It is
obvious from these figures that our estimate ?works?, even surprisingly well (note that the sample
size is not very large and we did not perform any parameter tuning). Even in the case of a step
function the estimate recovers the structure of the density. Note that this is a particularly difficult
case in our setting, because within the constant parts of the two steps, the kNN graphs of the left and
right step are indistinguishable. It is only in a small strip around the boundary between the two steps
that kNN graph will reveal non-uniform behavior. The simulations show that this is already enough
to reveal the overall structure of the step function.

7

Extensions

We have seen how to estimate the density in an unweighted, directed kNN graph. It is even possible
to extend this result to more general cases. Here is a sketch of the main ideas.
Estimating the dimension from the graph. The current density estimate requires that we know
the dimension d of the underlying space because we need to be able to compute the constants ?d
(volume of the unit ball) and vd (intersection of two unit balls). The dimension can be estimated
from the directed, unweighted kNN graph as follows. Denote by r the distance of x to its kth-nearest
neighbor, and by K the number of vertices that can be reached from x by a directed shortest path
of length 2. Then k/n ? P (B(x, r)) and K/n ? P (B(x, 2r)). If n is large enough and k small
enough, the density on these balls is approximately constant, which implies K/k ? 2d where d is
the dimension of the underlying space.
Recovering the directed graph from the undirected one. The current estimate is based on the
directed kNN graph, but many applications use undirected kNN graphs. However, it is possible to
recover the directed, unweighted kNN graph from the undirected, unweighted graph. Denote by
N (x) the vertices that have an undirected edge to x. If n is large and k small, then for any two
vertices x and y we can approximate |N (x) ? N (y)|/n ? P (B(x, r) ? B(y, r)). The latter is
monotonously decreasing with kx ? yk. To estimate the set Out(x) in order to recover the directed
kNN graph, we rank all points y ? N (x) according to |N (x) ? N (y)| and choose Out(x) as the
first k vertices in this ranking.
Point embedding. In this paper we focus on estimating the density from the unweighted kNN graph.
Another interesting problem is to recover an embedding of the vertices to Rd such that the kNN graph
based on the embedded vertices corresponds to the given kNN graph. This problem is closely related
to a classic problem in statistics, namely non-metric multidimensional scaling (Shepard, 1966, Borg
and Groenen, 2005), and more specifically to learning distances and embeddings from ranking and
comparison data (Schultz and Joachims, 2004, Agarwal et al., 2007, Ouyang and Gray, 2008, McFee
and Lanckriet, 2009, Shaw and Jebara, 2009, Shaw et al., 2011, Jamieson and Nowak, 2011) as well
as to ordinal (monotone) embeddings (Bilu and Linial, 2005, Alon et al., 2008, B?adoiu et al., 2008,
Gutin et al., 2009). However, we are not aware of any approach in the literature that can faithfully
embed unweighted kNN graphs and comes with performance guarantees. Based on our density
estimate, such an embedding can now easily be constructed. Given the unweighted kNN graph,
we assign edge weights w(Xi , Xj ) = (?
p?1/d (Xi ) + p??1/d (Xj ))/2 where p? is the estimate of the
underlying density. Then the shortest paths in this weighted kNN graph converge to the Euclidean
distances in the underlying space, and standard metric multidimensional scaling can be used to
construct an appropriate embedding. In the limit of n ? ?, this approach is going to recover the
original point embedding up to similarity transformations (translation, rotation or rescaling).
7

density, n=2000, k=50, dim=1

density, n=2000, k=50, dim=2

1

density, n=2000, k=50, dim=10

1.4

2.5

1.2
0.8

2
1

0.6

0.8

1.5

0.4

0.6

1

0.4
0.2

0.5
0.2

0
?3

?2

?1

0

1

2

3

log(p) true, n = 2000, k=50, dim=2

0
?3

?2

?1

0

1

2

0
?4

3

?2

0

2

4

density, n=2000, k=50, dim=2

log(p) estimated, n = 2000, k=50, dim=2
0.2

3

3

2

2

1

1

0

0

?1

?1

?2

?2
?2

0

2

log(p) true, n = 2000, k=50, dim=2

0.15

0.1

0.05

?2

0

2

0
?4

?2

0

2

4

density, n=2000, k=50, dim=2

log(p) estimated, n = 2000, k=50, dim=2
1.5

0.8

0.8

0.6

0.6

0.4

0.4

0.2

0.2

1

0.5

0.2

0.4

0.6

0.8

0.2

0.4

0.6

0.8

0
0

0.2

0.4

0.6

0.8

1

Figure 3: Densities and their estimates. Density model in the first row: the first dimension is sampled
from a mixture of Gaussians, the other dimensions from a uniform distribution. The figures plot the
first dimension of the data points versus the true (black) and estimated (green) density values. From
left to right, they show the case of 1, 2, and 10 dimensions, respectively. Second and third row:
2-dimensional densities. The left plots show the true log-density (a Gaussian and a step function),
the middle plots show the estimated log-density. The right figures plot the first coordinate of the
data points against the true (black) and estimated (green) density values. The black star in the left
plot depicts the anchor point X0 of the integration step.

8

Conclusions

In this paper we show how a density can be estimated from the adjacency matrix of an unweighted,
directed kNN graph, provided the graph is dense enough (k d+2 /(n2 logd n) ? ?). In this case, the
information about the underlying density is implicitly contained in unweighted kNN graphs, and,
at least in principle, accessible by machine learning algorithms. However, in most applications, k
is chosen much, much smaller, typically on the order k ? log(n). For such sparse graphs, our
density estimate fails because it is dominated by sampling noise that does not disappear as n ? ?.
This raises the question whether this failure is just an artifact of our particular construction or of
our proof, or whether a similar phenomenon is true more generally. If yes, then machine learning
algorithms on sparse unweighted kNN graphs would be highly problematic: If the information about
the underlying density is not present in the graph, it is hard to imagine how machine learning algorithms (for example, spectral clustering) could still be statistically consistent. General lower bounds
proving or disproving these speculations are an interesting open problem.
Acknowledgements
We would like to thank Gabor Lugosi for help with the proof of Theorem 1. This research was
partly supported by the German Research Foundation (grant LU1718/1-1 and Research Unit 1735
?Structural Inference in Statistics: Adaptation and Efficiency?).

8

References
S. Agarwal, J. Wills, L. Cayton, G. Lanckriet, D. Kriegman, and S. Belongie. Generalized nonmetric multidimensional scaling. In AISTATS, 2007.
M. Alamgir and U. von Luxburg. Shortest path distance in random k-nearest neighbor graphs. In
International Conference on Machine Learning (ICML), 2012.
N. Alon, M. B?adoiu, E. Demaine, M. Farach-Colton, M. Hajiaghayi, and A. Sidiropoulos. Ordinal
embeddings of minimum relaxation: general properties, trees, and ultrametrics. ACM Transactions on Algorithms, 4(4):46, 2008.
M. B?adoiu, E. Demaine, M. Hajiaghayi, A. Sidiropoulos, and M. Zadimoghaddam. Ordinal embedding: approximation algorithms and dimensionality reduction. In Approximation, Randomization
and Combinatorial Optimization. Algorithms and Techniques. Springer, 2008.
Y. Bilu and N. Linial. Monotone maps, sphericity and bounded second eigenvalue. Journal of
Combinatorial Theory, Series B, 95(2):283?299, 2005.
I. Borg and P. Groenen. Modern multidimensional scaling: Theory and applications. Springer,
2005.
D. Burago, Y. Burago, and S. Ivanov. A course in metric geometry. American Mathematical Society,
2001.
G. Gutin, E. Kim, M. Mnich, and A. Yeo. Ordinal embedding relaxations parameterized above tight
lower bound. arXiv preprint arXiv:0907.5427, 2009.
K. Jamieson and R. Nowak. Low-dimensional embedding using adaptively selected ordinal data. In
Conference on Communication, Control, and Computing, pages 1077?1084, 2011.
B. McFee and G. Lanckriet. Partial order embedding with multiple kernels. In International Conference on Machine Learning (ICML), 2009.
H. Ouyang and A. Gray. Learning dissimilarities by ranking: from SDP to QP. In International
Conference on Machine Learning (ICML), pages 728?735, 2008.
M. Schultz and T. Joachims. Learning a distance metric from relative comparisons. In Neural
Information Processing Systems (NIPS), 2004.
B. Shaw and T. Jebara. Structure preserving embedding. In International Conference on Machine
Learning (ICML), 2009.
B. Shaw, B. Huang, and T. Jebara. Learning a distance metric from a network. Neural Information
Processing Systems (NIPS), 2011.
R. Shepard. Metric structures in ordinal data. Journal of Mathematical Psychology, 3(2):287?315,
1966.

9

"
1996,2005,On the Accuracy of Bounded Rationality: How Far from Optimal Is Fast and Frugal?,Abstract Missing,"On the Accuracy of Bounded Rationality:
How Far from Optimal Is Fast and Frugal?

Michael Schmitt
Ludwig-Marum-Gymnasium
Schlossgartenstra?e 11
76327 Pfinztal, Germany
mschmittm@googlemail.com

Laura Martignon
Institut f?ur Mathematik und Informatik
P?adagogische Hochschule Ludwigsburg
Reuteallee 46, 71634 Ludwigsburg, Germany
martignon@ph-ludwigsburg.de

Abstract
Fast and frugal heuristics are well studied models of bounded rationality. Psychological research has proposed the take-the-best heuristic as a
successful strategy in decision making with limited resources. Take-thebest searches for a sufficiently good ordering of cues (features) in a task
where objects are to be compared lexicographically. We investigate the
complexity of the problem of approximating optimal cue permutations
for lexicographic strategies. We show that no efficient algorithm can approximate the optimum to within any constant factor, if P 6= NP. We
further consider a greedy approach for building lexicographic strategies
and derive tight bounds for the performance ratio of a new and simple
algorithm. This algorithm is proven to perform better than take-the-best.

1

Introduction

In many circumstances the human mind has to make decisions when time and knowledge
are limited. Cognitive psychology categorizes human judgments made under such constraints as being boundedly rational if they are ?satisficing? (Simon, 1982) or, more generally, if they do not fall too far behind the rational standards. A class of models for human
reasoning studied in the context of bounded rationality consists of simple algorithms termed
?fast and frugal heuristics?. These were the topic of major psychological research (Gigerenzer and Goldstein, 1996; Gigerenzer et al., 1999). Great efforts have been put into testing
these heuristics by empirical means in experiments with human subjects (Br?oder, 2000;
Br?oder and Schiffer, 2003; Lee and Cummins, 2004; Newell and Shanks, 2003; Newell
et al., 2003; Slegers et al., 2000) or in simulations on computers (Br?oder, 2002; Hogarth
and Karelaia, 2003; Nellen, 2003; Todd and Dieckmann, 2005). (See also the discussion
and controversies documented in the open peer commentaries on Todd and Gigerenzer,
2000.)
Among the fast and frugal heuristics there is an algorithm called ?take-the-best? (TTB)
that is considered a process model for human judgments based on one-reason decision
making. Which of the two cities has a larger population: (a) D?usseldorf (b) Hamburg?
This is the task originally studied by Gigerenzer and Goldstein (1996) where German cities
with a population of more than 100,000 inhabitants had to be compared. The available
information on each city consists of the values of nine binary cues, or attributes, indicating

Hamburg
Essen
D?usseldorf
Validity

Soccer Team
1
0
0
1

State Capital
1
0
1
1/2

License Plate
0
1
1
0

Table 1: Part of the German cities task of Gigerenzer and Goldstein (1996). Shown are
profiles and validities of three cues for three cities. Cue validities are computed from the
data as given here. The original data has different validities but the same cue ranking.
presence or absence of a feature. The cues being used are, for instance, whether the city is
a state capital, whether it is indicated on car license plates by a single letter, or whether it
has a soccer team in the national league. The judgment which city is larger is made on the
basis of the two binary vectors, or cue profiles, representing the two cities. TTB performs
a lexicographic strategy, comparing the cues one after the other and using the first cue that
discriminates as the one reason to yield the final decision. For instance, if one city has
a university and the other does not, TTB would infer that the first city is larger than the
second. If the cue values of both cities are equal, the algorithm passes on to the next cue.
TTB examines the cues in a certain order. Gigerenzer and Goldstein (1996) introduced
ecological validity as a numerical measure for ranking the cues. The validity of a cue is
a real number in the interval [0, 1] that is computed in terms of the known outcomes of
paired comparisons. It is defined as the number of pairs the cue discriminates correctly
(i.e., where it makes a correct inference) divided by the number of pairs it discriminates
(i.e., where it makes an inference, be it right or wrong). TTB always chooses a cue with
the highest validity, that is, it ?takes the best? among those cues not yet considered. Table 1
shows cue profiles and validities for three cities. The ordering defined by the size of their
population is given by
{h D?usseldorf , Essen i, h D?usseldorf , Hamburg i, h Essen , Hamburg i},
where a pair ha, bi indicates that a has less inhabitants than b. As an example for calculating
the validity, the state-capital cue distinguishes the first and the third pair but is correct only
on the latter. Hence, its validity has value 1/2.
The order in which the cues are ranked is crucial for success or failure of TTB. In the example of D?usseldorf and Hamburg, the car-license-plate cue would yield that D?usseldorf (D)
is larger than Hamburg (HH), whereas the soccer-team cue would correctly favor Hamburg.
Thus, how successful a lexicographic strategy is in a comparison task consisting of a partial ordering of cue profiles depends on how well the cue ranking minimizes the number of
incorrect comparisons. Specifically, the accuracy of TTB relies on the degree of optimality
achieved by the ranking according to decreasing cue validities. For TTB and the German
cities task, computer simulations have shown that TTB discriminates at least as accurate as
other models (Gigerenzer and Goldstein, 1996; Gigerenzer et al., 1999; Todd and Dieckmann, 2005). TTB made as many correct inferences as standard algorithms proposed by
cognitive psychology and even outperformed some of them.
Partial results concerning the accuracy of TTB compared to the accuracy of other strategies have been obtained analytically by Martignon and Hoffrage (2002). Here we subject
the problem of finding optimal cue orderings to a rigorous theoretical analysis employing
methods from the theory of computational complexity (Ausiello et al., 1999). Obviously,
TTB runs in polynomial time. Given a list of ordered pairs, it computes all cue validities
in polynomially many computing steps in terms of the size of the list. We define the optimization problem M INIMUM I NCORRECT L EXICOGRAPHIC S TRATEGY as the task of
minimizing the number of incorrect inferences for the lexicographic strategy on a given list
of pairs. We show that, unless P = NP, there is no polynomial-time approximation algo-

rithm that computes solutions for M INIMUM I NCORRECT L EXICOGRAPHIC S TRATEGY
that are only a constant factor worse than the optimum, unless P = NP. This means that
the approximating factor, or performance ratio, must grow with the size of the problem.
As an extension of TTB we consider an algorithm for finding cue orderings that was called
?TTB by Conditional Validity? in the context of bounded rationality. It is based on the
greedy method, a principle widely used in algorithm design. This greedy algorithm runs
in polynomial time and we derive tight bounds for it, showing that it approximates the
optimum with a performance ratio proportional to the number of cues. An important consequence of this result is a guarantee that for those instances that have a solution that discriminates all pairs correctly, the greedy algorithm always finds a permutation attaining this
minimum. We are not aware that this quality has been established for any of the previously
studied heuristics for paired comparison. In addition, we show that TTB does not have this
property, concluding that the greedy method of constructing cue permutations performs
provably better than TTB. For a more detailed account and further results we refer to the
complete version of this work (Schmitt and Martignon, 2006).

2

Lexicographic Strategies

A lexicographic strategy is a method for comparing elements of a set B ? {0, 1}n . Each
component 1, . . . , n of these vectors is referred to as a cue. Given a, b ? B, where a =
(a1 , . . . , an ) and b = (b1 , . . . , bn ), the lexicographic strategy searches for the smallest
cue index i ? {1, . . . , n} such that ai and bi are different. The strategy then outputs one
of ? < ? or ? > ? according to whether ai < bi or ai > bi assuming the usual order
0 < 1 of the truth values. If no such cue exists, the strategy returns ? = ?. Formally, let
diff : B ? B ? {1, . . . , n + 1} be the function where diff(a, b) is the smallest cue index
on which a and b are different, or n + 1 if they are equal, that is,
diff(a, b)

=

min{{i : ai 6= bi } ? {n + 1}}.

Then, the function S : B ? B ? {? < ?, ? = ?, ? > ?} computed by the lexicographic
strategy is
?
? ? < ? if diff(a, b) ? n and adiff(a,b) < bdiff(a,b) ,
? > ? if diff(a, b) ? n and adiff(a,b) > bdiff(a,b) ,
S(a, b) =
? ? = ? otherwise.

Lexicographic strategies may take into account that the cues come in an order that is different from 1, . . . , n. Let ? : {1, . . . , n} ? {1, . . . , n} be a permutation of the cues. It
gives rise to a mapping ? : {0, 1}n ? {0, 1}n that permutes the components of Boolean
vectors by ?(a1 , . . . , an ) = (a?(1) , . . . , a?(n) ). As ? is uniquely defined given ?, we simplify the notation and write also ? for ?. The lexicographic strategy under cue permutation
? passes through the cues in the order ?(1), . . . , ?(n), that is, it computes the function
S? : B ? B ? {? < ?, ? = ?, ? > ?} defined as
S? (a, b)

= S(?(a), ?(b)).

The problem we study is that of finding a cue permutation that minimizes the number of
incorrect comparisons in a given list of element pairs using the lexicographic strategy. An
instance of this problem consists of a set B of elements and a set of pairs L ? B ? B. Each
pair ha, bi ? L represents an inequality a ? b. Given a cue permutation ?, we say that the
lexicographic strategy under ? infers the pair ha, bi correctly if S? (a, b) ? {? < ?, ? = ?},
otherwise the inference is incorrect. The task is to find a permutation ? such that the number
of incorrect inferences in L using S? is minimal, that is, a permutation ? that minimizes
INCORRECT(?, L)

= |{ha, bi ? L : S? (a, b) = ? > ?}|.

3

Approximability of Optimal Cue Permutations

A large class of optimization problems, denoted APX, can be solved efficiently if the solution is required to be only a constant factor worse than the optimum (see, e.g., Ausiello
et al., 1999). Here, we prove that, if P 6= NP, there is no polynomial-time algorithm whose
solutions yield a number of incorrect comparisons that is by at most a constant factor larger
than the minimal number possible. It follows that the problem of approximating the optimal cue permutation is even harder than any problem in APX. The optimization problem
is formally stated as follows.
M INIMUM I NCORRECT L EXICOGRAPHIC S TRATEGY
Instance: A set B ? {0, 1}n and a set L ? B ? B.
Solution: A permutation ? of the cues of B.
Measure: The number of incorrect inferences in L for the lexicographic strategy under cue permutation ?, that is, INCORRECT(?, L).
Given a real number r > 0, an algorithm is said to approximate M INIMUM I NCORRECT
L EXICOGRAPHIC S TRATEGY to within a factor of r if for every instance (B, L) the algorithm returns a permutation ? such that
INCORRECT(?, L)

? r ? opt(L),

where opt(L) is the minimal number of incorrect comparisons achievable on L by any
permutation. The factor r is also known as the performance ratio of the algorithm. The
following optimization problem plays a crucial role in the derivation of the lower bound
for the approximability of M INIMUM I NCORRECT L EXICOGRAPHIC S TRATEGY.
M INIMUM H ITTING S ET
Instance: A collection C of subsets of a finite set U .
Solution: A hitting set for C, that is, a subset U ? ? U such that U ? contains at
least one element from each subset in C.
Measure: The cardinality of the hitting set, that is, |U ? |.
M INIMUM H ITTING S ET is equivalent to M INIMUM S ET C OVER. Bellare et al. (1993)
have shown that M INIMUM S ET C OVER cannot be approximated in polynomial time to
within any constant factor, unless P = NP. Thus, if P 6= NP, M INIMUM H ITTING S ET
cannot be approximated in polynomial time to within any constant factor as well.
Theorem 1. For every r, there is no polynomial-time algorithm that approximates M INI MUM I NCORRECT L EXICOGRAPHIC S TRATEGY to within a factor of r, unless P = NP.
Proof. We show that the existence of a polynomial-time algorithm that approximates M IN IMUM I NCORRECT L EXICOGRAPHIC S TRATEGY to within some constant factor implies
the existence of a polynomial-time algorithm that approximates M INIMUM H ITTING S ET
to within the same factor. Then the statement follows from the equivalence of M INIMUM
H ITTING S ET with M INIMUM S ET C OVER and the nonapproximability of the latter (Bellare et al., 1993). The main part of the proof consists in establishing a specific approximation preserving reduction, or AP-reduction, from M INIMUM H ITTING S ET to M INIMUM
I NCORRECT L EXICOGRAPHIC S TRATEGY. (See Ausiello et al., 1999, for a definition of
the AP-reduction.).
We first define a function f that is computable in polynomial time and maps each instance
of M INIMUM H ITTING S ET to an instance of M INIMUM I NCORRECT L EXICOGRAPHIC
S TRATEGY. Let 1 denote the n-bit vector with a 1 everywhere and 1i1 ,...,i? the vector
with 0 in positions i1 , . . . , i? and 1 elsewhere. Given the collection C of subsets of the set
U = {u1 , . . . , un }, the function f maps C to (B, L), where B ? {0, 1}n+1 is defined as
follows:

1. Let (1, 0) ? B.
2. For i = 1, . . . , n, let (1i , 1) ? B.
3. For every {ui1 , . . . , ui? } ? C, let (1i1 ,...,i? , 1) ? B.
Further, the set L is constructed as
L = {h(1, 0), (1i , 1)i : i = 1, . . . , n}?{h(1i1 ,...,i? , 1), (1, 0)i : {ui1 , . . . , ui? } ? C}. (1)
In the following, a pair from the first and second set on the right-hand side of equation (1)
is referred to as an element pair and a subset pair, respectively. Obviously, the function f
is computable in polynomial time. It has the following property.
Claim 1. Let f (C) = (B, L). If C has a hitting set of cardinality k or less then f (C) has
a cue permutation ? where INCORRECT(?, L) ? k.
To prove this, assume without loss of generality that C has a hitting set U ? of cardinality
exactly k, say U ? = {uj1 , . . . , ujk }, and let U \ U ? = {ujk+1 , . . . , ujn }. Then the cue
permutation
j1 , . . . , jk , n + 1, jk+1 , . . . , jn .
results in no more than k incorrect inferences in L. Indeed, consider an arbitrary subset
pair h(1i1 ,...,i? , 1), (1, 0)i. To not be an error, one of i1 , . . . , i? must occur in the hitting
set j1 , . . . , jk . Hence, the first cue that distinguishes this pair has value 0 in (1i1 ,...,i? , 1)
and value 1 in (1, 0), resulting in a correct comparison. Further, let h(1, 0), (1i , 1)i be an
element pair with ui 6? U ? . This pair is distinguished correctly by cue n + 1. Finally,
each element pair h(1, 0), (1i , 1)i with ui ? U ? is distinguished by cue i with a result
that disagrees with the ordering given by L. Thus, only element pairs with ui ? U ? yield
incorrect comparisons and no subset pair. Hence, the number of incorrect inferences is not
larger than |U ? |.
Next, we define a polynomial-time computable function g that maps each collection C of
subsets of a finite set U and each cue permutation ? for f (C) to a subset of U . Given that
f (C) = (B, L), the set g(C, ?) ? U is defined as follows:
1. For every element pair h(1, 0), (1i , 1)i ? L that is compared incorrectly by ?, let
ui ? g(C, ?).
2. For every subset pair h(1i1 ,...,i? , 1), (1, 0)i ? L that is compared incorrectly by ?,
let one of the elements ui1 , . . . , ui? ? g(C, ?).
Clearly, the function g is computable in polynomial time. It satisfies the following condition.
Claim 2. Let f (C) = (B, L). If INCORRECT(?, L) ? k then g(C, ?) is a hitting set of
cardinality k or less for C.
Obviously, if INCORRECT(?, L) ? k then g(C, ?) has cardinality at most k. To show that
it is a hitting set, assume the subset {ui1 , . . . , ui? } ? C is not hit by g(C, ?). Then neither
of ui1 , . . . , ui? is in g(C, ?). Hence, we have correct comparisons for the element pairs
corresponding to ui1 , . . . , ui? and for the subset pair corresponding to {ui1 , . . . , ui? }. As
the subset pair is distinguished correctly, one of the cues i1 , . . . , i? must be ranked before
cue n + 1. But then at least one of the element pairs for ui1 , . . . , ui? yields an incorrect
comparison. This contradicts the assertion that the comparisons for these element pairs are
all correct. Thus, g(C, ?) is a hitting set and the claim is established.
Assume now that there exists a polynomial-time algorithm A that approximates M INIMUM
I NCORRECT L EXICOGRAPHIC S TRATEGY to within a factor of r. Consider the algorithm
that, for a given instance C of M INIMUM H ITTING S ET as input, calls algorithm A with
input (B, L) = f (C), and returns g(C, ?) where ? is the output provided by A. Clearly,
this new algorithm runs in polynomial time. We show that it approximates M INIMUM

Algorithm 1 G REEDY C UE P ERMUTATION
Input: a set B ? {0, 1}n and a set L ? B ? B
Output: a cue permutation ? for n cues
I := {1, . . . , n};
for i = 1, . . . , n do
let j ? I be a cue where INCORRECT(j, L) = minj ? ?I INCORRECT(j ? , L);
?(i) := j;
I := I \ {j};
L := L \ {ha, bi : aj 6= bj }
end for.
H ITTING S ET to within a factor of r. By the assumed approximation property of algorithm
A, we have
INCORRECT(?, L)

? r ? opt(L).

Together with Claim 2, this implies that g(?, C) is a hitting set for C satisfying
|g(C, ?)|

? r ? opt(L).

From Claim 1 we obtain opt(L) ? opt(C) and, thus,
|g(C, ?)|

? r ? opt(C).

Thus, the proposed algorithm for M INIMUM H ITTING S ET violates the approximation
lower bound that holds for this problem under the assumption P 6= NP. This proves the
statement of the theorem.

4

Greedy Approximation of Optimal Cue Permutations

The so-called greedy approach to the solution of an approximation problem is helpful when
it is not known which algorithm performs best. It is a simple heuristic that in practice often
provides satisfactory solutions in many situations. The algorithm G REEDY C UE P ERMU TATION that we introduce here is based on the greedy method. The idea is to select the
first cue according to which single cue makes a minimum number of incorrect inferences
(choosing one arbitrarily if there are two or more). After that the algorithm removes those
pairs that are distinguished by the selected cue, which is reasonable as the distinctions
drawn by this cue cannot be undone by later cues. This procedure is then repeated on the
set of pairs left. The description of G REEDY C UE P ERMUTATION is given as Algorithm 1.
It employs an extension of the function INCORRECT applicable to single cues, such that
for a cue i we have
INCORRECT(i, L)

= |{ha, bi ? L : ai > bi }|.

It is evident that Algorithm 1 runs in polynomial time, but how good is it? The least one
should demand from a good heuristic is that, whenever a minimum of zero is attainable,
it finds such a solution. This is indeed the case with G REEDY C UE P ERMUTATION as
we show in the following result. Moreover, it asserts a general performance ratio for the
approximation of the optimum.
Theorem 2. The algorithm G REEDY C UE P ERMUTATION approximates M INIMUM I N CORRECT L EXICOGRAPHIC S TRATEGY to within a factor of n, where n is the number of
cues. In particular, it always finds a cue permutation with no incorrect inferences if one
exists.
Proof. We show by induction on n that the permutation returned by the algorithm makes
a number of incorrect inferences no larger than n ? opt(L). If n = 1, the optimal cue

h 001 , 010 i
h 010 , 100 i
h 010 , 101 i
h 100 , 111 i
Figure 1: A set of lexicographically ordered pairs with nondecreasing cue validities (1, 1/2,
and 2/3). The cue ordering of TTB (1, 3, 2) causes an incorrect inference on the first pair.
By Theorem 2, G REEDY C UE P ERMUTATION finds the lexicographic ordering.
permutation is definitely found. Let n > 1. Clearly, as the incorrect inferences of a cue
cannot be reversed by other cues, there is a cue j with
INCORRECT(j, L) ? opt(L).
The algorithm selects such a cue in the first round of the loop. During the rest of the
rounds, a permutation of n ? 1 cues is constructed for the set of remaining pairs. Let
j be the cue that is chosen in the first round, I ? = {1, . . . , j ? 1, j + 1, . . . , n}, and
L? = L \ {ha, bi : aj 6= bj }. Further, let optI ? (L? ) denote the minimum number of
incorrect inferences taken over the permutations of I ? on the set L? . Then, we observe that
opt(L) ? opt(L? ) = optI ? (L? ).
The inequality is valid because of L ? L? . (Note that opt(L? ) refers to the minimum taken
over the permutations of all cues.) The equality holds as cue j does not distinguish any pair
in L? . By the induction hypothesis, rounds 2 to n of the loop determine a cue permutation ? ?
with INCORRECT(? ? , L? ) ? (n ? 1) ? optI ? (L? ). Thus, the number of incorrect inferences
made by the permutation ? finally returned by the algorithm satisfies
INCORRECT(?, L) ? INCORRECT(j, L) + (n ? 1) ? optI ? (L? ),
which is, by the inequalities derived above, not larger than opt(L) + (n ? 1) ? opt(L) as
stated.
Corollary 3. On inputs that have a cue ordering without incorrect comparisons under the
lexicographic strategy, G REEDY C UE P ERMUTATION can be better than TTB.
Proof. Figure 1 shows a set of four lexicographically ordered pairs. According to Theorem 2, G REEDY C UE P ERMUTATION comes up with the given permutation of the cues.
The validities are 1, 1/2, and 2/3. Thus, TTB ranks the cues as 1, 3, 2 whereupon the first
pair is inferred incorrectly.
Finally, we consider lower bounds on the performance ratio of G REEDY C UE P ERMUTA TION . The proof of this claim is omitted here.
Theorem 4. The performance ratio of G REEDY C UE P ERMUTATION is at least
max{n/2, |L|/2}.

5

Conclusions

The result that the optimization problem M INIMUM I NCORRECT L EXICOGRAPHIC
S TRATEGY cannot be approximated in polynomial time to within any constant factor answers a long-standing question of psychological research into models of bounded rationality: How accurate are fast and frugal heuristics? It follows that no fast, that is, polynomialtime, algorithm can approximate the optimum well, under the widely accepted assumption
that P 6= NP. A further question is concerned with a specific fast and frugal heuristic: How
accurate is TTB? The new algorithm G REEDY C UE P ERMUTATION has been shown to perform provably better than TTB. In detail, it always finds accurate solutions when they exist,
in contrast to TTB. With this contribution we pose a challenge to cognitive psychology: to
study the relevance of the greedy method as a model for bounded rationality.

Acknowledgment. The first author has been supported in part by the Deutsche
Forschungsgemeinschaft (DFG).
References
Ausiello, G., Crescenzi, P., Gambosi, G., Kann, V., Marchetti-Spaccamela, A., and Protasi, M.
(1999). Complexity and Approximation: Combinatorial Problems and Their Approximability
Properties. Springer-Verlag, Berlin.
Bellare, M., Goldwasser, S., Lund, C., and Russell, A. (1993). Efficient probabilistically checkable
proofs and applications to approximation. In Proceedings of the 25th Annual ACM Symposium on
Theory of Computing, pages 294?304. ACM Press, New York, NY.
Br?oder, A. (2000). Assessing the empirical validity of the ?take-the-best? heuristic as a model of
human probabilistic inference. Journal of Experimental Psychology: Learning, Memory, and
Cognition, 26:1332?1346.
Br?oder, A. (2002). Take the best, Dawes? rule, and compensatory decision strategies: A regressionbased classification method. Quality & Quantity, 36:219?238.
Br?oder, A. and Schiffer, S. (2003). Take the best versus simultaneous feature matching: Probabilistic
inferences from memory and effects of representation format. Journal of Experimental Psychology: General, 132:277?293.
Gigerenzer, G. and Goldstein, D. G. (1996). Reasoning the fast and frugal way: Models of bounded
rationality. Psychological Review, 103:650?669.
Gigerenzer, G., Todd, P. M., and the ABC Research Group (1999). Simple Heuristics That Make Us
Smart. Oxford University Press, New York, NY.
Hogarth, R. M. and Karelaia, N. (2003). ?Take-the-best? and other simple strategies: Why and
when they work ?well? in binary choice. DEE Working Paper 709, Universitat Pompeu Fabra,
Barcelona.
Lee, M. D. and Cummins, T. D. R. (2004). Evidence accumulation in decision making: Unifying the
?take the best? and the ?rational? models. Psychonomic Bulletin & Review, 11:343?352.
Martignon, L. and Hoffrage, U. (2002). Fast, frugal, and fit: Simple heuristics for paired comparison.
Theory and Decision, 52:29?71.
Nellen, S. (2003). The use of the ?take the best? heuristic under different conditions, modeled with
ACT-R. In Detje, F., D?orner, D., and Schaub, H., editors, Proceedings of the Fifth International
Conference on Cognitive Modeling, pages 171?176, Universit?atsverlag Bamberg, Bamberg.
Newell, B. R. and Shanks, D. R. (2003). Take the best or look at the rest? Factors influencing
?One-Reason? decision making. Journal of Experimental Psychology: Learning, Memory, and
Cognition, 29:53?65.
Newell, B. R., Weston, N. J., and Shanks, D. R. (2003). Empirical tests of a fast-and-frugal heuristic:
Not everyone ?takes-the-best?. Organizational Behavior and Human Decision Processes, 91:82?
96.
Schmitt, M. and Martignon, L. (2006). On the complexity of learning lexicographic strategies. Journal of Machine Learning Research, 7(Jan):55?83.
Simon, H. A. (1982). Models of Bounded Rationality, Volume 2. MIT Press, Cambridge, MA.
Slegers, D. W., Brake, G. L., and Doherty, M. E. (2000). Probabilistic mental models with continuous
predictors. Organizational Behavior and Human Decision Processes, 81:98?114.
Todd, P. M. and Dieckmann, A. (2005). Heuristics for ordering cue search in decision making. In
Saul, L. K., Weiss, Y., and Bottou, L., editors, Advances in Neural Information Processing Systems
17, pages 1393?1400. MIT Press, Cambridge, MA.
Todd, P. M. and Gigerenzer, G. (2000). Pr?ecis of ?Simple Heuristics That Make Us Smart?. Behavioral and Brain Sciences, 23:727?741.

"
5491,2015,Robust Spectral Inference for Joint Stochastic Matrix Factorization,"Spectral inference provides fast algorithms and provable optimality for latent topic analysis. But for real data these algorithms require additional ad-hoc heuristics, and even then often produce unusable results. We explain this poor performance by casting the problem of topic inference in the framework of Joint Stochastic Matrix Factorization (JSMF) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist. We then propose a novel rectification method that learns high quality topics and their interactions even on small, noisy data. This method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality.","Robust Spectral Inference for Joint Stochastic Matrix
Factorization
David Mimno
Dept. of Information Science
Cornell University
Ithaca, NY 14850
mimno@cornell.edu

Moontae Lee, David Bindel
Dept. of Computer Science
Cornell University
Ithaca, NY 14850
{moontae,bindel}@cs.cornell.edu

Abstract
Spectral inference provides fast algorithms and provable optimality for latent topic
analysis. But for real data these algorithms require additional ad-hoc heuristics,
and even then often produce unusable results. We explain this poor performance
by casting the problem of topic inference in the framework of Joint Stochastic
Matrix Factorization (JSMF) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist. We then propose a novel
rectification method that learns high quality topics and their interactions even on
small, noisy data. This method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality.

1

Introduction

Summarizing large data sets using pairwise co-occurrence frequencies is a powerful tool for data
mining. Objects can often be better described by their relationships than their inherent characteristics. Communities can be discovered from friendships [1], song genres can be identified
from co-occurrence in playlists [2], and neural word embeddings are factorizations of pairwise cooccurrence information [3, 4]. Recent Anchor Word algorithms [5, 6] perform spectral inference on
co-occurrence statistics for inferring topic models [7, 8]. Co-occurrence statistics can be calculated
using a single parallel pass through a training corpus. While these algorithms are fast, deterministic,
and provably guaranteed, they are sensitive to observation noise and small samples, often producing
effectively useless results on real documents that present no problems for probabilistic algorithms.
We cast this general problem
Area = 0.000313
Area = 0.002602
Area = 0.000660
of learning overlapping latent
clusters as Joint-Stochastic Matrix Factorization (JSMF), a
subset of non-negative matrix
factorization that contains topic
modeling as a special case.
We explore the conditions necessary for inference from co- Figure 1: 2D visualizations show the low-quality convex hull
occurrence statistics and show found by Anchor Words [6] (left) and a better convex hull (middle)
that the Anchor Words algo- found by discovering anchor words on a rectified space (right).
rithms necessarily violate such
conditions. Then we propose a rectified algorithm that matches the performance of probabilistic
inference?even on small and noisy datasets?without losing efficiency and provable guarantees.
Validating on both real and synthetic data, we demonstrate that our rectification not only produces
better clusters, but also, unlike previous work, learns meaningful cluster interactions.
0.05

0.05

0.04

0.04

0.03

0.03

0.02

0.02

0.01

0.01

0.02

0.015

0.01

0

0

-0.01

-0.01

-0.02

-0.02

0.005

0

-0.005

-0.03

-0.04
-0.04

-0.01

-0.03

-0.02

0

0.02

0.04

1

0.06

0.08

-0.04
-0.04

-0.02

0

0.02

0.04

0.06

0.08

-0.015
-0.02

-0.01

0

0.01

0.02

0.03

Let the matrix C represent the co-occurrence of pairs drawn from N objects: Cij is the joint probability p(X1 = i, X2 = j) for a pair of objects i and j. Our goal is to discover K latent clusters by approximately decomposing C ? BAB T . B is the object-cluster matrix, in which each
column corresponds to a cluster and Bik = p(X = i|Z = k) is the probability of drawing an
object i conditioned on the object belonging to the cluster k; and A is the cluster-cluster matrix,
in which Akl = p(Z1 = k, Z2 = l) represents the joint probability of pairs of clusters. We
call the matrices C and A joint-stochastic (i.e., C ? J S N , A ? J S K ) due to their correspondence to joint distributions; B is column-stochastic. Example applications are shown in Table 1.
Table 1: JSMF applications, with anchor-word equivalents.
Anchor Word algorithms [5,
6] solve JSMF problems usDomain
Object
Cluster
Basis
ing a separability assumption:
Document
Word
Topic
Anchor Word
each topic contains at least
Image
Pixel
Segment
Pure Pixel
one ?anchor? word that has
Network
User
Community
Representative
non-negligible probability exLegislature Member Party/Group
Partisan
Playlist
Song
Genre
Signature Song
clusively in that topic. The algorithm uses the co-occurrence
patterns of the anchor words as a summary basis for the co-occurrence patterns of all other words.
The initial algorithm [5] is theoretically sound but unable to produce column-stochastic word-topic
matrix B due to unstable matrix inversions. A subsequent algorithm [6] fixes negative entries in B,
but still produces large negative entries in the estimated topic-topic matrix A. As shown in Figure 3,
the proposed algorithm infers valid topic-topic interactions.

2

Requirements for Factorization

In this section we review the probabilistic and statistical structures of JSMF and then define geometric structures of co-occurrence matrices required for successful factorization. C ? RN ?N is a
joint-stochastic matrix constructed from M training examples, each of which contain some subset
of N objects. We wish to find K  N latent clusters by factorizing C into a column-stochastic
matrix B ? RN ?K and a joint-stochastic matrix A ? RK?K , satisfying C ? BAB T .
Probabilistic structure. Figure 2 shows the event
space of our model. The distribution A over pairs of clusters is generated first from a stochastic process with a hyperparameter ?. If the m-th training example contains
a total of nm objects, our model views the example as
consisting of all possible nm (nm ? 1) pairs of objects.1
For each of these pairs, cluster assignments are sampled
from the selected distribution ((z1 , z2 ) ? A). Then an
actual object pair is drawn with respect to the corresponding cluster assignments (x1 ? Bz1 , x2 ? Bz2 ). Note that
this process does not explain how each training example
is generated from a model, but shows how our model understands the objects in the training examples.

?
Z1

X1

Z2

X2

A

Bk

nm (nm ? 1)
1?m?M

1?k?K

Figure 2: The JSMF
event space differs
1

from LDA?s. JSMF deals only with pairwise
co-occurrence events and does not generate
observations/documents.

Following [5, 6], our model views B as a set of parameters rather than random variables.2 The
primary learning task is to estimate B; we then estimate A to recover the hyperparameter ?. Due to
the conditional independence X1 ? X2 | (Z1 or Z2 ), the factorization C ? BAB T is equivalent to
XX
p(X1 , X2 |A; B) =
p(X1 |Z1 ; B)p(Z1 , Z2 |A)p(X2 |Z2 ; B).
z1

z2

Under the separability assumption, each cluster k has a basis object sk such that p(X = sk |Z =
k) > 0 and p(X = sk |Z 6= k) = 0. In matrix terms, we assume the submatrix of B comprised of
1

Due to the bag-of-words assumption, every object can pair with any other object in that example, except
itself. One implication of our work is better understanding the self-co-occurrences, the diagonal entries in the
co-occurrence matrix.
2
In LDA, each column of B is generated from a known distribution Bk ? Dir(?).

2

the rows with indices S = {s1 , . . . , sK } is diagonal. As these rows form a non-negative basis for
the row space of B, the assumption implies rank+ (B) = K = rank(B).3 Providing identifiability
to the factorization, this assumption becomes crucial for inference of both B and A. Note that JSMF
factorization is unique up to column permutation, meaning that no specific ordering exists among
the discovered clusters, equivalent to probabilistic topic models (see the Appendix).
Statistical structure. Let f (?) be a (known) distribution of distributions from which a cluster
distribution is sampled for each training example. Saying Wm ? f (?), we have M i.i.d samples
{W1 , . . . , WM } which are not directly observable. Defining the posterior cluster-cluster matrix
PM
1
T
?
T
4
A?M = M
m=1 Wm Wm and the expectation A = E[Wm Wm ], Lemma 2.2 in [5] showed that
A?M ?? A?

as M ?? ?.

(1)
?
Cm

Denote the posterior co-occurrence for the m-th training example by
and all examples by C ? .
P
M
1
?
T T
?
Then Cm
= BWm Wm
B , and C ? = M
m=1 Cm . Thus
!
M
1 X
?
T
C =B
Wm Wm B T = BA?M B T .
(2)
M m=1
Denote the noisy observation for the m-th training example by Cm , and all examples by C. Let
W = [W1 |...|WM ] be a matrix of topics. We will construct Cm so that E[C|W ] is an unbiased
estimator of C ? . Thus as M ? ?
C ?? E[C] = C ? = BA?M B T ?? BA? B T .

(3)

Geometric structure. Though the separability assumption allows us to identify B even from the
noisy observation C, we need to throughly investigate the structure of cluster interactions. This is
because it will eventually be related to how much useful information the co-occurrence between
corresponding anchor bases contains, enabling us to best use our training data. Say DN N n is the
set of n ? n doubly non-negative matrices: entrywise non-negative and positive semidefinite (PSD).
Claim A?M , A? ? DN N K and C ? ? DN N N
Proof Take any vector y ? RK . As A?M is defined as a sum of outer-products,
y

T

A?M y

M
X
1 X T T
1 X T
T
T
y Wm Wm
y=
(Wm y) (Wm
y) =
(non-negative) ? 0.
=
M m=1
M

(4)

Thus A?M ? PSDK . In addition, (A?M )kl = p(Z1 = k, Z2 = l) ? 0 for all k, l. Proving
A? ? DN N K is analogous by the linearity of expectation. Relying on double non-negativity of
A?M , Equation (3) implies not only the low-rank structure of C ? , but also double non-negativity of
C ? by a similar proof (see the Appendix).
The Anchor Word algorithms in [5, 6] consider neither double non-negativity of cluster interactions
nor its implication on co-occurrence statistics. Indeed, the empirical co-occurrence matrices collected from limited data are generally indefinite and full-rank, whereas the posterior co-occurrences
must be positive semidefinite and low-rank. Our new approach will efficiently enforce double nonnegativity and low-rankness of the co-occurrence matrix C based on the geometric property of its
posterior behavior. We will later clarify how this process substantially improves the quality of the
clusters and their interactions by eliminating noises and restoring missing information.

3

Rectified Anchor Words Algorithm

In this section, we describe how to estimate the co-occurrence matrix C from the training data, and
how to rectify C so that it is low-rank and doubly non-negative. We then decompose the rectified
C 0 in a way that preserves the doubly non-negative structure in the cluster interaction matrix.
3
4

rank+ (B) means the non-negative rank of the matrix B, whereas rank(B) means the usual rank.
PM
1
This convergence is not trivial while M
m=1 Wm ? E[Wm ] as M ? ? by the Central Limit Theorem.

3

Generating co-occurrence C. Let Hm be the vector of object counts for the m-th training example, and let pm = BWm where Wm is the document?s latent topic distribution. Then Hm is assumed
PN
(i)
to be a sample from a multinomial distribution Hm ? Multi(nm , pm ) where nm = i=1 Hm , and
recall E[Hm ] = nm pm = nm BWm and Cov(Hm ) = nm diag(pm ) ? pm pTm . As in [6], we
generate the co-occurrence for the m-th example by
Cm =

T
Hm Hm
? diag(Hm )
.
nm (nm ? 1)

(5)

The diagonal penalty in Eq. 5 cancels out the diagonal matrix term in the variance-covariance matrix,
T
making the estimator unbiased. Putting dm = nm (nm ? 1), that is E[Cm |Wm ] = d1m E[Hm Hm
]?
1
1
T
T
T
?
diag(E[H
])
=
(E[H
]E[H
]
+
Cov(H
)
?
diag(E[H
]))
=
B(W
W
)B
?
C
m
m
m
m
m
m
m
m.
dm
dm
?
Thus E[C|W ] = C by the linearity of expectation.
Rectifying co-occurrence C. While C is an unbiased estimator for C ? in our model, in reality the
two matrices often differ due to a mismatch between our model assumptions and the data5 or due
to error in estimation from limited data. The computed C is generally full-rank with many negative
eigenvalues, causing a large approximation error. As the posterior co-occurrence C ? must be lowrank, doubly non-negative, and joint-stochastic, we propose two rectification methods: Diagonal
Completion (DC) and Alternating Projection (AP). DC modifies only diagonal entries so that C
becomes low-rank, non-negative, and joint-stochastic; while AP enforces modifies every entry and
enforces the same properties as well as positive semi-definiteness. As our empirical results strongly
favor alternating projection, we defer the details of diagonal completion to the Appendix.
Based on the desired property of the posterior co-occurrence C ? , we seek to project our estimator
C onto the set of joint-stochastic, doubly non-negative, low rank matrices. Alternating projection
methods like Dykstra?s algorithm [9] allow us to project onto an intersection of finitely many convex
sets using projections onto each individual set in turn. In our setting, we consider the intersection
of three sets of symmetric N ? N matrices: the elementwise non-negative matrices N N N , the
normalized matrices N ORN whose entry sum is equal to 1, and the positive semi-definite matrices
with rank K, PSDN K . We project onto these three sets as follows:
P
1 ? i,j Cij T
+ T
?PSDN K (C) = U ?K U , ?N ORN (C) = C +
11 , ?N N N (C) = max{C, 0}.
N2
where C = U ?U T is an eigendecomposition and ?+
K is the matrix ? modified so that all negative
eigenvalues and any but the K largest positive eigenvalues are set to zero. Truncated eigendecompositions can be computed efficiently, and the other projections are likewise efficient. While N N N
and N ORN are convex, PSDN K is not. However, [10] show that alternating projection with a
non-convex set still works under certain conditions, guaranteeing a local convergence. Thus iterating three projections in turn until the convergence rectifies C to be in the desired space. We will
show how to satisfy such conditions and the convergence behavior in Section 5.
Selecting basis S. The first step of the factorization is to select the subset S of objects that satisfy
the separability assumption. We want the K best rows of the row-normalized co-occurrence matrix
C so that all other rows lie nearly in the convex hull of the selected rows. [6] use the GramSchmidt process to select anchors, which computes pivoted QR decomposition, but did not utilize the
sparsity of C. To scale beyond small vocabularies, they use random projections that approximately
preserve `2 distances between rows of C. For all experiments we use a new pivoted QR algorithm
(see the Appendix) that exploits sparsity instead of using random projections, and thus preserves
deterministic inference.6
Recovering object-cluster B. After finding the set of basis objects S, we can infer each entry of
B by Bayes? rule as in [6]. Let {p(Z1 = k|X1 = i)}K
k=1 be the coefficients that reconstruct the
i-th row of C in terms of the basis rows corresponding to S. Since Bik = p(X1 = i|Z1 = k),
5

There is no reason to expect real data to be generated from topics, much less exactly K latent topics.
To effectively use random projections, it is necessary to either find proper dimensions based on multiple
trials or perform low-dimensional random projection multiple times [25] and merge the resulting anchors.
6

4

P
we can use the corpus frequencies p(X1 = i) =
j Cij to estimate Bik ? p(Z1 = k|X1 =
i)p(X1 = i). Thus the main task for this step is to solve simplex-constrained QPs to infer a
set of such coefficients for each object. We use an exponentiated gradient algorithm to solve the
problem similar to [6]. Note that this step can be efficiently done in parallel for each object.

Recovering cluster-cluster A.
[6] recovered A by minimizing
kC ? BAB T kF ; but the inferred
A generally has many negative
entries, failing to model the
probabilistic interaction between
topics. While we can further
project A onto the joint-stochastic
matrices, this produces a large Figure 3: The algorithm of [6] (first panel) produces negative cluster
co-occurrence probabilities. A probabilistic reconstruction alone (this
approximation error.
22.842

-7.687

0.629

-2.723

-12.888

45.021

0.000

0.000

0.000

0.000

0.114

0.000

0.002

0.024

0.004

-7.687

43.605

-4.986

-7.788

-22.930

0.000

43.086

0.000

0.000

0.000

0.000

0.115

0.010

0.007

0.017

0.629

-4.986

12.782

-5.269

-2.998

0.000

0.000

52.828

0.000

0.000

0.002

0.010

0.162

0.016

0.012

-2.723

-7.788

-5.269

19.237

-3.267

0.000

0.000

0.000

17.527

0.000

0.024

0.007

0.016

0.072

0.014

-12.888

-22.930

-2.998

-3.267

42.367

0.000

0.000

0.000

0.000

76.153

0.004

0.017

0.012

0.014

0.328

23.46
1.00
0.84
0.67
0.50
0.34
0.17
0.00

?11.23
?22.93

paper & [5], second panel) removes negative entries but has no off-

We consider an alternate recovery diagonals and does not sum to one. Trying after rectification (this
method that again leverages the paper, third panel) produces a valid joint stochastic matrix.
separability assumption. Let CSS be the submatrix whose rows and columns correspond to the
selected objects S, and let D be the diagonal submatrix BS? of rows of B corresponding to S. Then
CSS = DADT = DAD =? A = D?1 CSS D?1 .
(6)
This approach efficiently recovers a cluster-cluster matrix A mostly based on the co-occrrurence
information between corresponding anchor basis, and produces no negative entries due to the stability of diagonal matrix inversion. Note that the principle submatrices of a PSD matrix are also
PSD; hence, if C ? PSDN then CSS , A ? PSDK . Thus, not only is the recovered A an unbiased
estimator for A?M , but also it is now doubly non-negative as A?M ? DN N K after the rectification.7

4

Experimental Results

Our Rectified Anchor Words algorithm with alternating projection fixes many problems in the baseline Anchor Words algorithm [6] while matching the performance of Gibbs sampling [11] and maintaining spectral inference?s determinism and independence from corpus size. We evaluate direct
measurement of matrix quality as well as indicators of topic utility. We use two text datasets:
NIPS full papers and New York Times news articles.8 We eliminate a minimal list of 347 English stop words and prune rare words based on tf-idf scores and remove documents with fewer
than five tokens after vocabulary curation. We also prepare two non-textual item-selection datasets:
users? movie reviews from the Movielens 10M Dataset,9 and music playlists from the complete
Yes.com dataset.10 We perform similar vocabulary curation and document tailoring, with the exception of frequent stop-object elimination. Playlists often contain the same songs multiple times,
but users are unlikely to review the same movies more than once, so we augment the movie dataset
so that each review contains 2 ? (stars) number of movies based on the half-scaled rating information that varies from 0.5 stars to 5 stars. Statistics of our datasets are shown in Table 2.
We run DC 30 times for each experiment, randomly
permuting the order of objects and using the median
Dataset
M
N
Avg. Len
results to minimize the effect of different orderings.
NIPS
1,348
5k
380.5
We also run 150 iterations of AP alternating PSDN K ,
NYTimes 269,325 15k
204.9
N ORN , and N N N in turn. For probabilistic Gibbs
Movies
63,041
10k
142.8
sampling, we use the Mallet with the standard option
Songs
14,653
10k
119.2
doing 1,000 iterations. All metrics are evaluated against
the original C, not against the rectified C 0 , whereas we use B and A inferred from the rectified C 0 .
Table 2: Statistics of four datasets.

7

We later realized that essentially same approach was previously tried in [5], but it was not able to generate
a valid topic-topic matrix as shown in the middle panel of Figure 3.
8
https://archive.ics.uci.edu/ml/datasets/Bag+of+Words
9
http://grouplens.org/datasets/movielens
10
http://www.cs.cornell.edu/?shuochen/lme

5

Qualitative results. Although [6] report comparable results to probabilistic algorithms for LDA,
the algorithm fails under many circumstances. The algorithm prefers rare and unusual anchor words
that form a poor basis, so topic clusters consist of the same high-frequency terms repeatedly, as
shown in the upper third of Table 3. In contrast, our algorithm with AP rectification successfully learns themes similar to the probabilistic algorithm. One can also verify that cluster interactions given in the third panel of Figure 3 explain how the five topics correlate with each other.
Similar to [12], we visualize the Table 3: Each line is a topic from NIPS (K = 5). Previous work
five anchor words in the co- simply repeats the most frequent words in the corpus five times.
occurrence space after 2D PCA
Arora et al. 2013 (Baseline)
of C. Each panel in Figure 1
neuron layer hidden recognition signal cell noise
neuron layer hidden cell signal representation noise
shows a 2D embedding of the
neuron layer cell hidden signal noise dynamic
NIPS vocabulary as blue dots and
neuron layer cell hidden control signal noise
five selected anchor words in red.
neuron layer hidden cell signal recognition noise
The first plot shows standard anThis paper (AP)
chor words and the original coneuron circuit cell synaptic signal layer activity
occurrence space. The second plot
control action dynamic optimal policy controller reinforcement
shows anchor words selected from
recognition layer hidden word speech image net
the rectified space overlaid on the
cell field visual direction image motion object orientation
original co-occurrence space. The
gaussian noise hidden approximation matrix bound examples
third plot shows the same anchor
Probabilistic LDA (Gibbs)
words as the second plot overlaid
neuron cell visual signal response field activity
on the AP-rectified space. The reccontrol action policy optimal reinforcement dynamic robot
recognition image object feature word speech features
tified anchor words provide better
hidden net layer dynamic neuron recurrent noise
coverage on both spaces, explaingaussian approximation matrix bound component variables
ing why we are able to achieve reasonable topics even with K = 5.
Rectification also produces better clusters in the non-textual movie dataset. Each cluster is notably
more genre-coherent and year-coherent than the clusters from the original algorithm. When K = 15,
for example, we verify a cluster of Walt Disney 2D Animations mostly from the 1990s and a cluster
of Fantasy movies represented by Lord of the Rings films, similar to clusters found by probabilistic
Gibbs sampling. The Baseline algorithm [6] repeats Pulp Fiction and Silence of the Lambs 15 times.
Quantitative results. We measure the intrinsic quality of inference and summarization with respect to the JSMF objectives as well as the extrinsic quality of resulting topics. Lines correspond to
four methods: ? Baseline for the algorithm in the previous work [6] without any rectification, 4 DC
for Diagonal Completion,  AP for Alternating Projection, and  Gibbs for Gibbs sampling.
Anchor objects should form a good basis for theremaining objects. We measure Recovery error
PN
PK
1
i kC i ?
k p(Z1 = k|X1 = i)C Sk k2 with respect to the original C matrix, not the
N
rectified matrix. AP reduces error in almost all cases and is more effective than DC. Although
we expect error to decrease as we increase the number of clusters K, reducing recovery error for
a fixed K by choosing better anchors is extremely difficult: no other subset selection algorithm
[13] decreased error by more than 0.001. A good
 matrix factorization should have small elementwise Approximation error kC ? BAB T kF . DC and AP preserve more of the information in
the original matrix C than the Baseline method, especially when K is small.11 We expect nontrivial interactions between clusters, even when wedo not explicitly model them as in [14]. Greater
PK
1
12
diagonal Dominancy K
k p(Z2 = k|Z1 = k) indicates lower correlation between clusters.
AP and Gibbs results are similar. We do not report held-out probability because we find that relative
results are determined by user-defined smoothing parameters [12, 24].

PK
1
Specificity K
k KL (p(X|Z = k)kp(X)) measures how much each cluster is distinct from
the corpus distribution. When anchors produce a poor basis, the conditional distribution of clus11
In the NYTimes corpus, 10?2 is a large error: each element is around 10?9 due to the number of normalized entries.
12
Dominancy in Songs corpus lacks any Baseline results at K > 10 because dominancy is undefined if an
algorithm picks a song that occurs at most once in each playlist as a basis object. In this case, the original
construction of CSS , and hence of A, has a zero diagonal element, making dominancy NaN.

6

Nips
Recovery

Approximation

?

Dominancy
1.0

?

?

?

?

Specificity
?

? ?

?

Dissimilarity

?

3

0.8
0.05

0.10

?

Coherence
?160

15

0.15
0.06

?

?

?

10 15

25

Category

AP

?

5

10 15

1

0.2

0

5
?

?

?

0.00

50 75100

0.4

?
?

?

5

?

?

?

?240

?

?
?

?

25

50 75100

5

10 15

25

?

50 75100

5

10 15

?

?

? ?

?

?

0

25

50 75100

?

?

?

?

?

?

?

5

Baseline
DC

?
?

0.03

?

?200

2

0.6

0.05

?

?

10

?

0.04

?

Gibbs

?280
?320

10 15

25

50 75100

5

10 15

25

50 75100

NYTimes
Recovery

0.25

Approximation
1.0

?

Specificity

?

0.20

0.20
0.15

Dominancy
?

0.8
?

?

?

?

0.05

?

5

?

10 15 25

?

? ??

15

5

?

? ?

1

10 15 25

?

?
?

0
5

10 15 25

50 75100150

?

Baseline
DC

?

?

??

50 75100150

AP
??

?
? ?

5

0.2

50 75100150

Category

?

?

?

?

?350

?

?

0.00

?

?

?

10

0.4

?
?

?300

2
?

?

? ?

3

?

0.05

Coherence

?

?

?

0.6

?

0.10

0.10

? ??

?

?

0.15

Dissimilarity

Gibbs

?400

?

?

?

? ??

?

5

10 15 25

50 75100150

5

10 15 25

50 75100150

5

10 15 25

50 75100150

Movies
Recovery

Approximation

Dominancy
1.00

?

?

?

?

?

? ?

Specificity
?

?

?

Dissimilarity

Coherence
?120

4

0.75

10
0.08

?

?

? ?
?

?

0.06

5

?

?
?
?

0.04

?

10 15

25

?

50 75100

5

1

?

?

?

0.25
0

10 15

25

50 75100

5

10 15

25

50 75100

5

? ?

?

?

?

10 15

25

?

?

0

50 75100

Gibbs

?210

?

0
5

Baseline
DC

?

?

?

?180

2

0.50

AP

?

10
?

Category

?

?150

3

?

5

?

15

0.10

?

5

?

?

10 15

?

? ?

25

?

?

?240
50 75100

5

10 15

25

50 75100

Songs
Recovery
0.150

Approximation

?

Dominancy

1.0

?

Specificity

4

0.8

?

?

?
?
?

?

0.075

?

?

0.005

0.6

?
?

0.050
5

10 15

25

50 75100

5

10 15

25

?

?

50 75100

10

?500

Category

AP

0.4

0
5

10 15

?

25

50 75100

?
?

?

5

?

?

10 15

? ?

25

?

?

?
?

?

DC
Gibbs

?700

?

0
50 75100

?

Baseline

?
?

5

1

0.000

?300

2

?

?
?

Coherence

15

?

3

0.010

?

0.100

Dissimilarity

20

5

0.015

0.125

?

5

?

?

10 15

?

?

?
?

25

50 75100

5

10 15

25

50 75100

Figure 4: Experimental results on real dataset. The x-axis indicates logK where K varies by 5 up to 25 topics
and by 25 up to 100 or 150 topics. Whereas the Baseline algorithm largely fails with small K and does not infer
quality B and A even with large K, Alternating Projection (AP) not only finds better basis vectors (Recovery),
but also shows stable and comparable behaviors to probabilistic inference (Gibbs) in every metric.

ters given objects becomes uniform, making p(X|Z) similar to p(X). Inter-topic Dissimilarity
counts the average number of objects in each cluster that do not occur in any other cluster?s top
20 objects. Our experiments validate that AP and Gibbs yield comparably specific and distinct
topics, while Baseline and DC simply repeat the corpus distribution as in Table 3. Coherence
PK P?T opk
D2 (x1 ,x2 )+ 
1
penalizes topics that assign high probability (rank > 20) to
k
x1 6=x2 log
K
D1 (x2 )
words that do not occur together frequently. AP produces results close to Gibbs sampling, and
far from the Baseline and DC. While this metric correlates with human evaluation of clusters [15]
?worse? coherence can actually be better because the metric does not penalize repetition [12].
In semi-synthetic experiments [6] AP matches Gibbs sampling and outperforms the Baseline, but
the discrepancies in topic quality metrics are smaller than in the real experiments (see Appendix).
We speculate that semi-synthetic data is more ?well-behaved? than real data, explaining why issues
were not recognized previously.

5

Analysis of Algorithm

Why does AP work? Before rectification, diagonals of the empirical C matrix may be far from
correct. Bursty objects yield diagonal entries that are too large; extremely rare objects that occur
at most once per document yield zero diagonals. Rare objects are problematic in general: the corresponding rows in the C matrix are sparse and noisy, and these rows are likely to be selected by
the pivoted QR. Because rare objects are likely to be anchors, the matrix CSS is likely to be highly
diagonally dominant, and provides an uninformative picture of topic correlations. These problems
are exacerbated when K is small relative to the effective rank of C, so that an early choice of a poor
anchor precludes a better choice later on; and when the number of documents M is small, in which
case the empirical C is relatively sparse and is strongly affected by noise. To mitigate this issue,
[24] run exhaustive grid search to find document frequency cutoffs to get informative anchors. As
7

model performance is inconsistent for different cutoffs and search requires cross-validation for each
case, it is nearly impossible to find good heuristics for each dataset and number of topics.
Fortunately, a low-rank PSD matrix cannot have too many diagonally-dominant rows, since this violates the low rank property. Nor can it have diagonal entries that are small relative to off-diagonals,
since this violates positive semi-definiteness. Because the anchor word assumption implies that
non-negative rank and ordinary rank are the same, the AP algorithm ideally does not remove the
information we wish to learn; rather, 1) the low-rank projection in AP suppresses the influence of
small numbers of noisy rows associated with rare words which may not be well correlated with the
others, and 2) the PSD projection in AP recovers missing information in diagonals. (As illustrated
in the Dominancy panel of the Songs corpus in Figure 4, AP shows valid dominancies even after
K > 10 in contrast to the Baseline algorithm.)
Why does AP converge? AP enjoys local linear convergence [10] if 1) the initial C is near the
convergence point C 0 , 2) PSDN K is super-regular at C 0 , and 3) strong regularity holds at C 0 . For
the first condition, recall that we rectified C 0 by pushing C toward C ? , which is the ideal convergence
point inside the intersection. Since C ? C ? as shown in (5), C is close to C 0 as desired.The proxregular sets13 are subsets of super-regular sets, so prox-regularity of PSDN K at C 0 is sufficient for
the second condition. For permutation invariant M ? RN , the spectral set of symmetric matrices
is defined as ??1 (M) = {X ? SN : (?1 (X), . . . , ?N (X)) ? M}, and ??1 (M) is prox-regular
if and only if M is prox-regular [16, Th. 2.4]. Let M be {x ? R+
n : |supp(x)| = K}. Since each
element in M has exactly K positive components and all others are zero, ??1 (M) = PSDN K . By
the definition of M and K < N , PM is locally unique almost everywhere, satisfying the second
condition almost surely. (As the intersection of the convex set PSDN and the smooth manifold of
rank K matrices, PSDN K is a smooth manifold almost everywhere.)
Checking the third condition a priori is challenging, but we expect noise in the empirical C to
prevent an irregular solution, following the argument of Numerical Example 9 in [10]. We expect
AP to converge locally linearly and we can verify local convergence of AP in practice. Empirically,
the ratio of average distances between two iterations are always ? 0.9794 on the NYTimes dataset
(see the Appendix), and other datasets were similar. Note again that our rectified C 0 is a result of
pushing the empirical C toward the ideal C ? . Because approximation factors of [6] are all computed
based on how far C and its co-occurrence shape could be distant from C ? ?s, all provable guarantees
of [6] hold better with our rectified C 0 .

6

Related and Future Work

JSMF is a specific structure-preserving Non-negative Matrix Factorization (NMF) performing spectral inference. [17, 18] exploit a similar separable structure for NMF problmes. To tackle hyperspectral unmixing problems, [19, 20] assume pure pixels, a separability-equivalent in computer vision.
In more general NMF without such structures, RESCAL [21] studies tensorial extension of similar
factorization and SymNMF [22] infers BB T rather than BAB T . For topic modeling, [23] performs
spectral inference on third moment tensor assuming topics are uncorrelated.
As the core of our algorithm is to rectify the input co-occurrence matrix, it can be combined with
several recent developments. [24] proposes two regularization methods for recovering better B.
[12] nonlinearly projects co-occurrence to low-dimensional space via t-SNE and achieves better
anchors by finding the exact anchors in that space. [25] performs multiple random projections to
low-dimensional spaces and recovers approximate anchors efficiently by divide-and-conquer strategy. In addition, our work also opens several promising research directions. How exactly do anchors
found in the rectified C 0 form better bases than ones found in the original space C? Since now the
topic-topic matrix A is again doubly non-negative and joint-stochastic, can we learn super-topics in
a multi-layered hierarchical model by recursively applying JSMF to topic-topic co-occurrence A?

Acknowledgments
This research is supported by NSF grant HCC:Large-0910664. We thank Adrian Lewis for valuable
discussions on AP convergence.
13

A set M is prox-regular if PM is locally unique.

8

References
[1] Alan Mislove, Bimal Viswanath, Krishna P. Gummadi, and Peter Druschel. You are who you know: Inferring user profiles in Online Social Networks. In Proceedings of the 3rd ACM International Conference
of Web Search and Data Mining (WSDM?10), New York, NY, February 2010.
[2] Shuo Chen, J. Moore, D. Turnbull, and T. Joachims. Playlist prediction via metric embedding. In ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 714?722, 2012.
[3] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. GloVe: Global vectors for word representation. In EMNLP, 2014.
[4] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In NIPS, 2014.
[5] S. Arora, R. Ge, and A. Moitra. Learning topic models ? going beyond SVD. In FOCS, 2012.
[6] Sanjeev Arora, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu, and
Michael Zhu. A practical algorithm for topic modeling with provable guarantees. In ICML, 2013.
[7] T. Hofmann. Probabilistic latent semantic analysis. In UAI, pages 289?296, 1999.
[8] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, pages
993?1022, 2003. Preliminary version in NIPS 2001.
[9] JamesP. Boyle and RichardL. Dykstra. A method for finding projections onto the intersection of convex
sets in Hilbert spaces. In Advances in Order Restricted Statistical Inference, volume 37 of Lecture Notes
in Statistics, pages 28?47. Springer New York, 1986.
[10] Adrian S. Lewis, D. R. Luke, and Jrme Malick. Local linear convergence for alternating and averaged
nonconvex projections. Foundations of Computational Mathematics, 9:485?513, 2009.
[11] T. L. Griffiths and M. Steyvers. Finding scientific topics. Proceedings of the National Academy of
Sciences, 101:5228?5235, 2004.
[12] Moontae Lee and David Mimno. Low-dimensional embeddings for interpretable anchor-based topic
inference. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pages 1319?1328. Association for Computational Linguistics, 2014.
[13] Mary E Broadbent, Martin Brown, Kevin Penner, I Ipsen, and R Rehman. Subset selection algorithms:
Randomized vs. deterministic. SIAM Undergraduate Research Online, 3:50?71, 2010.
[14] D. Blei and J. Lafferty. A correlated topic model of science. Annals of Applied Statistics, pages 17?35,
2007.
[15] David Mimno, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. Optimizing
semantic coherence in topic models. In EMNLP, 2011.
[16] A. Daniilidis, A. S. Lewis, J. Malick, and H. Sendov. Prox-regularity of spectral functions and spectral
sets. Journal of Convex Analysis, 15(3):547?560, 2008.
[17] Christian Thurau, Kristian Kersting, and Christian Bauckhage. Yes we can: simplex volume maximization
for descriptive web-scale matrix factorization. In CIKM?10, pages 1785?1788, 2010.
[18] Abhishek Kumar, Vikas Sindhwani, and Prabhanjan Kambadur. Fast conical hull algorithms for nearseparable non-negative matrix factorization. CoRR, pages ?1?1, 2012.
[19] Jos M. P. Nascimento, Student Member, and Jos M. Bioucas Dias. Vertex component analysis: A fast
algorithm to unmix hyperspectral data. IEEE Transactions on Geoscience and Remote Sensing, pages
898?910, 2005.
[20] C?ecile Gomez, H. Le Borgne, Pascal Allemand, Christophe Delacourt, and Patrick Ledru. N-FindR
method versus independent component analysis for lithological identification in hyperspectral imagery.
International Journal of Remote Sensing, 28(23):5315?5338, 2007.
[21] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning on
multi-relational data. In Proceedings of the 28th International Conference on Machine Learning (ICML11), ICML, pages 809?816. ACM, 2011.
[22] Da Kuang, Haesun Park, and Chris H. Q. Ding. Symmetric nonnegative matrix factorization for graph
clustering. In SDM. SIAM / Omnipress, 2012.
[23] Anima Anandkumar, Dean P. Foster, Daniel Hsu, Sham Kakade, and Yi-Kai Liu. A spectral algorithm
for latent Dirichlet allocation. In Advances in Neural Information Processing Systems 25: 26th Annual
Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December
3-6, 2012, Lake Tahoe, Nevada, United States., pages 926?934, 2012.
[24] Thang Nguyen, Yuening Hu, and Jordan Boyd-Graber. Anchors regularized: Adding robustness and
extensibility to scalable topic-modeling algorithms. In Association for Computational Linguistics, 2014.
[25] Tianyi Zhou, Jeff A Bilmes, and Carlos Guestrin. Divide-and-conquer learning by anchoring a conical
hull. In Advances in Neural Information Processing Systems 27, pages 1242?1250. 2014.

9

"
1091,2001,Probabilistic Inference of Hand Motion from Neural Activity in Motor Cortex,Abstract Missing,"Probabilistic Inference of Hand Motion from Neural
Activity in Motor Cortex

Y. Gao


M. J. Black



E. Bienenstock



S. Shoham



J. P. Donoghue



Division of Applied Mathematics, Brown University, Providence, RI 02912
Dept.
of Computer Science, Brown University, Box 1910, Providence, RI 02912

Princeton University, Dept. of Molecular Biology Princeton, NJ, 08544

Dept. of Neuroscience, Brown University, Providence, RI 02912
gao@cfm.brown.edu, black@cs.brown.edu, elie@dam.brown.edu,
sshoham@princeton.com, john donoghue@brown.edu

Abstract
Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. First, an array of electrodes provides training data of neural firing conditioned on hand kinematics. We learn a nonparametric representation of this firing activity using a Bayesian model
and rigorously compare it with previous models using cross-validation.
Second, we infer a posterior probability distribution over hand motion
conditioned on a sequence of neural test data using Bayesian inference.
The learned firing models of multiple cells are used to define a nonGaussian likelihood term which is combined with a prior probability for
the kinematics. A particle filtering method is used to represent, update,
and propagate the posterior distribution over time. The approach is compared with traditional linear filtering methods; the results suggest that it
may be appropriate for neural prosthetic applications.

1 Introduction
This paper explores the use of statistical learning methods and probabilistic inference techniques for modeling the relationship between the motion of a monkey?s arm and neural
activity in motor cortex. Our goals are threefold: (i) to investigate the nature of encoding
in motor cortex, (ii) to characterize the probabilistic relationship between arm kinematics
(hand position or velocity) and activity of a simultaneously recorded neural population, and
(iii) to optimally reconstruct (decode) hand trajectory from population activity to smoothly
control a prosthetic robot arm (cf [14]).
A multi-electrode array (Figure 1) is used to simultaneously record the activity of 24 neurons in the arm area of primary motor cortex (MI) in awake, behaving, macaque monkeys.
This activity is recorded while the monkeys manually track a smoothly and randomly mov-



 
   	 
  
 
  
 

 

C.


Connector

Acrylic

Bone

Silicone

  



	 
 
 
,.-0/1 2 3
! !"" #

()) *+

$ %&'
4
444
5
564

White Matter

Figure 1: Multi-electrode array. A. 10X10 matrix of electrodes. Separation 4007 m (size
4X4 mm). B. Location of array in the MI arm area. C. Illustration of implanted array
(courtesy N. Hatsopoulos).

ing visual target on a computer monitor [12]. Statistical learning methods are used to derive
Bayesian estimates of the conditional probability of firing for each cell given the kinematic variables (we consider only hand velocity here). Specifically, we use non-parametric
models of the conditional firing, learned using regularization (smoothing) techniques with
cross-validation. Our results suggest that the cells encode information about the position
and velocity of the hand in space. Moreover, the non-parametric models provide a better
explanation of the data than previous parametric models [6, 10] and provide new insight
into neural coding in MI.
Decoding involves the inference of the hand motion from the firing rate of the cells. In particular, we represent the posterior probability of the entire hand trajectory conditioned on
the observed sequence of neural activity (spike trains). The nature of this activity results in
ambiguities and a non-Gaussian posterior probability distribution. Consequently, we represent the posterior non-parametrically using a discrete set of samples [8]. This distribution
is predicted and updated in non-overlapping 50 ms time intervals using a Bayesian estimation method called particle filtering [8]. Experiments with real and synthetic data suggest
that this approach provides probabilistically sound estimates of kinematics and allows the
probabilistic combination of information from multiple neurons, the use of priors, and the
rigorous evaluation of models and results.

2 Methods: Neural Recording
The design of the experiment and data collection is described in detail in [12]. Summarizing, a ten-by-ten array of electrodes is implanted in the primary motor cortex (MI) of
a Macaque monkey (Figure 1) [7, 9, 12]. Neural activity in motor cortex has been shown
to be related to the movement kinematics of the animal?s arm and, in particular, to the
direction of hand motion [3, 6]. Previous behavioral tasks have involved reaching in one
of a fixed number of directions [3, 6, 14]. To model the relationship between continuous,
smooth, hand motion and neural activity, we use a more complex scenario where the monkey performs a continuous tracking task in which the hand is moved on a 2D tablet while
holding a low-friction manipulandum that controls the motion of a feedback dot viewed on
a computer monitor (Figure 2a) [12]. The monkey receives a reward upon completion of
a successful trial in which the manipulandum is moved to keep the feedback dot within a
pre-specified distance of the target. The path of the target is chosen to be a smooth random
walk that effectively samples the space of hand positions and velocities: measured hand
positions and velocities have a roughly Gaussian distribution (Figure 2b and c) [12]. Neural activity is amplified, waveforms are thresholded, and spike sorting is performed off-line
to isolate the activity of individual cells [9]. Recordings from 24 motor cortical cells are
measured simultaneously with hand kinematics.

Monitor

16

Target

14

25

12
20
10

15

8

Tablet
6
10

Trajectory

4
5
2

Manipulandum

0

0

a
b
c
Figure 2: Smooth tracking task. (a) The target moves with a smooth random walk. Distribution of the position (b) and velocity (c) of the hand. Color coding indicates the frequency
with which different parts of the space are visited. (b) Position: horizontal and vertical
axes represent and  position of the hand. (c) Velocity: the horizontal axis represents
direction, 
	 , and the vertical axis represents speed,  .



3

2.5

 


2

 

1.5



1

0.5







cell 3

0

cell 16

cell 19

Figure 3: Observed mean conditional firing rates in 50 ms intervals for three cells given
hand velocity. The horizontal axis represents the direction of movement,  , in radians
(?wrapping? around from  to  ). The vertical axis represents speed,  , and ranges from
0 cm/s to 12 cm/s. Color ranges from dark blue (no measurement) to red (approximately 3
spikes).

3 Modeling Neural Activity
Figure 3 shows the measured mean firing rate within 50 ms time intervals for three cells
conditioned on the subject?s hand velocity. We view the neural firing activity in Figure 3
as a stochastic and sparse realization of some underlying model that relates neural firing
to hand motion. Similar plots are obtained as a function of hand position. Each plot can
be thought of as a type of ?tuning function? [12] that characterizes the response of the cell
conditioned on hand velocity. In previous work, authors have considered a variety of
models of this data including a cosine tuning function [6] and a modified cosine function
[10]. Here we explore a non-parametric model of the underling activity and, adopting a
Bayesian formulation, seek a maximum a posterior (MAP) estimate of a cell?s conditional
firing.
Adopting
a Markov Random Field (MRF) assumption [4], let the velocity space, 

!""!#%$ , be discretized on a &(')'
*+&(')' grid. Let g be the array of true (unobserved) conditional neural firing and , be the corresponding observed mean firing. We seek the posterior
-/.
.98:-/.9;
-/.
probability
g 01,1235476
6<0>=?6@2A4CBDFE/G =?6<0H=?6?IJ2K2
(1)

0

0.18

0.16

?2
0.14

?4

0.12

0.1

?6
0.08

?8

0.06

0.04

?10
0.02

0
?3

?2

?1

0

1

2

3

?12
?3

?2

?1

0

1

2

3

a
b
Figure 4: Prior probability of firing variation ( = ). (a) Probability of firing variation computed from training data (blue). Proposed robust prior model (red) plotted for   ' .
(b) Logarithm of the distributions shown to provide detail.

8

;

where is a normalizing constant independent of g, 6 and = 6 are the observed and true
mean firing at velocity  respectively, = 6 I represents the firing rate for the  th neighboring
velocity of  , and the neighbors are taken to be the four nearest velocities (	

 ).

; term on the right hand side represents the likelihood of observing a particular firing
The first
- of the neural
rate 6 given that the true rate is =6 . Here we compare
two generative models
spiking process within 50 ms; a Poisson model,  , and a Gaussian model, 
 :
- .;


- .9;

&;  



=

0>= 2 

0H= 2 


&



?

.;



=A2 

 

!



The second term is a spatial prior probability that encodes our expectations about = ,
the variation of neural activity in velocity space. The MRF prior states that the firing,
=?6 , at velocity  depends only on the firing at neighboring velocities. We consider two
possible prior models for the distribution of = : Gaussian and ?robust?. A Gaussian prior
corresponds to an assumption that the firing rate varies smoothly. A robust prior assumes
a heavy-tailed distribution of the spatial variation (see Figure 4), = , (derivatives of the
firing rate in the  and  directions) and implies piecewise smooth data. The two spatial
priors are

-#"" .

=2  

.

 %$
 '&

- .

= 2



=2 


&
? 





.

= 2( 
 )!

The various models (cosine, a modified cosine (Moran and Schwartz [10]), GausG
sian+Gaussian, and Poisson+Robust) are fit to the training data as shown in Figure 5.
In the case of the Gaussian+Gaussian and Poisson+Robust models, the optimal value of
the  parameter is computed for each cell using cross validation. During cross-validation,
each time 10 trials out of 180 are left out for testing and the models are fit with the remaining training data. We then compute the log likelihood of the test data given the model. This
provides a measure of how well the model captures the statistical variation in the training
set and is used for quantitative comparison. The whole procedure is repeated 18 times for
different test data sets.
The solution to the Gaussian+Gaussian model can be computed in closed form but for
the Poisson+Robust model no closed form solution for g exists and an optimal Bayesian
estimate could be achieved with simulated annealing [4]. Instead, we derive an approximate

*

By ?Gaussian+Gaussian? we mean both the likelihood and prior terms are Gaussian whereas
?Poisson+Robust? implies a Poisson likelihood and robust spatial prior.

1.5

1.2
1
0.8
0.6
0.4
0.2

1

1

Cosine

0.5

0.5

1.2
1
0.8
0.6
0.4
0.2

1.5

0.8
0.6
0.4
0.2

0.8

0.9

1
0.5

0.75

0.6

0.8

0.7

0.4

0.8

0.3

cell 3

0.8
0.7
0.6
0.5

1

0.5

0.6

cell 16

Gaussian+Gaussian

0.65

0.7

0.7

Moran&Schwartz
(M&S)

Poisson+Robust

cell 19

Figure 5: Estimated firing rate for cells in Figure 3 using different models.
Method:
G+G over Cosine
G+G over M&S
P+R over Cosine
P+R over M&S

Log Likelihood Ratio
24.9181
15.8333
50.0685
32.2218

p-value
7.6294e-06
0.0047
7.6294e-06
7.6294e-06

Table 1: Numerical comparison; log likelihood ratio of pairs of models and the significance
level given by Wilcoxon signed rank test (Splus, MathSoft Inc., WA).
solution for g in (1) by minimizing the negative logarithm of the distribution using standard
regularization techniques [1, 13] with missing data, the learned prior model, and a Poisson
likelihood term [11]. Simple gradient descent [1] with deterministic annealing provides a
. logarithm of. the prior
. term can be approximated
reasonable solution. Note that the negative
=A2  =  &
=A2 12 which has been used
by the robust statistical error function
extensively in machine vision and image processing for smoothing data with discontinuities
[1, 5].
Figure 5 shows the various estimates of the receptive fields. Observe that the pattern of
firing is not Gaussian. Moreover, some cells appear to be tuned to motion direction,  , and
not to speed,  , resulting in vertically elongated patterns of firing. Other cells (e.g. cell 19)
appear to be tuned to particular directions and speeds; this type of activity is not well fit by
the parametric models.
Table 1 shows a quantitative comparison using cross-validation. The log likelihood ratio
(LLR) is used to compare each pair of models: LLR(model 1, model 2) = log( (observed
firing 0 model 1)/Pr(observed firing 0 model 2)). The positive values in Table 1 indicate
that the non-parametric models do a better job of explaining new data than the parametric
models with the Poisson+Robust fit providing the best description of the data. This P+R
model implies that the conditional firing rate is well described by regions of smooth activity
with relatively sharp discontinuities between them. The non-parametric models reduce the
strong bias of the parametric models with a slight increase in variance and hence achieve a
lower total error.

4 Temporal Inference
Given neural measurements our goal is to infer the motion of the hand over time. Related
approaches have exploited simple linear filtering methods which do not provide a probabilistic interpretation of the data that can facilitate analysis and support the principled
combination of multiple sources of information. Related probabilistic approaches have
exploited Kalman filtering [2]. We note here however, that the learned models of neural
activity are not-Gaussian and the dynamics of the hand motion may be non-linear. Furthermore with a small number of cells, our interpretation of the neural data may be ambiguous
and the posterior probability of the kinematic variables, given the neural activity, may be
best modeled by a non-Gaussian, multi-modal, distribution. To cope with these issues in
a sound probabilistic framework we exploit a non-parametric approach that uses factored
sampling to discretely approximate the posterior distribution, and particle filtering to propagate and update this distribution over time [8].



D

 

be the mean firing rate of cell
Let the state of the system be s7 ! ?# at time . Let 

 at time where the mean firing
rate
is
estimated
within
non-overlapping
50 ms temporal
	 
  G

windows. Also, let c  
   # represent the firing rate of all 
 cells at time .
D	

Similarly let 
represent the sequence of these firing rates for cell  up to time and let
	 
 G

C  
  
# represent the firing of all 
 cells up to time .


We assume that the temporal dynamics of the states, s , form a Markov chain for which the

state at time depends only on the state at the previous time instant:

-/.



- .

G 2

s 0S

s 0s

G2

where S  s   F s
(# denotes the state history. We also assume that given s , the current
observation c and the previous observations C  G are independent.
Using Bayes rule and the above assumptions, the probability of observing the state at time

given the history of firing can be written as

- .

8

s H0 CJ2



8 - .


c 0 s""2

-/.

sH0 C 

G2

(2)

- . that
where is -/
insures that the distribution integrates to one. The like D	
 a . normalizing term
lihood term c 0 s 2  DFE/G  0 s 2 assumes conditional independence of the individual
cells where the likelihood for the firing rate of an individual cell is taken to be a Poisson
distribution with the mean firing rate for the speed and velocity given by s determined by
the conditional firing models learned in the previous section. Plotting this likelihood term
for a range of states reveals
- . that its structure is highly non-Gaussian with multiple peaks.
The temporal prior term,

-/.

- .

sH0 C 

s 0C 

G2 

G2

can be written as


-/.

sH0 s 

G2

-/.

s

G 0C  G 2



s

G

(3)

.
where sH0 s  G 2 embodies the temporal dynamics of the hand velocity- which
are assumed
to be constant with Gaussian noise; that is, a diffusion process. Note, s  G 0 C  G 2 is the

posterior distribution
over the state space at time  & .
- .
The posterior, s 0 CJ2 , is represented with a discrete, weighted set, of ')'' random samples which are propagated in time using a standard particle filter (see [8] for details). Unlike
previous applications of particle filtering, the likelihood of firing for an individual cell in

trial No. 8, Vx in cm/s, blue:true, red:reconstruction

trial No. 8, Vx in cm/s, blue:true, red:reconstruction

10

10

5

5

0

0

-5
-10
125

-5

126

127

128

129

130
131
time in second

132

133

134

135

-10
125

126

127

128

129

Vy in cm/s
10

5

5

0

0

-5

-5

126

127

128

129

130

132

133

134

135

132

133

134

135

Vy in cm/s

10

-10
125

130
131
time in second

131

132

133

134

-10
125

126

127

128

129

130

131

a
b
Figure 6: Tracking results using 1008 synthetic cells showing horizontal velocity,  , (top)
and vertical velocity,  , (bottom). Blue indicates true velocity of hand. (a) Bayesian
estimate using particle filtering. Red curve shows expected value of the posterior. The


error is    '  
 for  and   ' '   for  . (b) Linear filtering method shown in

red;    ' 
	 for  and    ' '
for  .
50 ms provides very little information. For the posterior to be meaningful we must combine evidence from multiple cells. Our experiments indicate that the responses from our
24 cells are insufficient for this task. To demonstrate the feasibility of the particle filtering
method, we synthesized approximately 1000 cells by taking the learned models of the 24
cells and translating them along the  axis to generate a more complete covering of the
velocity space. Note that the assumption of such a set of cells in MI is quite reasonable
give the sampling of cells we have observed in multiple monkeys.
From the set of synthetic cells we then generate a synthetic spike train by taking a known
sequence of hand velocities and stochastically generating spikes using the learned conditional firing models with a Poisson generative model. Particle filtering is used to estimate
the posterior distribution over hand velocities given the synthetic neural data. The expected
value of the horizontal and vertical velocity is displayed in Figure 6a. For comparison, a
standard linear filtering method [6, 14] was trained on the synthetic data from 50 ms intervals. The resulting prediction is shown in Figure 6b. Linear filtering works well over
longer time windows which introduce lag. The Bayesian analysis provides a probabilistic
framework for sound causal estimates over short time intervals.
We are currently experimenting with modified particle filtering schemes in which linear
filtering methods provide a proposal distribution and importance sampling is used to construct a valid posterior distribution. We are also comparing these results with those of
various Kalman filters.

5 Conclusions
We have described a Bayesian model for neural activity in MI that relates this activity to
actions in the world. Quantitative comparison with previous models of MI activity indicate
that the non-parametric models computed using regularization more accurately describe
the neural activity. In particular, the robust spatial prior term suggests that neural firing in
MI is not a smooth function of velocity but rather exhibits discontinuities between regions

of high and low activity.
We have also described the Bayesian decoding of hand motion from firing activity using a
particle filter. Initial results suggest that measurements from several hundred cells may be
required for accurate estimates of hand velocity. The application of particle filtering to this
problem has many advantages as it allows complex, non-Gaussian, likelihood models that
may incorporate non-linear temporal properties of neural firing (e.g. refractory period).
Unlike previous linear filtering methods this Bayesian approach provides probabilistically
sound, causal, estimates in short time windows of 50ms. Current work is exploring correlations between cells [7] and the relationship between the neural activity and other kinematic
variables [12].
Acknowledgments. This work was supported by the Keck Foundation and by the National
Institutes of Health under grants #R01 NS25074 and #N01-NS-9-2322 and by the National
Science Foundation ITR Program award #0113679. We are very grateful to M. Serruya,
M. Fellows, L. Paninski, and N. Hatsopoulos who provided the neural data and valuable
insight.

References
[1] M. Black and A. Rangarajan. On the unification of line processes, outlier rejection, and robust
statistics with applications in early vision. IJCV, 19(1):57?92, 1996.
[2] E. Brown, L. Frank, D. Tang, M. Quirk, and M. Wilson. A statistical paradigm for neural spike
train decoding applied to position prediction from ensemble firing patterns of rat hippocampal
place cells. J. Neuroscience, 18(18):7411?7425, 1998.
[3] Q-G. Fu, D. Flament, J. Coltz, and T. Ebner. Temporal encoding of movement kinematics in the
discharge of primate primary motor and premotor neurons. J. of Neurophysiology, 73(2):836?
854, 1995.
[4] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions and Bayesian restoration
of images. PAMI, 6(6):721?741, November 1984.
[5] S. Geman and D. McClure. Statistical methods for tomographic image reconstruction. Bulletin
of the Int. Stat. Inst., LII-4:5?21, 1987.
[6] A. Georgopoulos, A. Schwartz, and R. Kettner. Neuronal population coding of movement
direction. Science, 233:1416?1419, 1986.
[7] N. Hatsopoulos, C. Ojakangas, L. Paninski, and J. Donoghue. Information about movement
direction obtained from synchronous activity of motor cortical neurons. Proc. Nat. Academy of
Sciences, 95:15706?15711, 1998.
[8] M. Isard and A. Blake. Condensation ? conditional density propagation for visual tracking.
IJCV, 29(1): 5?28, 1998.
[9] E. Maynard, N. Hatsopoulos, C. Ojakangas, B. Acuna, J. Sanes, R. Normann, and J. Donoghue.
Neuronal interaction improve cortical population coding of movement direction. J. of Neuroscience, 19(18):8083?8093, 1999.
[10] D. Moran and A. Schwartz. Motor cortical representation of speed and direction during reaching. J. Neurophysiol, 82:2676-2692, 1999.
[11] R. Nowak and E. Kolaczyk. A statistical multiscale framework for Poisson inverse problems.
IEEE Inf. Theory, 46(5):1811?1825, 2000.
[12] L. Paninski, M. Fellows, N. Hatsopoulos, and J. Donoghue. Temporal tuning properties for
hand position and velocity in motor cortical neurons. submitted, J. Neurophysiology, 2001.
[13] D. Terzopoulos. Regularization of inverse visual problems involving discontinuities. PAMI,
8(4):413?424, 1986.
[14] J. Wessberg, C. Stambaugh, J. Kralik, P. Beck, M. Laubach, J. Chapin, J. Kim, S. Biggs, M.
Srinivasan, and M. Nicolelis. Real-time prediction of hand trajectory by ensembles of cortical
neurons in primates. Nature, 408:361?365, 2000.

"
1817,2004,Newscast EM,Abstract Missing,"Newscast EM
Wojtek Kowalczyk
Department of Computer Science
Vrije Universiteit Amsterdam
The Netherlands
wojtek@cs.vu.nl

Nikos Vlassis
Informatics Institute
University of Amsterdam
The Netherlands
vlassis@science.uva.nl

Abstract
We propose a gossip-based distributed algorithm for Gaussian mixture
learning, Newscast EM. The algorithm operates on network topologies
where each node observes a local quantity and can communicate with
other nodes in an arbitrary point-to-point fashion. The main difference
between Newscast EM and the standard EM algorithm is that the M-step
in our case is implemented in a decentralized manner: (random) pairs
of nodes repeatedly exchange their local parameter estimates and combine them by (weighted) averaging. We provide theoretical evidence and
demonstrate experimentally that, under this protocol, nodes converge exponentially fast to the correct estimates in each M-step of the EM algorithm.

1

Introduction

Advances in network technology, like peer-to-peer networks on the Internet or sensor networks, have highlighted the need for efficient ways to deal with large amounts of data that
are distributed over a set of nodes. Examples are financial data reported on the Internet,
weather data observed by a set of sensors, etc. In particular, in many data mining applications we are interested in learning a global model from such data, like a probability
distribution or a clustering of the data, without first transferring all the data to a central
repository. Ideally, we would like to have a fully decentralized algorithm that computes
and disseminates aggregates of the data, with minimal processing and communication requirements and good fault-tolerant behavior.
A recent development in distributed systems technology is the use of gossip-based models
of computation [1, 2, 3]. Roughly, in a gossip-based protocol each node repeatedly contacts some other node at random and the two nodes exchange information. Gossip-based
protocols are very simple to implement, while they enjoy strong performance guarantees
as a result of randomization. Their use in data mining and machine learning applications is
currently finding inroads [4, 5].
In this paper we propose a gossip-based, fully decentralized implementation of the
Expectation-Maximization (EM) algorithm for Gaussian mixture learning [6]. Our algorithm, which we call ?Newscast EM?, assumes a set of data {xi } that are drawn independently from a common Gaussian mixture and are distributed over the nodes of a network
(one data point per node). Newscast EM utilizes a gossip-based protocol in its M-step to

learn a global Gaussian mixture model p(x) from the data. The main idea is to perform the
M-step in a number of cycles. Each node starts with a local estimate of the model parameters. Then, in every cycle, each node contacts some other node that is chosen at random
from a list of known nodes, and the two nodes replace their local model estimates by their
(weighted) averages. As we show below, under such a protocol the (erroneous) local models of the individual nodes converge exponentially fast to the (correct) global model in each
M-step of the algorithm.
Our approach is fundamentally different from other distributed exact implementations of
the EM algorithm that resort on global broadcasting [7] or routing trees [8]. In the latter,
for instance, data sufficient statistics are propagated through a spanning tree in the network,
combined with an incremental learning scheme as in [9]. A disadvantage of that approach
is that only one node is carrying out computations at any time step, whereas in Newscast
EM all nodes are running the same protocol in parallel. This results in a batch M-step that
has average runtime at most logarithmic in the number of nodes, as we will see next.

2

Gaussian mixtures and the EM algorithm

A k-component Gaussian mixture for a random vector x ? IRd is defined as the convex
combination
k
X
p(x) =
?s p(x|s)
(1)
s=1

?d/2

of k Gaussian densities p(x|s) = (2?)
|Cs |?1/2 exp[?(x ? ms )> Cs?1 (x ? ms )/2],
each parameterized by its mean ms and covariance matrix Cs . The components of the
mixture are indexed by the random variable s that takes values from 1 to k, and ? s = p(s)
defines a discrete prior distribution over the components. Given a set {x1 , . . . , xn } of
independent and identically distributed samples from p(x), the learning task is to estimate
k
the parameter vector
Pn ? = {?s , ms , Cs }s=1 of the k components that maximizes the loglikelihood L = i=1 log p(xi ; ?). Throughout we assume that the likelihood function is
bounded from above (e.g., by placing appropriate bounds on the components covariance
matrices).
Maximization of the data log-likelihood L can be carried out by the EM algorithm [6]
which can be seen as iteratively maximizing a lower bound of L [9]. This bound F is
a function of the current mixture parameters ? and a set of ?responsibility? distributions
{qi (s)}, i = 1, . . . , n, where each qi (s) corresponds to a data point xi and defines an
arbitrary discrete distribution over s. This lower bound is given by:
F=

n X
k
X
i=1 s=1



qi (s) log p(xi , s; ?) ? log qi (s) .

(2)

In the E-step of the EM algorithm, the responsibility qi (s) for each point xi is set to the
Bayes posterior p(s|xi ) given the parameters found in the previous step. In the M-step we
solve for the unknown parameters of the mixture by maximizing F for fixed qi (s). This
yields the following updates:
Pn
Pn
Pn
qi (s)xi x>
i
i=1 qi (s)
i=1 qi (s)xi
?s =
,
ms =
,
Cs = i=1
?ms m>
s . (3)
n
n?s
n?s
Note that the main operation of the M-step is averaging: ?s is the average of qi (s), ms
is the average of products qi (s)xi (divided by ?s ), and the covariance matrix Cs is the
>
average of matrices qi (s)xi x>
i (divided by ?s and decreased by ms ms ). This observation
is essential for the proposed algorithm, as we will shortly see.

3

Newscast computing and averaging

The proposed distributed EM algorithm for Gaussian mixture learning relies on the use of
the Newscast protocol for distributed computing [3]. Newscast is a gossip-based protocol
that applies to networks where arbitrary point-to-point communication between nodes is
possible, and it involves repeated data exchange between nodes using randomization: with
constant frequency each node contacts some other node at random, and the two nodes
exchange application-specific data as well as caches with addresses of other nodes. The
protocol is very robust, scalable, and simple to implement?its Java implementation is
only a few kBytes of code and can run on small network-enabled computing devices such
as mobile phones, PDAs, or sensors.
As with other gossip-based protocols [2], Newscast can be used for computing the mean
of a set of values that are distributed over a network. Suppose that values v1 , . . . , vn are
stored in the nodes of a network, one value per node. Moreover
Pn suppose that each node
knows the addresses of all other nodes. To compute ? = n1 i=1 vi , each node i initially
sets ?i = vi as its local estimate of ?, and then runs the following protocol for a number of
cycles:
Uniform Newscast (for node i)
1. Contact a node j = f (i) that is chosen uniformly at random from 1, . . . , n.
2. Nodes i and j update their estimates as follows: ?0i = ?0j = (?i + ?j )/2.
For the purpose of analysis we will assume that in each cycle every node initiates a single
contact (but in practice the algorithm can be fully asynchronous). Note that the mean of
the local estimates {?i } is always the correct mean ?, while for their variance holds:
Lemma 1. In each cycle of uniform Newscast the variance of the local estimates drops on
1
the average by factor ?, with ? ? 2?
.
e
Pn
Proof. 1 Let ?t = i=1 (?i ? ?)2 be the unnormalized variance of the local estimates ?i
at cycle t. Suppose, without loss of generality, that within cycle t nodes initiate contacts in
the order 1, 2, . . . , n. The new variance after node?s 1 contact is:
? + ?
2
1
f (1)
?1 = ?t ? (?1 ? ?)2 ? (?f (1) ? ?)2 + 2
??
(4)
2
1
1
(5)
= ?t ? (?1 ? ?)2 ? (?f (1) ? ?)2 + (?1 ? ?)(?f (1) ? ?).
2
2
Taking expectation over f , and using the fact that P [f (i) = j] = n1 for all i and j, gives:
n

1 X
1 
1
1
(?j ? ?)2 = 1 ?
? ? (?1 ? ?)2 . (6)
E[?1 |?t = ?] = ? ? (?1 ? ?)2 ?
2
2n j=1
2n
2
After n such exchanges, the variance ?t+1 is on the average:
n

1 n
1 X
1 n?i
E[?t+1 |?t = ?] = 1 ?
??
1?
(?i ? ?)2 .
2n
2 i=1
2n
Bounding the term (1 ?

1 n?i
2n )

by (1 ?

1 n
2n )

finally gives:
1 n
?
E[?t+1 |?t = ?] ?
1?
?? ? .
2
2n
2 e

1

(7)

1

See [3] for an alternative proof of the same bound.

(8)

Thus after t cycles of uniform Newscast,
? the original variance ?0 of the local estimates is
reduced on the average to ?t ? ?0 /(2 e)t . The fact that the variance drops at an exponential rate means that the nodes learn the correct average very fast. Indeed, using Chebyshev?s
inequality Pt [|?i ? ?| ? ?] ? ?t /(n?2 ) we see that for any ? > 0, the probability that
some node makes an estimation error larger than ? is dropping exponentially fast with the
number of cycles t. In particular, we can derive a bound on the number of cycles that are
needed in order to guarantee with high probability that all nodes know the correct answer
with some specific accuracy:
Theorem 1. With probability 1 ? ? , after d0.581(log n + 2 log ? + 2 log 1? + log 1? )e cycles
of uniform Newscast holds maxi |?i ? ?| ? ?, for any ? > 0 and data variance ? 2 .
?
Proof. Using Lemma 1 and the fact that ?0 = n? 2 , we obtain E[?t ] ? n? 2 /(2 e)t .
?
2
2
Setting ? = log( n?
?2 ? )/ log(2 e) we obtain E[?? ] ? ? ?. Using Markov inequality, with
2
probability at least 1 ? ? holds ?? ? ? . Therefore, since ?? is the sum of local terms, for
each of them must hold |?i ? ?| ? ?. It is straightforward to show by induction over ? that
the same inequality will hold for any time ? 0 > ? .
For example, for unit-variance data and a network with n = 104 nodes we need 49 cycles
to guarantee with probability 95% that each node is within 10?10 from the correct answer.
Note that in uniform Newscast, each node in the network is assumed to know the addresses
of all other nodes, and therefore can choose in each cycle one node uniformly at random
to exchange data with. In practice, however, each node can only have a limited cache of
addresses of other nodes. In this case, the averaging algorithm is modified as follows:
Non-uniform Newscast (for node i)
1. Contact a node j = f (i) that is appropriately chosen from i?s local cache.
2. Nodes i and j update their estimates as follows: ?0i = ?0j = (?i + ?j )/2.
3. Nodes i and j update their caches appropriately.
Step 3 implements a ?membership management? schedule which effectively defines a dynamically changing random graph topology over the network. In our experiments we
adopted the protocol of [10] which roughly operates as follows. Each entry k in node?s i
cache contains an ?age? attribute that indicates the number of cycles that have been elapsed
since node k created that entry. In step 1 above, node i contacts the node j with the largest
age from i?s cache, and increases by one the age of all other entries in i?s cache. Then
node i exchanges estimates with node j as in step 2. In step 3, both nodes i and j select a
random subset of their cache entries and mutually exchange them, filling empty slots and
discarding self-pointers and duplicates. Finally node i creates an entry with i?s address in
it and age zero, which is added in j?s cache. The resulting protocol is particularly effective
and, as we show in the experiments below, in some cases it even outperforms the uniform
Newscast. We refer to [10] for more details.

4

The Newscast EM algorithm

Newscast EM (NEM) is a gossip-based distributed implementation of the EM algorithm for
Gaussian mixture learning, that applies to the following setting. We are given a set of data
{xi } that are distributed over the nodes of a network (one data point per node). The data
are assumed independent samples from a common k-component Gaussian mixture p(x)
with (unknown) parameters ? = {?s , ms , Cs }ks=1 . The task is to learn the parameters of
the mixture with maximum likelihood in a decentralized manner: that is, all learning steps

should be performed locally at the nodes, and they should involve as little communication
as possible.
The NEM algorithm is a direct application of the averaging protocol of Section 3 for estimating the parameters ? of p(x) using the EM updates (3). The E-step of NEM is identical
to the E-step of the standard EM algorithm, and it can be performed by all nodes in parallel. The novel characteristic of NEM is the M-step which is implemented as a sequence
of gossip-based cycles: At the beginning of each M-step, each node i starts with a local
estimate ?i of the ?correct? parameter vector ? (correct according to EM and for the current
EM iteration). Then, in every cycle, each node contacts some other node at random, and
the two nodes replace their local estimates ?i by their (weighted) averages. At the end of
the M-step each node has converged (within machine precision) to the correct parameter ?.
To simplify notation, we will denote by ?i = {?si , msi , C?si } the local estimates of node i
for the parameters of component s at any point of the algorithm. The parameter C?si is
defined such that Csi = C?si ? msi m>
si . The complete algorithm, which runs identically
and in parallel for each node, is as follows:
Newscast EM (for node i)
1. Initialization. Set qi (s) to some random positive value and then normalize all qi (s) to
sum to 1 over all s.
2. M-step. Initialize i?s local estimates for each component s as follows: ?si = qi (s),
msi = xi , C?si = xi x>
i . Then repeat for ? cycles:
a. Contact a node j = f (i) from i?s local cache.
b. Nodes i and j update their local estimates for each component s as follows:
?si + ?sj
,
2
?si msi + ?sj msj
m0si = m0sj =
,
?si + ?sj
?si C?si + ?sj C?sj
0
0
.
C?si
= C?sj
=
?si + ?sj
0
0
?si
= ?sj
=

(9)
(10)
(11)

c. Nodes i and j update their caches appropriately.
3. E-step. Compute new responsibilities qi (s) = p(s|xi ) for each component s using the
M-step estimates ?si , msi , and Csi = C?si ? msi m>
si .
4. Loop. Go to step 2, unless a stopping criterion is satisfied that involves the parameter
estimates themselves or the energy F.2
A few observations about the algorithm are in order. First note that both the initialization
of the algorithm (step 1) as well as the E-step are completely local to each node. Similarly,
a stopping criterion involving the parameter estimates can be implemented locally if each
node caches its estimates from the previous EM-iteration. The M-step involves a total of
k[1 + d + d(d + 1)/2] averages, for each one of the k components and for dimensionality d, which are computed with the Newscast protocol. Given that all nodes agree on the
number ? of Newscast cycles in the M-step, and assuming that ? is large enough to guarantee convergence to the correct parameter estimates, the complete NEM algorithm can be
performed identically and in parallel by all nodes in the network.
It is easy to see that at any cycle of an M-step, and for any component s, the weighted
2

Note that F is a sum of local terms, and thus it can also be computed using the same protocol.

averages over all nodes of the local estimates are always the EM-correct estimates, i.e.,
Pn
i=1 ?si msi
P
= ms
(12)
n
i=1 ?si

and similarly for the C?si . Moreover, note that the weighted averages of the msi in (10)
and the C?si in (11), with weights given by (9), can be written as unweighted averages of
the corresponding products ?si msi and ?si C?si . In other words, each local estimate can be
written as the ratio of two local estimates that converge to the correct values at the same
exponential rate (as shown in the previous section). The above observations establish the
following:
Theorem 2. In every M-step of Newscast EM, each node converges exponentially fast to
the correct parameter estimates for each component of the mixture.
Similarly, the number of cycles ? for each M-step can be chosen according to Theorem 1.
However, note that in every M-step each node has to wait ? cycles before its local estimates
have converged, and only then can it use these estimates in a new next E-step. We describe
here a modification of NEM that allows a node to run a local E-step before its M-step has
converged. This ?partial? NEM algorithm is based on the following ?self-correction? idea:
instead of waiting until the M-step converges, after a small number of cycles each node runs
a local E-step, adjusts its responsibilities, and propagates appropriate corrections through
the network.
Such a scheme additionally requires that each node caches its responsibilities from the
previous E-step, denoted by q?i (s). The only modification is in the initialization of the Mstep: instead of fully resetting the local estimates as in step 2 above, a node makes the
following corrections to its current estimates ?si , msi , C?si for each component s:
0
?si
= ?si + qi (s) ? q?i (s),

m0si
0
C?si

(13)
0
q?i (s)]}/?si
,

= {msi ?si + xi [qi (s) ?
0
= {C?si ?si + xi x>
?i (s)]}/?si
.
i [qi (s) ? q

(14)
(15)

After these corrections, the Newscast averaging protocol is executed for a number of cycles (smaller than the number ? of the ?full? NEM). These corrections may increase the
variance of the local estimates, but in most cases the corresponding increase of variance is
relatively small. This results in speed-ups (often as large as 10-fold), however guaranteed
convergence is hard to establish.3

5

Experiments

To get an insight into the behavior of the presented algorithms we ran several experiments
using a Newscast simulator.4 In Fig. 1 we demonstrate the the performance of uniform
and non-uniform Newscast in typical averaging tasks involving zero-mean unit-variance
data. In Fig. 1(left) we plot the variance reduction rate ? (mean and one standard deviation
for 50 runs) as a function of the number of cycles, for averaging problems involving n =
105 data. Note
? that in uniform Newscast the observed rate is always below the derived
bound 1/(2 e) ? 0.303 from Lemma 1. Moreover note that in non-uniform Newscast the
variance drops faster than in uniform Newscast. This is due to the fact that the dynamic
cache exchange scheme of [10] results in in-degree network distributions that are very
peaked around the cache size. In practice this means that on the average each node is
3
This would require, for instance, that individual nodes have estimates of the total variance over
the network, which is not obvious how it can be done.
4
Available from http://www.cs.vu.nl/?steen/globesoul/sim.tgz

42.5
Uniform Newscast
Non?uniform (cache 20)

Uniform Newscast
Non?uniform (cache 20)

42

0.3

Cycles for convergence

Variance reduction rate

0.305

0.295

0.29

41
40.5
40
39.5

0.285

0

41.5

5

10

15
20
25
Number of cycles

30

35

40

39 3
10

4

5

10

10

6

10

Number of nodes

Figure 1: (Left) Variance reduction rate of uniform and non-uniform Newscast, in averaging tasks involving n = 105 nodes. (Right) Number of cycles to achieve convergence
within ? = 10?10 for unit-variance datasets of various sizes.
equally often contacted to by other nodes in each cycle of the protocol. We also observed
that the variance reduction rate is on the average unaffected by the network size, while
larger networks result in smaller deviations. For n = 8 ? 105 , for instance, the standard
deviation is half the one plotted above.
In Fig. 1(right) we plot the number of cycles that are required to achieve model accuracy at
all nodes within ? = 10?10 as a function of the network size. Note that all observed quantities are below the derived bound of Theorem 1, while non-uniform Newscast performs
slightly better than uniform Newscast.
We also ran experiments involving synthetic data drawn from Gaussian mixtures of different number of data points, where we observed results essentially identical to those obtained
by the standard (centralized) EM. We also performed some experiments with the ?partial?
NEM, where it turned out that in most cases we could obtain the same model accuracy with
a much smaller number of cycles (5?10 times than the ?full? NEM), but in some cases the
algorithm did not converge.

6

Summary and extensions

We presented Newscast EM, a distributed gossip-based implementation of the EM algorithm for learning Gaussian mixture models. Newscast EM applies on networks where
each one of a (large) number of nodes observes a local quantity, and can communicate with
other nodes in a point-to-point fashion. The algorithm utilizes a gossip-based protocol in
its M-step to learn a global Gaussian mixture model from the data: each node starts with
a local estimate of the parameters of the mixture and then, for a number of cycles till convergence, pairs of nodes repeatedly exchange their local parameter estimates and combine
them by (weighted) averaging. Newscast EM implements a batch M-step that has average runtime logarithmic in the network size. We believe that gossip-based protocols like
Newscast can be used in several other algorithms that learn models from distributed data.
Several extensions of the algorithm are possible. Here we have assumed that each node
in the network observes one data point. We can easily generalize this to situations where
each node observes (and stores) a collection of points, like in [8]. On the other hand, if the
locally observed data are too many, one may consider storing only some sufficient statistics of these data locally, and appropriately bound the energy F in each iteration to get a
convergent EM algorithm [11]. Another interesting extension is to replace the averaging

step 2 of uniform and non-uniform Newscast with weighted averaging (for some choice of
weights), and study the variance reduction rate in this case. Another interesting problem is
when the E-step cannot be performed locally at a node but it requires distributing some information over the network. This could be the case, for instance, when each node observes
only a few elements of a vector-valued quantity while, for instance, all nodes together observe the complete sample. We note that if the component models factorize, several useful
quantities can be computed by averaging in the log domain. Finally, it would be interesting
to investigate the applicability of the Newscast protocol in problems involving distributed
inference/learning in more general graphical models [12].
Acknowledgments
We want to thank Y. Sfakianakis for helping in the experiments, T. Pylak for making his
Newscast simulator publicly available, and D. Barber, Z. Ghahramani, and J.J. Verbeek for
their comments. N. Vlassis is supported by PROGRESS, the embedded systems research
program of the Dutch organization for Scientific Research NWO, the Dutch Ministry of
Economic Affairs and the Technology Foundation STW, project AES 5414.
References
[1] R. Karp, C. Schindelhauer, S. Shenker, and B. Vo? cking. Randomized rumour spreading. In
Proc. 41th IEEE Symp. on Foundations of Computer Science, Redondo Beach, CA, November
2000.
[2] D. Kempe, A. Dobra, and J. Gehrke. Gossip-based computation of aggregate information. In
Proc. 44th IEEE Symp. on Foundations of Computer Science, Cambridge, MA, October 2003.
[3] M. Jelasity, W. Kowalczyk, and M. van Steen. Newscast computing. Technical report, Dept. of
Computer Science, Vrije Universiteit Amsterdam, 2003. IR-CS-006.
[4] C. C. Moallemi and B. Van Roy. Distributed optimization in adaptive networks. In S. Thrun,
L. Saul, and B. Sch?olkopf, editors, Advances in Neural Information Processing Systems 16.
MIT Press, Cambridge, MA, 2004.
[5] D. Kempe and F. McSherry. A decentralized algorithm for spectral analysis. In Proc. 36th ACM
Symp. on Theory of Computing, Chicago, IL, June 2004.
[6] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via
the EM algorithm. J. Roy. Statist. Soc. B, 39:1?38, 1977.
[7] G. Forman and B. Zhang. Distributed data clustering can be efficient and exact. ACM SIGKDD
Explorations, 2(2):34?38, 2000.
[8] R. D. Nowak. Distributed EM algorithms for density estimation and clustering in sensor networks. IEEE Trans. on Signal Processing, 51(8):2245?2253, August 2003.
[9] R. M. Neal and G. E. Hinton. A view of the EM algorithm that justifies incremental, sparse, and
other variants. In M. I. Jordan, editor, Learning in graphical models, pages 355?368. Kluwer
Academic Publishers, 1998.
[10] S. Voulgaris, D. Gavidia, and M. van Steen. Inexpensive membership management for unstructured P2P overlays. Journal of Network and Systems Management, 2005. To appear.
[11] J. R. J. Nunnink, J. J. Verbeek, and N. Vlassis. Accelerated greedy mixture learning. In Proc.
Belgian-Dutch Conference on Machine Learning, Brussels, Belgium, January 2004.
[12] M. A. Paskin and C. E. Guestrin. Robust probabilistic inference in distributed systems. In Proc.
20th Int. Conf. on Uncertainty in Artificial Intelligence, Banff, Canada, July 2004.

"
4679,2014,Learning with Fredholm Kernels,"In this paper we propose a framework for supervised and semi-supervised learning based on reformulating the learning problem as a regularized Fredholm integral equation. Our approach fits naturally into the kernel framework and can be interpreted as constructing new data-dependent kernels, which we call Fredholm kernels. We proceed to discuss the noise assumption"" for semi-supervised learning and provide evidence evidence both theoretical and experimental that Fredholm kernels can effectively utilize unlabeled data under the noise assumption. We demonstrate that methods based on Fredholm learning show very competitive performance in the standard semi-supervised learning setting.""","Learning with Fredholm Kernels

Qichao Que Mikhail Belkin Yusu Wang
Department of Computer Science and Engineering
The Ohio State University
Columbus, OH 43210
{que,mbelkin,yusu}@cse.ohio-state.edu

Abstract
In this paper we propose a framework for supervised and semi-supervised learning
based on reformulating the learning problem as a regularized Fredholm integral
equation. Our approach fits naturally into the kernel framework and can be interpreted as constructing new data-dependent kernels, which we call Fredholm
kernels. We proceed to discuss the ?noise assumption? for semi-supervised learning and provide both theoretical and experimental evidence that Fredholm kernels
can effectively utilize unlabeled data under the noise assumption. We demonstrate
that methods based on Fredholm learning show very competitive performance in
the standard semi-supervised learning setting.

1

Introduction

Kernel methods and methods based on integral operators have become one of the central areas of
machine learning and learning theory. These methods combine rich mathematical foundations with
strong empirical performance. In this paper we propose a framework for supervised and unsupervised learning as an inverse problem based on solving the integral equation known as the Fredholm
problem of the first kind. We develop regularization based algorithms for solving these systems
leading to what we call Fredholm kernels.
In the basic setting of supervised learning we are given the data set (xi , yi ), where xi ? X, yi ? R.
We would like to construct a function f : X ? R, such that f (xi ) ? yi and f is ?nice enough?
to generalize to new data points. This is typically done by choosing f from a class of functions (a
Reproducing Kernel Hilbert Space (RKHS) corresponding to a positive definite kernel for the kernel
methods) and optimizing a certain loss function, such as the square loss or hinge loss.
In this paper we formulate a new framework for learning based on interpreting the learning problem
as a Fredholm integral equation. This formulation shares some similarities with the usual kernel
learning framework but unlike the standard methods also allows for easy incorporation of unlabeled
data. We also show how to interpret the resulting algorithm as a standard kernel method with a
non-standard data-dependent kernel (somewhat resembling the approach taken in [13]).
We discuss reasons why incorporation of unlabeled data may be desirable, concentrating in particular on what may be termed ?the noise assumption? for semi-supervised learning, which is related
but distint from manifold and cluster assumption popular in the semi-supervised learning literature.
We provide both theoretical and empirical results showing that the Fredholm formulation allows for
efficient denoising of classifiers.
To summarize, the main contributions of the paper are as follows:
(1) We formulate a new framework based on solving a regularized Fredholm equation. The framework naturally combines labeled and unlabeled data. We show how this framework can be expressed
as a kernel method with a non-standard data-dependent kernel.
1

(2) We discuss ?the noise assumption? in semi-supervised learning and provide some theoretical evidence that Fredholm kernels are able to improve performance of classifiers under this assumption.
More specifically, we analyze the behavior of several versions of Fredholm kernels, based on combining linear and Gaussian kernels. We demonstrate that for some models of the noise assumption,
Fredholm kernel provides better estimators than the traditional data-independent kernel and thus
unlabeled data provably improves inference.
(3) We show that Fredholm kernels perform well on synthetic examples designed to illustrate the
noise assumption as well as on a number of real-world datasets.
Related work. Kernel and integral methods in machine learning have a large and diverse literature
(e.g., [12, 11]). The work most directly related to our approach is [10], where Fredholm integral
equations were introduced to address the problem of density ratio estimation and covariate shift. In
that work the problem of density ratio estimation was expressed as a Fredholm integral equation and
solved using regularization in RKHS. This setting also relates to a line of work on on kernel mean
embedding where data points are embedded in Reproducing Kernel Hilbert Spaces using integral
operators with applications to density ratio estimation and other tasks [5, 6, 7]. A very interesting
recent work [9] explores a shrinkage estimator for estimating means in RKHS, following the SteinJames estimator originally used for estimating the mean in an Euclidean space. The results obtained
in [9] show how such estimators can reduce variance. There is some similarity between that work
and our theoretical results presented in Section 4 which also show variance reduction for certain
estimators of the kernel although in a different setting. Another line of related work is the class
of semi-supervised learning techniques (see [15, 2] for a comprehensive overview) related to manifold regularization [1], where an additional graph Laplacian regularizer is added to take advantage
of the geometric/manifold structure of the data. Our reformulation of Fredholm learning as a kernel, addressing what we called ?noise assumptions?, parallels data-dependent kernels for manifold
regularization proposed in [13].

2

Fredholm Kernels

We start by formulating learning framework proposed in this paper. Suppose we are given l labeled
pairs (x1 , y1 ), . . . , (xl , yl ) from the data distribution p(x, y) defined on X ? Y and u unlabeled
points xl+1 , . . . , xl+u from the marginal distribution pX (x) on X. For simplicity we will assume
that the feature space X is a Euclidean space RD , and the label set Y is either {?1, 1} for binary
classification or the real line R for regression. Semi-supervised learning algorithms aim to construct
a (predictor) function f : X ? Y by incorporating the information of unlabeled data distribution.
To this end, we introduce the integral operator KpX associated with a kernel function k(x, z). In our
setting k(x, z) does not have to be a positive semi-definite (or even symmetric) kernel.
Z
KpX : L2 ? L2 and KpX f (x) = k(x, z)f (z)pX (z)dz,
(1)
where L2 is the space of square-integrable functions. By the law of large numbers, the above operator can be approximated using unlabeled data from pX as
l+u

Kp?X f (x) =

1 X
k(x, xi )f (xi ).
l + u i=1

This approximation provides a natural way of incorporating unlabeled data into algorithms. In our
Fredholm learning framework, we will use functions in KpX H = {KpX f : f ? H}, where H is
an appropriate Reproducing Kernel Hilbert Space (RKHS) as classification or regression functions.
Note that unlike RKHS, this space of functions, KpX H, is density dependent.
In particular, this now allows us to formulate the following optimization problem for semi-supervised
classification/regression in a way similar to many supervised learning algorithms:
The Fredholm learning framework solves the following optimization problem1 :
l

1X
((Kp?X f )(xi ) ? yi )2 + ?kf k2H ,
f ?H l
i=1

f ? = arg min
1

(2)

We will be using the square loss to simplify the exposition. Other loss functions can also be used in Eqn 2.

2

The final classifier is c(x) = (Kp?X f ? ) (x), where Kp?X is the operator defined above. Eqn 2 is a
discretized and regularized version of the Fredholm integral equation KpX f = y, thus giving the
name of Fredholm learning framework.
Even though at a first glance this setting looks similar to conventional kernel methods, the extra
layer introduced by Kp?X makes significant difference, in particular, by allowing the integration
of information from unlabeled data distribution. In contrast, solutions to standard kernel methods
for most kernels, e.g., linear, polynomial or Gaussian kernels, are completely independent of the
unlabeled data. We note that our approach is closely related to [10] where a Fredholm equation is
used to estimated the density ratio for two probability distributions.
The Fredholm learning framework is a generalization of the standard kernel framework. In fact, if
the kernel k is the ?-function, then our formulation above is equivalent to the Regularized Kernel
Pl
Least Squares equation f ? = arg minf ?H 1l i=1 (f (xi ) ? yi )2 + ?kf k2H . We could also replace
the L2 loss in Eqn 2 by other loss functions, such as hinge loss, resulting in a SVM-like classifier.
Finally, even though Eqn 2 is an optimization problem in a potentially infinite dimensional function
space H, a standard derivation, using the Representer Theorem (See full version for details), yields
a computationally accessible solution as follows:
l+u

f ? (x) =

?1 T
1 X
T
kH (x, xj )vj , v = Kl+u
Kl+u KH + ?I
Kl+u y,
l + u j=1

(3)

where (Kl+u )ij = k(xi , xj ) for 1 ? i ? l, 1 ? j ? l + u, and (KH )ij = kH (xi , xj ) for
1 ? i, j ? l + u. Note that Kl+u is a l ? (l + u) matrix.
Fredholm kernels: a convenient reformulation. In fact we will see that Fredholm learning problem induces a new data-dependent kernel, which we will refer to as Fredholm kernel2 . To show this
connection, we use the following identity, which can be easily verified:
?1 T
?1
T
T
T
Kl+u
Kl+u KH + ?I
Kl+u = Kl+u
Kl+u KH Kl+u
+ ?I
.
T
Define KF = Kl+u KH Kl+u
to be the l ? l kernel matrix associated with a new kernel defined by

k?F (x, z) =

l+u
X
1
k(x, xi )kH (xi , xj )k(z, xj ),
(l + u)2 i,j=1

(4)

and we consider the unlabeled data are fixed for computing this new kernel. Using this new kernel
k?F , the final classifying function from Eqn 3 can be rewritten as:
l+u
l
X
1 X
?1
c? (x) =
k(x, xi )f ? (xi ) =
k?F (x, xs )?s , ? = (KF + ?I) y.
l + u i=1
s=1
Because of Eqn 4 we will sometimes refer to the kernels kH and k as the ?inner? and ?outer? kernels
respectively. It can be observed that this solution is equivalent to a standard kernel method, but using
a new data dependent kernel k?F , which we will call the Fredholm kernel, since it is induced from
the Fredholm problem formulated in Eqn 2.
Proposition 1. The Fredholm kernel defined in Eqn 4 is positive semi-definite as long as KH is
positive semi-definite for any set of data x1 , . . . , xl+u .
The proof is given in the full version. The ?outer? kernel k does not have to be either positive definite
or even symmetric. When using Gaussian kernel for k, discrete approximation in Eqn 4 might be
unstable when the kernel width is small, so we also introduce the normalized Fredholm kernel,
l+u
X
k(z, xj )
k(x, xi )
P
kH (xi , xj ) P
.
(5)
k?FN (x, z) =
k(x,
x
)
n
n
n k(z, xn )
i,j=1
It is easy to check that the resulting Fredholm kernel k?FN is still symmetric positive semi-definite.
Even though Fredholm kernel was derived using L2 loss here, it could also be derived when hinge
loss is used, which will be explained in full version.
2
We note that the term Fredholm Kernel has been used in mathematics ([8], page 103) and also in a different
learning context [14]. Our usage represents a different object.

3

3

The Noise Assumption and Semi-supervised Learning

In order for unlabeled data to be useful in classification tasks it is necessary for the marginal distribution of the unlabeled data to contain information about the conditional distribution of the labels.
Several ways in which such information can be encoded has been proposed including the ?cluster
assumption? [3] and the ?manifold assumption? [1]. The cluster assumption states that a cluster (or
a high density area) contains only (or mostly) points belonging to the same class. That is, if x1 and
x2 belong to the same cluster, the corresponding labels y1 , y2 should be the same. The manifold
assumption assumes that the regression function is smooth with respect to the underlying manifold
structure of the data, which can be interpreted as saying that the geodesic distance should be used
instead of the ambient distance for optimal classification. The success of algorithms based on these
ideas indicates that these assumptions do capture certain characteristics of real data. Still, better
understanding of unlabeled data may still lead to progress in data analysis.
The noise assumption. We propose to formulate a new assumption, the ?noise assumption?, which is that in the neighborhood of every point, the directions with low variance (for
the unlabeled data) are uninformative with respect to the class labels, and can be regarded as
noise. While intuitive, as far as we know, it has
not been explicitly formulated in the context
of semi-supervised learning algorithms, nor applied to theoretical analysis.

Figure 1: Left: only labelled points, and Right:
with unlabelled points.
Note that even if the noise variance is small along a single direction, it could still significantly decrease the performance of a supervised learning algorithm if the noise is high-dimensional. These
accumulated non-informative variations in particular increase the difficulty of learning a good classifier when the amount of labeled data is small. The first figure on right illustrates the issue of noise
with two labeled points. The seemingly optimal classification boundary (the red line) differs from
the correct one (in black) due to the noisy variation along the y axis for the two labeled points.
Intuitively unlabeled data shown in the right panel of Figure 1 can be helpful in this setting as low
variance directions can be estimated locally such that algorithms could suppress the influences of
the noisy variation when learning a classifier.
Connection to cluster and manifold assumptions. The noise assumption is compatible with the
manifold assumption within the manifold+noise model. Specifically, we can assume that the functions of interest vary along the manifold and are constant in the orthogonal direction. Alternatively,
we can think of directions with high variance as ?signal/manifold? and directions with low variance as ?noise?. We note that the noise assumption does not require the data to conform to a
low-dimensional manifold in the strict mathematical sense of the word. The noise assumption is
orthogonal to the cluster assumption. For example, Figure 1 illustrates a situation where data has no
clusters but the noise assumption applies.

4

Theoretical Results for Fredholm Kernels

Non-informative variation in data could degrade traditional supervised learning algorithms. We
will now show that Fredholm kernels can be used to replace traditional kernels to inject them with
?noise-suppression? power with the help of unlabeled data. In this section we will present two views
to illustrate how such noise suppression can be achieved. Specifically, in Section 4.1) we show that
under certain setup, linear Fredholm kernel suppresses principal components with small variance.
In Section 4.2) we prove that under certain conditions we are able to provide good approximations
to the ?true? kernel on the hidden underlying space.
To make our arguments more clear, we assume that there are infinite amount of unlabelled data; that
is, we know the marginal distribution of data exactly. We will then consider the following continuous
versions of the un-normalized andZnormalized
Fredholm kernels as in Eqn 4 and 5:
Z
kFU (x, z) =
k(x, u)kH (u, v)k(z, v)p(u)p(v)dudv
Z Z
k(x, u)
k(z, v)
R
kFN (x, z) =
kH (u, v) R
p(u)p(v)dudv.
k(x, w)p(w)dw
k(z, w)p(w)dw
4

(6)
(7)

Note, in the above equations and in what follows, we sometimes write p instead of pX for the
marginal distribution when its choice is clear from context. We will typically use kF to denote
appropriate normalized or unnormalized kernels depending on the context.
4.1

Linear Fredholm kernels and inner products

For this section, we consider the unormalized Fredholm kernel, that is kF = kFU . If the ?outer?
kernel k(u, v) is linear, i.e. k(u, v) = hu, vi, the resulting Fredholm kernel can be viewed as an
inner product. Specifically, the un-normalized Fredholm kernel from Eqn 6 can be rewritten as:
Z Z
T
kF (x, z) = x ?F z, where ?F =
ukH (u, v)v T p(u)p(v)dudv.
Thus kF (x, z) is simply an inner product which depends on both the unlabeled data distribution p(x)
and the ?inner? kernel kH . This inner product re-weights the standard norm in feature space based
on variances along the principal directions of the matrix ?F . We show that for the model when unlabeled data is sampled from a normal distribution this kernel can be viewed as a ?soft thresholding?
PCA, suppressing the directions with low variance. Specifically, we have the following3


2
Theorem 2. Let kH (x, z) = exp ? kx?zk
and assume the distribution pX for unlabeled data is
2t
a single multi-variate normal distribution, N (?, diag(?12 , . . . , ?d2 )). We have
!
s


D
4
Y
?14
?D
t
T
?? + diag
,..., 2
.
?F =
2?d2 + t
2?12 + t
2?D + t
d=1

Assuming that the data is mean-subtracted, i.e. ? = 0, we see that xT ?F z re-scales the projections
along the principal components
q when computing the inner product; that is, the rescaling factor for
the i-th principal direction is

Note that this rescaling factor
?4

?2

?i4
.
2?i2 +t
?i4
?
2?i2 +t

0 when ?i2  t. On the other hand when ?i2  t, we

have that 2?2i+t ? 2i . Hence t can be considered as a soft threshold that eliminates the effects of
i
principal components with small variances. When t is small the rescaling factors are approximately
2
), in which case ?F is is proportional to the covariance matrix
proportional to diag(?12 , ?22 , . . . , ?D
T
of the data XX .
4.2

Kernel Approximation With Noise

We have seen that one special case of Fredholm kernel could achieve the effect of principal components re-scaling by using linear kernel as the ?outer? kernel k. In this section we give a more general
interpretation of noise suppression by the Fredholm kernel.
First, we give a simple senario to provide some intuition behind the definition of Fredholm kernle. Consider a standard supervised learning setting which uses the solution f ? =
Pl
arg minf ?H 1l i=1 (f (xi )?yi )2 +?kf k2H as the classifier. Let
target
kH
denote the ideal kernel that we intend to use on the clean
data, which we call the target kernel from now on. Now suppose what we have are two noisy labelled points xe and ze for
?true? data x
? and z?, i.e. xe = x
? + ?x , ze = z? + ?z . The
target
evaluation of kH
(xe , ze ) can be quite different from the true
target
signal kH
(?
x, z?), leading to an suboptimal final classifier (the
red line in Figure 1 (a)). On the other hand, now consider the
RR
Fredholm kernel from Eqn 6 (or similarly from Eqn 7): kF (xe , ze ) =
k(xe , u)p(u) ? kH (u, v) ?
k(ze , v)p(v)dudv, and set the outer kernel k to be the Gaussian kernel, and the inner kernel kH to be
target
the same as target kernel kH
. We can think of kF (xe , ze ) as an averaging of kH (u, v) over all possible pairs of data u, v, weighted by k(xe , u)p(u) and k(ze , v)p(v) respectively. Specifically, points
3

The proof of this and other results can be found in the full version.

5

that are close to xe (resp. ze ) with high density will receive larger weights. Hence the weighted
averages will be biased towards x
? and z? respectively (which presumably lie in high density regions
around xe and ze ). The value of kF (xe , ze ) tends to provide a more accurate estimate of kH (?
x, z?).
See the right figure for an illustration where the arrows indicate points with stronger influences in the
computation of kF (xe , ze ) than kH (xe , ze ). As a result, the classifier obtained using the Fredholm
kernel will also be more resilient to noise and closer to the optimum.
The Fredholm learning framework is rather flexible in terms of the choices of kernels k and kH .
In the remainder of this section, we will consider a few specific scenarios and provide quantitative
analysis to show the noise robustness of the Fredholm kernel.
Problem setup. Assume that we have a ground-truth distribution over the subspace spanned by
the first d dimension of the Euclidean space RD . We will assume that this distribution is a single Gaussian N (0, ?2 Id ). Suppose this distribution is corrupted with Gaussian noise along the orthogonal subspace of dimension D ? d. That is, for any ?true? point x
? drawn from N (0, ?2 Id ),
2
its observation xe is drawn from N (?
x, ? ID?d ). Since the noise lies in a space orthogonal
to data distribution, this means that any observed point, labelled or unlabeled, is sampled from
pX = N (0, diag(?2 Id , ? 2 ID?d ). We will show that Fredholm kernel provides a better approximation to the ?original? kernel given unlabeled data than simply computing the kernel of noisy points.
We choose this basic setting to be able to state the theoretical results in a clean manner. Even though
this is a Gaussian distribution over a linear subspace with noise, this framework has more general
implications since local neighborhoods of manifolds are (almost) linear spaces.
Note: In this section we use normalized Fredholm kernel given in Eqn 7, that is kF = kFN for now
on. Un-normalized Fredholm kernel displays similar behavior, while the bounds are trickier.
target
Linear Kernel. First we consider the case where the target kernel kH
(u, v) is the linear kernel,
target
T
kH (u, v) = u v. We will set kH in Fredholm kernel to also be linear, and k to be the Gaussian
ku?vk2

kernel k(u, v) = e? 2t We will compare kF (xe , ze ) with the target kernel on the two observed
target
target
points, that is, with kH
(xe , ze ). The goal is to estimate kH
(?
x, z?). We will see that (1) both
target
kF (xe , ze ) and (appropriately scaled) kH (xe , ze ) are unbiased estimators of kH
(?
x, z?), however (2)
target
the variance of kF (xe , ze ) is smaller than that of kH (xe , ze ), making it a more precise estimator.
Theorem 3. Suppose the probability distribution for the unlabeled data pX
=
N (0, diag(?2 Id , ? 2 ID?d )). For Fredholm kernel defined in Eqn 7, we have
!


2 2
t
+
?
target
Exe ,ze (kH
(xe , ze )) = Exe ,ze
kF (xe , ze ) = x
?T z?
?2


2
target
t+?2
kF (xe , ze ) < Varxe ,ze (kH
(xe , ze )).
Moreover, when ? > ?, Varxe ,ze
?2
Remark: Note that we have a normalization constant for the Fredholm kernel to make it an unbiased
estimator of x
?T z?. In practice, choosing normalization is subsumed in selecting the regularization
parameter for kernel methods.
Thus we can see the Fredholm kernel provides an approximation of the ?true? linear kernel, but with
smaller variance compared to the actual linear kernel on noisy data.
Gaussian Kernel.  We now consider the case where the target kernel is the Gaussian kernel:
2
target
kH
(u, v) = exp ? ku?vk
. To approximate this kernel, we will set both k and kH to be Gaus2r
sian kernels. To simplify the presentation of results, we assume that k and kH have the same kernel
width t. The resulting Fredholm kernel turns out to also be a Gaussian kernel, whose kernel width
depends on the choice of t.
Our main result is the following. Again, similar to the case of linear kernel, the Fredholm estimation
target
target
kF (xe , ze ) and kH
(xe , ze ) are both unbiased estimator for the target kH
(?
x, z?) up to a constant;
but kF (xe , ze ) has a smaller variance.
Theorem 4. Suppose the probability distribution for the unlabeled
data pX
=

2
target
N (0, diag(?2 Id , ? 2 ID?d )). Given the target kernel kH
(u, v) = exp ? ku?vk
with
ker2r
nel width r > 0, we can choose t, given by the equation
6

t(t+?2 )(t+3?2 )
?4

= r, and two scaling

constants c1 , c2 , such that
target
target
?1
Exe ,ze (c?1
x, z?).
1 kH (xe , ze )) = Exe ,ze (c2 kF (xe , ze )) = kH (?
target
?1
and when ? > ?, we have Varxe ,ze (c?1
1 kH (xe , ze )) > Varxe ,ze (c2 kF (xe , ze )).

Remark. In practice, when applying kernel methods for real world applications, optimal kernel
width r is usually unknown and chosen by cross-validation or other methods. Similarly, for our
Fredholm kernel, one can also use cross-validation to choose the optimal t for kF .

5

Experiments

Using linear and Gaussian kernel for k or kH respectively, we will define three instances of the
Fredholm kernel as follows.


2
.
(1) FredLin1: k(x, z) = xT z and kH (x, z) = exp ? kx?zk
2r


kx?zk2
T
(2) FredLin2: k(x, z) = exp ? 2r
and kH (x, z) = x z.


2
(3) FredGauss: k(x, z) = kH (x, z) = exp ? kx?zk
.
2r
For the kernels in (2) and (3) that use the Gaussian kernel as outside
kernel k we can also define their normalized version, which we will
denote by by FredLin2(N) and FredGauss(N) respectively.
2

1.5

1

0.5

0

?0.5

?1

Synthetic examples. Noise and cluster assumptions.

?1.5

?2
?1

To isolate the ability of Fredholm kernels to deal with noise from
the cluster assumption, we construct two synthetic examples that
violate the cluster assumption, shown in Figure 2. The figures show
first two dimensions, with multi-variate Gaussian noise with variance ? 2 = 0.01 in R100 added. The classification boundaries are
indicated by the color. For each class, we provide several labeled
points and large amount of unlabeled data. Note that the classification boundary in the ?circle? example is non-linear.

?0.8

?0.6

?0.4

?0.2

0

0.2

0.4

0.6

0.8

1

1.5

1

0.5

0

?0.5

?1

?1.5

?1

?0.5

0

0.5

1

1.5

We compare Fredholm kernel based classifier with RLSC (Regularized Least Squares Classifier), and two widely used semisupervised methods, the transductive support vector machine and
Noise but not
LapRLSC. Since the examples violate the cluster assumption, the Figure 2:
cluster
assumption.
Gaussian
two existing semi-supervised learning algorithms, Transductive
100
noise
in
R
is
added.
Linear
SVM and LapRLSC, should not gain much from the unlabeled data.
For TSVM, we use the primal TSVM proposed in [4], and we will (above) and non-linear (beuse the implementation of LapRLSC given in [1]. Different num- low) class boundaries.
bers of labeled points are given for each class, together with another
2000 unlabeled points. To choose the optimal parameters for each method, we pick the parameters
based on their performance on the validation set, while the final classification error is computed on
the held-out testing data set. Results are reported in Table 1 and 2, in which Fredholm kernels show
clear improvement over other methods for synthetic examples in term of classification error.
Real-world Data Sets. Unlike artificial examples, it is usually difficult to verify whether certain
assumptions are satisfied in real-world problems. In this section, we examine the performance of
Fredholm kernels on several real-world data sets and compare it with the baseline algorithms mentioned above.
Linear Kernels. Here we consider text categorization and sentiment analysis, where linear methods
are known to perform well. We use the following data (represented by TF-IDF features):
(1) 20 news group: it has 11269 documents with 20 classes, and we select the first 10 categories
for our experiment. (2) Webkb: the original data set contains 7746 documents with 7 unbalanced
classes, and we pick the two largest classes with 1511 and 1079 instances respectively. (3) IMDB
movie review: it has 1000 positive reviews and 1000 negative reviews of movie on IMDB.com. (4)
Twitter sentiment data from Sem-Eval 2013: it contains 5173 tweets, with positive, neural and negative sentiment. We combine neutral and negative classes to set up a binary classification problem.
Results are reported in Table 3. In Table4, we use WebKB as an example to illustrate the change of
the performance as number of labeled points increases.
7

Number
of Labeled
8
16
32

RLSC
10.0(? 3.9)
9.1(? 1.9)
5.8(? 3.2)

TSVM
5.2(? 2.2)
5.1(? 1.1)
4.5(? 0.8)

Methods(Linear)
LapRLSC
FredLin1
10.0(? 3.5) 3.7(? 2.6)
9.1(? 2.2) 2.9(? 2.0)
6.0(? 3.2) 2.3(? 2.3)

FredLin2(N)
4.5(? 2.1)
3.6(? 1.9)
2.6(? 2.2)

Table 1: Prediction error of different classifiers for the?two lines? example.
Number
of Labeled
16
32
64

K-RLSC
17.4(? 5.0)
16.5(? 7.1)
8.7(? 1.7)

Methods(Gaussian)
TSVM
LapRLSC
32.2(? 5.2) 17.0(? 4.6)
29.9(? 9.3) 18.0(? 6.8)
20.3(? 4.2) 9.7(? 2.0)

FredGauss(N)
7.1(? 2.4)
6.0(? 1.6)
5.5(? 0.7)

Table 2: Prediction error of different classifiers for the ?circle? example.
Gaussian Kernel. We test our methods on hand-written digit recognition. The experiment use
subsets of two handwriting digits data sets MNIST and USPS: (1) the one from MNIST contains
10k digits in total with balanced examples for each class, and the one for USPS is the original testing
set containing about 2k images. The pixel values are normalized to [0, 1] as features. Results are
reported in Table 5. In Table 6, we show that as we add additional Gaussian noise to MNIST data,
Fredholm kernels start to show significant improvement.
Data Set
Webkb
20news
IMDB
Twitter

RLSC
16.9(? 1.4)
22.2(? 1.0)
30.0(? 2.0)
38.7(? 1.1)

TSVM
12.7(? 0.8)
21.0(? 0.9)
20.2(? 2.6)
37.6(? 1.4)

Methods(Linear)
FredLin1
FredLin2
13.0(? 1.3) 12.0(? 1.6)
20.5 (? 0.7) 20.5 (?0.7)
19.9(? 2.3) 21.7(? 2.9)
37.4(? 1.2) 37.4(? 1.2)

FredLin2(N)
12.0(? 1.6)
20.5(? 0.7)
21.7(? 2.7)
37.5(? 1.2)

Table 3: The error of various methods on the text data sets. 20 labeled data per class are given with
rest of the data set as unlabeled points. Optimal parameter for each method are used.
Number
of Labeled
10
20
80

RLSC
20.7(? 2.4)
16.9(? 1.4)
10.9(? 1.4)

TSVM
13.5(? 0.5)
12.7(? 0.8)
9.7(? 1.0)

Methods(Linear)
FredLin1
FredLin2
14.8(? 2.4) 14.6(? 2.4)
13.0(? 1.3) 12.0(? 1.6)
8.1(? 1.0)
7.9(? 0.9)

FredLin2(N)
14.6(? 2.3)
12.0(? 1.6)
7.9(? 0.9)

Table 4: Prediction error on Webkb with different number of labeled points.
Data Set
USPST
MNIST

K-RLSC
11.8(? 1.4)
14.3(? 1.2)

Methods(Gaussian)
LapRLSC
FredGauss
10.2 (?0.5) 12.4(? 1.8)
8.6(? 1.2)
12.2(?1.0)

FredGauss(N)
10.8(? 1.1)
13.0(? 0.9)

Table 5: Prediction error of nonlinear classifiers on the MNIST and USPS. 20 labeled data per class
are given with rest of the data set as unlabeled points. Optimal parameter for each method are used.
Number
of Labeled
10
20
40
80

K-RLSC
34.1(? 2.1)
27.2(? 1.1)
20.0(? 0.7)
15.6(? 0.4)

Methods(Gaussian)
LapRLSC
FredGauss
35.6 (?3.5) 27.9(? 1.6)
27.3 (?1.8) 21.9(? 1.2)
20.3 (?0.8) 17.3(? 0.5)
15.6 (?0.5) 14.8(? 0.6)

FredGauss(N)
29.0(? 1.5)
22.9(? 1.2)
18.4(? 0.4)
15.4(? 0.5)

Table 6: The prediction error of nonlinear classifiers on MNIST corrupted with Gaussian noise with
standard deviation 0.3, with different numbers of labeled points, from 10 to 80. Optimal parameter
for each method are used.
Acknowledgments. The work was partially supported by NSF Grants CCF-1319406 and RI
1117707. We thank the anonymous NIPS reviewers for insightful comments.

8

References
[1] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric
framework for learning from labeled and unlabeled examples. Journal of Machine Learning
Research, 7:2399?2434, 2006.
[2] Oliver Chapelle, Bernhard Sch?olkopf, and Alexander Zien, editors. Semi-Supervised Learning.
MIT Press, Cambridge, MA, 2006.
[3] Oliver Chapelle, Jason Weston, and Bernhard Sch?olkopf. Cluster kernels for semi-supervised
learning. In Advances in Neural Information Processing Systems 17, pages 585?592, 2003.
[4] Oliver Chapelle and Alexander Zien. Semi-supervised classification by low density separation.
In Robert G. Cowell and Zoubin Ghahramani, editors, AISTATS, pages 57?64, 2005.
[5] Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, and
Bernhard Sch?olkopf. Covariate shift by kernel mean matching. Dataset shift in machine
learning, pages 131?160, 2009.
[6] S. Gr?unew?alder, G. Lever, L. Baldassarre, S. Patterson, A. Gretton, and M. Pontil. Conditional mean embeddings as regressors. In Proceedings of the 29th International Conference on
Machine Learning, volume 2, pages 1823?1830, 2012.
[7] Steffen Grunewalder, Gretton Arthur, and John Shawe-Taylor. Smooth operators. In Proceedings of the 30th International Conference on Machine Learning, pages 1184?1192, 2013.
[8] Michiel Hazewinkel. Encyclopaedia of Mathematics, volume 4. Springer, 1989.
[9] Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Arthur Gretton, and Bernhard
Sch?olkopf. Kernel mean shrinkage estimators. arXiv preprint arXiv:1405.5505, 2014.
[10] Qichao Que and Mikhail Belkin. Inverse density as an inverse problem: the fredholm equation
approach. In Advances in Neural Information Processing Systems 26, pages 1484?1492, 2013.
[11] Bernhard Sch?olkopf and Alexander J Smola. Learning with kernels: Support vector machines,
regularization, optimization, and beyond. MIT press, 2001.
[12] John Shawe-Taylor and Nello Cristianini. Kernel methods for pattern analysis. Cambridge
university press, 2004.
[13] Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin. Beyond the point cloud: from transductive to semi-supervised learning. In Proceedings of the 22nd International Conference on
Machine Learning, pages 824?831, New York, NY, USA, 2005. ACM Press.
[14] SVN Vishwanathan, Alexander J Smola, and Ren?e Vidal. Binet-cauchy kernels on dynamical systems and its application to the analysis of dynamic scenes. International Journal of
Computer Vision, 73(1):95?119, 2007.
[15] Xiaojin Zhu. Semi-supervised learning literature survey. Technical report, Computer Science,
University of Wisconsin-Madison, 2005.

9

"
1520,2003,ARA*: Anytime A* with Provable Bounds on Sub-Optimality,Abstract Missing,"ARA*: Anytime A* with Provable Bounds on
Sub-Optimality

Maxim Likhachev, Geoff Gordon and Sebastian Thrun
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
{maxim+, ggordon, thrun}@cs.cmu.edu

Abstract
In real world planning problems, time for deliberation is often limited.
Anytime planners are well suited for these problems: they find a feasible solution quickly and then continually work on improving it until time
runs out. In this paper we propose an anytime heuristic search, ARA*,
which tunes its performance bound based on available search time. It
starts by finding a suboptimal solution quickly using a loose bound, then
tightens the bound progressively as time allows. Given enough time it
finds a provably optimal solution. While improving its bound, ARA*
reuses previous search efforts and, as a result, is significantly more efficient than other anytime search methods. In addition to our theoretical
analysis, we demonstrate the practical utility of ARA* with experiments
on a simulated robot kinematic arm and a dynamic path planning problem for an outdoor rover.

1

Introduction

Optimal search is often infeasible for real world problems, as we are given a limited amount
of time for deliberation and want to find the best solution given the time provided. In
these conditions anytime algorithms [9, 2] prove to be useful as they usually find a first,
possibly highly suboptimal, solution very fast and then continually work on improving
the solution until allocated time expires. Unfortunately, they can rarely provide bounds
on the sub-optimality of their solutions unless the cost of an optimal solution is already
known. Even less often can these algorithms control their sub-optimality. Providing suboptimality bounds is valuable, though: it allows one to judge the quality of the current
plan, decide whether to continue or preempt search based on the current sub-optimality, and
evaluate the quality of past planning episodes and allocate time for future planning episodes
accordingly. Control over the sub-optimality bounds helps in adjusting the tradeoff between
computation and plan quality.
A* search with inflated heuristics (actual heuristic values are multiplied by an inflation
factor  > 1) is sub-optimal but proves to be fast for many domains [1, 5, 8] and also provides a bound on the sub-optimality, namely, the  by which the heuristic is inflated [7].
To construct an anytime algorithm with sub-optimality bounds one could run a succession
of these A* searches with decreasing inflation factors. This naive approach results in a series of solutions, each one with a sub-optimality factor equal to the corresponding inflation

factor. This approach has control over the sub-optimality bound, but wastes a lot of computation since each search iteration duplicates most of the efforts of the previous searches.
One could try to employ incremental heuristic searches (e.g., [4]), but the sub-optimality
bounds for each search iteration would no longer be guaranteed.
To this end we propose the ARA* (Anytime Repairing A*) algorithm, which is an
efficient anytime heuristic search that also runs A* with inflated heuristics in succession
but reuses search efforts from previous executions in such a way that the sub-optimality
bounds are still satisfied. As a result, a substantial speedup is achieved by not re-computing
the state values that have been correctly computed in the previous iterations. We show the
efficiency of ARA* on two different domains. An evaluation of ARA* on a simulated robot
kinematic arm with six degrees of freedom shows up to 6-fold speedup over the succession
of A* searches. We also demonstrate ARA* on the problem of planning a path for a mobile
robot that takes into account the robot?s dynamics.
The only other anytime heuristic search known to us is Anytime A*, described in [8]. It
also first executes an A* with inflated heuristics and then continues to improve a solution.
However, the algorithm does not have control over its sub-optimality bound, except by
selecting the inflation factor of the first search. Our experiments show that ARA* is able
to decrease its bounds much more gradually and, moreover, does so significantly faster.
Another advantage of ARA* is that it guarantees to examine each state at most once during
its first search, unlike the algorithm of [8]. This property is important because it provides
a bound on the amount of time before ARA* produces its first plan. Nevertheless, as
mentioned later, [8] describes a number of very interesting ideas that are also applicable to
ARA*.

2
2.1

The ARA* Algorithm
A* with Weighted Heuristic

Normally, A* takes as input a heuristic h(s) which must be consistent. That is, h(s) ?
c(s, s0 ) + h(s0 ) for any successor s0 of s if s 6= sgoal and h(s) = 0 if s = sgoal . Here
c(s, s0 ) denotes the cost of an edge from s to s0 and has to be positive. Consistency, in
its turn, guarantees that the heuristic is admissible: h(s) is never larger than the true cost
of reaching the goal from s. Inflating the heuristic (that is, using  ? h(s) for  > 1)
often results in much fewer state expansions and consequently faster searches. However,
inflating the heuristic may also violate the admissibility property, and as a result, a solution
is no longer guaranteed to be optimal. The pseudocode of A* with inflated heuristic is
given in Figure 1 for easy comparison with our algorithm, ARA*, presented later.
A* maintains two functions from states to real numbers: g(s) is the cost of the current
path from the start node to s (it is assumed to be ? if no path to s has been found yet), and
f (s) = g(s)+?h(s) is an estimate of the total distance from start to goal going through s.
A* also maintains a priority queue, OPEN, of states which it plans to expand. The OPEN
queue is sorted by f (s), so that A* always expands next the state which appears to be on
the shortest path from start to goal. A* initializes the OPEN list with the start state, sstart
(line 02). Each time it expands a state s (lines 04-11), it removes s from OPEN. It then
updates the g-values of all of s?s neighbors; if it decreases g(s0 ), it inserts s0 into OPEN.
A* terminates as soon as the goal state is expanded.
01 g(sstart ) = 0; OPEN = ?;
02 insert sstart into OPEN with f (sstart ) =  ? h(sstart );
03 while(sgoal is not expanded)
04 remove s with the smallest f -value from OPEN;
05 for each successor s0 of s
06
if s0 was not visited before then
07
f (s0 ) = g(s0 ) = ?;
08
if g(s0 ) > g(s) + c(s, s0 )
09
g(s0 ) = g(s) + c(s, s0 );
10
f (s0 ) = g(s0 ) +  ? h(s0 );
11
insert s0 into OPEN with f (s0 );

Figure 1: A* with heuristic weighted by  ? 1

 = 2.5

 = 1.5

 = 1.0

 = 2.5

 = 1.5

 = 1.0

Figure 2: Left three columns: A* searches with decreasing . Right three columns: the corresponding
ARA* search iterations.

Setting  to 1 results in standard A* with an uninflated heuristic; the resulting solution
is guaranteed to be optimal. For  > 1 a solution can be sub-optimal, but the sub-optimality
is bounded by a factor of : the length of the found solution is no larger than  times the
length of the optimal solution [7].
The left three columns in Figure 2 show the operation of the A* algorithm with a
heuristic inflated by  = 2.5,  = 1.5, and  = 1 (no inflation) on a simple grid world. In
this example we use an eight-connected grid with black cells being obstacles. S denotes a
start state, while G denotes a goal state. The cost of moving from one cell to its neighbor
is one. The heuristic is the larger of the x and y distances from the cell to the goal. The
cells which were expanded are shown in grey. (A* can stop search as soon as it is about
to expand a goal state without actually expanding it. Thus, the goal state is not shown in
grey.) The paths found by these searches are shown with grey arrows. The A* searches with
inflated heuristics expand substantially fewer cells than A* with  = 1, but their solution is
sub-optimal.
2.2 ARA*: Reuse of Search Results
ARA* works by executing A* multiple times, starting with a large  and decreasing  prior
to each execution until  = 1. As a result, after each search a solution is guaranteed to be
within a factor  of optimal. Running A* search from scratch every time we decrease ,
however, would be very expensive. We will now explain how ARA* reuses the results of
the previous searches to save computation. We first explain the ImprovePath function (left
column in Figure 3) that recomputes a path for a given . In the next section we explain the
Main function of ARA* (right column in Figure 3) that repetitively calls the ImprovePath
function with a series of decreasing s.
Let us first introduce a notion of local inconsistency (we borrow this term from [4]). A
state is called locally inconsistent every time its g-value is decreased (line 09, Figure 1) and
until the next time the state is expanded. That is, suppose that state s is the best predecessor
for some state s0 : that is, g(s0 ) = mins00 ?pred(s0 ) (g(s00 )+c(s00 , s0 )) = g(s)+c(s, s0 ). Then,
if g(s) decreases we get g(s0 ) > mins00 ?pred(s0 ) (g(s00 ) + c(s00 , s0 )). In other words, the
decrease in g(s) introduces a local inconsistency between the g-value of s and the g-values
of its successors. Whenever s is expanded, on the other hand, the inconsistency of s is
corrected by re-evaluating the g-values of the successors of s (line 08-09, Figure 1). This
in turn makes the successors of s locally inconsistent. In this way the local inconsistency
is propagated to the children of s via a series of expansions. Eventually the children no
longer rely on s, none of their g-values are lowered, and none of them are inserted into
the OPEN list. Given this definition of local inconsistency it is clear that the OPEN list
consists of exactly all locally inconsistent states: every time a g-value is lowered the state
is inserted into OPEN, and every time a state is expanded it is removed from OPEN until
the next time its g-value is lowered. Thus, the OPEN list can be viewed as a set of states
from which we need to propagate local inconsistency.
A* with a consistent heuristic is guaranteed not to expand any state more than once.
Setting  > 1, however, may violate consistency, and as a result A* search may re-expand
states multiple times. It turns out that if we restrict each state to be expanded no more
than once, then the sub-optimality bound of  still holds. To implement this restriction we
check any state whose g-value is lowered and insert it into OPEN only if it has not been
previously expanded (line 10, Figure 3). The set of expanded states is maintained in the
CLOSED variable.

procedure fvalue(s)
01 return g(s) +  ? h(s);

procedure Main()
01? g(sgoal ) = ?; g(sstart ) = 0;
02? OPEN = CLOSED = INCONS = ?;
procedure ImprovePath()
03? insert sstart into OPEN with fvalue(sstart );
02 while(fvalue(sgoal ) > mins?OPEN (fvalue(s))) 04? ImprovePath();
03 remove s with the smallest fvalue(s) from OPEN; 05? 0 = min(, g(sgoal )/ mins?OPEN?INCONS (g(s)+h(s)));
04 CLOSED = CLOSED ? {s};
06? publish current 0 -suboptimal solution;
05 for each successor s0 of s
07? while 0 > 1
0
06
if s was not visited before then
08? decrease ;
07
g(s0 ) = ?;
09? Move states from INCONS into OPEN;
0
0
08
if g(s ) > g(s) + c(s, s )
10? Update the priorities for all s ? OPEN according to fvalue(s);
0
0
09
g(s ) = g(s) + c(s, s );
11? CLOSED = ?;
0
10
if s 6? CLOSED
12? ImprovePath();
11
insert s0 into OPEN with fvalue(s0 );
13? 0 = min(, g(sgoal )/ mins?OPEN?INCONS (g(s)+h(s)));
12
else
14? publish current 0 -suboptimal solution;
13
insert s0 into INCONS;

Figure 3: ARA*

With this restriction we will expand each state at most once, but OPEN may no longer
contain all the locally inconsistent states. In fact, it will only contain the locally inconsistent
states that have not yet been expanded. It is important, however, to keep track of all the
locally inconsistent states as they will be the starting points for inconsistency propagation
in the future search iterations. We do this by maintaining the set INCONS of all the locally
inconsistent states that are not in OPEN (lines 12-13, Figure 3). Thus, the union of INCONS
and OPEN is exactly the set of all locally inconsistent states, and can be used as a starting
point for inconsistency propagation before each new search iteration.
The only other difference between the ImprovePath function and A* is the termination
condition. Since the ImprovePath function reuses search efforts from the previous executions, sgoal may never become locally inconsistent and thus may never be inserted into
OPEN. As a result, the termination condition of A* becomes invalid. A* search, however,
can also stop as soon as f (sgoal ) is equal to the minimal f -value among all the states on
OPEN list. This is the condition that we use in the ImprovePath function (line 02, Figure 3). It also allows us to avoid expanding sgoal as well as possibly some other states
with the same f -value. (Note that ARA* no longer maintains f -values as variables since in
between the calls to the ImprovePath function  is changed, and it would be prohibitively
expensive to update the f -values of all the states. Instead, the fvalue(s) function is called
to compute and return the f -values only for the states in OPEN and sgoal .)
2.3

ARA*: Iterative Execution of Searches

We now introduce the main function of ARA* (right column in Figure 3) which performs a
series of search iterations. It does initialization and then repetitively calls the ImprovePath
function with a series of decreasing s. Before each call to the ImprovePath function a
new OPEN list is constructed by moving into it the contents of the set INCONS. Since
OPEN list has to be sorted by the current f -values of states it is also re-ordered (lines 09?10?, Figure 3). Thus, after each call to the ImprovePath function we get a solution that is
sub-optimal by at most a factor of .
As suggested in [8] a sub-optimality bound can also be computed as the ratio between
g(sgoal ), which gives an upper bound on the cost of an optimal solution, and the minimum
un-weighted f -value of a locally inconsistent state, which gives a lower bound on the cost
of an optimal solution. (This is a valid sub-optimality bound as long as the ratio is larger
than or equal to one. Otherwise, g(sgoal ) is already equal to the cost of an optimal solution.)
Thus, the actual sub-optimality bound for ARA* is computed as the minimum between 
and the ratio (lines 05? and 13?, Figure 3). At first, one may also think of using this actual
sub-optimality bound in deciding how to decrease  between search iterations (e.g., setting
 to 0 minus a small delta). Experiments, however, seem to suggest that decreasing  in
small steps is still more beneficial. The reason is that a small decrease in  often results
in the improvement of the solution, despite the fact that the actual sub-optimality bound of
the previous solution was already substantially less than the value of . A large decrease in
, on the other hand, may often result in the expansion of too many states during the next
search. (Another useful suggestion from [8], which we have not implemented in ARA*, is
to prune OPEN so that it never contains a state whose un-weighted f -value is larger than

or equal to g(sgoal ).)
Within each execution of the ImprovePath function we mainly save computation by
not re-expanding the states which were locally consistent and whose g-values were already
correct before the call to ImprovePath (Theorem 2 states this more precisely). For example,
the right three columns in Figure 2 show a series of calls to the ImprovePath function.
States that are locally inconsistent at the end of an iteration are shown with an asterisk.
While the first call ( = 2.5) is identical to the A* call with the same , the second call
to the ImprovePath function ( = 1.5) expands only 1 cell. This is in contrast to 15 cells
expanded by A* search with the same . For both searches the sub-optimality factor, ,
decreases from 2.5 to 1.5. Finally, the third call to the ImprovePath function with  set to
1 expands only 9 cells. The solution is now optimal, and the total number of expansions
is 23. Only 2 cells are expanded more than once across all three calls to the ImprovePath
function. Even a single optimal search from scratch expands 20 cells.
2.4 Theoretical Properties of the Algorithm
We now present some of the theoretical properties of ARA*. For the proofs of these and
other properties of the algorithm please refer to [6]. We use g ? (s) to denote the cost of an
optimal path from sstart to s. Let us also define a greedy path from sstart to s as a path
that is computed by tracing it backward as follows: start at s, and at any state si pick a state
si?1 = arg mins0 ?pred(si ) (g(s0 ) + c(s0 , si )) until si?1 = sstart .
Theorem 1 Whenever the ImprovePath function exits, for any state s with f (s) ?
mins0 ?OPEN (f (s0 )), we have g ? (s) ? g(s) ?  ? g ? (s), and the cost of a greedy path
from sstart to s is no larger than g(s).
The correctness of ARA* follows from this theorem: each execution of the ImprovePath function terminates when f (sgoal ) is no larger than the minimum f -value in
OPEN, which means that the greedy path from start to goal that we have found is within a
factor  of optimal. Since before each iteration  is decreased, and it, in its turn, is an upper
bound on 0 , ARA* gradually decreases the sub-optimality bound and finds new solutions
to satisfy the bound.
Theorem 2 Within each call to ImprovePath() a state is expanded at most once and only
if it was locally inconsistent before the call to ImprovePath() or its g-value was lowered
during the current execution of ImprovePath().
The second theorem formalizes where the computational savings for ARA* search
come from. Unlike A* search with an inflated heuristic, each search iteration in ARA*
is guaranteed not to expand states more than once. Moreover, it also does not expand states
whose g-values before a call to the ImprovePath function have already been correctly computed by some previous search iteration, unless they are in the set of locally inconsistent
states already and thus need to update their neighbors (propagate local inconsistency).

3

Experimental Study

3.1 Robotic Arm
We first evaluate the performance of ARA* on simulated 6 and 20 degree of freedom (DOF)
robotic arms (Figure 4). The base of the arm is fixed, and the task is to move its end-effector
to the goal while navigating around obstacles (indicated by grey rectangles). An action
is defined as a change of a global angle of any particular joint (i.e., the next joint further
along the arm rotates in the opposite direction to maintain the global angle of the remaining
joints.) We discretitize the workspace into 50 by 50 cells and compute a distance from each
cell to the cell containing the goal while taking into account that some cells are occupied
by obstacles. This distance is our heuristic. In order for the heuristic not to overestimate
true costs, joint angles are discretitized so as to never move the end-effector by more than
one cell in a single action. The resulting state-space is over 3 billion states for a 6 DOF
robot arm and over 1026 states for a 20 DOF robot arm, and memory for states is allocated
on demand.

(a) 6D arm trajectory for  = 3

(b) uniform costs

(c) non-uniform costs

(d) both Anytime A* and A*
(e) ARA*
(f) non-uniform costs
after 90 secs, cost=682, 0 =15.5 after 90 secs, cost=657, 0 =14.9
Figure 4: Top row: 6D robot arm experiments. Bottom row: 20D robot arm experiments (the
trajectories shown are downsampled by 6). Anytime A* is the algorithm in [8].

Figure 4a shows the planned trajectory of the robot arm after the initial search of ARA*
with  = 3.0. This search takes about 0.05 secs. (By comparison, a search for an optimal
trajectory is infeasible as it runs out of memory very quickly.) The plot in Figure 4b shows
that ARA* improves both the quality of the solution and the bound on its sub-optimality
faster and in a more gradual manner than either a succession of A* searches or Anytime
A* [8]. In this experiment  is initially set to 3.0 for all three algorithms. For all the experiments in this section  is decreased in steps of 0.02 (2% sub-optimality) for ARA* and
a succession of A* searches. Anytime A* does not control , and in this experiment it
apparently performs a lot of computations that result in a large decrease of  at the end. On
the other hand, it does reach the optimal solution first this way. To evaluate the expense of
the anytime property of ARA* we also ran ARA* and an optimal A* search in a slightly
simpler environment (for the optimal search to be feasible). Optimal A* search required
about 5.3 mins (2,202,666 state expanded) to find an optimal solution, while ARA* required about 5.5 mins (2,207,178 state expanded) to decrease  in steps of 0.02 from 3.0
until a provably optimal solution was found (about 4% overhead).
While in the experiment for Figure 4b all the actions have the same cost, in the experiment for Figure 4c actions have non-uniform costs: changing a joint angle closer to the
base is more expensive than changing a higher joint angle. As a result of the non-uniform
costs our heuristic becomes less informative, and so search is much more expensive. In
this experiment we start with  = 10, and run all algorithms for 30 minutes. At the end,
ARA* achieves a solution with a substantially smaller cost (200 vs. 220 for the succession
of A* searches and 223 for Anytime A*) and a better sub-optimality bound (3.92 vs. 4.46
for both the succession of A* searches and Anytime A*). Also, since ARA* controls  it
decreases the cost of the solution gradually. Reading the graph differently, ARA* reaches
a sub-optimality bound 0 = 4.5 after about 59 thousand expansions and 11.7 secs, while
the succession of A* searches reaches the same bound after 12.5 million expansions and
27.4 minutes (about 140-fold speedup by ARA*) and Anytime A* reaches it after over 4
million expansions and 8.8 minutes (over 44-fold speedup by ARA*). Similar results hold
when comparing the amount of work each of the algorithms spend on obtaining a solution
of cost 225. While Figure 4 shows execution time, the comparison of states expanded (not
shown) is almost identical. Additionally, to demonstrate the advantage of ARA* expanding
each state no more than once per search iteration, we compare the first searches of ARA*
and Anytime A*: the first search of ARA* performed 6,378 expansions, while Anytime
A* performed 8,994 expansions, mainly because some of the states were expanded up to

(a) robot with laser scanner

(b) 3D Map

(c) optimal 2D search

(d) optimal 4D search with A*
(e) 4D search with ARA*
(f) 4D search with ARA*
after 25 secs
after 0.6 secs ( = 2.5)
after 25 secs ( = 1.0)
Figure 5: outdoor robot navigation experiment (cross shows the position of the robot)

seven times before a first solution was found.
Figures 4d-f show the results of experiments done on a 20 DOF robot arm, with actions
that have non-uniform costs. All three algorithms start with  = 30. Figures 4d and 4e
show that in 90 seconds of planning the cost of the trajectory found by ARA* and the suboptimality bound it can guarantee is substantially smaller than for the other algorithms. For
example, the trajectory in Figure 4d contains more steps and also makes one extra change
in the angle of the third joint from the base of the arm (despite the fact that changing lower
joint angles is very expensive) in comparison to the trajectory in Figure 4e. The graph in
Figure 4f compares the performance of the three algorithms on twenty randomized environments similar to the environment in Figure 4d. The environments had random goal locations, and the obstacles were slid to random locations along the outside walls. The graph
shows the additional time the other algorithms require to achieve the same sub-optimality
bound that ARA* does. To make the results from different environments comparable we
normalize the bound by dividing it by the maximum of the best bounds that the algorithms
achieve before they run out of memory. Averaging over all environments, the time for
ARA* to achieve the best bound was 10.1 secs. Thus, the difference of 40 seconds at the
end of the Anytime A* graph corresponds to an overhead of about a factor of 4.
3.2 Outdoor Robot Navigation
For us the motivation for this work was efficient path-planning for mobile robots in large
outdoor environments, where optimal trajectories involve fast motion and sweeping turns
at speed. In such environments it is particularly important to take advantage of the robot?s
momentum and find dynamic rather than static plans. We use a 4D state space: xy position,
orientation, and velocity. High dimensionality and large environments result in very large
state-spaces for the planner and make it computationally infeasible for the robot to plan
optimally every time it discovers new obstacles or modelling errors. To solve this problem
we built a two-level planner: a 4D planner that uses ARA*, and a fast 2D (x, y) planner
that uses A* search and whose results serve as the heuristic for the 4D planner.1
1

To interleave search with the execution of the best plan so far we perform 4D search backward.
That is, the start of the search, sstart , is the actual goal state of the robot, while the goal of the search,
sgoal , is the current state of the robot. Thus, sstart does not change as the robot moves and the search
tree remains valid in between search iterations. Since heuristics estimate the distances to sgoal (the
robot position) we have to recompute them during the reorder operation (line 10?, Figure 3).

In Figure 5 we show the robot we used for navigation and a 3D laser scan [3] constructed by the robot of the environment we tested our system in. The scan is converted
into a map of the environment (Figure 5c, obstacles shown in black). The size of the environment is 91.2 by 94.4 meters, and the map is discretitized into cells of 0.4 by 0.4 meters.
Thus, the 2D state-space consists of 53808 states. The 4D state space has over 20 million
states. The robot?s initial state is the upper circle, while its goal is the lower circle. To
ensure safe operation we created a buffer zone with high costs around each obstacle. The
squares in the upper-right corners of the figures show a magnified fragment of the map with
grayscale proportional to cost. The 2D plan (Figure 5c) makes sharp 45 degree turns when
going around the obstacles, requiring the robot to come to complete stops. The optimal
4D plan results in a wider turn, and the velocity of the robot remains high throughout the
whole trajectory. In the first plan computed by ARA* starting at  = 2.5 (Figure 5e) the
trajectory is much better than the 2D plan, but somewhat worse than the optimal 4D plan.
The time required for the optimal 4D planner was 11.196 secs, whereas the time for
the 4D ARA* planner to generate the plan in Figure 5e was 556ms. As a result, the robot
that runs ARA* can start executing its plan much earlier. A robot running the optimal
4D planner would still be near the beginning of its path 25 seconds after receiving a goal
location (Figure 5d). In contrast, in the same amount of time the robot running ARA* has
advanced much further (Figure 5f), and its plan by now has converged to optimal ( has
decreased to 1).

4

Conclusions

We have presented the first anytime heuristic search that works by continually decreasing
a sub-optimality bound on its solution and finding new solutions that satisfy the bound on
the way. It executes a series of searches with decreasing sub-optimality bounds, and each
search tries to reuse as much as possible of the results from previous searches. The experiments show that our algorithm is much more efficient than any of the previous anytime
searches, and can successfully solve large robotic planning problems.
Acknowledgments
This work was supported by AFRL contract F30602?01?C?0219, DARPA?s MICA program.

References
[1] B. Bonet and H. Geffner. Planning as heuristic search. Artificial Intelligence, 129(12):5?33, 2001.
[2] T. L. Dean and M. Boddy. An analysis of time-dependent planning. In Proc. of the
National Conference on Artificial Intelligence (AAAI), 1988.
[3] D. Haehnel. Personal communication, 2003.
[4] S. Koenig and M. Likhachev. Incremental A*. In Advances in Neural Information
Processing Systems (NIPS) 14. Cambridge, MA: MIT Press, 2002.
[5] R. E. Korf. Linear-space best-first search. Artificial Intelligence, 62:41?78, 1993.
[6] M. Likhachev, G. Gordon, and S. Thrun. ARA*: Formal Analysis. Tech. Rep. CMUCS-03-148, Carnegie Mellon University, Pittsburgh, PA, 2003.
[7] J. Pearl. Heuristics: Intelligent Search Strategies for Computer Problem Solving.
Addison-Wesley, 1984.
[8] R. Zhou and E. A. Hansen. Multiple sequence alignment using A*. In Proc. of the
National Conference on Artificial Intelligence (AAAI), 2002. Student abstract.
[9] S. Zilberstein and S. Russell. Approximate reasoning using anytime algorithms. In
Imprecise and Approximate Computation. Kluwer Academic Publishers, 1995.

"
100,1995,Beating a Defender in Robotic Soccer: Memory-Based Learning of a Continuous Function,Abstract Missing,"Beating a Defender in Robotic Soccer:
Memory-Based Learning of a Continuous
FUnction
Peter Stone
Department of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213

Manuela Veloso
Department of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213

Abstract
Learning how to adjust to an opponent's position is critical to
the success of having intelligent agents collaborating towards the
achievement of specific tasks in unfriendly environments. This paper describes our work on a Memory-based technique for to choose
an action based on a continuous-valued state attribute indicating
the position of an opponent. We investigate the question of how an
agent performs in nondeterministic variations of the training situations. Our experiments indicate that when the random variations
fall within some bound of the initial training, the agent performs
better with some initial training rather than from a tabula-rasa.

1

Introduction

One of the ultimate goals subjacent to the development of intelligent agents is to
have multiple agents collaborating in the achievement of tasks in the presence of
hostile opponents. Our research works towards this broad goal from a Machine
Learning perspective. We are particularly interested in investigating how an intelligent agent can choose an action in an adversarial environment. We assume that
the agent has a specific goal to achieve. We conduct this investigation in a framework where teams of agents compete in a game of robotic soccer. The real system
of model cars remotely controlled from off-board computers is under development .
Our research is currently conducted in a simulator of the physical system.
Both the simulator and the real-world system are based closely on systems designed by the Laboratory for ComputationalIntelligence at the University of British
Columbia [Sahota et a/., 1995, Sahota, 1993]. The simulator facilitates the control
of any number of cars and a ball within a designated playing area. Care has been
taken to ensure that the simulator models real-world responses (friction, conserva-

897

Memory-based Learning of a Continuous Function

tion of momentum, etc.) as closely as possible. Figure l(a) shows the simulator
graphics.

IJ

-

0

(j

<0

-

~

<:P
(a)

I

IJ

-

~

(b)

Figure 1: (a) the graphic view of our simulator. (b) The initial position for all
of the experiments in this paper. The teammate (black) remains stationary, the
defender (white) moves in a small circle at different speeds, and the ball can move
either directly towards the goal or towards the teammate . The position of the ball
represents the position of the learning agent.
We focus on the question of learning to choose among actions in the presence of
an adversary. This paper describes our work on applying memory-based supervised
learning to acquire strategy knowledge that enables an agent to decide how to
achieve a goal. For other work in the same domain, please see [Stone and Veloso ,
1995b]. For an extended discussion of other work on incremental and memorybased learning [Aha and Salzberg, 1994 , Kanazawa, 1994, Kuh et al., 1991, Moore,
1991, Salganicoff, 1993, Schlimmer and Granger , 1986, Sutton and Whitehead , 1993,
Wettschereck and Dietterich, 1994, Winstead and Christiansen, 1994], particularly
as it relates to this paper, please see [Stone and Veloso, 1995a].
The input to our learning task includes a continuous-valued range of the position
of the adversary. This raises the question of how to discretize the space of values
into a set of learned features . Due to the cost of learning and reusing a large set of
specialized instances, we notice a clear advantage to having an appropriate degree
of generalization . For more details please see [Stone and Veloso, 1995a].
Here , we address the issue of the effect of differences between past episodes and the
current situation. We performed extensive experiments, training the system under
particular conditions and then testing it (with learning continuing incrementally) in
nondeterministic variations of the training situation. Our results show that when
the random variations fall within some bound of the initial training , the agent
performs better with some initial training rather than from a tabula-rasa. This
intuitive fact is interestingly well- supported by our empirical results.

2

Learning Method

The learning method we develop here applies to an agent trying to learn a function
with a continuous domain. We situate the method in the game of robotic soccer.
We begin each trial by placing a ball and a stationary car acting as the ""teammate""
in specific places on the field. Then we place another car, the ""defender,"" in front of
the goal. The defender moves in a small circle in front of the goal at some speed and
begins at some random point along this circle. The learning agent must take one
of two possible actions: shoot straight towards the goal, or pass to the teammate so

898

P. STONE, M. VELOSO

that the ball will rebound towards the goal. A snapshot of the experimental setup
is shown graphically in Figure 1(b).
The task is essentially to learn two functions, each with one continuous input variable, namely the defender's position. Based on this position, which can be represented unambiguously as the angle at which the defender is facing, ?, the agent tries
to learn the probability of scoring when shooting , Ps* (?), and the probability of scor(? ).1 If these functions were learned completely, which would
ing when passing ,
only be possible if the defender's motion were deterministic, then both functions
would be binary partitions: Ps*, P; : [0 .0,360.0) f--.+ {-1 , I}. 2 That is, the agent
would know without doubt for any given ? whether a shot, a pass, both, or neither
would achieve its goal. However, since the agent cannot have had experience for
every possible ?, and since the defender may not move at the same speed each time,
the learned functions must be approximations: Ps,Pp : [0 .0,360.0) f--.+ [-1.0 , 1.0] .

P;

In order to enable the agent to learn approximations to the functions Ps* and P*,
we gave it a memory in which it could store its experiences and from which it coufd
retrieve its current approximations Ps(?) and Pp( ?). We explored and developed
appropriate methods of storing to and retrieving from memory and an algorithm
for deciding what action to take based on the retrieved values.

2.1

Memory Model

Storing every individual experience in memory would be inefficient both in terms
of amount of memory required and in terms of generalization time. Therefore, we
store Ps and Pp only at discrete , evenly-spaced values of ?. That is, for a memory
of size M (with M dividing evenly into 360 for simplicity), we keep values of Pp(O)
and Ps(O) for 0 E {360n/M I 0 ~ n < M}. We store memory as an array ""Mem""
of size M such that Mem[n] has values for both Pp(360n/M) and Ps(360n/M) .
Using a fixed memory size precludes using memory-based techniques such as KNearest-Neighbors (kNN) and kernel regression which require that every experience
be stored, choosing the most relevant only at decision time. Most of our experiments
were conducted with memories of size 360 (low generalization) or of size 18 (high
generalization), i.e. M = 18 or M = 360. The memory size had a large effect on
the rate of learning [Stone and Veloso, 1995a].

2.1.1

Storing to Memory

With M discrete memory storage slots, the problem then arises as to how a specific
training example should be generalized. Training examples are represented here as
E.p,a,r, consisting of an angle ?, an action a, and a result r where ? is the initial
position of the defender, a is ""s"" or ""p"" for ""shoot"" or ""pass,"" and r is ""I"" or
""-I"" for ""goal"" or "" miss"" respectively. For instance, E 72 .345 ,p ,1 represents a pass
resulting in a goal for which the defender started at position 72.345 0 on its circle.
Each experience with 0 - 360/2M :::; ? < 0 + 360/2M affects Mem[O] in proportion to the distance 10 - ?I. In particular, Mem[O] keeps running sums of the
magnitudes of scaled results, Mem[O]. total-a-results, and of scaled positive results,
Mem[O].positive-a-results, affecting Pa(O), where ""a"" stands for ""s"" or ""p"" as before. Then at any given time , Pa (0) = -1 + 2 * positive-a-results
total-a-r esults . The ""-I"" is for
per convention, P * represents the target (optimal) function.
we think of
and
as functions from angles to probabilities, we will use
-1 rather than 0 as the lower bound of the range. This representation simplifies many of
our illustrative calculations.
1 As

2 Although

P;

P;

899

Memory-based Learning of a Continuous Function

the lower bound of our probability range, and the ""2*"" is to scale the result to this
range. Call this our adaptive memory storage technique:
Adaptive Memory Storage of
I _
(1 _ 14>-01 )

? r

- r

*

E4>,a,r

in Mem 0

360/M .

? Mem[O].total-a-results += r'o
? If r' > 0 Then Mem[O].positive-a-results
? P (0) = -1 + 2 * posittve-a-results.
a

+=

r'o

total-a-resuLts

For example, EllO,p,l wOilld set both total-p-results and positive-p-results for
Mem[120] (and Mem[100]) to 0.5 and consequently Pp(120) (and Pp(100)) to 1.0.
But then E l25 ,p,-1 would increment total-p-resultsfor Mem[120] by .75, while leaving positive-p-results unchanged. Thus Pp(120) becomes -1 + 2 * 1:~5 = -.2.
This method of storing to memory is effective both for time-varying concepts and
for concepts involving random noise. It is able to deal with conflicting examples
within the range of the same memory slot.
Notice that each example influences 2 different memory locations. This memory
storage technique is similar to the kNN and kernel regression function approximation
techniques which estimate f( ?) based on f( 0) possibly scaled by the distance from
o to ? for the k nearest values of O. In our linear continuum of defender position,
our memory generalizes training examples to the 2 nearest memory locations. 3
2.1.2

Retrieving from Memory

Since individual training examples affect multiple memory locations, we use a simple
technique for retrieving Pa (?) from memory when deciding whether to shoot or to
pass. We round ? to the nearest 0 for which Mem[O] is defined, and then take Pa (0)
as the value of Pa(?). Thus, each Mem[O] represents Pa(?) for 0 - 360/2M ~ ? <
o+ 360 /2M. Notice that retrieval is much simpler when using this technique than
when using kNN or kernel regression: we look directly to the closest fixed memory
position, thus eliminating the indexing and weighting problems involved in finding
the k closest training examples and (possibly) scaling their results.
2.2

Choosing an Action

The action selection method is designed to make use of memory to select the action
most probable to succeed, and to fill memory when no useful memories are available.
For example, when the defender is at position ?, the agent begins by retrieving Pp (?)
and Ps( ?) as described above. Then, it acts according to the following function:
If Pp_(<fJ) = P.(<fJ) (no basis for a decision), shoot or pass randomly.
else If Pp(<fJ) > 0 and Pp(<fJ) > Ps(<fJ), pass.
else If P.(<fJ) > 0 and P.(<fJ) > Pp(<fJ), shoot.
else If Pp(<fJ) = 0, (no previous passes) pass.
else If P.(<fJ) = 0, (no previous shots) shoot.
else (Pp(<fJ),P.(<fJ) < 0) shoot or pass randomly.

An action is only selected based on the memory values if these values indicate that
one action is likely to succeed and that it is better than the other. If, on the other
hand, neither value Pp(?) nor Ps(?) indicate a positive likelihood of success, then
an action is chosen randomly. The only exception to this last rule is when one of
3For particularly large values of M it is useful to generalize training examples to more
memory locations, particularly at the early stages of learning. However for the values of
M considered in this paper, we always generalize to the 2 nearest memory locations.

P.STONE,M.VELOSO

900

the values is zero,4 suggesting that there has not yet been any training examples for
that action at that memory location. In this case, there is a bias towards exploring
the untried action in order to fill out memory.

3

Experiments and Results

In this section, we present the results of our experiments. We explore our agent's
ability to learn time-varying and nondeterministic defender behavior .
While examining the results, keep in mind that even if the agent used the functions
and
to decide whether to shoot or to pass, the success rate would be significantly less than 100% (it would differ for different defender speeds): there were
many defender starting positions for which neither shooting nor passing led to a
goal (see Figure 2). For example, from our experiments with the defender moving

P;

P;

I

-

-

I

D ..... ? ?
Cd)

(b)

Figure 2: For different defender starting positions (solid rectangle), the agent can
score when (a) shooting, (b) passing, (c) neither, or (d) both.
at a constant speed of 50, 5 we found that an agent acting optimally scores 73.6%
of the time; an agent acting randomly scores only 41.3% of the time. These values
set good reference points for evaluating our learning agent's performance.
3.1

Coping with Changing Concepts

Figure 3 demonstrates the effectiveness of adaptive memory when the defender's
speed changes. In all of the experiments represented in these graphs, the agent
Success Rate vs. Defender Speed: Memory Size = 360

80r-~~--~~~--~~--~

75

Success Rate vs. Defender Speed: Memory Size = 18

BOr-~~--~~~--~~~--'

75
70

~... - -"".""""':::::'"" .:.::.-~

60

60

55

55

50

.........

~._.

~

65

65

50

45

45

First 1000 trials Next 1000 trials .. - ..
TheoreHcal optimum .....

40

35L-~~--~~~--~~~~

10

20

30

40 50
60
70
Defender Speed

First 1000 trials Next 1000 trials .-..
Theoretical optimum .. ..

40
80

90

100

35L-~~--~~~--~~~~

10

20

30

40 50 60
70
Defender Speed

80

90

100

Figure 3: For all trials shown in these graphs, the agent began with a memory
trained for a defender moving at constant speed 50.
started with a memory trained by attempting a single pass and a single shot with
the defender starting at each position 0 for which Mem[O] is defined and moving in
4Recall that a memory value of 0 is equivalent to a probability of .5, representing no
reason to believe that the action will succeed or fail.
SIn the simulator, ""50"" represents 50 cm/s. Subsequently, we omit the units.

Memory-based Learning of a Continuous Function

901

its circle at speed 50. We tested the agent 's performance with the defender moving
at various (constant) speeds.
With adaptive memory, the agent is able to unlearn the training that no longer
applies and approach optimal behavior: it re-Iearns the new setup. During the first
1000 trials the agent suffers from having practiced in a different situation (especially
for the less generalized memory, M = 360) , but then it is able to approach optimal
behavior over the next 1000 trials. Remember that optimal behavior, represented
in the graph, leads to roughly a 70% success rate, since at many starting positions,
neither passing nor shooting is successful.
From these results we conclude that our adaptive memory can effectively deal with
time-varying concepts. It can also perform well when the defender 's motion is
nondeterministic, as we show next.

3.2

Coping with Noise

To model nondeterministic motion by the defender, we set the defender's speed
randomly within a range. For each attempt this speed is constant, but it varies from
attempt to attempt. Since the agent observes only the defender's initial position,
from the point of view of the agent, the defender's motion is nondeterministic.
This set of experiments was designed to test the effectiveness of adaptive memory
when the defender's speed was both nondeterministic and different from the speed
used to train the existing memory. The memory was initialized in the same way as
in Section 3.1 (for defender speed 50). We ran experiments in which the defender's
speed varied between 10 and 50. We compared an agent with trained memory
against an agent with initially empty memories as shown in Figure 4.
70

Success Rate VS . Trial #: M=18, Defender speed 10-50
r-~---.--~--~--r--.--~---.--.

55
50
No initial memory
Full initial memory - -

45
40

L-~

50

_ _- L_ _~_ _~_ _L-~_ _~_ _~~

100 150 200 250 300 350 400 450 500
Trial Number

Figure 4: A comparison of the effectiveness of starting with an empty memory
versus starting with a memory trained for a constant defender speed (50) different
from that used during testing. Success rate is measured as goal percentage thus far.
The agent with full initial memory outperformed the agent with initially empty
memory in the short run. The agent learning from scratch did better over time
since it did not have any training examples from when the defender was moving
at a fixed speed of 50; but at first, the training examples for speed 50 were better
than no training examples. Thus, when you would like to be successful immediately
upon entering a novel setting, adaptive memory allows training in related situations
to be effective without permanently reducing learning capacity.

902

4

P. STONE, M. VELOSO

Conclusion

Our experiments demonstrated that online, incremental, supervised learning can be
effective at learning functions with continuous domains. We found that adaptive
memory made it possible to learn both time-varying and nondeterministic concepts.
We empirically demonstrated that short-term performance was better when acting
with a memory trained on a concept related to but different from the testing concept, than when starting from scratch. This paper reports experimental results on
our work towards multiple learning agents, both cooperative and adversarial, III a
continuous environment.
Future work on our research agenda includes simultaneous learning of the defender
and the controlling agent in an adversarial context. We will also explore learning
methods with several agents where teams are guided by planning strategies. In
this way we will simultaneously study cooperative and adversarial situations using
reactive and deliberative reasoning.
Acknow ledgements
We thank Justin Boyan and the anonymous reviewers for their helpful suggestions. This research is
sponsored by the Wright Laboratory, Aeronautical Systems Center, Air Force Materiel Command, USAF,
and the Advanced Research Projects Agency (ARPA) under grant number F33615-93-1-1330. The views
and conclusions contained in this document are those of the authors and should not be interpreted
as necessarily representing the official policies or endorsements, either expressed or implied, of Wright
Laboratory or the U. S. Government.

References
[Aha and Salzberg, 1994] David W . Aha and Steven L. Salzberg. Learning to catch: Applying nearest
neighbor algorithms to dynamic control tasks. In P. Cheeseman and R. W. Oldford, editors, Selecttng
Models from Data : Artificial Intelhgence and StattStics IV. SpringIer-Verlag, New York, NY, 1994.
[Kanazawa, 1994] Keiji Kanazawa. Sensible decisions: Toward a theory of decision-theoretic information
invariants. In Proceedings of the Twelfth National Conference on Art~ficial Intelligence, pages 973978, 1994 .
[Kuh et al., 1991] A. Kuh, T. Petsche, and R.L. Rivest. Learning time-varying concepts. In Advances
in Neural Information Processing Systems 3 , pages 183-189. Morgan Kaufman, December 1991.
[Moore, 1991] A.W . Moore . Fast, robust adaptive control by learning only forward models. In Advances
in Neural Information Processing Systems 3. Morgan Kaufman, December 1991.
[Sahota et al., 1995] Michael K. Sahota, Alan K. Mackworth, Rod A. Barman, and Stewart J. Kingdon.
Real-time control of soccer-playing robots using off-board vision : the dynamite testbed. In IEEE
Internahonal Conference on Systems, Man, and Cybernetics, pages 3690-3663, 1995.
[Sahota, 1993] Michael K . Sahota. Real-time intelligent behaviour in dynamic environments: Soccerplaying robots. Master's thesis, University of British Columbia, August 1993.
[Salganicoff, 1993] Marcos Salganicoff. Density-adaptive learning and forgetting. In Proceedmgs of the
Tenth International Conference on Machine Learning, pages 276-283, 1993.
[Schlimmer and Granger, 1986] J.C. Schlimmer and R.H. Granger. Beyond incremental processing:
Tracking concept drift. In Proceedings of the Fiffth National Conference on Artifictal Intelligence ,
pages 502-507. Morgan Kaufman, Philadelphia, PA, 1986.
[Stone and Veloso, 1995a] Peter Stone and Manuela Veloso. Beating a defender in robotic soccer:
Memory-based learning of a continuous function. Technical Report CMU-CS-95-222, Computer Science Department, Carnegie Mellon University, 1995.
[Stone and Veloso, 1995b] Peter Stone and Manuela Veloso. Broad learning from narrow training: A case
study in robotic soccer. Technical Report CMU-CS-95-207, Computer Science Department, Carnegie
Mellon University, 1995.
[Sutton and Whitehead, 1993] Richard S. Sutton and Steven D. Whitehead. Online learning with random representations. In ProceedIngs of the Tenth International Conference on Machine Learnmg,
pages 314-321, 1993.
[Wettschereck and Dietterich, 1994] Dietrich Wettschereck and Thomas Dietterich. Locally adaptive
nearest neighbor algorithms. In J. D . Cowan, G. Tesauro, and J. Alspector, editors, Advances in
Neural Informatton Processing Systems 6, pages 184-191, San Mateo, CA, 1994. Morgan Kaufmann.
[Winstead and Christiansen, 1994] Nathaniel S. Winstead and Alan D. Christiansen. Pinball: Planning
and learning in a dynamic real-time environment. In AAAI-9-4 Fall Symposium on Control of the
Physical World by Intelligent Agents, pages 153-157, New Orleans, LA, November 1994.

"
1235,2001,The Concave-Convex Procedure (CCCP),Abstract Missing,"The Concave-Convex Procedure (CCCP)

A. L. Yuille and Anand Rangarajan *
Smith-Kettlewell Eye Research Institute,
2318 Fillmore Street,
San Francisco, CA 94115, USA.
Tel. (415) 345-2144. Fax. (415) 345-8455.
Email yuille@ski.org

* Prof. Anand Rangarajan. Dept. of CISE, Univ. of Florida Room 301, CSE
Building Gainesville, FL 32611-6120 Phone: (352) 392 1507 Fax: (352) 392 1220
e-mail: anand@cise.ufl.edu

Abstract
We introduce the Concave-Convex procedure (CCCP) which constructs discrete time iterative dynamical systems which are guaranteed to monotonically decrease global optimization/energy functions. It can be applied to (almost) any optimization problem and
many existing algorithms can be interpreted in terms of CCCP. In
particular, we prove relationships to some applications of Legendre
transform techniques. We then illustrate CCCP by applications to
Potts models, linear assignment, EM algorithms, and Generalized
Iterative Scaling (GIS). CCCP can be used both as a new way to
understand existing optimization algorithms and as a procedure for
generating new algorithms.

1

Introduction

There is a lot of interest in designing discrete time dynamical systems for inference
and learning (see, for example, [10], [3], [7], [13]).
This paper describes a simple geometrical Concave-Convex procedure (CCCP) for
constructing discrete time dynamical systems which can be guaranteed to decrease
almost any global optimization/energy function (see technical conditions in section (2)).
We prove that there is a relationship between CCCP and optimization techniques
based on introducing auxiliary variables using Legendre transforms. We distinguish
between Legendre min-max and Legendre minimization. In the former, see [6], the
introduction of auxiliary variables converts the problem to a min-max problem
where the goal is to find a saddle point. By contrast, in Legendre minimization, see
[8], the problem remains a minimization one (and so it becomes easier to analyze

convergence). CCCP relates to Legendre minimization only and gives a geometrical
perspective which complements the algebraic manipulations presented in [8].
CCCP can be used both as a new way to understand existing optimization algorithms and as a procedure for generating new algorithms. We illustrate this by
giving examples from Potts models, EM, linear assignment, and Generalized Iterative Scaling. Recently, CCCP has also been used to construct algorithms to
minimize the Bethe/Kikuchi free energy [13].
We introduce CCCP in section (2) and relate it to Legendre transforms in section (3). Then we give examples in section (4).

2

The Concave-Convex Procedure (CCCP)

The key results of CCCP are summarized by Theorems 1,2, and 3.
Theorem 1 shows that any function , subject to weak conditions, can be expressed
as the sum of a convex and concave part (this decomposition is not unique). This
implies that CCCP can be applied to (almost) any optimization problem.
Theorem 1. Let E(x) be an energy function with bounded Hessian [J2 E(x)/8x8x.
Then we can always decompose it into the sum of a convex function and a concave
function.

Proof. Select any convex function F(x) with positive definite Hessian with eigenvalues bounded below by f > o. Then there exists a positive constant A such that
the Hessian of E(x) + AF(x) is positive definite and hence E(x) + AF(x) is convex. Hence we can express E(x) as the sum of a convex part, E(x) + AF(x) , and a
concave part -AF(x).

Figure 1: Decomposing a function into convex and concave parts. The original function (Left Panel) can be expressed as the sum of a convex function (Centre Panel)
and a concave function (Right Panel). (Figure courtesy of James M. Coughlan).
Our main result is given by Theorem 2 which defines the CCCP procedure and
proves that it converges to a minimum or saddle point of the energy.
Theorem 2 . Consider an energy function E(x) (bounded below) of form E(x) =
Evex (x) + E cave (x) where Evex (x), E cave (x) are convex and concave functions of x
respectively. Then the discrete iterative CCCP algorithm ;zt f-7 ;zt+1 given by:
\1Evex
(x-t+l ) _- -\1Ecave
(x-t ),

(1)

is guaranteed to monotonically decrease the energy E(x) as a function of time and
hence to converge to a minimum or saddle point of E(x).

Proof. The convexity and concavity of Evex (.) and Ecave (.) means that Evex (X2) 2:
Evex (xd + (X2 -xd? ~ Evex (xd and Ecave (X4) :S Ecave (X3) + (X4 -X3)? ~ Ecave (X3 ),
for all X1 ,X2,X3,X4. Now set Xl = xt+l,X2 = xt,X3 = xt,X4 = xt+1. Using the
algorithm definition (i.e. ~Evex (xt+1) = -~Ecave (xt)) we find that Evex (xt+ 1) +
Ecave (xt+1) :S Evex (xt) + Ecave (xt), which proves the claim.
We can get a graphical illustration of this algorithm by the reformulation shown in
figure (2) (suggested by James M. Coughlan). Think of decomposing the energy
function E(x) into E 1(x) - E 2(x) where both E 1(x) and E 2(x) are convex. (This
is equivalent to decomposing E(x) into a a convex term E 1(x) plus a concave term
-E2(X)) . The algorithm proceeds by matching points on the two terms which have
the same tangents. For an input Xo we calculate the gradient ~ E2 (xo) and find the
point Xl such that ~ E1 (xd = ~ E2 (xo). We next determine the point X2 such that
~E1(X2) = ~E2 (X1)' and repeat.
7~------~--------~------,

60 50 40 -

30 -

20 -

o

10

O L---~=-~O-=~~~~----~
10

XO

Figure 2: A CCCP algorithm illustrated for Convex minus Convex. We want to
minimize the function in the Left Panel. We decompose it (Right Panel) into
a convex part (top curve) minus a convex term (bottom curve). The algorithm
iterates by matching points on the two curves which have the same tangent vectors,
see text for more details. The algorithm rapidly converges to the solution at x = 5.0.
We can extend Theorem 2 to allow for linear constraints on the variables X, for
example Li et Xi = aM where the {en, {aM} are constants. This follows directly
because properties such as convexity and concavity are preserved when linear constraints are imposed. We can change to new coordinates defined on the hyperplane
defined by the linear constraints. Then we apply Theorem 1 in this coordinate
system.
Observe that Theorem 2 defines the update as an implicit function of xt+ 1. In many
cases, as we will show, it is possible to solve for xt+1 directly. In other cases we may
need an algorithm, or inner loop , to determine xt+1 from ~Evex (xt+1). In these
cases we will need the following theorem where we re-express CCCP in terms of
minimizing a time sequence of convex update energy functions Et+1 (xt+1) to obtain
the updates xt+1 (i.e. at the tth iteration of CCCP we need to minimize the energy
Et+1 (xt+1 )). We include linear constraints in Theorem 3.
Theorem 3. Let E(x) = Evex (x) + E cave (x) where X is required to satisfy the linear
constraints Li et Xi = aM, where the {et}, {aM} are constants. Then the update rule

for xt+1 can be formulated as minimizing a time sequence of convex update energy

functions Et+1 (;rt+1):

(2)
where the lagrange parameters P'J1} impose linear comnstraints.

Proof. Direct calculation.
The convexity of EH1 (;rt+1) implies that there is a unique minimum corresponding
to ;rt+1. This means that if an inner loop is needed to calculate ;rt+1 then we can
use standard techniques such as conjugate gradient descent (or even CCCP).

3

Legendre Transformations

The Legendre transform can be used to reformulate optimization problems by introducing auxiliary variables [6]. The idea is that some of the formulations may
be more effective (and computationally cheaper) than others. We will concentrate
on Legendre minimization, see [7] and [8], instead of Legendre min-max emphasized
in [6]. An advantage of Legendre minimization is that mathematical convergence
proofs can be given. (For example, [8] proved convergence results for the algorithm
implemented in [7].)
In Theorem 4 we show that Legendre minimization algorithms are equivalent to
CCCP. The CCCP viewpoint emphasizes the geometry of the approach and complements the algebraic manipulations given in [8]. (Moreover, our results of the
previous section show the generality of CCCP while, by contrast, the Legendre
transform methods have been applied only on a case by case basis).
Definition 1. Let F(x) be a convex function. For each value y let F*(ff) =
minx{F(x) +y?x.}. Then F*(Y) is concave and is the Legendre transform of F(x).
Moreover, F (x) = max y { F* (y) - y. x} .
Property 1. F(.) and F*(.) are related by

a:; (fJ)

= {~~} - 1(_Y), -~~(x) =

{a{y* } -1 (x). (By { a{y* } -1 (x) we mean the value y such that a{y* (y) = x.)
Theorem 4. Let E1 (x) = f(x) + g(x) and E 2(x, Y) = f(x) + x? Y + h(i/), where
f(.), h(.) are convex functions and g(.) is concave. Then applying CCCP to E1 (x) is
equivalent to minimizing E2 (x, Y) with respect to x and y alternatively (for suitable
choices of g(.) and h(.).

Proof. We can write E1(X) = f(x) +miny{g*(Y) +x?y} where g*(.) is the Legendre
transform of g( .) (identify g(.) with F*( .) and g*(.) with F(.) in definition 1). Thus
minimizing E1 (x) with respect to x is equivalent to minimizing E1 (x, Y) = f(x) +
x . y + g* (Y) with respect to x and y. (Alternatively, we can set g* (Y) = h(Y)
in the expression for E 2(x,i/) and obtain a cost function E 2(x) = f(x) + g(x).)
Alternatively minimization over x and y gives: (i) of/ax = y to determine Xt+1 in
terms of Yt, and (ii) ag* / ay = x to determine Yt in terms of Xt which, by Property
1 of the Legendre transform is equivalent to setting y = -ag / ax. Combining these
two stages gives CCCP:
f (_)
a
ag (_)
ax Xt+1 = - ax Xt .

4

Examples of CCCP

We now illustrate CCCP by giving four examples: (i) discrete time dynamical
systems for the mean field Potts model, (ii) an EM algorithm for the elastic net,
(iii) a discrete (Sinkhorn) algorithm for solving the linear assignment problem, and
(iv) the Generalized Iterative Scaling (GIS) algorithm for parameter estimation.
Example 1.
Discrete Time Dynamical Systems for the Mean Field Potts
Model. These attempt to minimize discrete energy functions of form E[V] =
2: i ,j,a,b Tij ab Via V)b + 2: ia Bia Vi a, where the {Via} take discrete values {a, I} with
linear constraints 2:i Via = 1, Va.

Discussion. Mean field algorithms minimize a continuous effective energy E ett [S; T]
to obtain a minimum of the discrete energy E[V] in the limit as T f-7 a. The
{Sial are continuous variables in the range [0 ,1] and correspond to (approximate)
estimates of the mean states of the {Via}. As described in [12}, to ensure that the
minima of E[V] and E ett [S; T] all coincide (as T f-7 0) it is sufficient that T ijab
be negative definite. Moreover, this can be attained by adding a term -K 2: ia
to E[V] (for sufficiently large K) without altering the structure of the minima of
E[V] . Hence, without loss of generality we can consider 2: i ,j,a,b Tijab Via V)b to be a
concave function .

Vi!

We impose the linear constraints by adding a Lagrange multiplier term
2: a Pa {2: i Via - I} to the energy where the {Pa} are the Lagrange multipliers. The
effective energy becomes:

ia

i,j,a ,b

ia

a

We can then incorporate the Lagrange multiplier term into the convex part.
This gives: Evex [S] = T2: ia SialogSia + 2:aPa{2:iSia -I} and Ecave[S] =
2: i jab TijabSiaSjb + 2: ia BiaS ia ? Taking derivatives yields:
Evex [S] =

TI~~Sia + Pa

&g

&:s::~ (StH) = Bia?

&t E cave [S] = 2 2: j ,b TijabSjb + Bia? Applying eeep by setting
&:5;:e (st) gives T{l + log Sia (t + I)} + Pa = -2 2: j ,b TijabSjb(t)-

and

We solve for the Lagrange multipliers {Pal by imposing the constraints
1, Va. This gives a discrete update rule:

2:i Sia(t + 1) =

Sia (t + 1) =

e(-1/T){2 2:.J, b TijabSjb(t)+Oia}

'
.
2: c e( -1/T){2 2: j,b TijcbSjb(tl+Oi
c}

(4)

Algorithms of this type were derived in [lO}, [3} using different design principles.

Our second example relates to the ubiquitous EM algorithm. In general EM and
CCCP give different algorithms but in some cases they are identical. The EM algorithm seeks to estimate a variable f* = argmaxt log 2:{I} P(f, l), where {f}, {l} are
variables that depend on the specific problem formulation. It was shown in [4] that
this is equivalent to minimizing the following effective energy with respect to the
variables f and P(l): E ett [! , P(l)] = - ~ 2:1 P(l) log P(f, l) + ~ 2:{I} P(l) log P(l).
To apply CCCP to an effective energy like this we need either: (a) to decompose
E ett [!, P(l)] into convex and concave functions of f, P(l), or (b) to eliminate either

variable and obtain a convex concave decomposition in the remaining variable (d.
Theorem 4). We illustrate (b) for the elastic net [2]. (See Yuille and Rangarajan,
in preparation, for an illustration of (a)).
Example 2. The elastic net attempts to solve the Travelling Salesman Problem
(TSP) by finding the shortest tour through a set of cities at positions {Xi }' The
elastic net is represented by a set of nodes at positions {Ya} with variables {Sial
that determine the correspondence between the cities and the nodes of the net. Let
E el I [S, 171 be the effective energy for the elastic net, then the {y} variables can be
eliminated and the resulting Es[S] can be minimized using GGGP. (Note that the
standard elastic net only enforces the second set of linear constraints).

Discussion. The elastic net energy function can be expressed as [11]:

ia

a,b

where we impose the conditions L: a Sia = 1, V i and

i,a

L:i Sia =

1, V a.

The EM algorithm can be applied to estimate the {Ya}. Alternatively we can solve
for the {Ya} variables to obtain Yb = L:i a PabSiaXi where {Pab } = {Jab + 2')'Aab} -1.
We substitute this back into E ell [S, 171 to get a new energy Es[S] given by:

(6)
i ,j,a,b

i,a

Once again this is a sum of a concave and a convex part (the first term is concave
because of the minus sign and the fact that {Pba } and Xi . Xj are both positive semidefinite.) We can now apply GGGP and obtain the standard EM algorithm for this
problem. (See Yuille and Rangarajan, in preparation, for more details).

Our final example is a discrete iterative algorithm to solve the linear assignment
problem. This algorithm was reported by Kosowsky and Yuille in [5] where it was
also shown to correspond to the well-known Sinkhorn algorithm [9]. We now show
that both Kosowsky and Yuille's linear assignment algorithm, and hence Sinkhorn's
algorithm are examples of CCCP (after a change of variables).
Example 3. The linear assignment problem seeks to find the permutation matrix
{TI ia } which minimizes the energy E[m = L: ia TI ia A ia , where {Aia} is a set of
assignment values. As shown in [5} this is equivalent to minimizing the (convex)
Ep[P] energy given by Ep[P] = L: aPa + ~ L:i log L:a e-,B(Aia+Pa) , where the solution is given by TI;a = e-,B(Aia+Pa) / L:b e-,B(Aib+Pb) rounded off to the nearest
integer (for sufficiently large fJ). The iterative algorithm to minimize Ep[P] (which
can be re-expressed as Sinkhorn's algorithm, see [5}) is of form:

(7)

and can be re-expressed as GGGP.

Discussion. By performing the change of coordinates fJPa = - log r a V a (for r a

>

0, Va) we can re-express the Ep[P] energy as:

(8)

Observe that the first term of Er [r] is convex and the second term is concave (this
can be verified by calculating the Hessian). Applying CCCP gives the update rule:
1

rt+l =
a

2:= 2:::
i

e-,BAia
e-,BAibrt'

b

(9)

b

which corresponds to equation (7).
Example 4. The Generalized Iterative Scaling (GIS) Algorithm [ll for estimating

parameters in parallel.
Discussion. The GIS algorithm is designed to estimate the parameter

Xof a distri-

bution P(x : X) = eX.?(x) IZ[X] so that 2:::x P(x; X)?(x) = h, where h are observation data (with components indexed by j.t). It is assumed that ?fJ,(x) ::::: 0, V j.t,x,
hfJ, ::::: 0, V j.t, and 2:::fJ, ?fJ, (x) = 1, V x and 2:::fJ, hfJ, = 1. (All estimation problems of
this type can be transformed into this form [lj).
Darroch and Ratcliff [ll prove that the following GIS algorithm is guaranteed to
converge to value X* that minimizes the (convex) cost function E(X) = log Z[X]-X.h
and hence satisfies 2::: x P(x; X*)?(x) = h. The GIS algorithms is given by:
Xt+! =

Xt -

log ht

+ log h,

(10)

where ht = 2::: x P(x; Xt )?(x) {evaluate log h componentwise: (log h)fJ, = log hf),')
To show that GIS can be reformulated as CCCP, we introduce a new variable
iJ = eX (componentwise). We reformulate the problem in terms of minimizing
the cost function E,B [iJ] = log Z[log(iJ)] - h . (log iJ). A straightforward calculation shows that -h . (log iJ) is a convex function of iJ with first derivative being
-hi iJ (where the division is componentwise). The first derivative of log Z[log(iJ)] is
(II iJ) 2::: x ?(x)P(x: log ,8) (evaluated componentwise). To show that log Z[log(iJ)] is
concave requires computing its Hessian and applying the Cauchy-Schwarz inequality
using the fact that 2:::fJ, ?fJ,(x) = 1, V x and that ?fJ,(x) ::::: 0, V j.t,x. We can therefore apply CCCP to E,B [iJ] which yields l/iJH1 = l/iJt x Ilh x ht (componentwise) ,
which is GIS (by taking logs and using log ,8 = X).

5

Conclusion

CCCP is a general principle which can be used to construct discrete time iterative
dynamical systems for almost any energy minimization problem. It gives a geometric perspective on Legendre minimization (though not on Legendre min-max).
We have illustrated that several existing discrete time iterative algorithms can be reinterpreted in terms of CCCP (see Yuille and Rangarajan, in preparation, for other

examples). Therefore CCCP gives a novel way ofthinking about and classifying existing algorithms. Moreover, CCCP can also be used to construct novel algorithms.
See, for example, recent work [13] where CCCP was used to construct a double loop
algorithm to minimize the Bethe/Kikuchi free energy (which are generalizations of
the mean field free energy).
There are interesting connections between our results and those known to mathematicians. After this work was completed we found that a result similar to Theorem
2 had appeared in an unpublished technical report by D. Geman. There also are
similarities to the work of Hoang Tuy who has shown that any arbitrary closed
set is the projection of a difference of two convex sets in a space with one more
dimension. (See http://www.mai.liu.se/Opt/MPS/News/tuy.html).

Acknowledgements
We thank James Coughlan and Yair Weiss for helpful conversations. Max Welling
gave useful feedback on this manuscript. We thank the National Institute of Health
(NEI) for grant number R01-EY 12691-01.

References
[1] J.N. Darroch and D. Ratcliff. ""Generalized Iterative Scaling for Log-Linear Models"".
The Annals of Mathematical Statistics. Vol. 43. No.5, pp 1470-1480. 1972.
[2] R. Durbin, R. Szeliski and A.L. Yuille."" An Analysis of an Elastic net Approach to
the Traveling Salesman Problem"". Neural Computation. 1 , pp 348-358. 1989.
[3] LM. Elfadel ""Convex potentials and their conjugates in analog mean-field optimization"". Neural Computation. Volume 7. Number 5. pp. 1079-1104. 1995.
[4] R. Hathaway. ""Another Interpretation of the EM Algorithm for Mixture Distributions"" . Statistics and Probability Letters. Vol. 4, pp 53-56. 1986.
[5] J. Kosowsky and A.L. Yuille. ""The Invisible Hand Algorithm: Solving the Assignment
Problem with Statistical Physics"". Neural Networks. , Vol. 7, No.3 , pp 477-490. 1994.
[6] E. Mjolsness and C. Garrett. ""Algebraic Transformations of Objective Functions"".
Neural Networks. Vol. 3, pp 651-669.
[7] A. Rangarajan, S. Gold, and E. Mjolsness. ""A Novel Optimizing Network Architecture with Applications"" . Neural Computation, 8(5), pp 1041-1060. 1996.
[8] A. Rangarajan, A.L. Yuille, S. Gold. and E. Mjolsness."" A Convergence Proof for
the Softassign Quadratic assignment Problem"". In Proceedings of NIPS '96. Denver.
Colorado. 1996.
[9] R. Sinkhorn. ""A Relationship Between Arbitrary Positive Matrices and Doubly
Stochastic Matrices"". Ann. Math. Statist .. 35, pp 876-879. 1964.
[10] F.R. Waugh and R.M . Westervelt. ""Analog neural networks with local competition:
L Dynamics and stability"". Physical Review E, 47(6), pp 4524-4536. 1993.
[11] A.L. Yuille. ""Generalized Deformable Models, Statistical Physics and Matching Problems,"" Neural Computation, 2 pp 1-24. 1990.
[12] A.L. Yuille and J.J. Kosowsky. ""Statistical Physics Algorithms that Converge."" Neural Computation. 6, pp 341-356. 1994.
[13] A.L. Yuille. ""A Double-Loop Algorithm to Minimize the Bethe and Kikuchi Free
Energies"" . Neural Computation. In press. 2002.

"
7195,1994,Predicting the Risk of Complications in Coronary Artery Bypass Operations using Neural Networks,Abstract Missing,"Predicting the Risk of Complications in Coronary
Artery Bypass Operations using Neural Networks
Richard P. Lippmann, Linda Kukolich
MIT Lincoln Laboratory
244 Wood Street
Lexington, MA 02173-0073

Dr. David Shahian
Lahey Clinic
Burlington, MA 01805

Abstract
Experiments demonstrated that sigmoid multilayer perceptron (MLP)
networks provide slightly better risk prediction than conventional
logistic regression when used to predict the risk of death, stroke, and
renal failure on 1257 patients who underwent coronary artery bypass
operations at the Lahey Clinic. MLP networks with no hidden layer and
networks with one hidden layer were trained using stochastic gradient
descent with early stopping. MLP networks and logistic regression used
the same input features and were evaluated using bootstrap sampling
with 50 replications. ROC areas for predicting mortality using
preoperative input features were 70.5% for logistic regression and
76.0% for MLP networks. Regularization provided by early stopping
was an important component of improved perfonnance. A simplified
approach to generating confidence intervals for MLP risk predictions
using an auxiliary ""confidence MLP"" was developed. The confidence
MLP is trained to reproduce confidence intervals that were generated
during training using the outputs of 50 MLP networks trained with
different bootstrap samples.

1 INTRODUCTION
In 1992 there were roughly 300,000 coronary artery bypass operations perfonned in the
United States at a cost of roughly $44,000 per operation. The $13.2 billion total cost of
these operations is a significant fraction of health care spending in the United States. This
has led to recent interest in comparing the quality of cardiac surgery across hospitals using
risk-adjusted procedures and large patient populations. It has also led to interest in better
assessing risks for individual patients and in obtaining improved understanding of the patient and procedural characteristics that affect cardiac surgery outcomes.

1056

Richard P. Lippmann, Linda Kukolich, David Shahian

INPUT
FEATURES
~

-...
-....
...
~

-~

SELECT I-FEATURES
I-AND
REPLACE
MISSING
FEATURES r-r--

CLASSIFY

-...

CONFIDENCE
NETWORK

...
~

RISK
PROBABILITY

CONFIDENCE
INTERVAL

Figure 1. Block diagram of a medical risk predictor.
This paper describes a experiments that explore the use of neural networks to predict the
risk of complications in coronary artery bypass graft (CABO) surgery. Previous approaches
to risk prediction for bypass surgery used linear or logistic regression or a Bayesian approach which assumes input features used for risk prediction are independent (e.g. Edwards, 1994; Marshall, 1994; Higgins, 1992; O'Conner, 1992). Neural networks have the
potential advantages of modeling complex interactions among input features, of allowing
both categorical and continuous input features, and of allowing more flexibility in fitting
the expected risk than a simple linear or logistic function.

2 RISK PREDICTION AND FEATURE SELECTION
A block diagram of the medical risk prediction system used in these experiments is shown
in Figure 1. Input features from a patient's medical record are provided as 105 raw inputs,
a smaller subset of these features is selected, missing features in this subset are replaced
with their most likely values from training data, and a reduced input feature vector is fed to
a classifier and to a ""confidence network"". The classifier provides outputs that estimate the
probability or risk of one type of complication. The confidence network provides upper and
lower bounds on these risk estimates. Both logistic regression and multilayer sigmoid neural network (MLP) classifiers were evaluated in this study. Logistic regression is the most
common approach to risk prediction. It is structurally equivalent to a feed-forward network
with linear inputs and one output unit with a sigmoidal nonlinearity. Weights and offsets are
estimated using a maximum likelihood criterion and iterative ""batch"" training. The reference logistic regression classifier used in these experiments was implemented with the SPlus glm function (Mathsoft, 1993) which uses iteratively reweighted least squares for
training and no extra regularization such as weight decay. Multilayer feed-forward neural
networks with no hidden nodes (denoted single-layer MLPs) and with one hidden layer and
from 1 to 10 hidden nodes were also evaluated as implemented using LNKnet pattern classification software (Lippmann, 1993). An MLP committee classifier containing eight members trained using different initial random weights was also evaluated.
All classifiers were evaluated using a data base of 1257 patients who underwent coronary
artery bypass surgery from 1990 to 1994. Classifiers were used to predict mortality, postoperative strokes, and renal failure. Predictions were made after a patient's medical history
was obtained (History), after pre-surgical tests had been performed (Post-test), immediately
before the operation (preop), and immediately after the operation (Postop). Bootstrap sampling (Efron, 1993) was used to assess risk prediction accuracy because there were so few

Predicting the Risk of Complications in Coronary Artery Bypass Operations

1057

patients with complications in this data base. The number of patients with complications
was 33 or 2.6% for mortality, 25 or 2.0% for stroke, and 21 or 1.7% for renal failure. All
experiments were performed using 50 bootstrap training sets where a risk prediction technique is trained with a bootstrap training set and evaluated using left-out patterns.
NComplications
NHigh

mSTORY
Age
COPD (Chronic Obs. Pul. Disease)

% True Hits

27/674
71126

5.6%

8nl
61105
6/21

11.3%
5.7%
26.6%

211447
111115
10/127
7/64

4.7%
6.6%
7.9%
10.9%

121113
91184

10.6%
4.9%

4.0%

POST?TEST
Pulmonary Ventricular Congestion
X-ray Cardiomegaly
X-ray Pulmonary Edema

PREOP
NTG (Nitroglycerin)
IABP (Intraaortic Balloon Pump)
Urgency Status
MI When

POSTOP
Blood Used (Packed Cells)
Perfusion Time

Figure 2. Features selected to predict mortality.
The initial set of 105 raw input features included binary (e.g. MalelFemale), categorical
(e.g. MI When: none, old, recent, evolving), and continuous valued features (e.g. Perfusion
Time, Age). There were many missing and irrelevant features and all features were only
weakly predictive. Small sets of features were selected for each complication using the following procedures: (1) Select those 10 to 40 features experience and previous studies indicate are related to each complication, (2) Omit features if a univariate contingency table
analysis shows the feature is not important, (3) Omit features that are missing for more than
5% of patients, (4) Order features by number of true positives, (5) Omit features that are
similar to other features keeping the most predictive, and (7) Add features incrementally as
a patient's hospital interaction progresses. This resulted in sets of from 3 to 11 features for
the three complications. Figure 2 shows the 11 features selected to predict mortality. The
first column lists the features, the second column presents a fraction equal to the number of
complications when the feature was ""high"" divided by the number of times this feature was
""high"" (A threshold was assigned for continuous and categorical features that provided
good separation), and the last column is the second column expressed as a percentage. Classifiers were provided identical sets of input features for all experiments. Continuous inputs
to all classifiers were normalized to have zero mean and unit variance, categorical inputs
ranged from -(D-1)/2 to (D-1)/2 in steps of 1.0, where D is the number of categories, and
binary inputs were -0.5 or 0.5 .

3 PERFORMANCE COMPARISONS
Risk prediction was evaluated by plotting and computing the area under receiver operating
characteristic (ROC) curves and also by using chi-square tests to determine how accurately
classifiers could stratify subjects into three risk categories. Automated experiments were
performed using bootstrap sampling to explore the effect of varying the training step size

J058

Richard P. Lippmann. Linda Kukolich, David Shahian

100
~ 80
:~
~

c(/) 60
Q)

en

~ 40

""

I

HISTORY (68.6%)

?fl. 20

o

o

20

40

60
80 100 0
20
40
60
% FALSE ALARMS (100 - Specificity)

80

100

Figure 3. Fifty preoperative bootstrap ROCs predicting mortality using an MLP
classifier with two hidden nodes and the average ROC (left), and average ROCS
for mortality using history, preoperative, and postoperative features (right).
from 0.005 to 0.1; of using squared-error, cross-entropy, and maximum likelihood cost
functions; of varying the number of hidden nodes from 1 to 8; and of stopping training after
from 5 to 40 epochs. ROC areas varied little as parameters were varied. Risk stratification,
which measures how well classifier outputs approximate posterior probabilities, improved
substantially with a cross-entropy cost function (instead of squared error), with a smaller
stepsize (0.01 instead of 0.05 or 0.1) and with more training epochs (20 versus 5 or 10). An
MLP classifier with two hidden nodes provided good overall performance across complications and patient stages with a cross-entropy cost function, a stepsize of 0.01, momentum
of 0.6, and stochastic gradient descent stopping after 20 epochs. A single-layer MLP provided good performance with similar settings, but stopping after 5 epochs. These settings
were used for all experiments. The left side of Figure 3 shows the 50 bootstrap ROCs created using these settings for a two-hidden-node MLP when predicting mortality with preoperative features and the ROC created by averaging these curves. There is a large
variability in these ROes due to the small amount of training data. The ROC area varies
from 67% to 85% (cr=4.7) and the sensitivity with 20% false alarms varies from 30% to
79%. Similar variability occurs for other complications. The right side of Figure 3 shows
average ROCs for mortality created using this MLP with history, preoperative, and postoperative features. As can be seen, the ROC area and prediction accuracy increases from
68.6% to 79.2% as more input features become available.
Figure 4 shows ROC areas across all complications and patient stages. Only three and two
patient stages are shown for stroke and renal failure because no extra features were added
at the missing stages for these complications. ROC areas are low for all complications and
range from 62% to 80%. ROC areas are highest using postoperative features, lowest using
only history features, and increase as more features are added. ROC areas are highest for
mortality (68 to 80%) and lower for stroke (62 to 71 %) and renal failure (62 to 67% ).The
MLP classifier with two hidden nodes (MLP) always provided slightly higher ROC areas
than logistic regression. The average increase with the MLP classifier was 2.7 percentage

Predicting the Risk of Complications in Coronary Artery Bypass Operations

100
90

III Logistic

MORTALITY

-80

C

~ 70
~ 60

8a:

1059

[3

Single?Layer MLP

?

MLP

?

MLP?Commillee

50
40

30

HISTORY

POSTTEST PRE?OP
PATIENT STAGE

POST?OP

100
90

-

80

C
?
w 70

100

STROKE

90

80

I=2a

I=2a

70

~ 60

8a:

RENAL FAILURE

60

50

50

40

40

30

30

HISTORY

POSTTEST
PATIENT STAGE

POST?OP

HISTORY
POST?OP
PATIENT STAGE

Figure 4. ROC areas across all complications and patient stages for logistic
regression, single-layer MLP classifier, two-layer MLP classifier with two hidden
nodes, and a committee classifier containing eight two-layer MLP classifiers
trained using different random starting weights.
points (the increase ranged from 0.3 to 5.5 points). The single-layer MLPclassifier also provided good performance. The average ROC area with the single-layer MLP was only 0.6
percentage points below that of the MLP with two hidden nodes. The committee using eight
two-layer MLP classifiers performed no better than an individual two-layer MLP classifier.
Classifier outputs were used to bin or stratify each patient into one of four risk levels (05%, 5-10%, and 10-100%) by treating the output as an estimate of the complication posterior probability. Figure 5 shows the accuracy of risk stratification for the MLP classifier for
all complications. Each curve was obtained by averaging 50 individual curves obtained using bootstrap sampling as with the ROC curves. Individual curves were obtained by placing
each patient into one of the three risk bins based on the MLP output. The x's represent the
average MLP output for all patients in each bin. Open squares are the true percentage of
patients in each bin who experienced a complication. The bars represent ?2 binomial deviations about the true patient percentages. Risk prediction is accurate if the x's are close to
the squares and within the confidence intervals. As can be seen, risk prediction is accurate
and close to the actual number of patients who experienced complications. It is difficult,
however, to assess risk prediction given the limited numbers of patients in the two highest
bins. For example, in Figure 5, the median number of patients with complications was only
2 out of 20 in the middle bin and 2 out of 13 in the upper bin. Good and similar risk stratification, as measured by a chi-square test, was provided by all classifiers. Differences between classifier predictions and true patient percentages were small and not statistically
significant.

1060

Richard P. Lippmann, Linda Kukolich, David Shahian

~ ~------------------------------~
MORTALITY

0- PATIENT COUNT

X - MLP OUTPUT

30

10

o ____.M____________________________
40

~ ~--------------------------r_--_,
RENAL FAILURE

30
20
10

o

~

__

_w~

0-5

________

~

__________

5?10

~

__

~

10-100

BIN PROBABILITY RANGE ('?o)

Figure 5. Accuracy of MLP risk stratification for three complications using
preoperative features. Open squares are true percentages of patients in each bin
with a complication, x's are MLP predictions, bars represent ?2 binomial
standard deviation confidence intervals.

4 CONFIDENCE MLP NETWORKS
Estimating the confidence in the classification decision produced by a neural network is a
critical issue that has received relatively little study. Not being able to provide a confidence
measure makes it difficult for physicians and other professionals to accept the use of complex networks. Bootstrap sampling (Efron, 1993) was selected as an approach to generate
confidence intervals for medical risk prediction because 1) It can be applied to any type of
classifier, 2) It measures variability due to training algorithms, implementation differences,
and limited training data, and 3) It is simple to implement and apply. As shown in the top
half of Figure 6, 50 bootstrap sets of training data are created from the original training data
by resampling with replacement. These bootstrap training sets are used to train 50 bootstrap
MLP classifiers using the same architecture and training procedures that were selected for
the risk prediction MLP. When a pattern is fed into these classifiers, their outputs provide
an estimate of the distribution of the output of the risk prediction MLP. Lower and upper
confidence bounds for any input are obtained by sorting these outputs and selecting the 10%
and 90% cumulative levels.
It is computationally expensive to have to maintain and query 50 bootstrap MLPs whenever
confidence bounds are desired. A simpler approach is to train a single confidence MLP to
replicate the confidence bounds predicted by the 50 bootstrap MLPs, as shown in the bot-

Predicting the Risk of Complications in Coronary Artery Bypass Operations

1061

OUTPUT
STATISTICS

UPPER
LIMIT

/
INPUT
PATTERN

RISK
PREDICTION
MLP

CONFIDENCE
MLP

LOWER
LIMIT

Figure 6. A confidence MLP trained using 50 bootstrap MLPs produces upper and
lower confidence bounds for a risk prediction MLP.
tom half of Figure 6. The the confidence MLP is fed the input pattern and the output of the
risk prediction MLP and produces at its output the confidence intervals that would have
been produced by 50 bootstrap MLPs. The confidence MLP is a mapping or regression network that replaces the 50 bootstrap networks. It was found that confidence networks with
one hidden layer, two hidden nodes, and a linear output could accurately reproduce the upper and lower confidence intervals created by 50 bootstrap two-layer MLP networks. The
confidence network outputs were almost always within ?15% of the actual bootstrap
bounds. Upper and lower bounds produced by these confidence networks for all patients
using preoperative features predicting mortality are show in Figure 7. Bounds are high (? 10
percentage points) when the complication risk is near 20% and drop to lower values (?0.4
percentage points) when the risk is near 1%. This relatively simple approach makes it possible to create and replicate confidence intervals for many types of classifiers.

5 SUMMARY AND FUTURE PLANS
MLP networks provided slightly better risk prediction than conventional logistic regression
when used to predict the risk of death, stroke, and renal failure on 1257 patients who underwent coronary artery bypass operations. Bootstrap sampling was required to compare
approaches and regularization provided by early stopping was an important component of
improved performance. A simplified approach to generating confidence intervals for MLP
risk predictions using an auxiliary ""confidence MLP"" was also developed. The confidence
MLP is trained to reproduce the confidence bounds that were generated during training by
50 MLP networks trained using bootstrap samples. Current research is validating these results using larger data sets, exploring approaches to detect outlier patients who are so different from any training patient that accurate risk prediction is suspect, developing approaches to explaining which input features are important for an individual patient, and
determining why MLP networks provide improved performance.

Richard P. Lippmann, Linda Kukolich, David Shahian

1062

~r------.------.-------r------'
T

30

T

'#

... T

...

TT

025

I-

J;....

5

:J20

T

T

UPPER

W

..

(,)

Z 15
W
Q

u::
o(,)
Z

10

5

COMPLICATION RISK%

Figure 7. Upper and lower confidence bounds for all patients and preoperative
mortality risk predictions calculated using two MLP confidence networks.

ACKNOWLEDGMENT
This work was sponsored by the Department of the Air Force. The views expressed are
those of the authors and do not reflect the official policy or position of the U.S. Government. We wish to thank Stephanie Moisakis and Anne Nilson at the Lahey Clinic and
Yuchun Lee at Lincoln Laboratory for assistance in organizing and preprocessing the data.

BIBLIOGRAPHY
F. Edwards, R. Clark, and M. Schwartz. (1994) Coronary Artery Bypass Grafting: The
Society of Thoracic Surgeons National Database Experience. In Annals Thoracic Surgery,
Vol. 57, 12-19.
Bradley Efron and Robert J. Tibshirani. (1993) An Introduction to the Bootstrap. Monographs on Statistics and Applied Probability 57, New York: Chapman and Hall (1993).
T. Higgins, F. Estafanous, et. al. (1992) Stratification of Morbidity and Mortality Outcome
by Preoperative Risk Factors in Coronary Artery Bypass Patients. In Journal of the American Medical Society, Vol. 267, No. 17,2344-2348.
R. Lippmann, L. Kukolich, and E. Singer. (1993) LNKnet: Neural Network, Machine
Learning, and Statistical Software for Pattern Classification. In Lincoln Laboratory Journal, Vol. 6, No.2, 249-268.
Marshall Guillenno, Laurie W. Shroyer, et al. (1994) Bayesian-Logit Model for Risk
Assessment in Coronary Artery Bypass Grafting, In Annals Thoracic Surgery, Vol. 57,
1492-5000.
G. O'Conner, S. Plume, et. al. (1992) Multivariate Prediction of In-Hospital Mortality
Associated with Coronary Artery Bypass Surgery. In Circulation, Vol. 85, No.6, 21102118.
Statistical Sciences. (1993) S-PLUS Guide to Statistical and Mathematical Analyses, Version 3.2, Seattle: StatSci, a division of MathSoft, Inc.

"
6126,2016,Spatio-Temporal Hilbert Maps for Continuous Occupancy Representation in Dynamic Environments,"We consider the problem of building continuous occupancy representations in  dynamic environments for robotics applications. The problem has hardly been discussed previously due to the complexity of patterns in urban environments,  which have both spatial and temporal dependencies. We address the problem  as learning a kernel classifier on an efficient feature space. The key novelty of  our approach is the incorporation of variations in the time domain into the spatial  domain. We propose a method to propagate motion uncertainty into the kernel using a hierarchical model. The main benefit of this approach is that it can directly predict  the occupancy state of the map in the future from past observations, being a valuable  tool for robot trajectory planning under uncertainty. Our approach preserves the  main computational benefits of static Hilbert maps ? using stochastic gradient  descent for fast optimization of model parameters and incremental updates as  new data are captured. Experiments conducted in road intersections of an urban  environment demonstrated that spatio-temporal Hilbert maps can accurately model  changes in the map while outperforming other techniques on various aspects.","Spatio?Temporal Hilbert Maps for Continuous
Occupancy Representation in Dynamic Environments

Ransalu Senanayake
University of Sydney
rsen4557@uni.sydney.edu.au
Simon O?Callaghan
Data61/CSIRO, Australia
simon.ocallaghan@data61.csiro.au

Lionel Ott
University of Sydney
lionel.ott@sydney.edu.au
Fabio Ramos
University of Sydney
fabio.ramos@sydney.edu.au

Abstract
We consider the problem of building continuous occupancy representations in
dynamic environments for robotics applications. The problem has hardly been
discussed previously due to the complexity of patterns in urban environments,
which have both spatial and temporal dependencies. We address the problem
as learning a kernel classifier on an efficient feature space. The key novelty of
our approach is the incorporation of variations in the time domain into the spatial
domain. We propose a method to propagate motion uncertainty into the kernel using
a hierarchical model. The main benefit of this approach is that it can directly predict
the occupancy state of the map in the future from past observations, being a valuable
tool for robot trajectory planning under uncertainty. Our approach preserves the
main computational benefits of static Hilbert maps ? using stochastic gradient
descent for fast optimization of model parameters and incremental updates as
new data are captured. Experiments conducted in road intersections of an urban
environment demonstrated that spatio-temporal Hilbert maps can accurately model
changes in the map while outperforming other techniques on various aspects.

1

Introduction

We are in the climax of driverless vehicles research where the perception and learning are no longer
trivial problems due to the transition from controlled test environments to real world complex
interactions with other road users. Online mapping environments is vital for action planing. In such
applications, the state of the observed world with respect to the vehicle changes over time, making
modeling and predicting into the future challenging. Despite this, there is a plethora of mapping
techniques for static environments but only very few instances of truly dynamic mapping methods.
Most existing techniques merely consider a static representation, and as parallel processes, initialize
target trackers for the dynamic objects in the scene, updating the map with new information. This
approach can be effective from a computational point of view, but it disregards crucial relationships
between time and space. By treating the dynamics as a separate problem from the space representation,
such methods cannot perform higher level inference tasks such as what are the most likely regions of
the environment to be occupied in the future, or when and where a dynamic object is most likely to
appear.
In occupancy grid maps (GM) [1], the space is divided into a fixed number of non-overlapping
cells and the likelihood of occupancy for each individual cell is estimated independently based on
sensor measurements. Considering the main drawbacks of the GM, discretization of the world and
disregarding spatial relationship among cells, Gaussian process occupancy map (GPOM) [2] enabled
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

continuous probabilistic representation. In spite of its profound formulation, it is less pragmatic
for online learning due to O(N 3 ) computational cost in both learning and inference, where N is
the number of data points. Recently, as an alternative, static Hilbert maps (SHMs) [3, 4] was
proposed, borrowing the two main advantages of GPOMs but at a much lower computational cost.
As a parametric technique, SHMs have a constant cost for updating the model with new observations.
Additionally, the parameters can be learned using stochastic gradient descent (SGD) which made it
computationally attractive and capable of handling large datasets. Nonetheless, all these techniques
assume a static environment.
Although attempts to adapt occupancy grid maps to dynamic environments and identify periodic
patterns exist [5], to the best of our knowledge, only dynamic Gaussian processes occupancy maps
(DGPOM) [6] can model occupancy in dynamic environments in a continuous fashion. There,
velocity estimates are linearly added to the inputs of the GP kernel. This approach, similar to the
proposed method, can make occupancy predictions into the future. However, being a non-parametric
model, the cost of inverting the covariance matrix in DGPOM grows over time and hence the model
cannot be run in real-time.
In this paper, we propose a method for building continuous spatio-temporal Hilbert maps (STHM)
using ?hinged? features. This method builds on the main ideas behind SHM and generalize it to
dynamic environments. To this end, we formulate a novel methodology to permeate the variability
in the temporal domain into the spatial domain, rather than considering time merely as another
dimension. This approach can be used to predict the occupancy state of the world, interpolating
not only in space but also in time. The representation is demonstrated in highly dynamic urban
environments of busy intersections with cars moving and turning in both directions obeying traffic
lights. In Section 2, we lay the foundation by introducing SHMs and then, we discuss the proposed
method in Section 3, followed by experiments and discussions in Section 4.

2

Static Hilbert maps (SHMs)

A static Hilbert map (SHM) [3] is a continuous probabilistic occupancy representation of the space,
given a collection of range sensor measurements. As in almost all autonomous vehicles, we assume a
training dataset consisting of locations with associated occupancy information obtained from a range
sensor ? in the case of a laser scanner (i.e. LIDAR), points along the beam are unoccupied while the
end point is occupied ? the model predicts the occupancy state of different locations given by query
points.
D
The SHM model: Formally, let the training dataset be defined as D = {xi , yi }N
i=1 with xi ? R
being a point in 2D or 3D space, and yi ? {?1, +1} the associated occupancy status. SHM
predicts the probability of occupancy for a new point x? calculated as p(y? |x? , w, D), given a set of
parameters w and the dataset D. This discriminative model takes the form of a logistic regression
classifier with an elastic net regularizer operating on basis functions mapping the point coordinates to
a Hilbert space defined by a kernel k(x, x0 ) : X ? X ? R where x, x0 ? X = {location}. This is
equivalent to kernel logistic regression [7] which is known to be computationally expensive due to
the need of computing the kernel matrix between all points in the dataset. The crucial insight to
make the method computationally efficient is to first approximate the kernel by a dot product of
basis functions such that k(x, x0 ) ? ?(x)> ?(x0 ). This can be done using the random kitchen sinks
procedure [8, 9] or by directly defining efficient basis functions. Note that, [3] assumes a linear
machine w> ?(x). Learning w is done by minimizing the regularized negative-log-likelihood using
stochastic gradient descent (SGD) [10]. The probability that a query point x? is not occupied is
?1
given by p(y? = ?1|x? , w, D) = 1 + exp(w> ?(x? ))
, while the probability of being occupied
is given by p(y? = +1|x? , w, D) = 1 ? p(y? = ?1|x? , w, D).

3

Spatio-temporal hinged features (HF-STHM)

In this section, SHMs are generalized into the spatio-temporal domain. Though augmenting the inputs of the SHM kernel X = {location} as X = {(location, time)} or X =
{(location, time, velocity)} is the naive method to build instantaneous maps, they cannot be used for
predicting into the future mainly because they are not cable of capturing complex and miscellaneous
spatio-temporal dependencies. As discussed in Section 3.3, in our approach, the uncertainty of
2

Figure 1: Motion centroids are collected over time from raw data (Section 3.1) and individual GPs are
trained (input: centroids, output: motion information) to learn GP-hyperparameters (Section 3.2 and
Figure 4). Then, the motion of data points at time t? (= 0 for present, > 0 for future, < 0
for past) are queried using the trained GPs and this motion distribution is fed into the kernel
(Section 3.3). This implicitly embeds motion information into the spatial observations. Then a
kernelized logistic regression model logistic(w> ?) is trained to learn w. For a new query point
in space, ?(longitude, latitude) is calculated using Equation 6 followed by sigmoidal(w> ?) to
obtain the occupancy probability. These steps are repeated for each new laser scan.

dynamic objects is incorporated into the map. This uncertainty is estimated using an underlying
Gaussian process (GP) regression model described in Section 3.2. The inputs for the GP are obtained
using a further underlying model based on motion cluster data association which is discussed in Section 3.1. This way, locations are no more deterministic but each location has a probability distribution
and hence the kernel inputs become X = {mean and variance of location}. Sections 3.1?3.3
explain this three-step hierarchical framework in the bottom-to-top approach which are executed
sequentially as new data are received. The method is summarized in Figure 1.
Assumptions: WOLOG, we assume that the sensor is not moving; the general case where the
sensor moves is trivial if the motion of the platform is known. From a robotics perspective, we treat
localization as a separate process and assume it is given for the purpose of introducing the method.
Notation: In this section, unless otherwise stated, input x = (x, y, t) are the longitude, latitude and
time components and, s = (x, y) are merely the spatial coordinates. A motion vector (displacement)
is denoted by v = (vx , vy ), where vx and vy are the motion in x and y directions, respectively. A
motion field is a mapping from space and time to a motion vector, (x, y, t) 7? (vx , vy ).
3.1

Motion observations

As the first step, motion observations are extracted from laser scans. Due to occlusions and sensor
noise, extracting dynamic parts of a scene is not straightforward. Similarly, as the shapes of observed
objects change over time (because the only measurement in laser is depth), morphology based object
tracking algorithms and optical flow [11, 12] which are commonly used in computer vision are
unsuitable. Therefore, we devise as a method that is robust to occlusions and noise without relying on
the shape of the objects present in the scene. To obtain motion observations, taking raw laser scans as
inputs and output motion vectors, the following two steps are performed.
3.1.1

Computing centroids of dynamic objects

As shown in Figure 2, firstly, a SHM is built from the raw scanner data at time t and then it is binarized
to produce a grid map containing occupied and free cells. Based on this grid map, observable areas
where dynamic objects can appear are extracted. Next, dynamic objects are obtained by performing
logical conjunction between an adaptive binary mask and the raw laser data. The final step is the
computation of the centroid for each of these components.
3.1.2

Associating centroids of consecutive frames

Having obtained N centroids for frame t and, M centroids for frame t ? 1 from the previous step,
we formulate the centroid?association as the integer program in Equation 1.
3

(a)

(b)

Figure 2: The various steps involved in computing motion observations discussed in Section 3.1 is
shown in (a). The mask (lower left of (b)) is generated by applying morphological operations to the
raw scans (top row). Taking the intersection between the mask and a raw scan yields the potential
dynamic objects in a scene at a given time (middle row). The final centroid association of such
connected components across two consecutive frames is shown in the bottom right frame.

where dij is the Euclidean distance between
dij aij
(1a) two centroids and aij are the elements of the
minimize
assignment matrix. In order to obtain valid asi=1 j=1
signment solutions aij , we impose that only one
M
X
centroid from frame t can be assigned to one
subject to
aij = 1, j = 1, . . . , N (1b) centroid in frame t ? 1, Equation 1b, and the
i=1
vice versa with Equation 1c. Finally, we only
N
allow integer solutions, Equation 1d. The soX
aij = 1, i = 1, . . . , M (1c) lution to the above problem is obtained using
j=1
the Hungarian method [13]. The asymptotically
aij ? {0, 1},
(1d) cubic computational complexity
does not thwart online learning as the number of vehicles in the field of vision is typically very low
(say, < 10). This forms the basis for obtaining the motion field which is described in the next section.
M X
N
X

3.2

Motion prediction using Gaussian process regression

In this section we describe the construction of a model to predict the motion field as a mapping
(x, y, t) ? (vx , vy ). We adopt a Bayesian approach that can provide uncertainty of a query point
with a little amount of data. A Gaussian process (GP) regression model is instantiated for each new
moving object and motion observations are collected over time until the object disappears from the
robot?s view. Each GP model has a different number of data points which grows over time during its
lifespan. Nevertheless, this stage does not suffer from O(N 3 ) asymptotic cost of GPs because objects
appear and disappear from the mapped area (say, the number of GPs < 20 and N < 50 for each GP).
Let us denote displacements collected over time t = {t ? T, . . . , t ? 2, t ? 1, t} for any such moving
object as V = {vt?T , vt?2 , vt?1 , . . . , vt }. A Gaussian process (GP) prior is placed on f , such that
f ? GP(0, kGP (t, t0 )), and V = f (t) +  is an additive noise  ? N (0, ? 2 ). This way we can
model non-linear relationships between motion and time. As v are observations in 2D, the model is a
two dimensional output GP. However, it is also possible to disregard the correlation between response
variables vx and vy for simplicity. So as to capture the variations in motions, we adopt a polynomial
covariance function of degree 3. Further, as commonly used in kriging methods in geostatistics [14],
we explicitly augment the input with a quadratic term ?t = [t, t2 ]> and build kGP (t, t0 ) = (?t?t0 + 1)3 ,
to improve (verified in pilot experiments) the prediction. Unlike squared-exponential kernels which
definitely decay beyond the range of data points, polynomial kernels are suitable for extrapolation
into the near future. However, note that polynomials of unnecessarily higher orders would result in
over-fitting.
4

The predictive distribution for the motion of a point in the locality of an individual GP at a given time,
v? ? N (E, V), can be then predicted using standard GP prediction equations [15] (Figure 4). Note
that hyperparameters of each GP has to be optimized before making any predictions. The associated
distribution for the position of a point transformed by p(v(x)) is then,

s ? N (?, ?) ? N (E, V) ? N

x
y




+ E, V


?N

x + ?x
y + ?y

 
?xx
,
?yx

?xy
?yy


(2)

where we used s(x) to denote the spatial coordinates of x such that s(x) = (x, y).
3.3

Feature embedding

With the predicted spatial coordinates for each point x at time t? , represented as N (?, ?), obtained
in the previous step, the HF-STHM (hinged feature STHM) can now be constructed. As there is
uncertainty in the motion of a point, this uncertainty needs to be propagated into the map.
Denoting H for a reproducing kernel Hilbert space (RKHS) of functions f : S ? R with a
reproducing kernel k : SR ? S ? R, the mean map ? from probability space P into H is obtained [16]
as ? : P ? H, P 7? S k(s, ?)dP(s). Then, the kernel between two distributions can be written as,
Z Z
hk(si , ?), k(sj , ?)iH dPi (si )dPj (sj )

k(Pi , Pj ) =
Z Z
=

k(si , sj )dP(si )dP(sj )

(3)

Z Z
=

k(si , sj )p(si ; ?i , ?i )p(sj ; ?j , ?j )dsi dsj ,

where h?, ?i denotes the dot product and Pi := P(si ) = N (?i , ?i ) in a probability space P.
Theorem 1 [17] If a squared exponential kernel, k(si , sj ) = exp{? 12 (si ? sj )> L?1 (si ? sj )}, is
endowed with P = N (s; ?, ?), then there exists an analytical solution in the form,



?1/2
1
?1
>
?1


k(Pi , Pj ) = I + L (?i + ?j )
exp ? (?i ? ?j ) (L + ?i + ?j ) (?i ? ?j ) ,
2

(4)

where I is the identity matrix and L is the matrix of length scale parameters which determines how
fast the magnitude of the exponential decays with ?.
Corollary 1 For point estimates ?s of Pj ,




1
?1 ?1/2
>
?1

k(Pi , ?s) = I + L ?
exp ? (? ? ?s) (L + ?) (? ? ?s) .
2

(5)


Corollary 1 is now used to compute k p(s), ?s which defines the feature embedding for HF-STHM.
Note that Corollary 1 is equivalent to centering (hinging) the kernels at M fixed points ?s in space
which allows capturing different spatial dependencies over the map dimensions. The pooled-length
scales L + ? of these ?hinged? kernels change over time. Typically, these ?s can be obtained by
a pre-defined regular grid. Finally, the feature mapping for each spatial location is obtained by
concatenating multiple kernels hinged at supports:


>
?hinged (x) = k p(s), ?s1 , . . . , k p(s), ?sM
,
5

(6)

The method to predict occupancy maps at each iteration is summarized in Algorithm 1. As in SHM,
the length-scale of the hinged-feature kernels and the regularization parameter has to be picked
heuristically or using grid-search.
Data: Set of consecutive laser scans
Result: Continuous occupancy map at time t? at any arbitrary resolution
while true do
Extract motion observations V (Section 3.1);
Build the motion vector field from V using Gaussian process regression (Section 3.2);
Generate motion predictions p(v) for t? (Section 3.2);
Compute the feature mapping (Equation 6);
Update w of the logistic regression model similar to Section 2;
Generate a new spatial map by querying at a desirable resolution similar to Section 2;
end
Algorithm 1: Querying maps for t? using HF-STHM algorithm.
Being a parametric model, this method can be used to predict past (t? < 0), present (t? = 0) and
future (t? > 0) occupancy maps using a fixed number of parameters (M + 1). However, in practice,
it may not be required to generate future or past maps at every time step. However, it is required to
incorporate new laser data and update w using SGD at each iteration. Therefore, GP predictions
and probabilistic feature embedding can be skipped by setting ? = 0, whenever it is not required to
predict future or past maps as the uncertainty of knowing the current location for any laser reflection
is zero.

4

Experiments and Discussion

In this section we demonstrate how HF-STHM can be effectively used for mapping in dynamic
environments. Our main dataset1 , named as dataset 1, consists of laser scans, each with 180 beams
covering 1800 angle and 30 m radius, collected from a busy intersection [6]. Figure 3 [6] shows an
aerial view of the area and the location of the sensor. In Section 4.4, we used an additional dataset1
(dataset 2) of a larger intersection, as this section verifies an important part of our algorithm.
4.1

Motion model

Figure 4 shows a real instance where a vehicle breaks and how the GP model is cable of predicting
its future locations with associated uncertainty. Although the GP has two outputs vx and vy , only
predictions along the direction of motion vx is shown for clarity. There can be several such GP models
at a given time as a new GP model is initialized for each new moving object (centroid association)
entering the environment and is removed as it disappears. The GP model not only extrapolates the
motion into the future, but also provides an estimate of the predictive uncertainty which is crucial for
the probabilistic feature embedding techniques discussed in Section 3.3. This location uncertainty
around past observations is negligible while it is increasingly high as the more time steps ahead
into the future we attempt to predict. However, the variance may also slightly change with the
number of data points in the GP and the variability of the motion. As opposed to the two-frame
based velocity calculation technique employed in DGPOM, our method uses motion data of dynamic
objects collected over several frames which makes the predictions more accurate as it does not make
assumptions about the motion of objects such as constant velocity.
4.2

Supports for hinged features

Although in Section 3.3 we suggested to hinge the kernels using a regular grid, we compare it with
kernels hinged in random locations in this experiment. As shown in Table 1, the area-under-ROCcurve (AUC) averaged over randomly selected maps at t? = 0 are more accurate for regular grid
because random supports cannot cover the entire realm, especially if the number of supports is small.
Similarly, a random support based map may not be qualitatively appealing. In general, regular grid
requires less amount of features to ensure a qualitatively and quantitatively better map.
1

https://goo.gl/f9cTDr

6

Table 1: Average AUC ?
supports for hinged features

Motion map
quiver plot

Laser returns
from street walls

Sensor's
location

Figure 3: Ariel view of dataset
1 environment

4.3

Figure 4: GP model

No. of
supports
250
500
1000
5000

Regular
grid
0.95
0.98
0.99
0.99

Random
grid
0.83
0.88
0.94
0.98

Point estimate vs. distribution embedding

It is important to understand if distribution embedding discussed in Section 3.3 indeed improves
accuracy over point embedding. In order to see this, the accuracy between dynamic clusters of
future maps and corresponding ground truth laser values should be compared. Since automatically
identifying dynamic clusters is not possible, we semi-automatically extracted them. To this end,
dynamic clusters of each predict-ahead map were manually delimited using python graphical user
interface tools and negative-log-loss (NLL) between those dynamic clusters and corresponding ground
truth laser values were evaluated. Because the maps are probabilistic, NLL is more representative
than AUC.
Keeping all other variables unaltered, the average decrements of NLL from point estimates to
distribution embedding of randomly selected instances for query time steps t? = 1 to 5 were
0.11, 0.22, 0.34, 0.83, 0.50, 1.36 (note! log scale) where t? > 0 represents future. Therefore, embedding both mean and variance, rather than merely mean, is crucial for a higher accuracy. Intuitively,
though we can never predict the exact future location of a moving vehicle, it is possible to predict the
probability of its presence at different locations in the space.
4.4

Spatial maps vs. spatio-temporal maps

In order to showcase the importance of spatio-temporal models (HF-STHM) over spatial models
(SHM), NLL values of a subset of dataset were calculated similar to Section 4.3 for compare dynamic
occupancy grid map (DGM), SHM and HF-STHM. SHM and HF-STHM used 1000 bases. DGM
is an extension to [1] which calculates occupancy probability based on few past time steps. In this
experiment we considered 10 past time steps and 1 m grid-cell resolution for DGM.
The experiments were performed for datasets 1 and 2 and results are given in Table 2. The smaller
the NLL, the better the accuracy is. HF-STHM outperforms SHM and this effect becomes more
prominent for higher t? . DGM struggles in dynamic environments because of the fixed grid-size,
assumptions about cell independence and it was not explicitly designed for predicting into the future.
NLL of DGM increases with t? as it keeps memory in a decaying-fashion for 10-consecutive-paststeps. Since SHM does not update positions of objects (as it is a spatial model), NLL also increases
with t? . In HF-STHM, NLL increases with t? because predictive variance increases with t? in
addition to mean error. Figure 5 presents a qualitative comparison.
Table 2: NLL - predictions using dynamic occupancy grid map
(DGM), static Hilbert map (SHM) and the proposed method (HFSTHM) for future time steps.
Dataset 1
Dataset 2
Time
DGM SHM STHM
DGM SHM STHM
t? = 0 11.20 0.11
0.12
6.00
0.18
0.09
t? = 1 17.69 0.15
0.15
10.16 0.29
0.12
t? = 2 19.88 0.28
0.18
12.71 0.82
0.34
t? = 3 25.24 0.61
0.19
16.54 1.85
0.57
t? = 4 26.84 1.18
0.48
20.76 2.96
0.16 Table 3: AUC of prediction
t? = 5 27.44 1.46
0.89
25.25 4.00
1.10
t? = 6 34.54 2.00
1.68
26.78 4.90
1.30

7

Figure 5: SHM and HF-STHM for t? -ahead predictions. The robot is at (0,0) facing up. The
white points are ground truth laser reflections. Observe that, in HF-STHM, moving objects are
predicted-ahead and uncertainty of dynamic areas grows as t? increases. Differences are encircled
for t? = 7.

4.5

Predicting into the future and retrieving old maps

In order to assess the ability of our method to predict the future locations of dynamic objects, we
compare the map obtained when predicting a certain number of time steps ahead (t? ) with the
measurements made at that time. Then the average is computed and the AUC as a function of how
far ahead the model makes predictions. The experiment was carried out similar to [6]We compare
our model with DGPOM (AUC values obtained from [6]) as this is the only other method capable of
this type of prediction. According to Figure 3 we can see that both methods perform comparably
when t? < 2. However, if we predict further ahead our method maintains high quality while DGPOM
start to suffer somewhat. One explanation for this is the way motion predictions are integrated in our
method. As discussed in Section 4.3, we embed distributions rather than point observations to the
model and hence it allows us to better deal with the uncertainty of the motion of the dynamic objects.
On the other hand, our motion model can capture non-linear patterns.
In addition to predicting into the future, our method is also capable of extrapolating few steps into the
past merely by changing the time index t to negative instead of positive. This allows us to retrieve past
maps without having to store the complete dataset. In contrast to DGPOM, the parametric nature and
amenability to optimization using SGD makes our method much more efficient in both performing
inference and updating with new observations.
4.6

Runtime

To add a new observation, i.e. a new laser scan, into the HF-STHM map it takes around 0.5 s with the
extraction of the dynamic objects taking up the majority of the time. To query a single map with 0.1 m
resolution takes around 0.5 s as well. These numbers are for a simple Python based implementation.

5

Conclusions and future work

This paper presented hinged features to model occupancy state of dynamic environments, by generalizing static Hilbert maps into dynamic environments. The method requires only a small number
of data points (180) per frame to model the occupancy of a dynamic environment (30 meter radius)
at any resolution. To this end, uncertainty of motion predictions were embedded into the map in a
probabilistic manner by considering spatio-temporal relationships. Because of the hierarchical nature,
the proposed feature embedding technique is amenable for more sophisticated motion prediction
models and sensor fusion techniques. The power of this method can be used for planning and safe
navigation where knowing the future state of the world is always advantageous. Furthermore, it can
be used as a general tool for learning behaviors of moving objects and how they interact with the
space around them.
8

References
[1] A. Elfes, ?Sonar-based real-world mapping and navigation,? IEEE Journal of Robotics and
Automation, vol. RA-3(3), pp. 249?265, 1987.
[2] S. T. O?Callaghan and F. T. Ramos, ?Gaussian process occupancy maps,? The International
Journal of Robotics Research (IJRR), vol. 31, no. 1, pp. 42?62, 2012.
[3] F. Ramos and L. Ott, ?Hilbert maps: scalable continuous occupancy mapping with stochastic
gradient descent,? in Proceedings of Robotics: Science and Systems (RSS), 2015.
[4] K. Doherty, J. Wang, and B. Englot, ?Probabilistic map fusion for fast, incremental occupancy
mapping with 3d hilbert maps,? in IEEE International Conference on Robotics and Automation
(ICRA), pp. 0?0, 2016.
[5] T. Krajn?k, P. Fentanes, G. Cielniak, C. Dondrup, and T. Duckett, ?Spectral analysis for longterm robotic mapping,? in IEEE International Conference on Robotics and Automation (ICRA),
2014.
[6] S. O?Callaghan and F. Ramos, ?Gaussian Process Occupancy Maps for Dynamic Environment,?
in Proceedings of the International Symposium of Experimental Robotics (ISER), 2014.
[7] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning. Springer Series
in Statistics, New York, NY, USA: Springer New York Inc., 2001.
[8] A. Rahimi and B. Recht, ?Random features for large-scale kernel machines,? in Neural Information Processing Systems (NIPS), 2008.
[9] A. Rahimi and B. Recht, ?Weighted sums of random kitchen sinks: Replacing minimization
with randomisation in learning,? in Neural Information Processing Systems (NIPS), 2009.
[10] L. Bottou and O. Bousquet, ?The tradeoffs of large scale learning,? in Neural Information
Processing Systems (NIPS), 2008.
[11] D. Fleet and Y. Weiss, ?Optical flow estimation,? in Handbook of Mathematical Models in
Computer Vision (MMCV), pp. 237?257, Springer, 2006.
[12] B. D. Lucas, T. Kanade, et al., ?An iterative image registration technique with an application
to stereo vision.,? in International Joint Conference on Artificial Intelligence (IJCAI), vol. 81,
pp. 674?679, 1981.
[13] H. Kuhn, ?The hungarian method for the assignment problem,? Naval research logistics quarterly, 1955.
[14] H. Wackernagel, Multivariate geostatistics: an introduction with applications. Springer-Verlag
Berlin Heidelberg, 2003.
[15] C. Rasmussen and C. Williams, Gaussian Processes for Machine Learning. The MIT Press,
2006.
[16] A. Smola, A. Gretton, L. Song, and B. Sch?lkopf, ?A hilbert space embedding for distributions,?
in International Conference Algorithmic Learning Theory (COLT), pp. 13?31, Springer-Verlag,
2007.
[17] A. Girard, C. E. Rasmussen, J. Quinonero-Candela, and R. Murray-Smith, ?Gaussian process
priors with uncertain inputs: Application to multiple-step ahead time series forecasting,? in
Neural Information Processing Systems (NIPS), 2002.

9

"
1628,2003,From Algorithmic to Subjective Randomness,Abstract Missing,"From Algorithmic to Subjective Randomness

Thomas L. Griffiths & Joshua B. Tenenbaum
{gruffydd,jbt}@mit.edu
Massachusetts Institute of Technology
Cambridge, MA 02139

Abstract
We explore the phenomena of subjective randomness as a case study in
understanding how people discover structure embedded in noise. We
present a rational account of randomness perception based on the statistical problem of model selection: given a stimulus, inferring whether the
process that generated it was random or regular. Inspired by the mathematical definition of randomness given by Kolmogorov complexity, we
characterize regularity in terms of a hierarchy of automata that augment
a finite controller with different forms of memory. We find that the regularities detected in binary sequences depend upon presentation format,
and that the kinds of automata that can identify these regularities are informative about the cognitive processes engaged by different formats.

1

Introduction

People are extremely good at finding structure embedded in noise. This sensitivity to patterns and regularities is at the heart of many of the inductive leaps characteristic of human
cognition, such as identifying the words in a stream of sounds, or discovering the presence
of a common cause underlying a set of events. These acts of everyday induction are quite
different from the kind of inferences normally considered in machine learning and statistics: human cognition usually involves reaching strong conclusions on the basis of limited
data, while many statistical analyses focus on the asymptotics of large samples.
The ability to detect structure embedded in noise has a paradoxical character: while it is
an excellent example of the kind of inference at which people excel but machines fail, it
also seems to be the source of errors in tasks at which machines regularly succeed. For
example, a common demonstration conducted in introductory psychology classes involves
presenting students with two binary sequences of the same length, such as HHTHTHTT and
HHHHHHHH, and asking them to judge which one seems more random. When students
select the former, they are told that their judgments are irrational: the two sequences are
equally random, since they have the same probability of being produced by a fair coin. In
the real world, the sense that some random sequences seem more structured than others
can lead people to a variety of erroneous inferences, whether in a casino or thinking about
patterns of births and deaths in a hospital [1].
Here we show how this paradox can be resolved through a proper understanding of what our
sense of randomness is designed to compute. We will argue that our sense of randomness is
actually extremely well-calibrated with a rational statistical computation ? just not the one
to which it is usually compared. While previous accounts criticize people?s randomness

judgments as poor estimates of the probability of an outcome, we claim that subjective
randomness, together with other everyday inductive leaps, can be understood in terms of the
statistical problem of model selection: given a set of data, evaluating hypotheses about the
process that generated it. Solving this model selection problem for small datasets requires
two ingredients: a set of hypotheses about the processes by which the data could have been
generated, and a rational statistical inference by which these hypotheses are evaluated.
We will model subjective randomness as an inference comparing the probability of a sequence under a random process, P (X|random), with the probability of that sequence
under a regular process, P (X|regular). In previous work we have shown that defining
P (X|regular) using a restricted form of Kolmogorov complexity, in which regularity is
characterized in terms of a simple computing machine, can provide a good account of human randomness judgments for binary sequences [2]. Here, we explore the consequences
of manipulating the conditions under which these sequences are presented. We will show
that the kinds of regularity to which people are sensitive depend upon whether the full sequence is presented simultaneously, or its elements are presented sequentially. By exploring how these regularities can be captured by different kinds of automata, we extend our
rational analysis of the inference involved in subjective randomness to a rational characterization of the processes underlying it: certain regularities can only be detected by automata
with a particular form of memory access, and identifying the conditions under which regularities are detectable provides insight into how characteristics of human memory interact
with rational statistical inference.

2

Kolmogorov complexity and randomness

A natural starting point for a formal account of subjective randomness is Kolmogorov complexity, which provides a mathematical definition of the randomness of a sequence in terms
of the length of the shortest computer program that would produce that sequence. The idea
of using a code based upon the length of computer programs was independently proposed
in [3], [4] and [5], although it has come to be associated with Kolmogorov. A sequence
X has Kolmogorov complexity K(X) equal to the length of the shortest program p for a
(prefix) universal Turing machine U that produces X and then halts,
K(X) =

min

p:U (p)=X

`(p),

(1)

where `(p) is the length of p in bits. Kolmogorov complexity identifies a sequence X
as random if `(X) ? K(X) is small: random sequences are those that are irreducibly
complex [4]. While not necessarily following the form of this definition, psychologists
have preserved its spirit in proposing that the perceived randomness of a sequence increases
with its complexity (eg. [6]). Kolmogorov complexity can also be used to define a variety
of probability distributions, assigning probability to events based upon their complexity.
One such distribution is algorithmic probability, in which the probability of X is
R(X) = 2?K(X) =

max

p:U (p)=X

2?`(p) .

(2)

There is no requirement that R(X) sum to one over all sequences; many probability distributions that correspond to codes are unnormalized, assigning the missing probability to an
undefined sequence.
There are three problems with using Kolmogorov complexity as the basis for a computational model of subjective randomness. Firstly, the Kolmogorov complexity of any particular sequence X is not computable [4], presenting a practical challenge for any modelling
effort. Secondly, while the universality of an encoding scheme based on Turing machines
is attractive, many of the interesting questions in cognition come from the details: issues of
representation and processing are lost in the asymptotic equivalence of coding schemes, but

play a key role in people?s judgments. Finally, Kolmogorov complexity is too permissive in
what it considers a regularity. The set of regularities identified by people are a strict subset
of those that might be expressed in short computer programs. For example, people are very
unlikely to be able to tell the difference between a binary sequence produced by a linear
congruential random number generator (a very short program) and a sequence produced by
flipping a coin, but these sequences should differ significantly in Kolmogorov complexity.
Restricting the set of regularities does not imply that people are worse than machines at
recognizing patterns: reducing the size of the set of hypotheses increases inductive bias,
making it possible to identify the presence of structure from smaller samples.

3

A statistical account of subjective randomness

While there are problems with using Kolmogorov complexity as the basis for a rational
theory of subjective randomness, it provides a clear definition of regularity. In this section
we will present a statistical account of subjective randomness in terms of a comparison between random and regular sources, where regularity is defined by analogues of Kolmogorov
complexity for simpler computing machines.
3.1

Subjective randomness as model selection

One of the most basic problems that arises in statistical inference is identifying the source
of a set of observations, based upon a set of hypotheses. This is the problem of model
selection. Model selection provides a natural basis for a statistical theory of subjective
randomness, viewing these judgments as the consequence of an inference to the process
that produced a set of observations. On seeing a stimulus X, we consider two hypotheses:
X was produced by a random process, or X was produced by a regular process. The
decision about the source of X can be formalized as a Bayesian inference,
P (random|X)
P (X|random) P (random)
=
,
P (regular|X)
P (X|regular) P (regular)

(3)

in which the posterior odds in favor of a random generating process are obtained from the
likelihood ratio and the prior odds. The only part of the right hand side of the equation
affected by X is the likelihood ratio, so we define the subjective randomness of X as
random(X) = log

P (X|random)
,
P (X|regular)

(4)

being the evidence that X provides towards the conclusion that it was produced by a random process.
3.2

The nature of regularity

In order to define random(X), we need to specify P (X|random) and P (X|regular). When
evaluating binary sequences, it is natural to set P (X|random) = ( 21 )`(X) . Taking the
logarithm in base 2, random(X) is ?`(X) ? log 2 P (X|regular), depending entirely on
P (X|regular). We obtain random(X) = K(X) ? `(X), the difference between the complexity of a sequence and its length, if we choose P (X|regular) = R(X), the algorithmic probability defined in Equation 2. This is identical to the mathematical definition of
randomness given by Kolmogorov complexity. However, the key point of this statistical
approach is that we are not restricted to using R(X): we have a measure of the randomness
of X for any choice of P (X|regular).
The choice of P (X|regular) will reflect the stimulus domain, and express the kinds of
regularity which people can detect in that domain. For binary sequences, a good candidate for specifying P (X|regular) is a hidden Markov model (HMM), a probabilistic finite

1

H

2

T

5

4

H

T
T

H
6

3

Figure 1: Finite state automaton used to define P (X|regular) to give random(X) ? DP .
Solid arrows are transitions consistent with repeating a motif, which are taken with probability ?. Dashed arrows are motif changes, using the prior determined by ?.

state automaton. In fact, specifying P (X|regular)in terms of a particular HMM results in
random(X) being equivalent to the ?Difficulty Predictor? (DP) [6] a measure of sequence
complexity that has been extremely successful in modelling subjective randomness judgments. DP measures the complexity of a sequence in terms of the number of repeating (eg.
HHHH) and alternating (eg. HTHT) subsequences it contains, adding one point for each
repeating subsequence and two points for each alternating subsequence. For example, the
sequence TTTHHHTHTH is a run of tails, a run of heads, and an alternating sub-sequence,
DP = 4. If there are several partitions into runs and alternations, DP is calculated on the
partition that results in the lowest score.
In [2], we showed that random(X) ? DP if P (X|regular) is specified by a particular HMM. This HMM produces sequences by motif repetition, using the transition graph
shown in Figure 1. The model emits sequences by choosing a motif, a sequence of symbols
of length k, with probability proportional to ?k , and emitting symbols consistent with that
motif with probability ?, switching to a new motif with probability 1 ? ?. In Figure 1,
state 1 repeats the motif H, state 2 repeats T, and the remaining states repeat the alternating motifs HT and TH. The randomness of a sequence under this definition of regularity
depends on ? and ?, but is generally affected by the number of repeating and alternating
subsequences. The equivalence to DP, in which a sequence scores a single point for each
repeating subsequence
and two points for each alternating subsequence, results from taking
?
? = 0.5 and ? = 3?1
2 , and choosing the the state sequence for the HMM that maximizes
the probability of the sequence.
Just as the algorithmic probability R(X) is a probability distribution defined by the length
of programs for a universal Turing machine, this choice of P (X|regular) can be seen as
specifying the length of ?programs? for a particular finite state automaton. The output of a
finite state automaton is determined by its state sequence, just as the output of a universal
Turing machine is determined by its program. However, since the state sequence is the
same length as the sequence itself, this alone does not provide a meaningful measure of
complexity. In our model, probability imposes a metric on state sequences, dictating a
greater cost for moves between certain states, which translates into a code length through
the logarithm. Since we find the state sequence most likely to have produced X, and thus
the shortest code length, we have an analogue of Kolmogorov complexity defined on a
finite state automaton.
3.3

Regularities and automata

Using a hidden Markov model to specify P (X|regular) provides a measure of complexity
defined in terms of a finite state automaton. However, the kinds of regularities people can
detect in binary sequences go beyond the capacity of a finite state automaton. Here, we
consider three additional regularities: symmetry (eg. THTHHTHT), symmetry in the com-

Finite state automaton
(motif repetition)

Queue automaton
(duplication)
Pushdown automaton
(symmetry)

Stack automaton

Turing machine
(all computable)

Figure 2: Hierarchy of automata used to define measures of complexity. Of the regularities
discussed in this paper, each automaton can identify all regularities identified by those
automata to its left as well as those stated in parentheses beneath its name.

plement (eg. TTTTHHHH), and the perfect duplication of subsequences (eg. HHHTHHHT
vs. HHHTHHHTH). These regularities identify formal languages that cannot be recognized
by a finite state automaton, suggesting that we might be able to develop better models of
subjective randomness by defining P (X|regular) in terms of more sophisticated automata.
The automata we will consider in this paper form a hierarchy, shown in Figure 2. This
hierarchy expresses the same content as Chomsky?s [7] hierarchy of computing machines
? the regularities identifiable by each machine are a strict superset of those identifiable
to the machine to the left ? although it features a different set of automata. The most
restricted set of regularities are those associated with the finite state automaton, and the
least restricted are those associated with the Turing machine. In between are the pushdown
automaton, which augments a finite controller with a stack memory, in which the last item
added is the first to be accessed; the queue automaton,1 in which the memory is a queue, in
which the first item added is the first to be accessed; and the stack automaton, in which the
memory is a stack but any item in the stack can be read by the controller [9, 10]. The key
difference between these kinds of automata is the memory available to the finite controller,
and exploring measures of complexity defined in terms of these automata thus involves
assessing the kind of memory required to identify regularities.
Each of the automata shown in Figure 2 can identify a different set of regularities. The
finite state automaton is only capable of identifying motif repetition, while the pushdown
automaton can identify both kinds of symmetry, and the queue automaton can identify
duplication. The stack automaton can identify all of these regularities, and the Turing
machine can identify all computable regularities. For each of the sub-Turing automata,
we can use these constraints to specify a probabilistic model for P (X|regular). For example, the probabilistic model corresponding to the pushdown automaton generates regular sequences by three methods: repetition, producing sequences with probabilities determined by the HMM introduced above; symmetry, where half of the sequence is produced
by the HMM and the second half is produced by reflection; and complement symmetry,
where the second half is produced by reflection and exchanging H and T. We then take
P (X|regular) = maxZ,M P (X, Z|M )P (M ), where M is the method of production and
Z is the state sequence for the HMM. Similar models can be defined for the queue and
stack automata, with the queue automaton allowing generation by repetition or duplication,
and the stack automaton allowing any of these four methods. Each regularity introduced
into the model requires a further parameter in specifying P (M ), so the hierarchy shown
in Figure 2 also expresses the statistical structure of this set of models: each model is a
special case of the model to its right, in which some regularities are eliminated by setting
P (M ) to zero. We can use this structure to perform model selection with likelihood ratio
tests, determining which model gives the best account of a particular dataset using just the
difference in the log-likelihoods. We apply this method in the next section.
1

An unrestricted queue automaton is equivalent to a Turing machine. We will use the phrase to
refer to an automaton in which the number of queue operations that can be performed for each input
symbol is limited, which is generally termed a quasi real time queue automaton [8].

4

Testing the models

The models introduced in the previous section differ in the memory systems with which
they augment the finite controller. The appropriateness of any one measure of complexity
to a particular task may thus depend upon the memory demands placed upon the participant. To explore this hypothesis, we conducted an experiment in which participants make
randomness judgments after either seeing a sequence in its entirety, or seeing each element
one after another. We then used model selection to determine which measure of complexity gave the best account of each condition, illustrating how the strategy of defining
more restricted forms of complexity can shed light into the cognitive processes underlying
regularity detection.
4.1

Experimental methods

There were two conditions in the experiment, corresponding to Simultaneous and Sequential presentation of stimuli. The stimuli were sequences of heads (H) and tails (T) presented
in 130 point fixed width sans-serif font on a 19? monitor at 1280 ? 1024 pixel resolution.
In the Simultaneous condition, all eight elements of the sequence appeared on the display
simultaneously. In the Sequential condition, the elements appeared one by one, being displayed for 300ms with a 300ms inter-stimulus interval.
The participants were 40 MIT undergraduates, randomly assigned to the two conditions.
Participants were instructed that they were about to see sequences which had either been
produced by a random process (flipping a fair coin) or by other processes in which the
choice of heads and tails was not random, and had to classify these sequences according
to their source. After a practice session, each participant classified all 128 sequences of
length 8, in random order, with each sequence randomly starting with either a head or a
tail. Participants took breaks at intervals of 32 sequences.
4.2

Results and Discussion

We analyzed the results by fitting the models corresponding to the four automata described above, using all motifs up to length 4 to specify the basic model. We computed
random(X) for each stimulus as in Eq. (4), with P (X|regular) specified by the probabilistic model corresponding to each of the automata. We then converted this log-likelihood
ratio into the posterior probability of a random generating process, using
P (random|X) =

1
1 + exp{?? random(X) ? ?}

where ? and ? are parameters weighting the contribution of the likelihoods and the priors respectively. We then optimized ?, ?, ?, ? and the parameters contributing to P (M )
for each model, maximizing the likelihood of the classifications of the sequences by the
20 participants in each of the 2 conditions. The results of the model-fitting are shown in
Figure 3(a) and (b), which indicate the relationship between the posterior probabilities predicted by the model and the proportion of participants who classified a sequence as random.
The correlation coefficients shown in the figure provide a relatively good indicator of the
fit of the models, and each sequence is labelled according to the regularity it expresses,
showing how accommodating particular regularities contributes to the fit.
The log-likelihood scores obtained from fitting the models can be used for model selection, testing whether any of the parameters involved in the models are unnecessary. Since
the models form a nested hierarchy, we can use likelihood ratio tests to evaluate whether
introducing a particular regularity (and the parameters associated with it) results in a statistically significant improvement in fit. Specifically, if model 1 has log-likelihood L1 and
df1 parameters, and model 2 has log-likelihood L2 and df2 > df1 parameters, 2(L2 ? L1 )

1

P(random|x)

Finite state
(a)

0.5

Pushdown

r=0.69

0

0

r=0.79

1

P(random|x)

0.5

0

Repetition
Symmetry
Complement
Duplication
Pushdown
r=0.70

r=0.70

0

Stack
r=0.83

0.5
1
Simultaneous data
Finite state

(b)

Queue
r=0.76

0.5
Sequential data

(c) 57.43 (1df, p < 0.0001)

Stack

r=0.76

r=0.77

1

Queue

Finite state
87.76 (2df, p < 0.0001)

Queue

75.41 (2df, p < 0.0001)
Stack

Pushdown

45.08 (1df, p < 0.0001)

(d) 33.24 (1df, p < 0.0001)

Queue

5.69 (2df, p = 0.0582)

Pushdown

31.42 (1df, p < 0.0001)

Finite state
1.82 (2df, p = 0.4025)

Stack

Figure 3: Experimental results for (a) the Simultaneous and (b) the Sequential condition,
showing the proportion of participants classifying a sequence as ?random? (horizontal axis)
and P (random|X) (vertical axis) as assessed by the four models. Points are labelled according to their parse under the Stack model. (c) and (d) show the model selection results
for the Simultaneous and Sequential conditions respectively, showing the four automata
with edges between them labelled with ?2 score (df, p-value) for improvement in fit.

should have a ?2 (df2 ? df1 ) distribution under the null hypothesis of no improvement in
fit. We evaluated the pairwise likelihood ratio tests for the four models in each condition,
with the results shown in Figure 3(c) and (d). Additional regularities always improved the
fit for the Simultaneous condition, while adding duplication, but not symmetry, resulted in
a statistically significant improvement in the Sequential condition.
The model selection results suggest that the best model for the Simultaneous condition
is the stack automaton, while the best model for the Sequential condition is the queue
automaton. These results indicate the importance of presentation format in determining
subjective randomness, as well as the benefits of exploring measures of complexity defined
in terms of a range of computing machines. The stack automaton can evaluate regularities
that require checking information in arbitrary positions in a sequence, something that is
facilitated by a display in which the entire sequence is available. In contrast, the queue
automaton can only access information in the order that it enters memory, and gives a
better match to the task in which working memory is required. This illustrates an important
fact about cognition ? that human working memory operates like a queue rather than a stack
? that is highlighted by this approach.
The final parameters of the best-fitting models provide some insight into the relative importance of the different kinds of regularities under different presentation conditions. For the
Simultaneous condition, ? = 0.66, ? = 0.12, ? = 0.26, ? = ?1.98 and motif repetition,
symmetry, symmetry in the complement, and duplication were given probabilities of 0.748,
0.208, 0.005, and 0.039 respectively. Symmetry is thus a far stronger characteristic of reg-

ularity than either symmetry in the complement or duplication, when entire sequences are
viewed simultaneously. For the Sequential condition, ? = 0.70, ? = 0.11, ? = 0.38, ? =
?1.24, and motif repetition was given a probability of 0.962 while duplication had a probability of 0.038, with both forms of symmetry being given zero probability since the queue
model provided the best fit. Values of ? > 0.5 for both models indicates that regular sequences tend to repeat motifs, rather than rapidly switching between them, and the low ?
values reflect a preference for short motifs.

5

Conclusion

We have outlined a framework for understanding the rational basis of the human ability to
find structure embedded in noise, viewing this inference in terms of the statistical problem of model selection. Solving this problem for small datasets requires two ingredients:
strong prior beliefs about the hypothetical mechanisms by which the data could have been
generated, and a rational statistical inference by which these hypotheses are evaluated.
When assessing the randomness of binary sequences, which involves comparing random
and regular sources, people?s beliefs about the nature of regularity can be expressed in
terms of probabilistic versions of simple computing machines. Different machines capture
regularity when sequences are presented simultaneously and when their elements are presented sequentially, and the differences between these machines provide insight into the
cognitive processes involved in the task. Analyses of the rational basis of human inference
typically either ignore questions about processing or introduce them as relatively arbitrary
constraints. Here, we are able to give a rational characterization of process as well as inference, evaluating a set of alternatives that all correspond to restrictions of Kolmogorov
complexity to simple general-purpose automata.
Acknowledgments. This work was supported by a Stanford Graduate Fellowship to the first author.
We thank Charles Kemp and Michael Lee for useful comments.

References
[1] D. Kahneman and A. Tversky. Subjective probability: A judgment of representativeness. Cognitive Psychology, 3:430?454, 1972.
[2] T. L. Griffiths and J. B. Tenenbaum. Probability, algorithmic complexity and subjective randomness. In Proceedings of the 25th Annual Conference of the Cognitive Science Society, Hillsdale,
NJ, 2003. Erlbaum.
[3] R. J. Solomonoff. A formal theory of inductive inference. Part I. Information and Control,
7:1?22, 1964.
[4] A. N. Kolmogorov. Three approaches to the quantitative definition of information. Problems of
Information Transmission, 1:1?7, 1965.
[5] G. J. Chaitin. On the length of programs for computing finite binary sequences: statistical
considerations. Journal of the ACM, 16:145?159, 1969.
[6] R. Falk and C. Konold. Making sense of randomness: Implicit encoding as a bias for judgment.
Psychological Review, 104:301?318, 1997.
[7] N. Chomsky. Threee models for the description of language. IRE Transactions on Information
Theory, 2:113?124, 1956.
[8] A. Cherubini, C. Citrini, S. C. Reghizzi, and D. Mandrioli. QRT FIFO automata, breadth-first
grammars and their relations. Theoretical Comptuer Science, 85:171?203, 1991.
[9] S. Ginsburg, S. A. Greibach, and M. A. Harrison. Stack automata and compiling. Journal of
the ACM, 14:172?201, 1967.
[10] A. V. Aho. Indexed grammars ? an extension of context-free grammars. Journal of the ACM,
15:647?671, 1968.

"
